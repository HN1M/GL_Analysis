# app_v0.17.py (ê±°ë˜ì²˜ ìƒì„¸ ë¶„ì„ ì˜¤ë¥˜ ìˆ˜ì •)
# --- BEGIN: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---
try:
    from infra.env_loader import boot as _llm_boot
    _llm_boot()  # í‚¤ ë¡œë“œ + ìƒíƒœ ë¡œê·¸
except Exception as _e:
    # ìµœì•…ì˜ ê²½ìš°ì—ë„ ì•±ì€ ëœ¨ê²Œ í•˜ê³ , ìƒíƒœë¥¼ stderrë¡œë§Œ ì•Œë¦¼
    import sys
    print(f"[env_loader] ì´ˆê¸°í™” ì‹¤íŒ¨: {_e}", file=sys.stderr)
# --- END: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
import plotly.graph_objects as go
from utils.helpers import find_column_by_keyword, add_provenance_columns, add_period_tag
from analysis.integrity import analyze_reconciliation, run_integrity_module
from analysis.contracts import LedgerFrame, ModuleResult
from analysis.trend import create_monthly_trend_figure, run_trend_module
from analysis.timeseries import (
    run_timeseries_module,          # â† ë³´ê³ ì„œ íƒ­ì—ì„œ ê³„ì† ì‚¬ìš©
    create_timeseries_figure        # â† ê·¸ë˜í”„ ë Œë” ê·¸ëŒ€ë¡œ ì‚¬ìš©
)
from analysis.ts_v2 import (
    run_timeseries_minimal,
    compute_series_stats,     # â† NEW
    build_anomaly_table,      # â† NEW
    add_future_shading        # â† NEW (ì‹œê° ìŒì˜)
)
from analysis.aggregation import aggregate_monthly, month_end_00
from analysis.anomaly import run_anomaly_module, compute_amount_columns
from analysis.correlation import run_correlation_module
from analysis.vendor import (
    create_pareto_figure,
    create_vendor_heatmap,
    create_vendor_detail_figure,
    run_vendor_module,
)
from analysis.report import run_final_analysis, build_methodology_note
from analysis.embedding import (
    ensure_rich_embedding_text,
    perform_embedding_and_clustering,
    perform_embedding_only,
    unify_cluster_names_with_llm,
)
from analysis.anomaly import calculate_grouped_stats_and_zscore
from services.llm import LLMClient
from services.cache import get_or_embed_texts
import services.cycles_store as cyc
from config import EMB_USE_LARGE_DEFAULT, HDBSCAN_RESCUE_TAU, EMB_MODEL_SMALL
try:
    from config import PM_DEFAULT
except Exception:
    PM_DEFAULT = 500_000_000
from utils.viz import add_materiality_threshold, add_pm_badge
from services.cluster_naming import (
    make_synonym_confirm_fn,
    unify_cluster_labels_llm,
)

# --- KRW ì…ë ¥(ì²œë‹¨ìœ„ ì½¤ë§ˆ) ìœ í‹¸: ì½œë°± ê¸°ë°˜ìœ¼ë¡œ ì•ˆì •í™” ---
def _krw_input(label: str, key: str, *, default_value: int, help_text: str = "") -> int:
    """
    í•œêµ­ ì›í™” ì…ë ¥ ìœ„ì ¯(ì²œë‹¨ìœ„ ì½¤ë§ˆ). í•µì‹¬ ê·œì¹™:
    1) ìœ„ì ¯ í‚¤(pm_value__txt ë“±)ë¥¼ ëŸ° ë£¨í”„ì—ì„œ ì§ì ‘ ëŒ€ì…í•˜ì§€ ì•ŠëŠ”ë‹¤.
    2) ì½¤ë§ˆ ì¬í¬ë§·ì€ on_change ì½œë°± ì•ˆì—ì„œë§Œ ìˆ˜í–‰í•œë‹¤.
    3) ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ê°’ì€ st.session_state[key]ì— ë³´ê´€í•œë‹¤.
    """
    txt_key = f"{key}__txt"  # ì‹¤ì œ text_input ìœ„ì ¯ì´ ë°”ì¸ë”©ë˜ëŠ” í‚¤

    # ì´ˆê¸° ì…‹ì—…: ìˆ«ì/ë¬¸ì ìƒíƒœë¥¼ ìœ„ì ¯ ìƒì„± ì „ì— ì¤€ë¹„
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if txt_key not in st.session_state:
        st.session_state[txt_key] = f"{int(st.session_state[key]):,}"

    # ì½œë°±: í¬ì»¤ìŠ¤ ì•„ì›ƒ/Enter ì‹œ ì½¤ë§ˆ í¬ë§·ì„ ì ìš©í•˜ê³  ìˆ«ì ìƒíƒœë¥¼ ë™ê¸°í™”
    def _on_blur_format():
        raw_now = st.session_state.get(txt_key, "")
        digits = re.sub(r"[^\d]", "", str(raw_now or ""))
        val = int(digits) if digits else 0
        if val < 0:
            val = 0
        st.session_state[key] = int(val)            # ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ìƒíƒœ
        st.session_state[txt_key] = f"{int(val):,}"  # ìœ„ì ¯ í‘œì‹œ í…ìŠ¤íŠ¸(ì½¤ë§ˆ)

    # ìœ„ì ¯ ìƒì„±
    raw = st.text_input(
        label,
        value=st.session_state[txt_key],
        key=txt_key,
        help=help_text,
        placeholder="ì˜ˆ: 500,000,000",
        on_change=_on_blur_format,
    )

    # ë¼ì´ë¸Œ íƒ€ì´í•‘ ë™ì•ˆì—ë„ ê·¸ë˜í”„ê°€ ì¦‰ì‹œ ë°˜ì˜ë˜ë„ë¡ ì •ìˆ˜ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸(ìœ„ì ¯ í‚¤ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    digits_live = re.sub(r"[^\d]", "", str(raw or ""))
    live_val = int(digits_live) if digits_live else 0
    if live_val < 0:
        live_val = 0
    st.session_state[key] = int(live_val)

    return int(st.session_state[key])


# ì‚¬ì´í´ í”„ë¦¬ì…‹ì„ ê³„ì • ì„ íƒê¸°ë¡œ ì£¼ì…í•˜ëŠ” í—¬í¼
def _apply_cycles_to_picker(*, upload_id: str, cycles_state_key: str, accounts_state_key: str, master_df: pd.DataFrame):
    """ì„ íƒëœ ì‚¬ì´í´ì˜ ê³„ì •ë“¤ì„ ê³„ì • ë©€í‹°ì…€ë ‰íŠ¸ì— í•©ì³ ë„£ì–´ì¤€ë‹¤."""
    cycles_map = cyc.get_effective_cycles(upload_id)
    chosen_cycles = st.session_state.get(cycles_state_key, []) or []
    # ì§€ì›: KO ë¼ë²¨ ë˜ëŠ” ì½”ë“œ ë¼ë²¨ â€” ê³µì‹ KO ë¼ë²¨ ì§‘í•© ê¸°ì¤€ìœ¼ë¡œ íŒë³„
    KO_LABELS = set(cyc.CYCLE_KO.values())
    if chosen_cycles and all(lbl in KO_LABELS for lbl in chosen_cycles):
        codes = cyc.accounts_for_cycles_ko(cycles_map, chosen_cycles)
    else:
        codes = cyc.accounts_for_cycles(cycles_map, chosen_cycles)
    names = (master_df[master_df['ê³„ì •ì½”ë“œ'].astype(str).isin(codes)]['ê³„ì •ëª…']
                .dropna().astype(str).unique().tolist())
    cur = set(st.session_state.get(accounts_state_key, []) or [])
    st.session_state[accounts_state_key] = sorted(cur.union(names))


# --- NEW: Correlation UI helpers (DRY) ---
from analysis.correlation import run_correlation_module  # í‘œì¤€(ê¸°ë³¸) ìƒê´€ ëª¨ë“ˆ

def _render_corr_basic_tab(*, upload_id: str):
    """
    ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„(íˆíŠ¸ë§µ/ê°•í•œ ìƒê´€ìŒ/ì œì™¸ê³„ì •)ì„ ë Œë”í•©ë‹ˆë‹¤.
    - ê¸°ì¡´ 'ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„' íƒ­ì˜ êµ¬í˜„ì„ ê·¸ëŒ€ë¡œ ì˜®ê²¨, ìƒê´€ íƒ­ì˜ 'ê¸°ë³¸' ì„œë¸Œíƒ­ì—ì„œ ì‚¬ìš©.
    - state keyëŠ” 'corr_basic_*' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ì¶©ëŒ ë°©ì§€.
    """
    import services.cycles_store as cyc
    mdf = st.session_state.master_df
    acct_names = sorted(mdf['ê³„ì •ëª…'].dropna().astype(str).unique().tolist())
    st.subheader("ê³„ì • ê°„ ìƒê´€ íˆíŠ¸ë§µ(ê¸°ë³¸)")
    colA, colB = st.columns([2,1])
    with colA:
        picked_accounts = st.multiselect(
            "ìƒê´€ ë¶„ì„ ëŒ€ìƒ ê³„ì •(2ê°œ ì´ìƒ ì„ íƒ)",
            acct_names,
            default=[],
            help="ì„ íƒí•œ ê³„ì •ë“¤ ê°„ ì›”ë³„ íë¦„ì˜ í”¼ì–´ìŠ¨ ìƒê´€ì„ ê³„ì‚°í•©ë‹ˆë‹¤.",
            key="corr_basic_accounts"
        )
    with colB:
        cycles_map_now = cyc.get_effective_cycles(upload_id)
        if cycles_map_now:
            picked_cycles = st.multiselect(
                "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ", list(cyc.CYCLE_KO.values()),
                default=[], key="corr_basic_cycles"
            )
            st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_corr_basic", on_click=_apply_cycles_to_picker,
                      kwargs=dict(upload_id=upload_id,
                                  cycles_state_key="corr_basic_cycles",
                                  accounts_state_key="corr_basic_accounts",
                                  master_df=st.session_state.master_df))
    corr_thr = st.slider(
        "ìƒê´€ ì„ê³„ì¹˜(ê°•í•œ ìƒê´€ìŒ í‘œ ì „ìš©)",
        min_value=0.50, max_value=0.95, step=0.05, value=0.70,
        help="ì ˆëŒ€ê°’ ê¸°ì¤€ ì„ê³„ì¹˜ ì´ìƒì¸ ê³„ì •ìŒë§Œ í‘œì— í‘œì‹œí•©ë‹ˆë‹¤.",
        key="corr_basic_thr"
    )

    if len(picked_accounts) < 2:
        st.info("ê³„ì •ì„ **2ê°œ ì´ìƒ** ì„ íƒí•˜ë©´ íˆíŠ¸ë§µì´ í‘œì‹œë©ë‹ˆë‹¤.")
        return

    # ìŠ¤ì½”í”„ ì ìš©ëœ LedgerFrameì„ ì¬ì‚¬ìš©
    lf_use = _lf_by_scope()

    # ê³„ì •ëª… â†’ ì½”ë“œ
    codes = (
        mdf[mdf['ê³„ì •ëª…'].isin(picked_accounts)]['ê³„ì •ì½”ë“œ']
        .astype(str).tolist()
    )
    cmod = run_correlation_module(
        lf_use,
        accounts=codes,
        corr_threshold=float(corr_thr),
        cycles_map=cyc.get_effective_cycles(upload_id),
    )
    _push_module(cmod)
    for w in cmod.warnings:
        st.warning(w)

    # íˆíŠ¸ë§µ(+í˜¸ë²„ ê³„ì •ëª…)
    if 'heatmap' in cmod.figures:
        fig = cmod.figures['heatmap']
        try:
            name_map = dict(zip(
                mdf["ê³„ì •ì½”ë“œ"].astype(str),
                mdf["ê³„ì •ëª…"].astype(str)
            ))
            tr = fig.data[0]
            x_codes = list(map(str, getattr(tr, 'x', [])))
            y_codes = list(map(str, getattr(tr, 'y', [])))
            x_names = [name_map.get(c, c) for c in x_codes]
            y_names = [name_map.get(c, c) for c in y_codes]
            tr.update(x=x_names, y=y_names)
            fig.update_traces(hovertemplate="ê³„ì •: %{y} Ã— %{x}<br>ìƒê´€ê³„ìˆ˜: %{z:.3f}<extra></extra>")
        except Exception:
            pass
        st.plotly_chart(fig, use_container_width=True, key=f"corr_basic_heatmap_{'_'.join(codes)}_{int(corr_thr*100)}")

    # ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ
    if 'strong_pairs' in cmod.tables and not cmod.tables['strong_pairs'].empty:
        st.markdown("**ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ**")
        st.dataframe(cmod.tables['strong_pairs'], use_container_width=True, height=320)

    # ì œì™¸ëœ ê³„ì •
    if 'excluded_accounts' in cmod.tables and not cmod.tables['excluded_accounts'].empty:
        with st.expander("ì œì™¸ëœ ê³„ì • ë³´ê¸°(ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)", expanded=False):
            exc = cmod.tables['excluded_accounts'].copy()
            if 'ê³„ì •ì½”ë“œ' in exc.columns:
                name_map = dict(zip(
                    mdf["ê³„ì •ì½”ë“œ"].astype(str),
                    mdf["ê³„ì •ëª…"].astype(str)
                ))
                exc['ê³„ì •ì½”ë“œ'] = exc['ê³„ì •ì½”ë“œ'].astype(str)
                exc['ê³„ì •ëª…'] = exc['ê³„ì •ì½”ë“œ'].map(name_map)
                cols = ['ê³„ì •ëª…', 'ê³„ì •ì½”ë“œ'] + [c for c in exc.columns if c not in ('ê³„ì •ëª…','ê³„ì •ì½”ë“œ')]
                exc = exc[cols]
            st.dataframe(exc, use_container_width=True)


def _render_corr_advanced_tab(*, upload_id: str):
    """
    ê³ ê¸‰ ìƒê´€ê´€ê³„ ë¶„ì„(ë°©ë²•/ì‹œì°¨/ë¡¤ë§ ì•ˆì •ì„± ë“±)ì„ ë Œë”í•©ë‹ˆë‹¤.
    - ê¸°ì¡´ 'ìƒê´€ê´€ê³„(ê³ ê¸‰)' íƒ­ì˜ ì½”ë“œë¥¼ ì„œë¸Œíƒ­ìš© í•¨ìˆ˜ë¡œ ëª¨ë“ˆí™”.
    - state keyëŠ” ê¸°ì¡´ 'corr_adv_*' ìœ ì§€(í˜¸í™˜).
    """
    import services.cycles_store as cyc
    st.subheader("ê³ ê¸‰ ìƒê´€ê´€ê³„")
    lf_adv = _lf_by_scope()  # ìŠ¤ì½”í”„ ì¼ê´€ì„± ìœ ì§€
    if lf_adv is None:
        st.info("ì›ì¥ì„ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        return

    mdf_adv = st.session_state.master_df
    acct_names_adv = sorted(mdf_adv['ê³„ì •ëª…'].dropna().astype(str).unique().tolist())
    colA, colB = st.columns(2)
    with colA:
        picked_accounts_adv = st.multiselect("ë¶„ì„ ê³„ì •(ë‹¤ì¤‘ ì„ íƒ)", options=acct_names_adv, key="corr_adv_accounts")
    with colB:
        picked_cycles_adv = st.multiselect("ì‚¬ì´í´ í”„ë¦¬ì…‹(ì„ íƒ ì‹œ ê³„ì • ìë™ ë°˜ì˜)", options=list(cyc.CYCLE_KO.values()), key="corr_adv_cycles")
        if st.button("í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_preset_corr_adv"):
            mapping = cyc.get_effective_cycles(upload_id)
            codes = cyc.accounts_for_cycles_ko(mapping, picked_cycles_adv)
            code_to_name = (
                mdf_adv[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']].assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str)).drop_duplicates()
                    .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict()
            )
            cur_set = set(st.session_state.get("corr_adv_accounts", []))
            cur_set.update({code_to_name.get(c, c) for c in codes})
            st.session_state["corr_adv_accounts"] = sorted(cur_set)

    method = st.selectbox("ìƒê´€ ë°©ì‹", ["pearson", "spearman", "kendall"], index=0, key="corr_adv_method")
    corr_threshold = st.slider("ì„ê³„ì¹˜(|r|)", 0.1, 0.95, 0.70, 0.05, key="corr_adv_thr")
    c1, c2 = st.columns(2)
    with c1:
        max_lag = st.slider("ìµœëŒ€ ì‹œì°¨(ê°œì›”)", 0, 12, 6, 1, key="corr_adv_maxlag")
    with c2:
        rolling_window = st.slider("ë¡¤ë§ ìœˆë„ìš°(ê°œì›”)", 3, 24, 6, 1, key="corr_adv_rollwin")

    if st.button("ë¶„ì„ ì‹¤í–‰", key="run_corr_adv"):
        try:
            from analysis.corr_advanced import run_corr_advanced as run_corr_adv
            # âœ… UI(ê³„ì •ëª…) â†’ ì½”ë“œ ë³€í™˜
            _names = st.session_state.get("corr_adv_accounts", picked_accounts_adv) or []
            _codes = (
                mdf_adv[mdf_adv['ê³„ì •ëª…'].isin(_names)]['ê³„ì •ì½”ë“œ']
                .astype(str).drop_duplicates().tolist()
            )
            mr = run_corr_adv(
                lf_adv,
                _codes,
                method=st.session_state.get("corr_adv_method", "pearson"),
                corr_threshold=float(st.session_state.get("corr_adv_thr", 0.70)),
                max_lag=int(st.session_state.get("corr_adv_maxlag", 6)),
                rolling_window=int(st.session_state.get("corr_adv_rollwin", 6)),
            )
            st.subheader("íˆíŠ¸ë§µ")
            if "heatmap" in mr.figures:
                st.plotly_chart(mr.figures["heatmap"], use_container_width=True)
            if "strong_pairs" in mr.tables:
                st.subheader("ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ")
                st.dataframe(mr.tables["strong_pairs"], use_container_width=True)
            if "lagged_pairs" in mr.tables:
                st.subheader("ìµœì  ì‹œì°¨ ìƒê´€(Top)")
                st.dataframe(mr.tables["lagged_pairs"], use_container_width=True)
            if "rolling_stability" in mr.tables:
                st.subheader("ë¡¤ë§ ì•ˆì •ì„±(ë³€ë™ì„± ë‚®ì€ ìˆœ)")
                st.dataframe(mr.tables["rolling_stability"], use_container_width=True)
        except Exception as _e:
            st.warning(f"ê³ ê¸‰ ìƒê´€ ë¶„ì„ ì‹¤íŒ¨: {_e}")


# --- 3. UI ë¶€ë¶„ ---
st.set_page_config(page_title="AI ë¶„ì„ ì‹œìŠ¤í…œ v0.18", layout="wide")
st.title("í›ˆ's GLë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("---")

for key in ['mapping_confirmed', 'analysis_done']:
    if key not in st.session_state:
        st.session_state[key] = False

# --- NEW: ëª¨ë“ˆ ê²°ê³¼ ìˆ˜ì§‘ìš© ì»¨í…Œì´ë„ˆ ---
if 'modules' not in st.session_state:
    st.session_state['modules'] = {}

def _push_module(mod: ModuleResult):
    """ModuleResultë¥¼ ì„¸ì…˜ì— ìˆ˜ì§‘(ë™ëª… ëª¨ë“ˆì€ ìµœì‹ ìœ¼ë¡œ êµì²´)."""
    try:
        if mod and getattr(mod, "name", None):
            st.session_state['modules'][str(mod.name)] = mod
    except Exception:
        pass


# (removed) number_input ê¸°ë°˜ ëŒ€ì²´ êµ¬í˜„: ì‰¼í‘œ ë¯¸í‘œì‹œÂ·í‚¤ ì¶©ëŒ ìœ ë°œ ê°€ëŠ¥ì„± â†’ ë‹¨ì¼ êµ¬í˜„ìœ¼ë¡œ í†µì¼

with st.sidebar:
    st.header("1. ë°ì´í„° ì¤€ë¹„")
    uploaded_file = st.file_uploader("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.", type=["xlsx", "xlsm"])
    if 'last_file' not in st.session_state or st.session_state.last_file != uploaded_file:
        st.session_state.mapping_confirmed = False
        st.session_state.analysis_done = False
        st.session_state.last_file = uploaded_file

    st.markdown("---")
    st.header("2. ë¶„ì„ ê¸°ê°„")
    default_scope = st.session_state.get("period_scope", "ë‹¹ê¸°")
    st.session_state.period_scope = st.radio(
        "ë¶„ì„ ìŠ¤ì½”í”„(íŠ¸ë Œë“œ ì œì™¸):",
        options=["ë‹¹ê¸°", "ë‹¹ê¸°+ì „ê¸°"],
        index=["ë‹¹ê¸°","ë‹¹ê¸°+ì „ê¸°"].index(default_scope),
        horizontal=True,
        help="ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ ëª¨ë“ˆì— ì ìš©ë©ë‹ˆë‹¤. íŠ¸ë Œë“œëŠ” ì„¤ê³„ìƒ CY vs PY ë¹„êµ ìœ ì§€."
    )
    st.markdown("---")
    st.header("3. Embedding / Clustering")
    st.session_state.use_large_embedding = st.toggle(
        "Use Large Embedding (cost â†‘)",
        value=st.session_state.get("use_large_embedding", EMB_USE_LARGE_DEFAULT),
        help="Large model improves semantics but is slower and more expensive."
    )
    st.session_state.rescue_tau = st.slider(
        "Noise rescue Ï„ (cosine)",
        min_value=0.60, max_value=0.90, step=0.01,
        value=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
        help="Reassign -1 (noise) to nearest cluster if similarity â‰¥ Ï„."
    )
    st.markdown("---")
    st.header("4. Materiality")
    pm_val = _krw_input(
        "Performance Materiality (KRW)",
        key="pm_value",
        default_value=PM_DEFAULT,
        help_text="Used for KIT (PM exceed) and integrated risk scoring."
    )
    st.caption("â“˜ The PM threshold is drawn as a red dotted line on applicable charts. "
               "Y-axis scaling may change to accommodate this line.")

    # ğŸ§¹ ìºì‹œ ê´€ë¦¬
    with st.expander("ğŸ§¹ ìºì‹œ ê´€ë¦¬", expanded=False):
        if st.button("ì„ë² ë”© ìºì‹œ ë¹„ìš°ê¸°"):
            import shutil
            from services.cache import _model_dir
            for m in ["text-embedding-3-small", "text-embedding-3-large"]:
                try:
                    shutil.rmtree(_model_dir(m), ignore_errors=True)
                except Exception as e:
                    st.warning(f"{m} ì‚­ì œ ì‹¤íŒ¨: {e}")
            st.success("ì„ë² ë”© ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ë°ì´í„° ìºì‹œ ë¹„ìš°ê¸°"):
            st.cache_data.clear()
            st.success("Streamlit ë°ì´í„° ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ìºì‹œ ì •ë³´ ë³´ê¸°"):
            from services.cache import get_cache_info
            try:
                st.write(get_cache_info("text-embedding-3-small"))
                st.write(get_cache_info("text-embedding-3-large"))
            except Exception as e:
                st.info(f"ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")


@st.cache_data(show_spinner=False)
def _read_excel(_file, sheet_name=None):
    return pd.read_excel(_file, sheet_name=sheet_name)


@st.cache_data(show_spinner=False)
def _read_xls(_file):
    # pickle ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ë§Œ ìºì‹œ â†’ ì‹œíŠ¸ëª… ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    return pd.ExcelFile(_file).sheet_names

# (removed duplicated definition) _krw_input â€” ìœ„ì˜ ë‹¨ì¼ ë²„ì „ë§Œ ìœ ì§€

def _apply_scope(df: pd.DataFrame, scope: str) -> pd.DataFrame:
    """ìŠ¤ì½”í”„ ì ìš© ì‹œ ê²°ì¸¡ ì»¬ëŸ¼ ë°©ì–´: 'period_tag' ë¯¸ì¡´ì¬ë©´ ì›ë³¸ ë°˜í™˜.
    df.get('period_tag','')ê°€ ë¬¸ìì—´ì„ ë°˜í™˜í•  ê²½ìš° .eq í˜¸ì¶œ AttributeErrorë¥¼ ë°©ì§€í•œë‹¤.
    """
    if df is None or df.empty or 'period_tag' not in df.columns:
        return df
    if scope == "ë‹¹ê¸°":
        return df[df['period_tag'].eq('CY')]
    if scope == "ë‹¹ê¸°+ì „ê¸°":
        return df[df['period_tag'].isin(['CY', 'PY'])]
    return df

# === ê³µìš©: í‘œ ë†’ì´ ìë™ ì œí•œ(í–‰ ìˆ˜ ê¸°ë°˜) ===
def _auto_table_height(df: pd.DataFrame, max_rows: int = 8,
                       row_px: int = 28, header_px: int = 38, pad_px: int = 12) -> int:
    """
    í‘œ ë†’ì´ë¥¼ 'í‘œì‹œ í–‰ ìˆ˜' ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•´ ë„˜ê¸°ê¸° ìœ„í•œ ìœ í‹¸.
    - max_rows: ìµœëŒ€ í‘œì‹œ í–‰ìˆ˜
    - ì‹¤íŒ¨ ì‹œ 300pxë¡œ í´ë°±
    """
    try:
        n = int(min(max(len(df), 1), max_rows))
        return int(header_px + n * row_px + pad_px)
    except Exception:
        return 300

def _lf_by_scope() -> LedgerFrame:
    """ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì½”í”„ ì ìš© LedgerFrame."""
    hist = st.session_state.get('lf_hist')
    scope = st.session_state.get('period_scope', 'ë‹¹ê¸°')
    if hist is None:
        return None
    return LedgerFrame(df=_apply_scope(hist.df, scope), meta=hist.meta)

# (removed) êµ¬ë²„ì „ í…ìŠ¤íŠ¸ì…ë ¥ + Â±step / âœ–reset ë³€í˜•ë“¤ â€” ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ë²„íŠ¼ë¥˜ ì‚­ì œ ë° ë‹¨ì¼í™”


if uploaded_file is not None:
    if not st.session_state.mapping_confirmed:
        # ... ì»¬ëŸ¼ ë§¤í•‘ UI ...
        try:
            st.info("2ë‹¨ê³„: ì—‘ì…€ì˜ ì»¬ëŸ¼ì„ ë¶„ì„ í‘œì¤€ í•„ë“œì— ë§ê²Œ ì§€ì •í•´ì£¼ì„¸ìš”.")
            sheet_names = _read_xls(uploaded_file)
            first_ledger_sheet = next((s for s in sheet_names if 'ledger' in s.lower()), None)
            if first_ledger_sheet is None:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            ledger_cols = _read_excel(uploaded_file, sheet_name=first_ledger_sheet).columns.tolist()
            ledger_map = {}
            st.markdown("#### **Ledger ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            cols = st.columns(6)
            ledger_fields = {'íšŒê³„ì¼ì': 'ì¼ì', 'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê±°ë˜ì²˜': 'ê±°ë˜ì²˜', 'ì ìš”': 'ì ìš”', 'ì°¨ë³€': 'ì°¨ë³€', 'ëŒ€ë³€': 'ëŒ€ë³€'}
            for i, (key, keyword) in enumerate(ledger_fields.items()):
                with cols[i]:
                    is_optional = key == 'ê±°ë˜ì²˜'
                    default_col = find_column_by_keyword(ledger_cols, keyword)
                    options = ['ì„ íƒ ì•ˆ í•¨'] + ledger_cols if is_optional else ledger_cols
                    default_index = options.index(default_col) if default_col in options else 0
                    ledger_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", options, index=default_index, key=f"map_ledger_{key}")
            st.markdown("---")
            st.markdown("#### **Master ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            master_cols = _read_excel(uploaded_file, sheet_name='Master').columns.tolist()
            master_map = {}
            cols = st.columns(7)
            master_fields = {'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…': 'ê³„ì •ëª…', 'BS/PL': 'BS/PL', 'ì°¨ë³€/ëŒ€ë³€': 'ì°¨ë³€/ëŒ€ë³€', 'ë‹¹ê¸°ë§ì”ì•¡': 'ë‹¹ê¸°ë§', 'ì „ê¸°ë§ì”ì•¡': 'ì „ê¸°ë§', 'ì „ì „ê¸°ë§ì”ì•¡': 'ì „ì „ê¸°ë§'}
            for i, (key, keyword) in enumerate(master_fields.items()):
                with cols[i]:
                    default_col = find_column_by_keyword(master_cols, keyword)
                    default_index = master_cols.index(default_col) if default_col in master_cols else 0
                    master_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", master_cols, index=default_index, key=f"map_master_{key}")
            if st.button("âœ… ë§¤í•‘ í™•ì¸ ë° ë°ì´í„° ì²˜ë¦¬", type="primary"):
                st.session_state.ledger_map = ledger_map
                st.session_state.master_map = master_map
                st.session_state.mapping_confirmed = True
                st.rerun()
        except Exception as e:
            st.error(f"ì—‘ì…€ íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    else:  # ë§¤í•‘ í™•ì¸ í›„
        try:
            ledger_map, master_map = st.session_state.ledger_map, st.session_state.master_map
            master_df = _read_excel(uploaded_file, sheet_name='Master')
            sheet_names = _read_xls(uploaded_file)
            ledger_sheets = [s for s in sheet_names if 'ledger' in s.lower()]
            if not ledger_sheets:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            all_parts = []
            for s in ledger_sheets:
                part = _read_excel(uploaded_file, sheet_name=s)
                part['source_sheet'] = s
                part = add_provenance_columns(part)
                all_parts.append(part)
            ledger_df = pd.concat(all_parts, ignore_index=True)
            # row_id: íŒŒì¼ëª…|ì‹œíŠ¸:í–‰  (ì„¸ì…˜/ì¬ì‹¤í–‰ì—ë„ ì•ˆì •)
            try:
                base = Path(getattr(uploaded_file, "name", "uploaded.xlsx")).stem
                if 'row_id' in ledger_df.columns:
                    ledger_df['row_id'] = base + "|" + ledger_df['row_id'].astype(str)
            except Exception:
                pass
            ledger_df.rename(columns={v: k for k, v in ledger_map.items() if v != 'ì„ íƒ ì•ˆ í•¨'}, inplace=True)
            master_df.rename(columns={v: k for k, v in master_map.items()}, inplace=True)

            # ğŸ”§ ë³‘í•© ì „ì— íƒ€ì…/í¬ë§·ì„ ë¨¼ì € í†µì¼
            for df_ in [ledger_df, master_df]:
                if 'ê³„ì •ì½”ë“œ' in df_.columns:
                    df_['ê³„ì •ì½”ë“œ'] = (
                        df_['ê³„ì •ì½”ë“œ']
                        .astype(str)
                        .str.replace(r'\.0$', '', regex=True)
                        .str.strip()
                    )

            master_essentials = master_df[['ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…']].drop_duplicates()
            ledger_df = pd.merge(ledger_df, master_essentials, on='ê³„ì •ì½”ë“œ', how='left')
            ledger_df['ê³„ì •ëª…'] = ledger_df['ê³„ì •ëª…'].fillna('ë¯¸ì§€ì • ê³„ì •')

            ledger_df['íšŒê³„ì¼ì'] = pd.to_datetime(ledger_df['íšŒê³„ì¼ì'], errors='coerce')
            ledger_df.dropna(subset=['íšŒê³„ì¼ì'], inplace=True)
            for col in ['ì°¨ë³€', 'ëŒ€ë³€']:
                ledger_df[col] = pd.to_numeric(ledger_df[col], errors='coerce').fillna(0)
            for col in ['ë‹¹ê¸°ë§ì”ì•¡', 'ì „ê¸°ë§ì”ì•¡', 'ì „ì „ê¸°ë§ì”ì•¡']:
                if col in master_df.columns:
                    master_df[col] = pd.to_numeric(master_df[col], errors='coerce').fillna(0)
                else:
                    master_df[col] = 0
            ledger_df['ê±°ë˜ê¸ˆì•¡'] = ledger_df['ì°¨ë³€'] - ledger_df['ëŒ€ë³€']
            ledger_df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] = abs(ledger_df['ê±°ë˜ê¸ˆì•¡'])
            ledger_df['ì—°ë„'] = ledger_df['íšŒê³„ì¼ì'].dt.year
            ledger_df['ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.month
            # âœ… ë¶„ì„ ê·œì¹™: ê³„ì • ì„œë¸Œì…‹ ë¶„ì„ ì‹œì—ë„ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•œ í¸ì˜ íŒŒìƒ
            ledger_df['ì—°ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
            # âœ… period_tag ì¶”ê°€(CY/PY/Other)
            ledger_df = add_period_tag(ledger_df)
            if 'ê±°ë˜ì²˜' not in ledger_df.columns:
                ledger_df['ê±°ë˜ì²˜'] = 'ì •ë³´ ì—†ìŒ'
            ledger_df['ê±°ë˜ì²˜'] = ledger_df['ê±°ë˜ì²˜'].fillna('ì •ë³´ ì—†ìŒ').astype(str)

            if st.button("ğŸš€ ì „ì²´ ë¶„ì„ ì‹¤í–‰", type="primary"):
                with st.spinner('ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                    # âœ… ì •í•©ì„±ì€ ì‚¬ìš©ì ê¸°ê°„ ì„ íƒê³¼ ë¬´ê´€í•˜ê²Œ ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
                    st.session_state.recon_status, st.session_state.recon_df = analyze_reconciliation(ledger_df, master_df)
                    # âœ… í‘œì¤€ LedgerFrame êµ¬ì„±(ì •í•©ì„±ì€ í•­ìƒ ì „ì²´ ê¸°ì¤€: DF_hist)
                    lf_hist = LedgerFrame(df=ledger_df, meta={
                        "file_name": getattr(uploaded_file, "name", "uploaded.xlsx"),
                        "master_df": master_df,
                    })
                    # ì´ˆê¸°ì—” focus=hist (í›„ì† ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì í•„í„° ì—°ê²°)
                    lf_focus = lf_hist

                    st.session_state.master_df = master_df
                    st.session_state.ledger_df = ledger_df
                    st.session_state.lf_hist = lf_hist
                    st.session_state.lf_focus = lf_focus
                    st.session_state.analysis_done = True
                st.rerun()

            if st.session_state.analysis_done:
                st.success("âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                # --- ê³„ì •â†’ì‚¬ì´í´ ë§¤í•‘ ê²€í† /ìˆ˜ì • ---
                upload_id = getattr(uploaded_file, "name", "uploaded.xlsx")
                # ì—…ë¡œë“œ ì§í›„ 1íšŒ: í”„ë¦¬ì…‹ ì—†ìœ¼ë©´ ë£°ë² ì´ìŠ¤ ìë™ ìƒì„±
                names_dict = (
                    master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']]
                        .drop_duplicates()
                        .assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str))
                        .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict()
                )
                if not cyc.get_effective_cycles(upload_id):
                    cyc.build_cycles_preset(upload_id, names_dict, use_llm=False)

                with st.expander("ğŸ§­ ê³„ì • â†’ ì‚¬ì´í´ ë§¤í•‘ ê²€í† /ìˆ˜ì •", expanded=False):
                    cur_map = cyc.get_effective_cycles(upload_id)
                    map_df = master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']].drop_duplicates().copy()
                    map_df['ê³„ì •ì½”ë“œ'] = map_df['ê³„ì •ì½”ë“œ'].astype(str)
                    map_df['ì‚¬ì´í´(í‘œì‹œ)'] = map_df['ê³„ì •ì½”ë“œ'].map(lambda c: cyc.code_to_ko(cur_map.get(c, 'Other')))

                    st.caption("ì‚¬ì´í´ ë¼ë²¨ì„ ìˆ˜ì •í•œ ë’¤ ì €ì¥ì„ ëˆ„ë¥´ì„¸ìš”. (í‘œì‹œëŠ” í•œê¸€, ë‚´ë¶€ëŠ” ì½”ë“œë¡œ ì €ì¥ë©ë‹ˆë‹¤)")
                    edited = st.data_editor(
                        map_df, hide_index=True, use_container_width=True,
                        column_config={
                            "ì‚¬ì´í´(í‘œì‹œ)": st.column_config.SelectboxColumn(
                                options=list(cyc.CYCLE_KO.values()), required=True
                            )
                        }
                    )

                    c1, c2, c3 = st.columns(3)
                    with c1:
                        if st.button("ğŸ’¾ ë§¤í•‘ ì €ì¥", type="primary", key="btn_save_cycles_map"):
                            new_map_codes = {
                                str(r['ê³„ì •ì½”ë“œ']): cyc.ko_to_code(r['ì‚¬ì´í´(í‘œì‹œ)'])
                                for _, r in edited.iterrows()
                            }
                            cyc.set_cycles_map(upload_id, new_map_codes)
                            st.success(f"ì €ì¥ë¨: {len(new_map_codes):,}ê°œ ê³„ì •")
                    with c2:
                        if st.button("ğŸ¤– LLM ì¶”ì²œ ë³‘í•©", help="ë£°ë² ì´ìŠ¤ ê²°ê³¼ ìœ„ì— LLM ì œì•ˆì„ ë®ì–´ì”Œì›ë‹ˆë‹¤", key="btn_merge_llm_cycles"):
                            cyc.build_cycles_preset(upload_id, names_dict, use_llm=True)
                            st.success("LLM ì¶”ì²œì„ ë³‘í•©í–ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                    with c3:
                        if st.button("â†º ë£°ë² ì´ìŠ¤ë¡œ ì´ˆê¸°í™”", key="btn_reset_rule_cycles"):
                            cyc.build_cycles_preset(upload_id, names_dict, use_llm=False)
                            st.success("ë£°ë² ì´ìŠ¤ë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                with st.expander("ğŸ” ë¹ ë¥¸ ì§„ë‹¨(ë°ì´í„° í’ˆì§ˆ ì²´í¬)", expanded=False):
                    df = st.session_state.ledger_df.copy()
                    issues = []

                    invalid_date = int(df['íšŒê³„ì¼ì'].isna().sum())
                    if invalid_date > 0:
                        issues.append(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ(NaT): {invalid_date:,}ê±´")

                    if 'ê±°ë˜ì²˜' in df.columns:
                        missing_vendor = int((df['ê±°ë˜ì²˜'].isna() | (df['ê±°ë˜ì²˜'] == 'ì •ë³´ ì—†ìŒ')).sum())
                        if missing_vendor > 0:
                            issues.append(f"â„¹ï¸ ê±°ë˜ì²˜ ì •ë³´ ì—†ìŒ/ê²°ì¸¡: {missing_vendor:,}ê±´")

                    zero_abs = int((df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] == 0).sum())
                    issues.append(f"â„¹ï¸ ê¸ˆì•¡ ì ˆëŒ€ê°’ì´ 0ì¸ ì „í‘œ: {zero_abs:,}ê±´")

                    unlinked = int(df['ê³„ì •ëª…'].eq('ë¯¸ì§€ì • ê³„ì •').sum())
                    if unlinked > 0:
                        issues.append(f"â— Masterì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì „í‘œ(ê³„ì •ëª… ë¯¸ì§€ì •): {unlinked:,}ê±´")

                    st.write("**ì²´í¬ ê²°ê³¼**")
                    if issues:
                        for line in issues:
                            st.write("- " + line)
                    else:
                        st.success("ë¬¸ì œ ì—†ì´ ê¹”ë”í•©ë‹ˆë‹¤!")
                tab_integrity, tab_vendor, tab_anomaly, tab_ts, tab_report, tab_corr = st.tabs(["ğŸŒŠ ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„", "ğŸ¢ ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„", "ğŸ”¬ ì´ìƒ íŒ¨í„´ íƒì§€", "ğŸ“‰ ì‹œê³„ì—´ ì˜ˆì¸¡", "ğŸ§  ë¶„ì„ ì¢…í•© ëŒ€ì‹œë³´ë“œ", "ğŸ“Š ìƒê´€ê´€ê³„"])

                # (ì´ì „ ë²„ì „) ëŒ€ì‹œë³´ë“œ íƒ­ì€ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì œê±°ë¨
                with tab_integrity:  # ...
                    st.header("ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    st.subheader("1. ë°ì´í„° ì •í•©ì„± ê²€ì¦ ê²°ê³¼")
                    mod = st.session_state.get('modules', {}).get('integrity')
                    status = (getattr(mod, 'summary', {}) or {}).get('overall_status', 'Pass') if mod else 'Pass'
                    result_df = (getattr(mod, 'tables', {}) or {}).get('reconciliation') if mod else st.session_state.get('recon_df')
                    if status == "Pass":
                        st.success("âœ… ëª¨ë“  ê³„ì •ì˜ ë°ì´í„°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤.")
                    elif status == "Warning":
                        st.warning("âš ï¸ ì¼ë¶€ ê³„ì •ì—ì„œ ì‚¬ì†Œí•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.error("ğŸš¨ ì¼ë¶€ ê³„ì •ì—ì„œ ì¤‘ëŒ€í•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    def highlight_status(row):
                        if row.ìƒíƒœ == 'Fail':
                            return ['background-color: #ffcccc'] * len(row)
                        elif row.ìƒíƒœ == 'Warning':
                            return ['background-color: #fff0cc'] * len(row)
                        return [''] * len(row)

                    format_dict = {col: '{:,.0f}' for col in result_df.select_dtypes(include=np.number).columns}
                    st.dataframe(result_df.style.apply(highlight_status, axis=1).format(format_dict), use_container_width=True)
                    st.markdown("---")
                    st.subheader("2. ê³„ì •ë³„ ì›”ë³„ ì¶”ì´ (PY vs CY)")
                    # âœ… ìë™ ì¶”ì²œ ì œê±°: ì‚¬ìš©ìê°€ ê³„ì •ì„ ì„ íƒí•œ ê²½ìš°ì—ë§Œ ê·¸ë˜í”„ ë Œë”
                    account_list = st.session_state.master_df['ê³„ì •ëª…'].unique()
                    selected_accounts = st.multiselect(
                        "ë¶„ì„í•  ê³„ì •ì„ ì„ íƒí•˜ì„¸ìš” (1ê°œ ì´ìƒ í•„ìˆ˜)",
                        account_list, default=[],
                        key="trend_accounts_pick"
                    )
                    # â–¼ ì‚¬ì´í´ í”„ë¦¬ì…‹(ì„ íƒ ì‹œ ìœ„ ë©€í‹°ì…€ë ‰íŠ¸ì— ê³„ì • ìë™ ë°˜ì˜)
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ(ì„ íƒí•˜ë©´ ìœ„ ê³„ì • ëª©ë¡ì— ìë™ ë°˜ì˜)",
                            list(cyc.CYCLE_KO.values()),
                            default=[], key="trend_cycles_pick"
                        )
                        st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_trend", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="trend_cycles_pick",
                                              accounts_state_key="trend_accounts_pick",
                                              master_df=st.session_state.master_df))
                    if not selected_accounts:
                        st.info("ê³„ì •ì„ 1ê°œ ì´ìƒ ì„ íƒí•˜ë©´ ì›”ë³„ ì¶”ì´ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        lf_use = st.session_state.get('lf_focus') or st.session_state.get('lf_hist')
                        # ì„ íƒëœ ê³„ì •ëª…ì„ ê³„ì •ì½”ë“œë¡œ ë³€í™˜
                        mdf = st.session_state.master_df
                        accounts_codes = (
                            mdf[mdf['ê³„ì •ëª…'].isin(selected_accounts)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        mod = run_trend_module(lf_use, accounts=accounts_codes)
                        _push_module(mod)
                        for w in mod.warnings:
                            st.warning(w)
                        if mod.figures:
                            for title, fig in mod.figures.items():
                                # PM ì„ê³„ì„ (í•­ìƒ í‘œì‹œ; ë²”ìœ„ ë°–ì´ë©´ ìë™ í™•ì¥)
                                st.plotly_chart(
                                    add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT))),
                                    use_container_width=True,
                                    key=f"trend_{title}"
                                )
                        else:
                            st.info("í‘œì‹œí•  ì¶”ì´ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("3. ê³„ì • ê°„ ìƒê´€ íˆíŠ¸ë§µ")
                    st.info("ì´ ê¸°ëŠ¥ì€ ìƒë‹¨ì˜ **ğŸ“Š ìƒê´€ê´€ê³„ â†’ 'ê¸°ë³¸' ì„œë¸Œíƒ­**ìœ¼ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.")

                with tab_vendor:
                    st.header("ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")

                    st.subheader("ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± (ê³„ì •ë³„)")
                    master_df_res = st.session_state.master_df
                    account_list_vendor = master_df_res['ê³„ì •ëª…'].unique()
                    selected_accounts_vendor = st.multiselect("ë¶„ì„í•  ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”.", account_list_vendor, default=[], key="vendor_accounts_pick")
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles_vendor = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ", list(cyc.CYCLE_KO.values()),
                            default=[], key="vendor_cycles_pick"
                        )
                        st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_vendor", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="vendor_cycles_pick",
                                              accounts_state_key="vendor_accounts_pick",
                                              master_df=st.session_state.master_df))

                    # ğŸ”§ ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„° â€” KRW ì…ë ¥(ì»¤ë°‹ ì‹œ ì‰¼í‘œ ì •ê·œí™”)
                    min_amount_vendor = _krw_input(
                        "ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„°",
                        key="vendor_min_amount",
                        default_value=0,
                        help_text="CY ê¸°ì¤€ ê±°ë˜ê¸ˆì•¡ í•©ê³„ê°€ ì´ ê°’ ë¯¸ë§Œì¸ ê±°ë˜ì²˜ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°ë©ë‹ˆë‹¤."
                    )
                    include_others_vendor = st.checkbox("ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°", value=True)

                    if selected_accounts_vendor:
                        selected_codes = (
                            master_df_res[master_df_res['ê³„ì •ëª…'].isin(selected_accounts_vendor)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        lf_use = _lf_by_scope()
                        vmod = run_vendor_module(
                            lf_use,
                            account_codes=selected_codes,
                            min_amount=float(min_amount_vendor),
                            include_others=bool(include_others_vendor),
                        )
                        _push_module(vmod)
                        if vmod.figures:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'pareto' in vmod.figures:
                                    figp = vmod.figures['pareto']
                                    figp = add_materiality_threshold(figp, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figp, use_container_width=True, key=f"vendor_pareto_{'_'.join(selected_accounts_vendor) or 'all'}")
                            with col2:
                                if 'heatmap' in vmod.figures:
                                    figh = add_pm_badge(vmod.figures['heatmap'], float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figh, use_container_width=True, key=f"vendor_heatmap_{'_'.join(selected_accounts_vendor) or 'all'}")
                        else:
                            st.warning("ì„ íƒí•˜ì‹  ê³„ì •ì—ëŠ” ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        for w in vmod.warnings:
                            st.warning(w)
                    else:
                        st.info("ê³„ì •ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ê³„ì •ì˜ ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± ë¶„ì„ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("ê±°ë˜ì²˜ë³„ ì„¸ë¶€ ë¶„ì„ (ì „ì²´ ê³„ì •)")
                    full_ledger_df = st.session_state.ledger_df
                    vendor_list = sorted(full_ledger_df[full_ledger_df['ê±°ë˜ì²˜'] != 'ì •ë³´ ì—†ìŒ']['ê±°ë˜ì²˜'].unique())

                    if len(vendor_list) > 0:
                        options = ['ì„ íƒí•˜ì„¸ìš”...'] + vendor_list
                        selected_vendor = st.selectbox("ìƒì„¸ ë¶„ì„í•  ê±°ë˜ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.", options, index=0)

                        if selected_vendor != 'ì„ íƒí•˜ì„¸ìš”...':
                            all_months_in_data = pd.period_range(
                                start=full_ledger_df['íšŒê³„ì¼ì'].min(),
                                end=full_ledger_df['íšŒê³„ì¼ì'].max(),
                                freq='M'
                            ).strftime('%Y-%m').tolist()
                            detail_fig = create_vendor_detail_figure(full_ledger_df, selected_vendor, all_months_in_data)
                            # PM line on vendor detail (stacked bars)
                            try:
                                detail_fig = add_materiality_threshold(detail_fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                            except Exception:
                                pass
                            if detail_fig:
                                st.plotly_chart(detail_fig, use_container_width=True, key=f"vendor_detail_{selected_vendor}")
                    else:
                        st.info("ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                with tab_anomaly:
                    st.header("ì´ìƒ íŒ¨í„´ íƒì§€")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    mdf = st.session_state.master_df
                    acct_names = mdf['ê³„ì •ëª…'].unique()
                    pick = st.multiselect("ëŒ€ìƒ ê³„ì • ì„ íƒ(ë¯¸ì„ íƒ ì‹œ ìë™ ì¶”ì²œ)", acct_names, default=[])
                    topn = st.slider("í‘œì‹œ ê°œìˆ˜(ìƒìœ„ |Z|)", min_value=10, max_value=500, value=20, step=10)
                    if st.button("ì´ìƒì¹˜ ë¶„ì„ ì‹¤í–‰"):
                        lf_use = _lf_by_scope()
                        codes = None
                        if pick:
                            codes = mdf[mdf['ê³„ì •ëª…'].isin(pick)]['ê³„ì •ì½”ë“œ'].astype(str).tolist()
                        amod = run_anomaly_module(lf_use, target_accounts=codes, topn=topn, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                        _push_module(amod)
                        for w in amod.warnings: st.warning(w)
                        if 'anomaly_top' in amod.tables:
                            _tbl = amod.tables['anomaly_top'].copy()
                            fmt = {}
                            if 'ë°œìƒì•¡' in _tbl.columns: fmt['ë°œìƒì•¡'] = '{:,.0f}'
                            if 'Z-Score' in _tbl.columns: fmt['Z-Score'] = '{:.2f}'
                            st.dataframe(_tbl.style.format(fmt), use_container_width=True)
                        if 'zscore_hist' in amod.figures:
                            st.plotly_chart(amod.figures['zscore_hist'], use_container_width=True, key="anomaly_hist")

                with tab_ts:
                    st.header("ì‹œê³„ì—´ ì˜ˆì¸¡")
                    with st.expander("ğŸ§­ í•´ì„ ê°€ì´ë“œ", expanded=False, icon=":material/help:"):
                        st.markdown(
                            """
### ìš©ì–´
- **z(í‘œì¤€í™” ì§€ìˆ˜)**: `z = (ì‹¤ì¸¡ âˆ’ ì˜ˆì¸¡) / Ïƒ`  
  - ì›”ë³„ ì˜ˆì¸¡ ì”ì°¨(ì‹¤ì¸¡-ì˜ˆì¸¡)ë¥¼ í‘œì¤€í™”í•œ ì§€ìˆ˜ì…ë‹ˆë‹¤. **ì´ìƒ íŒ¨í„´ íƒì§€ì˜ Z-Scoreì™€ ë‹¤ë¥¸ ê°œë…ì…ë‹ˆë‹¤.**
  - |z|â‰ˆ2ëŠ” **ì´ë¡€ì **, |z|â‰ˆ3ì€ **ë§¤ìš° ì´ë¡€ì **ì…ë‹ˆë‹¤.  
- **Ïƒ(í‘œì¤€í¸ì°¨) ì§‘ê³„**: ìµœê·¼ *k=6ê°œì›”* ì”ì°¨ì˜ í‘œì¤€í¸ì°¨ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ì§§ìœ¼ë©´ ì‹œì‘~í˜„ì¬ê¹Œì§€ì˜ **expanding Ïƒ**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
- **ìœ„í—˜ë„(0~1)** = `min(1, 0.5Â·|z|/3 + 0.3Â·PMëŒ€ë¹„ + 0.2Â·KIT)`  
  - PMëŒ€ë¹„ = `min(1, |ì‹¤ì¸¡âˆ’ì˜ˆì¸¡| / PM)`,  **KIT** = PM ì´ˆê³¼ ì—¬ë¶€(True/False)
- **Flow / Balance**: *Flow*ëŠ” **ì›” ë°œìƒì•¡(Î”ì”ì•¡)**, *Balance*ëŠ” **ì›”ë§ ì”ì•¡**ì…ë‹ˆë‹¤. *(BS ê³„ì •ì€ Balance ê¸°ì¤€ë„ ë³‘í–‰ ê³„ì‚°í•©ë‹ˆë‹¤.)*
- **ì •ìƒì„±**: ì‹œê³„ì—´ì˜ í‰ê· /ë¶„ì‚°ì´ ì‹œê°„ì— ë”°ë¼ **ì•ˆ ë³€í•¨**(ARIMAê°€ íŠ¹íˆ ì„ í˜¸).
- **MAE**: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨(ì› ë‹¨ìœ„). **ì‘ì„ìˆ˜ë¡ ì •í™•**.
- **MAPE**: ìƒëŒ€ ì˜¤ì°¨(%). **ê·œëª¨ ë‹¤ë¥¸ ê³„ì • ë¹„êµ**ì— ìœ ìš©.
- **AIC/BIC**: ëª¨ë¸ ë³µì¡ë„ê¹Œì§€ ê³ ë ¤í•œ **ì •ë³´ëŸ‰ ì§€í‘œ**. **ì‘ì„ìˆ˜ë¡ ìš°ìˆ˜**.

### ì°¨íŠ¸ ì½ê¸°
- ì‹¤ì„ =ì‹¤ì¸¡, ì ì„ =ì˜ˆì¸¡(**MoR**: EMA/MA/ARIMA/Prophet ì¤‘ ìë™ ì„ íƒ)  
- íšŒìƒ‰ ì ì„ : **ì—°(êµµê²Œ)** / **ë¶„ê¸°(ì–‡ê²Œ)** ê²½ê³„ì„ , ë¶‰ì€ ì ì„ : **PM ê¸°ì¤€ì„ **

### ì‚¬ìš©í•œ ì˜ˆì¸¡ëª¨ë¸
- **MA(ì´ë™í‰ê· )**: ìµœê·¼ *n*ê°œì›” **ë‹¨ìˆœ í‰ê· **. **ì§§ì€ ë°ì´í„°/ë³€ë™ ì™„ë§Œ**í•  ë•Œ ì•ˆì •ì .
- **EMA(ì§€ìˆ˜ì´ë™í‰ê· )**: **ìµœê·¼ê°’ ê°€ì¤‘** í‰ê· . **ìµœê·¼ ì¶”ì„¸ ë°˜ì˜**ì´ í•„ìš”í•  ë•Œ ìœ ë¦¬.
- **ARIMA(p,d,q)**: **ìê¸°ìƒê´€** ê¸°ë°˜. **ê³„ì ˆì„±ì´ ì•½(ë˜ëŠ” ì œê±° ê°€ëŠ¥)**í•˜ê³  **ë°ì´í„°ê°€ ì¶©ë¶„**í•  ë•Œ ê°•í•¨.
- **Prophet**: **ì—°/ë¶„ê¸° ê³„ì ˆì„±Â·íœ´ì¼íš¨ê³¼**ê°€ ëšœë ·í•  ë•Œ ì í•©(ì´ìƒì¹˜ì— ë¹„êµì  ê²¬ê³ ).

> :blue[**ëª¨ë¸ì€ ê³„ì •Ã—ê¸°ì¤€(Flow/Balance)ë³„ë¡œ êµì°¨ê²€ì¦ ì˜¤ì°¨(MAPE/MAE)ì™€ (ê°€ëŠ¥í•˜ë©´) ì •ë³´ëŸ‰(AIC/BIC)ì„ ì¢…í•©í•´ ìë™ ì„ íƒë©ë‹ˆë‹¤.**]
"""
                        )

                    # 0) ì›ì¥/ì„¸ì…˜ í™•ë³´
                    master_df: pd.DataFrame = st.session_state.get("master_df", pd.DataFrame())
                    if master_df.empty:
                        st.info("ì›ì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    # --- state bootstrap --- (ìœ„ì ¯ ìƒì„± ì „ì— ì‹¤í–‰)
                    st.session_state.setdefault("ts_accounts_names", [])
                    st.session_state.setdefault("ts_cycles_ko", [])
                    st.session_state.setdefault("ts_acc_buffer", None)
                    st.session_state.setdefault("ts_acc_needs_update", False)

                    # --- preset ì£¼ì… í›…: rerun ì§í›„, ë©€í‹°ì…€ë ‰íŠ¸ ìƒì„± 'ì´ì „'ì— 1íšŒ ì£¼ì… ---
                    if st.session_state.ts_acc_needs_update and st.session_state.ts_acc_buffer is not None:
                        st.session_state.ts_accounts_names = st.session_state.ts_acc_buffer
                        st.session_state.ts_acc_needs_update = False

                    upload_id = getattr(uploaded_file, 'name', '_default')

                    # ë‘ ì»¨í…Œì´ë„ˆë¡œ ì‹œê° ìˆœì„œëŠ” ìœ ì§€(ê³„ì • ìœ„, í”„ë¦¬ì…‹ ì•„ë˜) + ì½”ë“œ ìˆœì„œ ì œì–´
                    box_accounts = st.container()
                    box_preset = st.container()

                    # Helper: í”„ë¦¬ì…‹(KO ë¼ë²¨) â†’ ê³„ì •ëª… ë¦¬ìŠ¤íŠ¸ë¡œ í™•ì¥
                    def expand_cycles_to_account_names(*, upload_id: str, cycles_ko: list[str], master_df: pd.DataFrame) -> list[str]:
                        try:
                            mapping = cyc.get_effective_cycles(upload_id)
                            codes = cyc.accounts_for_cycles_ko(mapping, cycles_ko)
                            df_map = master_df[["ê³„ì •ì½”ë“œ","ê³„ì •ëª…"]].dropna().copy()
                            df_map["ê³„ì •ì½”ë“œ"] = df_map["ê³„ì •ì½”ë“œ"].astype(str)
                            code_to_name = df_map.drop_duplicates("ê³„ì •ì½”ë“œ").set_index("ê³„ì •ì½”ë“œ")["ê³„ì •ëª…"].astype(str).to_dict()
                            names = [code_to_name.get(str(c), str(c)) for c in codes]
                            # ìœ ë‹ˆí¬+ìˆœì„œë³´ì¡´
                            return list(dict.fromkeys([n for n in names if n]))
                        except Exception:
                            return []

                    # (ì•„ë˜) í”„ë¦¬ì…‹ ì˜ì—­: ë²„íŠ¼ìœ¼ë¡œ ë²„í¼ë§Œ ê°±ì‹ 
                    with box_preset:
                        st.markdown("#### ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ(ì„ íƒ ì‹œ ìœ„ ê³„ì • ëª©ë¡ì— **ì ìš© ë²„íŠ¼**ìœ¼ë¡œ ë°˜ì˜)")
                        chosen_cycles = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹",
                            options=list(cyc.CYCLE_KO.values()),
                            key="ts_cycles_ko",
                        )
                        if st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="ts_apply_preset"):
                            names_from_cycles = expand_cycles_to_account_names(
                                upload_id=upload_id,
                                cycles_ko=st.session_state.ts_cycles_ko,
                                master_df=master_df,
                            )
                            merged = list(dict.fromkeys([
                                *st.session_state.ts_accounts_names,
                                *names_from_cycles,
                            ]))
                            st.session_state.ts_acc_buffer = merged
                            st.session_state.ts_acc_needs_update = True
                            st.rerun()

                    # (ìœ„) ê³„ì • ì˜ì—­: ë©€í‹°ì…€ë ‰íŠ¸ ê·¸ë¦¬ê¸°(ê°’ ì£¼ì…ì€ ìƒë‹¨ í›…ì´ ë‹´ë‹¹)
                    with box_accounts:
                        all_account_names = (
                            master_df["ê³„ì •ëª…"].dropna().astype(str).sort_values().unique().tolist()
                        )
                        picked_names = st.multiselect(
                            "ëŒ€ìƒ ê³„ì •(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
                            options=all_account_names,
                            key="ts_accounts_names",
                            help="ì„ íƒí•œ ê³„ì •ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ í…Œì´ë¸”/ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
                        )

                    # ë¯¸ë˜ ì˜ˆì¸¡ ê°œì›” ìˆ˜ ìŠ¬ë¼ì´ë” ì¶”ê°€
                    forecast_horizon = st.slider(
                        "ë¯¸ë˜ ì˜ˆì¸¡ ê°œì›” ìˆ˜(ì‹œê°í™”ìš©)", min_value=0, max_value=12, value=0, step=1,
                        help="í‘œë³¸ N<6ì´ë©´ ìë™ìœ¼ë¡œ 0ìœ¼ë¡œ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤."
                        )

                    if not picked_names:
                        st.info("ì‹œê³„ì—´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ì„ íƒí•œ ê³„ì •/ê¸°ê°„ì— ë°ì´í„° ì—†ìŒ)")
                        st.stop()

                    # 2) ê³„ì •ëª… â†’ ê³„ì •ì½”ë“œ
                    name_to_code = (
                        master_df.dropna(subset=["ê³„ì •ëª…","ê³„ì •ì½”ë“œ"]).astype({"ê³„ì •ëª…":"string","ê³„ì •ì½”ë“œ":"string"})
                                 .drop_duplicates(subset=["ê³„ì •ëª…"]).set_index("ê³„ì •ëª…")["ê³„ì •ì½”ë“œ"].to_dict()
                    )
                    want_codes = [name_to_code.get(n) for n in picked_names if n in name_to_code]

                    # 3) ì •ì‹ ì‹œê³„ì—´ íŒŒì´í”„ë¼ì¸: ledger â†’ ì›”ë³„ì§‘ê³„(flow) â†’ balance(opening+ëˆ„ì ) â†’ ì˜ˆì¸¡/ì§„ë‹¨/ê·¸ë¦¼
                    lf_use = st.session_state.get('lf_hist')
                    st.caption("â“˜ ì‹œê³„ì—´ ë¶„ì„ì€ ì¢Œì¸¡ ìŠ¤ì½”í”„ ì„¤ì •ê³¼ ë¬´ê´€í•˜ê²Œ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    if lf_use is None:
                        st.info("ì›ì¥ì„ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
                        st.stop()

                    # (1) ë¶„ì„ ëŒ€ìƒ ìŠ¬ë¼ì´ìŠ¤
                    ldf = lf_use.df.copy()
                    ldf = ldf[ldf['ê³„ì •ì½”ë“œ'].astype(str).isin([str(x) for x in want_codes])]

                    # (2) í•„ìˆ˜ íŒŒìƒ: ë°œìƒì•¡/ìˆœì•¡ ë³´ì¥
                    from analysis.anomaly import compute_amount_columns
                    ldf = compute_amount_columns(ldf)

                    # (3) ë‚ ì§œ/ê¸ˆì•¡ ì»¬ëŸ¼ í”½ì—…(ì—†ìœ¼ë©´ ì•ˆì „ ì¢…ë£Œ)
                    from analysis.timeseries import DATE_CANDIDATES, AMT_CANDIDATES
                    date_col = next((c for c in DATE_CANDIDATES if c in ldf.columns), None)
                    amount_col = next((c for c in AMT_CANDIDATES if c in ldf.columns), None)
                    if not date_col or not amount_col:
                        st.error(
                            "í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
                            f"- ë‚ ì§œ í›„ë³´: {DATE_CANDIDATES}\n- ê¸ˆì•¡ í›„ë³´: {AMT_CANDIDATES}\n\n"
                            f"í˜„ì¬ ì»¬ëŸ¼: {list(ldf.columns)}"
                        )
                        st.stop()

                    # (4) opening(ì „ê¸°ë§ì”ì•¡) ë§µ êµ¬ì„±
                    opening_map = {}
                    if "ì „ê¸°ë§ì”ì•¡" in master_df.columns and "ê³„ì •ì½”ë“œ" in master_df.columns:
                        opening_map = (
                            master_df[["ê³„ì •ì½”ë“œ","ì „ê¸°ë§ì”ì•¡"]]
                            .dropna(subset=["ê³„ì •ì½”ë“œ"])
                            .assign(ì „ê¸°ë§ì”ì•¡=lambda d: pd.to_numeric(d["ì „ê¸°ë§ì”ì•¡"], errors="coerce").fillna(0.0))
                            .groupby("ê³„ì •ì½”ë“œ")["ì „ê¸°ë§ì”ì•¡"].first().to_dict()
                        )

                    # (5) BS/PL í”Œë˜ê·¸
                    is_bs_map = {}
                    if "BS/PL" in master_df.columns and "ê³„ì •ì½”ë“œ" in master_df.columns:
                        is_bs_map = (
                            master_df.dropna(subset=["ê³„ì •ì½”ë“œ","BS/PL"])
                            .astype({"ê³„ì •ì½”ë“œ":"string","BS/PL":"string"})
                            .drop_duplicates(subset=["ê³„ì •ì½”ë“œ"])
                            .assign(is_bs=lambda d: d["BS/PL"].str.upper().eq("BS"))
                            .set_index("ê³„ì •ì½”ë“œ")["is_bs"].to_dict()
                        )

                    # (5.5) ì°¨ë³€/ëŒ€ë³€ í”Œë˜ê·¸(ëŒ€ë³€ ê³„ì •ì€ ë¶€í˜¸ ë°˜ì „)
                    is_credit_map = {}
                    if "ì°¨ë³€/ëŒ€ë³€" in master_df.columns and "ê³„ì •ì½”ë“œ" in master_df.columns:
                        is_credit_map = (
                            master_df.dropna(subset=["ê³„ì •ì½”ë“œ","ì°¨ë³€/ëŒ€ë³€"])
                                     .assign(ê³„ì •ì½”ë“œ=lambda d: d["ê³„ì •ì½”ë“œ"].astype(str),
                                             credit=lambda d: d["ì°¨ë³€/ëŒ€ë³€"].astype(str).str.contains("ëŒ€ë³€"))
                                     .drop_duplicates(subset=["ê³„ì •ì½”ë“œ"])
                                     .set_index("ê³„ì •ì½”ë“œ")["credit"].to_dict()
                        )

                    # (6) ëª¨ë¸ ì„ íƒ(ë ˆì§€ìŠ¤íŠ¸ë¦¬)
                    st.caption("ëª¨í˜•: EMA(ê³ ì •). ë³µì¡ ëŸ¬ë„ˆëŠ” ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    backend = "ema"

                    PM = float(st.session_state.get("pm_value", PM_DEFAULT))
                    # (7) ê³„ì •ë³„ ì‹¤í–‰: ê²°ê³¼ ìˆ˜ì§‘ìš© ë²„í¼
                    gathered_flow = []
                    gathered_balance = []
                    results_per_account = {}

                    for code in want_codes:
                        sub = ldf[ldf["ê³„ì •ì½”ë“œ"].astype(str) == str(code)].copy()
                        if sub.empty:
                            continue
                        acc_name = (master_df[master_df["ê³„ì •ì½”ë“œ"].astype(str)==str(code)]["ê³„ì •ëª…"].dropna().astype(str).head(1).tolist() or [str(code)])[0]
                        is_bs = bool(is_bs_map.get(str(code), False))

                        PM = float(st.session_state.get("pm_value", PM_DEFAULT))
                        # âœ¨ ëŒ€ë³€ê³„ì •ì´ë©´ ë¶€í˜¸ ë°˜ì „
                        sign = -1.0 if bool(is_credit_map.get(str(code), False)) else 1.0
                        # ëª¨ë¸ ì…ë ¥ ì»¬ëŸ¼ì„ ìˆ˜ì¹˜í™” + ë°˜ì „
                        try:
                            sub[amount_col] = pd.to_numeric(sub[amount_col], errors="coerce").fillna(0.0) * float(sign)
                        except Exception:
                            sub[amount_col] = pd.to_numeric(sub.get(amount_col, 0.0), errors="coerce").fillna(0.0) * float(sign)
                        # BS ì”ì•¡ìš© openingë„ ë™ì¼ ê¸°ì¤€ìœ¼ë¡œ ë°˜ì „
                        opening = 0.0
                        if isinstance(opening_map, dict):
                            opening = float(opening_map.get(str(code), 0.0)) * float(sign)

                        out = run_timeseries_minimal(
                            sub,
                            account_name=acc_name,
                            date_col=date_col,
                            amount_col=amount_col,
                            is_bs=bool(is_bs),
                            opening=opening,
                            pm_value=PM
                        )

                        # (ìˆ˜ì§‘) í†µí•© ìš”ì•½í‘œ(1í–‰) + ê·¸ë˜í”„(ë‹¤í¬ì¸íŠ¸) ë¶„ë¦¬
                        if not out.empty:
                            tmp = out.copy()
                            tmp.insert(0, "ê³„ì •", acc_name)

                            # ê·¸ë˜í”„/ì§„ë‹¨ìš©(ì „ êµ¬ê°„)
                            results_per_account[acc_name] = tmp

                            # === ìš”ì•½í–‰(ë§ˆì§€ë§‰ 1í–‰) + í†µê³„ì—´ ì¶”ê°€ ===
                            for ms in ("flow", "balance"):
                                dfm = tmp[tmp["measure"].eq(ms)]
                                if dfm.empty:
                                    continue
                                stats = compute_series_stats(dfm)
                                last_row = dfm.tail(1).copy()
                                last_row["MAE"]  = stats["MAE"]
                                last_row["MAPE"] = stats["MAPE"]
                                last_row["RMSE"] = stats["RMSE"]
                                last_row["N"]    = stats["N"]

                                if ms == "flow":
                                    gathered_flow.append(last_row)
                                else:
                                    gathered_balance.append(last_row)

                    # === ê³µìš©: í‘œ ë†’ì´ ìë™ ê³„ì‚° ===
                    def _auto_table_height(df: pd.DataFrame, *, min_rows=3, max_rows=10, row_px=32, header_px=40, padding_px=16) -> int:
                        n = 0 if df is None else int(len(df))
                        n = max(min_rows, min(max_rows, n))
                        return header_px + n * row_px + padding_px

                    # === NEW: í†µí•© í…Œì´ë¸”(ê·¸ë˜í”„ë³´ë‹¤ ìœ„ì— í•œ ë²ˆë§Œ) ===
                    def _render_table(blocks, title):
                        if not blocks:
                            return
                        tbl = pd.concat(blocks, ignore_index=True)

                        show_cols = ["ê³„ì •","date","actual","predicted","error","z","risk","model",
                                     "MAE","MAPE","RMSE","N"]  # â† í†µê³„ ì—´ ì¶”ê°€
                        for c in show_cols:
                            if c not in tbl.columns:
                                tbl[c] = np.nan

                        # ì‚¬ìš©ì ì¹œí™” ë¼ë²¨/ì •ë ¬
                        tbl = (tbl.rename(columns={
                            "date":"ì¼ì","actual":"ì‹¤ì¸¡","predicted":"ì˜ˆì¸¡",
                            "error":"ì”ì°¨","risk":"ìœ„í—˜ë„","model":"ëª¨ë¸(MoR)"
                        })
                            .sort_values(["ê³„ì •","ì¼ì"])
                        )

                        st.subheader(title)
                        fmt = {}
                        for c in ["ì‹¤ì¸¡","ì˜ˆì¸¡","ì”ì°¨","MAE","RMSE"]:
                            if c in tbl.columns: fmt[c] = "{:,.0f}"
                        if "MAPE" in tbl.columns: fmt["MAPE"] = "{:.2f}%"
                        if "z" in tbl.columns: fmt["z"] = "{:.2f}"
                        if "ìœ„í—˜ë„" in tbl.columns: fmt["ìœ„í—˜ë„"] = "{:.2f}"

                        # í‘œ ë†’ì´: í–‰ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìë™(ìµœëŒ€ 420px)
                        rows = max(1, len(tbl))
                        height = min(420, 42 + rows * 28)

                        st.dataframe(tbl[["ê³„ì •","ì¼ì","ì‹¤ì¸¡","ì˜ˆì¸¡","ì”ì°¨","z","ìœ„í—˜ë„","ëª¨ë¸(MoR)","MAE","MAPE","RMSE","N"]]
                                     .style.format(fmt),
                                     use_container_width=True, height=height)

                        st.download_button(
                            "CSV ë‹¤ìš´ë¡œë“œ", data=tbl.to_csv(index=False).encode("utf-8-sig"),
                            file_name=f"timeseries_summary_{'flow' if 'Flow' in title else 'balance'}.csv",
                            mime="text/csv"
                        )

                    def _auto_height(df: pd.DataFrame, max_rows: int = 12) -> int:
                        rows = int(min(len(df), max_rows))
                        base_row = 34  # ì²´ê°ê°’
                        header = 38
                        pad = 8
                        return header + rows * base_row + pad

                    def _render_table_combined(flow_blocks, balance_blocks, title="ì„ íƒê³„ì • ìš”ì•½ (Flow+Balance)"):
                        import pandas as pd
                        import numpy as np
                        blocks = []

                        def _prep(df, label):
                            if df is None or len(df) == 0:
                                return None
                            x = pd.concat(df, ignore_index=True)
                            
                            # ë¨¼ì € renameì„ í•´ì„œ ì»¬ëŸ¼ëª…ì„ í†µì¼
                            rename_map = {
                                "account": "ê³„ì •", "date": "ì¼ì",
                                "actual": "ì‹¤ì¸¡", "predicted": "ì˜ˆì¸¡",
                                "error": "ì”ì°¨", "risk": "ìœ„í—˜ë„",
                                "model": "ëª¨ë¸(MoR)"
                            }
                            for k, v in rename_map.items():
                                if k in x.columns and v not in x.columns:  # ì¤‘ë³µ ë°©ì§€
                                    x.rename(columns={k: v}, inplace=True)
                            
                            # ê·¸ ë‹¤ìŒ 'ê¸°ì¤€' ì»¬ëŸ¼ ì¶”ê°€
                            x.insert(0, "ê¸°ì¤€", label)
                            return x

                        f = _prep(flow_blocks, "ë°œìƒì•¡(Flow)")
                        b = _prep(balance_blocks, "ì”ì•¡(Balance)")
                        if f is not None: blocks.append(f)
                        if b is not None: blocks.append(b)
                        if not blocks:
                                return

                        tbl = pd.concat(blocks, ignore_index=True)

                        # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (í˜¹ì‹œ ìˆë‹¤ë©´)
                        tbl = tbl.loc[:, ~tbl.columns.duplicated()]

                        # âœ… z ë¼ë²¨ ë³€ê²½(í‘œì—ì„œë§Œ)
                        col_map = {
                            "date":"ì¼ì","actual":"ì‹¤ì¸¡","predicted":"ì˜ˆì¸¡","error":"ì”ì°¨",
                            "z":"z(ì‹œê³„ì—´)","risk":"ìœ„í—˜ë„","model":"ëª¨ë¸(MoR)"
                        }
                        for k, v in col_map.items():
                            if k in tbl.columns:
                                tbl.rename(columns={k: v}, inplace=True)

                        want_cols = ["ê¸°ì¤€", "ê³„ì •", "ì¼ì", "ì‹¤ì¸¡", "ì˜ˆì¸¡", "ì”ì°¨", "z(ì‹œê³„ì—´)", "ìœ„í—˜ë„", "ëª¨ë¸(MoR)"]
                        show_cols = [c for c in want_cols if c in tbl.columns]
                        tbl = tbl[show_cols].copy()

                        # í¬ë§·
                        fmt = {"ì‹¤ì¸¡":"{:,.0f}","ì˜ˆì¸¡":"{:,.0f}","ì”ì°¨":"{:,.0f}","ìœ„í—˜ë„":"{:.2f}","z(ì‹œê³„ì—´)":"{:.2f}"}

                        st.subheader(title)

                        # ì¸ë±ìŠ¤ ìˆ¨ê¹€ + í†µì¼ëœ ë†’ì´
                        tbl = tbl.reset_index(drop=True)
                        try:
                            st.dataframe(
                                tbl.style.format(fmt),
                                use_container_width=True,
                                hide_index=True,
                                height=_auto_height(tbl)
                            )
                        except TypeError:
                            st.dataframe(
                                tbl.style.format(fmt),
                                use_container_width=True,
                                height=_auto_height(tbl)
                            )

                        # CSV
                            st.download_button(
                            "CSV ë‹¤ìš´ë¡œë“œ",
                            data=tbl.to_csv(index=False).encode("utf-8-sig"),
                            file_name="timeseries_summary_all.csv",
                                mime="text/csv"
                            )

                    def _render_outlier_alert(results_per_account: dict, *, topn: int = 10, z_thr: float = 2.0):
                        """
                        results_per_account: {ê³„ì •ëª… -> DataFrame}, DataFrameì€ ìµœì†Œ ì»¬ëŸ¼
                          ['date','actual','predicted','error','z','risk','model','measure'] ê°€ì •
                          measure âˆˆ {'flow','balance'}
                        """
                        import pandas as pd
                        import numpy as np
                        rows = []
                        for acc_name, df in (results_per_account or {}).items():
                            if df is None or df.empty or "z" not in df.columns:
                                continue
                            dfx = df.copy()
                            # ê³„ì •ëª… ë³´ê°• (í˜¹ì‹œ ëˆ„ë½ ëŒ€ë¹„)
                            if "ê³„ì •" not in dfx.columns:
                                dfx["ê³„ì •"] = acc_name
                            # ê¸°ì¤€ ë¼ë²¨(ë°œìƒì•¡/ì”ì•¡)
                            dfx["ê¸°ì¤€"] = dfx.get("measure", "").map(
                                {"flow": "ë°œìƒì•¡(Flow)", "balance": "ì”ì•¡(Balance)"}
                            ).fillna("ë°œìƒì•¡(Flow)")
                            # |z| í•„í„°
                            dfx = dfx[dfx["z"].abs() >= float(z_thr)]
                            if not dfx.empty:
                                rows.append(dfx)

                        # í—¤ë” + í…Œì´ë¸”
                        import streamlit as st
                        st.subheader(f"ì´ìƒì›” ì•Œë¦¼ (ìƒìœ„ {topn}ê±´, ê¸°ì¤€ |z| â‰¥ {z_thr:.1f})")

                        if not rows:
                            st.info(f"ì´ìƒì›” ì—†ìŒ(ê¸°ì¤€ |z| â‰¥ {z_thr:.1f})")
                            return

                        out = pd.concat(rows, ignore_index=True)

                        # ì •ë ¬: |z| ë‚´ë¦¼ì°¨ìˆœ â†’ |ì”ì°¨| ë³´ì¡°
                        out = out.sort_values(
                            by=["z", "error"],
                            key=lambda s: s.abs() if s.name in ("z", "error") else s,
                            ascending=[False, False]
                        ).head(int(topn))

                        # í‘œì‹œ ì»¬ëŸ¼/í•œê¸€ëª… + z ë¼ë²¨ ë³€ê²½
                        rename = {"date": "ì¼ì", "actual": "ì‹¤ì¸¡", "predicted": "ì˜ˆì¸¡",
                                  "error": "ì”ì°¨", "z": "z(ì‹œê³„ì—´)", "risk": "ìœ„í—˜ë„", "model": "ëª¨ë¸"}
                        for k, v in rename.items():
                            if k in out.columns:
                                out.rename(columns={k: v}, inplace=True)

                        show_cols = [c for c in ["ê³„ì •", "ì¼ì", "ì‹¤ì¸¡", "ì˜ˆì¸¡", "ì”ì°¨", "z(ì‹œê³„ì—´)", "ìœ„í—˜ë„", "ëª¨ë¸", "ê¸°ì¤€"] if c in out.columns]
                        out = out[show_cols]

                        fmt = {"ì‹¤ì¸¡":"{:,.0f}","ì˜ˆì¸¡":"{:,.0f}","ì”ì°¨":"{:,.0f}","ìœ„í—˜ë„":"{:.2f}","z(ì‹œê³„ì—´)":"{:.2f}"}

                        try:
                            st.dataframe(
                                out.style.format(fmt),
                                use_container_width=True,
                                hide_index=True,
                                height=_auto_height(out)
                            )
                        except TypeError:
                            # Streamlit êµ¬ë²„ì „ í˜¸í™˜
                            st.dataframe(
                                out.reset_index(drop=True).style.format(fmt),
                                use_container_width=True,
                                height=_auto_height(out)
                            )

                    # === NEW: ì„ íƒê³„ì • í†µê³„ ë° ì´ìƒì›” ë¦¬ìŠ¤íŠ¸ ë Œë”ë§ í•¨ìˆ˜ ì •ì˜ ===
                    def _safe_div(a, b):
                        try:
                            b = np.where(np.abs(b) < 1e-9, 1.0, b)
                            return a / b
                        except Exception:
                            return np.nan

                    _render_table_combined(gathered_flow, gathered_balance, title="ì„ íƒê³„ì • ìš”ì•½ (Flow+Balance)")

                    # í†µí•© ì´ìƒì›” ì•Œë¦¼ (|z| â‰¥ 2.0 ê³ ì •)
                    _render_outlier_alert(results_per_account, topn=10, z_thr=2.0)

                    # ============ ğŸ” ì‹œê³„ì—´ íŒŒì´í”„ë¼ì¸ ì§„ë‹¨(í˜„í™©íŒ) ============ #
                    with st.expander("ğŸ” ì‹œê³„ì—´ íŒŒì´í”„ë¼ì¸ ì§„ë‹¨(í˜„í™©íŒ)", expanded=False):
                        st.caption("ê° ë‹¨ê³„ë³„ë¡œ í¬ì¸íŠ¸ ìˆ˜/íƒ€ì…/ì •ê·œí™” ìƒíƒœë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤. ê·¸ë˜í”„ê°€ ì•ˆ ëœ¨ë©´ ì–´ë””ì„œ ëŠê²¼ëŠ”ì§€ ì—¬ê¸°ì„œ í™•ì¸í•˜ì„¸ìš”.")
                        # 0) ì›ë³¸ ìŠ¬ë¼ì´ìŠ¤ ìš”ì•½
                        st.markdown("**0) ì…ë ¥(ì›ì¥ ìŠ¬ë¼ì´ìŠ¤) ìš”ì•½**")
                        try:
                            st.write({
                                "ì„ íƒê³„ì • ìˆ˜": len(want_codes),
                                "ì›ì¥ í–‰ìˆ˜(ì„ íƒê³„ì •)": int(len(ldf)),
                                "date_col": date_col,
                                "amount_col": amount_col,
                                "date_dtype": str(ldf[date_col].dtype),
                                "amount_dtype": str(ldf[amount_col].dtype),
                                "NaT(ë‚ ì§œ)": int(pd.to_datetime(ldf[date_col], errors="coerce").isna().sum()),
                                "NaN(ê¸ˆì•¡)": int(pd.to_numeric(ldf[amount_col], errors="coerce").isna().sum()),
                                "ê¸°ê°„": f"{pd.to_datetime(ldf[date_col], errors='coerce').min()} ~ {pd.to_datetime(ldf[date_col], errors='coerce').max()}",
                            })
                            st.dataframe(
                                ldf[[date_col, "ê³„ì •ì½”ë“œ", "ê³„ì •ëª…", amount_col]].head(5),
                                use_container_width=True,
                                height=_auto_table_height(ldf.head(5))
                            )
                        except Exception as _e:
                            st.warning(f"ì…ë ¥ ìš”ì•½ ì‹¤íŒ¨: {_e}")

                        # 1) ê³„ì •Ã—ì›” ì§‘ê³„ í™•ì¸
                        st.markdown("**1) ì›”ë³„ ì§‘ê³„ ìƒíƒœ**")
                        try:
                            _tmp = ldf[[date_col, amount_col]].copy()
                            _tmp = _tmp.rename(columns={date_col: 'íšŒê³„ì¼ì', amount_col: 'ê±°ë˜ê¸ˆì•¡'})
                            _grp = aggregate_monthly(_tmp, date_col='íšŒê³„ì¼ì', amount_col='ê±°ë˜ê¸ˆì•¡').rename(columns={"amount":"flow"})
                            _grp["date"] = pd.to_datetime(_grp["date"], errors="coerce")
                            _norm_ok = int((_grp["date"].dt.hour.eq(0) & _grp["date"].dt.minute.eq(0)).sum())
                            st.write({
                                "ì§‘ê³„ í¬ì¸íŠ¸ ìˆ˜": int(len(_grp)),
                                "ì›”ë§ 00:00:00 ë¹„ìœ¨": f"{_norm_ok}/{len(_grp)}",
                                "ì˜ˆ: ì²« 3í–‰": None
                            })
                            st.dataframe(
                                _grp.head(3),
                                use_container_width=True,
                                height=_auto_table_height(_grp.head(3))
                            )
                            # (ë³´ë„ˆìŠ¤) ê²½ê³„ì„  ì˜ˆìƒ ê°œìˆ˜
                            try:
                                rng = pd.date_range(pd.to_datetime(ldf[date_col]).min(), pd.to_datetime(ldf[date_col]).max(), freq="M")
                                q_ends = [m for m in rng if m.month in (3,6,9,12)]
                                y_ends = [m for m in rng if m.month == 12]
                                st.write({"ê²½ê³„ì„ (ë¶„ê¸°ë§) ì˜ˆìƒ ê°œìˆ˜": len(q_ends), "ê²½ê³„ì„ (ì—°ë§) ì˜ˆìƒ ê°œìˆ˜": len(y_ends)})
                            except Exception as _ee:
                                st.info(f"ê²½ê³„ì„  ê°œìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {_ee}")
                        except Exception as _e:
                            st.warning(f"ì›”ë³„ ì§‘ê³„ ìƒíƒœ ê³„ì‚° ì‹¤íŒ¨: {_e}")

                        # 2) ëŸ¬ë„ˆ ê²°ê³¼ ìš”ì•½
                        st.markdown("**2) ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ìƒíƒœ(run_timeseries_minimal Â· EMA)**")
                        try:
                            if not (gathered_flow or gathered_balance):
                                st.warning("ëŸ¬ë„ˆ ì¶œë ¥(gathered_*)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìƒë‹¨ ì…ë ¥/ì§‘ê³„ ë‹¨ê³„ í™•ì¸ í•„ìš”.")
                            else:
                                parts = []
                                if gathered_flow: parts += gathered_flow
                                if gathered_balance: parts += gathered_balance
                                parts = [p for p in parts if isinstance(p, pd.DataFrame) and not p.empty]
                                if not parts:
                                    st.warning("ëŸ¬ë„ˆ ì¶œë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.(ìœ íš¨í•œ DataFrame ì—†ìŒ)")
                                else:
                                    _all = pd.concat(parts, ignore_index=True)
                                    st.write({
                                        "ê³„ì •Ã—ê¸°ì¤€(measure) ê°œìˆ˜": int(_all[["ê³„ì •","measure"]].drop_duplicates().shape[0]) if set(["ê³„ì •","measure"]).issubset(_all.columns) else 0,
                                        "actual ì¡´ì¬": bool("actual" in _all.columns),
                                        "predicted ì¡´ì¬": bool("predicted" in _all.columns),
                                        "flow í¬ì¸íŠ¸": int(_all[_all.get("measure","flow").eq("flow")].shape[0]) if "measure" in _all.columns else int(_all.shape[0]),
                                        "balance í¬ì¸íŠ¸": int(_all[_all.get("measure","flow").eq("balance")].shape[0]) if "measure" in _all.columns else 0,
                                    })
                                    st.dataframe(
                                        _all.head(5),
                                        use_container_width=True,
                                        height=_auto_table_height(_all.head(5))
                                    )
                        except Exception as _e:
                            st.warning(f"ëŸ¬ë„ˆ ì¶œë ¥ ìš”ì•½ ì‹¤íŒ¨: {type(_e).__name__}: {_e}")

                        # ë¶€í˜¸ ë³´ì • ê°€ë“œ í‘œì‹œ
                        st.markdown("**ë¶€í˜¸ ë³´ì • ê°€ë“œ**")
                        try:
                            pipeline_norm = any(c in ldf.columns for c in ["ë°œìƒì•¡_norm", "amount_norm", "__sign", "sign"])
                            plot_sign_flip = False  # í”Œë¡¯ ë ˆë²¨ ë°˜ì „ì€ í•˜ì§€ ì•ŠìŒ
                            st.write({
                                "pipeline_norm": bool(pipeline_norm),
                                "plot_sign_flip": bool(plot_sign_flip),
                                "guard_ok": bool(pipeline_norm and not plot_sign_flip)
                            })
                            if pipeline_norm and plot_sign_flip:
                                st.warning("ê²½ê³ : íŒŒì´í”„ë¼ì¸ê³¼ í”Œë¡¯ì—ì„œ ëª¨ë‘ ë¶€í˜¸ë¥¼ ë§Œì§€ë©´ ì´ì¤‘ ë°˜ì „ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
                        except Exception as _e:
                            st.info(f"ë¶€í˜¸ ë³´ì • ê°€ë“œ ì ê²€ ì‹¤íŒ¨: {_e}")

                        # 3) ê·¸ë¦¼ ì…ë ¥ ì „ ì ê²€(ê³„ì •ë³„)
                        st.markdown("**3) ê·¸ë¦¼ ì…ë ¥ ì‚¬ì „ ì ê²€(create_timeseries_figure ì§ì „)**")
                        try:
                            for acc_name, df_all in results_per_account.items():
                                for ms in (["flow","balance"] if df_all["measure"].eq("balance").any() else ["flow"]):
                                    dfx = df_all[df_all["measure"].eq(ms)]
                                    st.write(f"- {acc_name} / {ms}: N={len(dfx)} Â· ì»¬ëŸ¼={list(dfx.columns)} Â· ë‚ ì§œë²”ìœ„={pd.to_datetime(dfx['date']).min()}~{pd.to_datetime(dfx['date']).max()}")
                                    st.dataframe(
                                        dfx[["date","actual","predicted"]].head(3),
                                        use_container_width=True,
                                        height=_auto_table_height(dfx.head(3))
                                    )
                        except Exception as _e:
                            st.warning(f"ê·¸ë¦¼ ì…ë ¥ ì ê²€ ì‹¤íŒ¨: {_e}")

                    # === ì—°/ë¶„ê¸° ê²½ê³„ì„  ê¸°ë³¸ í‘œì‹œ ===
                    show_dividers = True

                    # === ê³„ì •Ã—ê¸°ì¤€ í†µê³„ ìš”ì•½ ===
                    def _safe_stats_block(df_in: pd.DataFrame) -> dict:
                        s = {}
                        try:
                            a = pd.to_numeric(df_in.get("actual"), errors="coerce")
                            p = pd.to_numeric(df_in.get("predicted"), errors="coerce")
                            e = a - p
                            s["N"] = int(len(df_in))
                            s["MAE"] = float(np.nanmean(np.abs(e)))
                            denom = a.replace(0, np.nan)
                            s["MAPE(%)"] = float(np.nanmean(np.abs(e / denom)) * 100.0)
                            s["RMSE"] = float(np.sqrt(np.nanmean((e**2))))
                            z = pd.to_numeric(df_in.get("z"), errors="coerce")
                            s["|z|_max"] = float(np.nanmax(np.abs(z))) if z is not None else np.nan
                            s["last_z"] = float(z.iloc[-1]) if (z is not None and len(z) > 0) else np.nan
                            s["last_err"] = float(e.iloc[-1]) if len(e) > 0 else np.nan
                            s["model"] = str(df_in.get("model").iloc[-1]) if "model" in df_in.columns and len(df_in) > 0 else ""
                        except Exception:
                            pass
                        return s

                    stats_rows = []
                    for acc_name, df_all in results_per_account.items():
                        for ms in (["flow","balance"] if df_all["measure"].eq("balance").any() else ["flow"]):
                            d = df_all[df_all["measure"].eq(ms)]
                            if d.empty:
                                continue
                            row = {"ê³„ì •": acc_name, "ê¸°ì¤€": ("ë°œìƒì•¡(Flow)" if ms=="flow" else "ì”ì•¡(Balance)")}
                            row.update(_safe_stats_block(d))
                            stats_rows.append(row)

                    if stats_rows:
                        stats_df = pd.DataFrame(stats_rows)[
                            ["ê³„ì •","ê¸°ì¤€","N","MAE","MAPE(%)","RMSE","|z|_max","last_z","last_err","model"]
                        ]
                        fmt = {"MAE":"{:,.0f}","MAPE(%)":"{:.2f}","RMSE":"{:,.0f}","|z|_max":"{:.2f}","last_z":"{:.2f}","last_err":"{:,.0f}"}
                        st.subheader("ê³„ì •ë³„ í†µê³„ ìš”ì•½")
                        st.dataframe(
                            stats_df.style.format(fmt),
                            use_container_width=True,
                            height=_auto_table_height(stats_df)
                        )

                    # === ê·¸ë˜í”„ ë Œë”(ì•„ë˜): ê³„ì •ë³„ë¡œ í‘œì‹œ ===
                    for acc_name, df_all in results_per_account.items():
                        for measure in (["flow","balance"] if (df_all["measure"].eq("balance").any()) else ["flow"]):
                            dfm = df_all[df_all["measure"]==measure].rename(columns={"account":"ê³„ì •"})
                            title = f"{acc_name} â€” {'ë°œìƒì•¡(Flow)' if measure=='flow' else 'ì”ì•¡(Balance)'}"

                            # í‘œë³¸ìˆ˜ ê²Œì´íŠ¸: N<6ì´ë©´ ë¯¸ë˜ ìŒì˜ ë¹„í™œì„±í™”
                            stats = compute_series_stats(dfm)
                            _hz = int(forecast_horizon or 0)
                            if stats["N"] < 6:
                                _hz = 0

                            fig, stats_d = create_timeseries_figure(
                                dfm, measure=measure, title=title,
                                pm_value=PM, show_dividers=True   # â† ë¶„ê¸°/ì—° ê²½ê³„ì„  ì¼œê¸°
                            )

                            # ë¯¸ë˜ êµ¬ê°„ ìŒì˜(ì‹œê°í™” ì „ìš©)
                            if _hz > 0 and not dfm.empty:
                                last_date = pd.to_datetime(dfm["date"]).max()
                                fig = add_future_shading(fig, last_date, horizon_months=_hz)

                            st.subheader(title)
                            if fig is not None:
                                st.plotly_chart(fig, use_container_width=True)

                            # MoR ë¡œê·¸ í‘œì‹œ
                            log = (results_per_account.get(acc_name, pd.DataFrame())).attrs.get("mor_log", {})
                            if stats_d or log:
                                st.caption(
                                    f"ëª¨í˜•:{dfm['model'].iloc[-1] if not dfm.empty else '-'} Â· "
                                    f"ì„ ì •ê·¼ê±°:{log.get('metric','-')} "
                                    f"(MAPE={log.get('mape_best',''):g}%, MAE={log.get('mae_best',''):,.0f}) Â· "
                                    f"í‘œë³¸ì›”:{log.get('n_months','-')}"
                                    + ("" if _hz == forecast_horizon else " Â· (í‘œë³¸ ë¶€ì¡±ìœ¼ë¡œ ë¯¸ë˜ìŒì˜ ë¹„í™œì„±)")
                                )
                # âš ï¸ ê¸°ì¡´ tab5(ìœ„í—˜í‰ê°€) ë¸”ë¡ ì „ì²´ ì‚­ì œë¨
                
                with tab_corr:
                    st.header("ìƒê´€ê´€ê³„")
                    upload_id = getattr(uploaded_file, 'name', '_default')
                    # í•œ íƒ­ ë‚´ ìˆœì°¨ ë Œë”(ì„œë¸Œíƒ­ ì‚¬ìš© ê¸ˆì§€)
                    st.subheader("ê¸°ë³¸ ìƒê´€ê´€ê³„")
                    _render_corr_basic_tab(upload_id=upload_id)
                    st.markdown("---")
                    st.subheader("ê³ ê¸‰ ìƒê´€ê´€ê³„")
                    _render_corr_advanced_tab(upload_id=upload_id)
                with tab_report:
                    st.header("ğŸ§  ë¶„ì„ ì¢…í•© ëŒ€ì‹œë³´ë“œ")
                    # --- Preview: modules session quick view ---
                    modules_list_preview = list(st.session_state.get('modules', {}).values())
                    with st.expander("ğŸ” ëª¨ë“ˆë³„ ìš”ì•½/ì¦ê±° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        if not modules_list_preview:
                            st.info("ëª¨ë“ˆ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ê° ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                        else:
                            for mr in modules_list_preview:
                                try:
                                    st.subheader(f"â€¢ {getattr(mr, 'name', 'module')}")
                                    if getattr(mr, 'summary', None):
                                        st.json(mr.summary)
                                    evs = list(getattr(mr, 'evidences', []))
                                    if evs:
                                        st.write("Evidence ìƒ˜í”Œ (ìƒìœ„ 3)")
                                        for ev in evs[:3]:
                                            try:
                                                st.write(f"- reason={ev.reason} | risk={float(ev.risk_score):.2f} | amount={float(ev.financial_impact):,.0f}")
                                            except Exception:
                                                st.write("- (í‘œì‹œ ì‹¤íŒ¨)")
                                    if getattr(mr, 'tables', None):
                                        try: st.caption(f"tables: {list(mr.tables.keys())}")
                                        except Exception: pass
                                    if getattr(mr, 'figures', None):
                                        try: st.caption(f"figures: {list(mr.figures.keys())}")
                                        except Exception: pass
                                except Exception:
                                    st.caption("(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨)")
                    # LLM í‚¤ ë¯¸ê°€ìš©ì´ì–´ë„ ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„± ê°€ëŠ¥
                    LLM_OK = False
                    try:
                        from services.llm import openai_available
                        LLM_OK = bool(openai_available())
                    except Exception:
                        LLM_OK = False
                    if not LLM_OK:
                        st.info("ğŸ”Œ OpenAI Key ì—†ìŒ: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ë¯¸ì‚¬ìš©)")
                    rendered_report = False

                    # === ëª¨ë¸/í† í°/ì»¨í…ìŠ¤íŠ¸ ì˜µì…˜ UI ===
                    colm1, colm2, colm3 = st.columns([1,1,1])
                    with colm1:
                        llm_model_choice = st.selectbox(
                            "LLM ëª¨ë¸", options=["gpt-5", "gpt-4o"], index=1,
                            help="gpt-5 ë¯¸ê°€ìš© ì‹œ ìë™ìœ¼ë¡œ gpt-4oë¡œ ëŒ€ì²´í•˜ì„¸ìš”(ì½”ë“œì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬)."
                        )
                    with colm2:
                        desired_tokens = st.number_input(
                            "ë³´ê³ ì„œ ìµœëŒ€ ì¶œë ¥ í† í°", min_value=512, max_value=32000, value=16000, step=512,
                            help="ì‹¤ì œ ì „ì†¡ê°’ì€ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ì™€ ì…ë ¥ í† í°ì„ ê³ ë ¤í•´ ì•ˆì „ í´ë¨í”„ë©ë‹ˆë‹¤."
                        )
                    with colm3:
                        ctx_topk = st.number_input("ì»¨í…ìŠ¤íŠ¸ Evidence Top-K(ëª¨ë“ˆë³„)", min_value=5, max_value=100, value=20, step=5)
                        st.caption("ìš”ì•½/ë„í‘œëŠ” ìµœì†Œí™”í•˜ê³  ì¦ê±°ëŠ” ìƒìœ„ Top-Kë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        st.session_state['ctx_topk'] = int(ctx_topk)

                    # ì„ íƒí•œ ëª¨ë¸/í† í°ì„ ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ í•˜ë‹¨ í˜¸ì¶œë¶€ì—ì„œ ì‹¤ì œ ì‚¬ìš©
                    st.session_state['llm_model'] = llm_model_choice
                    st.session_state['llm_max_tokens'] = int(desired_tokens)

                    # --- ì…ë ¥ ì˜ì—­ ---
                    mdf = st.session_state.master_df
                    ldf = st.session_state.ledger_df

                    # â‘  ê³„ì • ì„ íƒ(í•„ìˆ˜) â€” ìë™ ì¶”ì²œ ì œê±°
                    acct_names_all = sorted(mdf['ê³„ì •ëª…'].dropna().unique().tolist())
                    pick_accounts = st.multiselect(
                        "ë³´ê³ ì„œ ëŒ€ìƒ ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”. (ìµœì†Œ 1ê°œ)",
                        options=acct_names_all,
                        default=[],
                        key="report_accounts_pick"
                    )
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles_report = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ", list(cyc.CYCLE_KO.values()),
                            default=[], key="report_cycles_pick"
                        )
                        st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_report", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="report_cycles_pick",
                                              accounts_state_key="report_accounts_pick",
                                              master_df=st.session_state.master_df))
                    # â‘¡ ì˜µì…˜ ì œê±°: í•­ìƒ ìˆ˜í–‰ í”Œë˜ê·¸
                    opt_knn_evidence = True
                    opt_patterns = True
                    opt_patterns_py = True

                    # â‘¢ ì‚¬ìš©ì ë©”ëª¨(ì„ íƒ)
                    manual_ctx = st.text_area(
                        "ë³´ê³ ì„œì— ì¶”ê°€í•  ë©”ëª¨/ì£¼ì˜ì‚¬í•­(ì„ íƒ)",
                        placeholder="ì˜ˆ: 5~7ì›” ëŒ€í˜• ìº í˜ì¸ ì§‘í–‰ ì˜í–¥, 3ë¶„ê¸°ë¶€í„° ë‹¨ê°€ ì¸ìƒ ì˜ˆì • ë“±"
                    )

                    # â‘£ ì„ íƒ ê³„ì •ì½”ë“œ ë§¤í•‘
                    pick_codes = (
                        mdf[mdf['ê³„ì •ëª…'].isin(st.session_state['report_accounts_pick'])]['ê³„ì •ì½”ë“œ']
                        .astype(str).tolist()
                    )

                    colA, colB, colC = st.columns([1,1,1])
                    with colA: st.write("ì„ íƒ ê³„ì •ì½”ë“œ:", ", ".join(pick_codes) if pick_codes else "-")
                    with colB: st.write("ê¸°ì¤€ ì—°ë„(CY):", int(ldf['ì—°ë„'].max()))
                    with colC: st.write("ë³´ê³ ì„œ ê¸°ì¤€:", "Current Year GL")

                    # ë²„íŠ¼ì€ ê³„ì • ë¯¸ì„ íƒ ì‹œ ë¹„í™œì„±í™”
                    btn = st.button("ğŸ“ ë³´ê³ ì„œ ìƒì„±", type="primary", disabled=(len(pick_codes) == 0))
                    if len(pick_codes) == 0:
                        st.info("ê³„ì • 1ê°œ ì´ìƒ ì„ íƒ ì‹œ ë²„íŠ¼ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

                    if btn:
                        import time
                        from analysis.anomaly import compute_amount_columns
                        from analysis.embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
                        from analysis.report import build_report_context, run_final_analysis, build_methodology_note, run_offline_fallback_report
                        from services.llm import LLMClient
                        from analysis.anomaly import ensure_zscore

                        t0 = time.perf_counter()
                        with st.status("ë³´ê³ ì„œ ì¤€ë¹„ ì¤‘...", expanded=True) as s:
                            # Step 1) ë°ì´í„° ìŠ¬ë¼ì´ì‹±
                            s.write("â‘  ìŠ¤ì½”í”„ ì ìš© ë° ë°ì´í„° ìŠ¬ë¼ì´ì‹±(CY/PY)â€¦")
                            cur_year = ldf['ì—°ë„'].max()
                            df_cy = ldf[(ldf['period_tag'] == 'CY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            df_py = ldf[(ldf['period_tag'] == 'PY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            s.write(f"    â”” CY {len(df_cy):,}ê±´ / PY {len(df_py):,}ê±´")

                            # Step 2) í•„ìˆ˜ íŒŒìƒ(ë°œìƒì•¡/ìˆœì•¡)
                            s.write("â‘¡ ê¸ˆì•¡ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±(ë°œìƒì•¡/ìˆœì•¡)â€¦")
                            df_cy = compute_amount_columns(df_cy)
                            df_py = compute_amount_columns(df_py)

                            # Step 3) (ì„ íƒ) íŒ¨í„´ìš”ì•½: ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ (LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œì—ë§Œ)
                            cl_ok = False
                            if LLM_OK and opt_patterns and not df_cy.empty:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰(ì„ íƒ)â€¦")
                                # ì…ë ¥ í…ìŠ¤íŠ¸ í’ë¶€í™” + ì„ë² ë”© + HDBSCAN (ìµœëŒ€ N ì œí•œìœ¼ë¡œ ì•ˆì „ê°€ë“œ)
                                df_cy_small = df_cy.copy()
                                max_rows = 8000
                                if len(df_cy_small) > max_rows:
                                    df_cy_small = df_cy_small.sample(max_rows, random_state=42)
                                    s.write(f"    â”” ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                df_cy_small = ensure_rich_embedding_text(df_cy_small)
                                try:
                                    llm_service = LLMClient(model=st.session_state.get('llm_model', 'gpt-4o'))
                                    emb_client = llm_service.client  # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´
                                    naming_function = llm_service.name_cluster
                                    # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë„¤ì´ë°ì„ í•„ìˆ˜ë¡œ ìš”êµ¬
                                    df_clu, ok = perform_embedding_and_clustering(
                                        df_cy_small, emb_client,
                                        name_with_llm=True, must_name_with_llm=True,
                                        use_large=bool(st.session_state.get("use_large_embedding", False)),
                                        rescue_tau=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
                                        embed_texts_fn=get_or_embed_texts,
                                        naming_fn=naming_function,
                                    )
                                    if ok:
                                        # ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì´ë¦„ì„ LLMìœ¼ë¡œ í†µí•©
                                        df_clu, name_map = unify_cluster_names_with_llm(
                                            df_clu,
                                            sim_threshold=0.90,
                                            emb_model=st.session_state.get('embedding_model', None) or EMB_MODEL_SMALL,
                                            embed_texts_fn=get_or_embed_texts,
                                            confirm_pair_fn=make_synonym_confirm_fn(emb_client, st.session_state.get('llm_model', 'gpt-4o')),
                                        )
                                        # ì¶”ê°€ LLM ë¼ë²¨ í†µí•©(JSON ë§¤í•‘ ë°©ì‹) â€” CYì˜ cluster_groupì€ ìœ ì§€
                                        try:
                                            raw_map = unify_cluster_labels_llm(df_clu['cluster_name'].dropna().unique().tolist(), emb_client)
                                            if raw_map:
                                                df_clu['cluster_name'] = df_clu['cluster_name'].map(lambda x: raw_map.get(str(x), x))
                                                # â— cluster_groupëŠ” unify_cluster_names_with_llm()ì´ ì •í•œ canonicalì„ ìœ ì§€
                                        except Exception:
                                            pass
                                        # ê°„ë‹¨ ìš”ì•½(ìƒìœ„ 5ê°œ)
                                        topc = (df_clu.groupby('cluster_group')['ë°œìƒì•¡']
                                                .agg(['count','sum']).sort_values('sum', ascending=False).head(5))
                                        s.write("    â”” í´ëŸ¬ìŠ¤í„° ìƒìœ„ 5ê°œ ìš”ì•½:")
                                        st.dataframe(
                                            topc.rename(columns={'count':'ê±´ìˆ˜','sum':'ë°œìƒì•¡í•©ê³„'})
                                                .style.format({'ë°œìƒì•¡í•©ê³„':'{:,.0f}'}),
                                            use_container_width=True
                                        )
                                        # í’ˆì§ˆ ì§€í‘œ(ë…¸ì´ì¦ˆìœ¨Â·í´ëŸ¬ìŠ¤í„° ìˆ˜ ë“±) ê¸°ë¡
                                        try:
                                            n = int(len(df_clu))
                                            noise_rate = float((df_clu['cluster_id'] == -1).mean()) if n else 0.0
                                            n_clusters = int(df_clu.loc[df_clu['cluster_id'] != -1, 'cluster_id'].nunique())
                                            if n_clusters > 0:
                                                avg_size = float(df_clu[df_clu['cluster_id'] != -1].groupby('cluster_id').size().mean())
                                            else:
                                                avg_size = 0.0
                                            rescue_rate = float(df_clu.get('rescued', False).mean()) if 'rescued' in df_clu.columns else 0.0
                                            model_used = df_clu.attrs.get('embedding_model', 'unknown')
                                            umap_on = bool(df_clu.attrs.get('umap_used', False))
                                            s.write(
                                                f"    â”” Quality: N={n:,}, noise={noise_rate*100:.1f}%, "
                                                f"clusters={n_clusters}, avg_size={avg_size:.1f}, rescued={rescue_rate*100:.1f}%"
                                            )
                                            s.write(
                                                f"    â”” Model/UMAP: {model_used} | UMAP={'on' if umap_on else 'off'} | Ï„={float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)):.2f}"
                                            )
                                            # ëŒ€ì‹œë³´ë“œ ì¹´ë“œìš© í’ˆì§ˆ ì§€í‘œ ì €ì¥
                                            st.session_state['cluster_quality'] = {
                                                "N": n,
                                                "noise_rate": noise_rate,
                                                "n_clusters": n_clusters,
                                                "avg_size": avg_size,
                                                "rescued_rate": rescue_rate,
                                                "model": model_used,
                                                "umap": umap_on,
                                                "tau": float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)),
                                            }
                                        except Exception:
                                            pass
                                        # ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸ì— ë°˜ì˜: group/label ë™ì‹œ ë¶€ì°©
                                        df_cy = df_cy.merge(
                                            df_clu[['row_id','cluster_id','cluster_name','cluster_group']],
                                            on='row_id', how='left'
                                        )
                                        # í•„ìš” ì‹œ vectorë„ í•¨ê»˜ ë³‘í•© ê°€ëŠ¥:
                                        # df_cy = df_cy.merge(df_clu[['row_id','vector']], on='row_id', how='left')
                                        # (í˜„ì¬ëŠ” perform_embedding_only ë‹¨ê³„ì—ì„œ CY/PY dfì— vectorê°€ ì§ì ‘ ë¶€ì—¬ë¨)
                                        # --- PY clustering and alignment (optional) ---
                                        if opt_patterns_py and not df_py.empty:
                                            try:
                                                from analysis.embedding import cluster_year, align_yearly_clusters, unify_cluster_labels_llm
                                                # sampling guard similar to CY
                                                df_py_small = df_py.copy()
                                                max_rows = 8000
                                                if len(df_py_small) > max_rows:
                                                    df_py_small = df_py_small.sample(max_rows, random_state=42)
                                                    s.write(f"    â”” PY ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                                df_py_clu = cluster_year(
                                                    df_py_small, emb_client, embed_texts_fn=get_or_embed_texts
                                                )
                                                # ê°€ëŠ¥í•œ ê²½ìš° row_id ê¸°ì¤€ìœ¼ë¡œ PY ê²°ê³¼ ì»¬ëŸ¼ì„ df_pyì— ë³‘í•©
                                                if not df_py_clu.empty and 'row_id' in df_py.columns:
                                                    df_py = df_py.merge(df_py_clu, on='row_id', how='left', suffixes=("", "_pyclu"))
                                                # ì •ë ¬: PY í´ëŸ¬ìŠ¤í„°ë¥¼ CY í´ëŸ¬ìŠ¤í„°ì— ë§¤í•‘
                                                if 'cluster_id' in df_py_clu.columns:
                                                    mapping = align_yearly_clusters(df_clu, df_py_clu, sim_threshold=0.70)
                                                    # cluster_id â†’ (aligned_cy_cluster, aligned_sim)
                                                    cy_id_to_name = df_clu.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()
                                                    def _get_pair(cid):
                                                        try:
                                                            if pd.isna(cid):
                                                                return (np.nan, np.nan)
                                                            cid_int = int(cid)
                                                            return mapping.get(cid_int, (np.nan, np.nan))
                                                        except Exception:
                                                            return (np.nan, np.nan)
                                                    if 'cluster_id' in df_py.columns:
                                                        pairs = df_py['cluster_id'].map(_get_pair)
                                                        df_py[['aligned_cy_cluster', 'aligned_sim']] = pd.DataFrame(pairs.tolist(), index=df_py.index)
                                                        # ì´ë¦„ì€ CYì˜ ì´ë¦„ìœ¼ë¡œ ì •ë ¬(ê°€ëŠ¥í•œ ê²½ìš°)
                                                        df_py['cluster_name'] = df_py['aligned_cy_cluster'].map(cy_id_to_name).fillna(df_py.get('cluster_name'))
                                                # ìµœì¢… ë¼ë²¨ ì •í•©: ì „ì²´ ì´ë¦„ ì§‘í•© ê¸°ì¤€ìœ¼ë¡œ í†µí•©; CYì˜ cluster_groupì€ ìœ ì§€, PYëŠ” canonicalë¡œ ì •ë ¬
                                                try:
                                                    all_names = pd.Series([], dtype=object)
                                                    if 'cluster_name' in df_cy.columns:
                                                        all_names = pd.concat([all_names, df_cy['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    if 'cluster_name' in df_py.columns:
                                                        all_names = pd.concat([all_names, df_py['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    all_names = all_names.dropna().unique().tolist()
                                                    canon = unify_cluster_labels_llm(all_names, emb_client)
                                                    if canon:
                                                        if 'cluster_name' in df_cy.columns:
                                                            df_cy['cluster_name'] = df_cy['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_name' in df_py.columns:
                                                            df_py['cluster_name'] = df_py['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_group' in df_py.columns:
                                                            df_py['cluster_group'] = df_py['cluster_name']
                                                except Exception:
                                                    pass
                                            except Exception as e:
                                                s.write(f"    â”” PY í´ëŸ¬ìŠ¤í„°ë§/ì •ë ¬ ìŠ¤í‚µ: {e}")
                                        # ì»¨í…ìŠ¤íŠ¸ì— ë³„ë„ ë…¸íŠ¸ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                                        cl_ok = True
                                    else:
                                        s.write("    â”” LLM í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ â†’ ë³´ê³ ì„œ ìƒì„± ìš”ê±´ ë¯¸ì¶©ì¡±")
                                except Exception as e:
                                    s.write(f"    â”” ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
                            else:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§: LLM ë¯¸ê°€ìš© ë˜ëŠ” ì˜µì…˜ ë¹„í™œì„± â†’ ìŠ¤í‚µ")

                            # Step 3-1) (ì˜µì…˜ A) ê·¼ê±° ì¸ìš©(KNN)ìš© ì„ë² ë”©ë§Œ ìˆ˜í–‰ (LLM ê°€ëŠ¥ ì‹œ)
                            if LLM_OK and opt_knn_evidence:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš©ìš© ì„ë² ë”©(CY/PY)â€¦")
                                from analysis.embedding import perform_embedding_only, ensure_rich_embedding_text
                                emb_client2 = LLMClient().client
                                df_cy = ensure_rich_embedding_text(df_cy)
                                df_py = ensure_rich_embedding_text(df_py)
                                df_cy = perform_embedding_only(
                                    df_cy, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                                df_py = perform_embedding_only(
                                    df_py, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                            elif not LLM_OK:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš© ì„ë² ë”©: LLM ë¯¸ê°€ìš© â†’ ìŠ¤í‚µ")

                            # Step 3-2) Z-Score: ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ í•¨
                            s.write("â‘¢-2 Z-Score ê³„ì‚°/ê²€ì¦â€¦")
                            df_cy, z_ok = ensure_zscore(df_cy, pick_codes)
                            df_py, _    = ensure_zscore(df_py, pick_codes)  # ì „ê¸°ì—ë„ Z-Score ê³„ì‚°(ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©)
                            if not z_ok:
                                s.write("    â”” Z-Score ë¯¸ê³„ì‚° ë˜ëŠ” ì „ë¶€ ê²°ì¸¡")

                            # âœ… ê²Œì´íŠ¸ ì™„í™”: Z-Scoreë§Œ í™•ë³´ë˜ë©´ ë³´ê³ ì„œ ì§„í–‰.
                            #    (í´ëŸ¬ìŠ¤í„° ì‹¤íŒ¨ ì‹œ ê´€ë ¨ ì„¹ì…˜ì€ ìë™ ì¶•ì•½/ìƒëµ)
                            if not z_ok:
                                st.error("ë³´ê³ ì„œ ìƒì„± ì¤‘ë‹¨: Z-Score ì—†ìŒ.")
                                s.update(label="ë³´ê³ ì„œ ìš”ê±´ ë¯¸ì¶©ì¡±", state="error")
                                st.stop()
                            if not cl_ok:
                                s.write("    â”” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì—†ìŒ â†’ ë¦¬í¬íŠ¸ì—ì„œ í´ëŸ¬ìŠ¤í„° ì„¹ì…˜ì€ ìƒëµ/ì¶•ì•½ë©ë‹ˆë‹¤.")

                            # Step 4) ì»¨í…ìŠ¤íŠ¸ ìƒì„±(ì „ ëª¨ë“ˆ í¬í•¨) + ë°©ë²•ë¡  ë…¸íŠ¸
                            s.write("â‘£ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±(ì „ ëª¨ë“ˆ)â€¦")
                            from analysis.report_adapter import wrap_dfs_as_module_result
                            from analysis.report import generate_rag_context_from_modules
                            from analysis.integrity import run_integrity_module
                            from analysis.timeseries import run_timeseries_module

                            # (1) ì„¸ì…˜ ì´ˆê¸°í™” ë° ê³µí†µ ê°’ ì¤€ë¹„
                            st.session_state['modules'] = {}
                            lf_use = _lf_by_scope()
                            pm_use = float(st.session_state.get('pm_value', PM_DEFAULT))

                            # (2) ì£¼ìš” ëª¨ë“ˆ ì‹¤í–‰ ë° ìˆ˜ì§‘
                            if lf_use is not None:
                                # ì´ìƒì¹˜
                                try:
                                    amod = run_anomaly_module(lf_use, target_accounts=pick_codes or None,
                                                              topn=int(st.session_state.get('ctx_topk', 20)), pm_value=pm_use)
                                    _push_module(amod)
                                except Exception as _e:
                                    st.warning(f"anomaly ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ì¶”ì„¸(ì„ íƒ ê³„ì • í•„ìš”)
                                try:
                                    if pick_codes:
                                        _push_module(run_trend_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"trend ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ê±°ë˜ì²˜
                                try:
                                    if pick_codes:
                                        _push_module(run_vendor_module(lf_use, account_codes=pick_codes,
                                                                       min_amount=0.0, include_others=True))
                                except Exception as _e:
                                    st.warning(f"vendor ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ìƒê´€(2ê°œ ì´ìƒì¼ ë•Œë§Œ)
                                try:
                                    if len(pick_codes) >= 2:
                                        _push_module(run_correlation_module(lf_use, accounts=pick_codes,
                                                                            corr_threshold=0.70,
                                                                            cycles_map=cyc.get_effective_cycles()))
                                except Exception as _e:
                                    st.warning(f"correlation ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ì •í•©ì„±(ModuleResult) â€” ì„ íƒ ê³„ì • í•„í„° ì ìš©
                                try:
                                    _push_module(run_integrity_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"integrity ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # NEW: ì‹œê³„ì—´ í¬í•¨(ì§‘ê³„â†’DTO ë˜í•‘)
                                try:
                                    if not df_cy.empty:
                                        ts = pd.concat([df_cy, df_py], ignore_index=True)
                                        ts["date"] = month_end_00(ts["íšŒê³„ì¼ì"])  # ì›”ë§ 00:00:00 ì •ê·œí™”
                                        ts["account"] = ts["ê³„ì •ì½”ë“œ"].astype(str)
                                        ts["amount"] = ts.get("ë°œìƒì•¡", 0.0).astype(float)
                                        ts_in = ts.groupby(["account","date"], as_index=False)["amount"].sum()
                                        df_ts = run_timeseries_module(ts_in, account_col="account", date_col="date", amount_col="amount",
                                                                      pm_value=pm_use, output="flow", make_balance=False)
                                        summ_ts = {
                                            "n_series": int(df_ts["account"].nunique()) if not df_ts.empty else 0,
                                            "n_points": int(len(df_ts)),
                                            "max_abs_z": float(df_ts["z"].abs().max()) if ("z" in df_ts.columns and not df_ts.empty) else 0.0,
                                        }
                                        _push_module(ModuleResult(name="timeseries", summary=summ_ts,
                                                                  tables={"ts": df_ts}, figures={}, evidences=[], warnings=([] if not df_ts.empty else ["insufficient_points"])))
                                except Exception as _e:
                                    st.warning(f"timeseries ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")

                            # (3) ë ˆê±°ì‹œ DFë„ ì–´ëŒ‘í„°ë¡œ í•¨ê»˜ í¬í•¨(ê²½ëŸ‰ ì»¨í…ìŠ¤íŠ¸ìš©)
                            mr_ctx = wrap_dfs_as_module_result(df_cy, df_py, name="report_ctx")
                            modules_list = list(st.session_state.get('modules', {}).values()) + [mr_ctx]
                            # (4) ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„±(Top-K ì ìš©) â€” ì‹ ê·œ ê²½ë¡œë§Œ ì‚¬ìš© + ë©”ëª¨ ì£¼ì…
                            ctx = generate_rag_context_from_modules(
                                modules_list,
                                pm_value=pm_use,
                                topk=int(st.session_state.get('ctx_topk', 20)),
                                manual_note=(manual_ctx or "")
                            )

                            # (ìƒë‹¨ ê³µí†µ ë¯¸ë¦¬ë³´ê¸°ë¡œ ëŒ€ì²´)
                            note = build_methodology_note(report_accounts=pick_codes)

                            # Step 5) LLM í˜¸ì¶œ ì „ ì ê²€(ê¸¸ì´/í† í°)
                            s.write("â‘¤ LLM í”„ë¡¬í”„íŠ¸ ì ê²€â€¦")
                            prompt_len = len(ctx) + len(note)
                            s.write(f"    â”” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {prompt_len:,} chars")
                            try:
                                import tiktoken
                                enc = tiktoken.get_encoding("cl100k_base")
                                est_tokens = len(enc.encode(ctx)) + len(enc.encode(note))
                                s.write(f"    â”” ì˜ˆìƒ í† í° ìˆ˜: ~{est_tokens:,} tokens")
                            except Exception:
                                s.write("    â”” tiktoken ë¯¸ì„¤ì¹˜: í† í° ì¶”ì • ìƒëµ")

                            # Step 6) ë³´ê³ ì„œ ìƒì„±: LLM ê°€ëŠ¥í•˜ë©´ ì‹œë„, ì‹¤íŒ¨/ë¶ˆê°€ ì‹œ ì˜¤í”„ë¼ì¸ í´ë°±
                            final_report = None
                            if LLM_OK:
                                s.write("â‘¥ LLM ìš”ì•½ ìƒì„± í˜¸ì¶œâ€¦")
                                try:
                                    t_llm0 = time.perf_counter()
                                    llm_client = LLMClient(model=st.session_state.get('llm_model'))
                                    final_report = run_final_analysis(
                                        context=ctx + "\n" + note,
                                        account_codes=pick_codes,
                                        model=st.session_state.get('llm_model'),
                                        max_tokens=int(st.session_state.get('llm_max_tokens', 16000)),
                                        generate_fn=llm_client.generate,
                                    )
                                    s.write(f"    â”” LLM ì™„ë£Œ (ê²½ê³¼ {time.perf_counter()-t_llm0:.1f}s)")
                                except Exception as e:
                                    s.write(f"    â”” LLM ì‹¤íŒ¨: {e} â†’ ì˜¤í”„ë¼ì¸ í´ë°±ìœ¼ë¡œ ì „í™˜")

                            if final_report is None:
                                s.write("â‘¥-í´ë°±: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±â€¦")
                                final_report = run_offline_fallback_report(
                                    current_df=df_cy,
                                    previous_df=df_py,
                                    account_codes=pick_codes,
                                    pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                                )

                            s.update(label="ë³´ê³ ì„œ ì¤€ë¹„ ì™„ë£Œ", state="complete")

                            # ê²°ê³¼ ì¶œë ¥ ë° ì„¸ì…˜ ë³´ì¡´
                            st.session_state['last_report'] = final_report
                            st.session_state['last_context'] = ctx + "\n" + note
                            st.session_state['last_dfcy'] = df_cy
                            st.session_state['last_dfpy'] = df_py

                            st.success("ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                            st.markdown(final_report)

                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])

                        # ZIP ë‹¨ì¼ ë‹¤ìš´ë¡œë“œ + RAW ë¯¸ë¦¬ë³´ê¸°
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()

                        raw_df = _build_raw_evidence(st.session_state['last_dfcy'])
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")

                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_current"  # ê³ ìœ  í‚¤(í˜„ì¬ ê²°ê³¼)
                        )

                        st.caption(f"â± ì´ ì†Œìš”: {time.perf_counter()-t0:.1f}s")
                        rendered_report = True

                    # === ìºì‹œëœ ì´ì „ ê²°ê³¼ ë Œë”(ë²„íŠ¼ ë¯¸í´ë¦­ ì‹œì—ë§Œ) ===
                    if st.session_state.get('last_report') and not btn:
                        st.success("ë³´ê³ ì„œê°€ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                        st.markdown(st.session_state['last_report'])
                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])
                        # RAW ë¯¸ë¦¬ë³´ê¸° + ZIP ë²„íŠ¼ ì¬ì¶œë ¥
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()
                        raw_df = _build_raw_evidence(st.session_state.get('last_dfcy', pd.DataFrame()))
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")
                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_cached"  # ê³ ìœ  í‚¤(ìºì‹œ ê²°ê³¼)
                        )
                        # (ê°€ëŠ¥ ì‹œ) í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì¹´ë“œ í‘œì‹œ
                        cq = st.session_state.get("cluster_quality")
                        if cq:
                            st.markdown("---")
                            st.subheader("í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ìš”ì•½")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("Noise rate", f"{cq['noise_rate']*100:.1f}%")
                            c2.metric("#Clusters", f"{cq['n_clusters']}")
                            c3.metric("Avg size", f"{cq['avg_size']:.1f}")
                            c4.metric("Rescued", f"{cq['rescued_rate']*100:.1f}%")
                            st.caption(f"Model: {cq['model']} | UMAP: {'on' if cq['umap'] else 'off'} | Ï„={cq['tau']:.2f} | N={cq['N']:,}")
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            if st.button("ë§¤í•‘ ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸°"):
                st.session_state.mapping_confirmed = False
                st.rerun()
else:
    st.info("â¬…ï¸ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")


