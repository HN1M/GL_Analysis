# app_v0.17.py (ê±°ë˜ì²˜ ìƒì„¸ ë¶„ì„ ì˜¤ë¥˜ ìˆ˜ì •)
# --- BEGIN: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---
try:
    from infra.env_loader import boot as _llm_boot
    _llm_boot()  # í‚¤ ë¡œë“œ + ìƒíƒœ ë¡œê·¸
except Exception as _e:
    # ìµœì•…ì˜ ê²½ìš°ì—ë„ ì•±ì€ ëœ¨ê²Œ í•˜ê³ , ìƒíƒœë¥¼ stderrë¡œë§Œ ì•Œë¦¼
    import sys
    print(f"[env_loader] ì´ˆê¸°í™” ì‹¤íŒ¨: {_e}", file=sys.stderr)
# --- END: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
from utils.helpers import find_column_by_keyword, add_provenance_columns, add_period_tag
from analysis.integrity import analyze_reconciliation, run_integrity_module
from analysis.contracts import LedgerFrame, ModuleResult
from analysis.trend import create_monthly_trend_figure, run_trend_module
from analysis.timeseries import (
    model_registry,
    run_timeseries_module_with_flag,
    create_timeseries_figure
)
from analysis.anomaly import run_anomaly_module, compute_amount_columns
from analysis.correlation import run_correlation_module
from analysis.vendor import (
    create_pareto_figure,
    create_vendor_heatmap,
    create_vendor_detail_figure,
    run_vendor_module,
)
from analysis.report import run_final_analysis, build_methodology_note
from analysis.embedding import (
    ensure_rich_embedding_text,
    perform_embedding_and_clustering,
    perform_embedding_only,
)
from analysis.anomaly import calculate_grouped_stats_and_zscore
from services.llm import LLMClient
from services.cache import get_or_embed_texts
from services.cycles_store import get_effective_cycles
from config import EMB_USE_LARGE_DEFAULT, HDBSCAN_RESCUE_TAU
try:
    from config import PM_DEFAULT
except Exception:
    PM_DEFAULT = 500_000_000
from utils.viz import add_materiality_threshold, add_pm_badge

# --- KRW ì…ë ¥(ì²œë‹¨ìœ„ ì½¤ë§ˆ) ìœ í‹¸: ì½œë°± ê¸°ë°˜ìœ¼ë¡œ ì•ˆì •í™” ---
def _krw_input(label: str, key: str, *, default_value: int, help_text: str = "") -> int:
    """
    í•œêµ­ ì›í™” ì…ë ¥ ìœ„ì ¯(ì²œë‹¨ìœ„ ì½¤ë§ˆ). í•µì‹¬ ê·œì¹™:
    1) ìœ„ì ¯ í‚¤(pm_value__txt ë“±)ë¥¼ ëŸ° ë£¨í”„ì—ì„œ ì§ì ‘ ëŒ€ì…í•˜ì§€ ì•ŠëŠ”ë‹¤.
    2) ì½¤ë§ˆ ì¬í¬ë§·ì€ on_change ì½œë°± ì•ˆì—ì„œë§Œ ìˆ˜í–‰í•œë‹¤.
    3) ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ê°’ì€ st.session_state[key]ì— ë³´ê´€í•œë‹¤.
    """
    txt_key = f"{key}__txt"  # ì‹¤ì œ text_input ìœ„ì ¯ì´ ë°”ì¸ë”©ë˜ëŠ” í‚¤

    # ì´ˆê¸° ì…‹ì—…: ìˆ«ì/ë¬¸ì ìƒíƒœë¥¼ ìœ„ì ¯ ìƒì„± ì „ì— ì¤€ë¹„
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if txt_key not in st.session_state:
        st.session_state[txt_key] = f"{int(st.session_state[key]):,}"

    # ì½œë°±: í¬ì»¤ìŠ¤ ì•„ì›ƒ/Enter ì‹œ ì½¤ë§ˆ í¬ë§·ì„ ì ìš©í•˜ê³  ìˆ«ì ìƒíƒœë¥¼ ë™ê¸°í™”
    def _on_blur_format():
        raw_now = st.session_state.get(txt_key, "")
        digits = re.sub(r"[^\d]", "", str(raw_now or ""))
        val = int(digits) if digits else 0
        if val < 0:
            val = 0
        st.session_state[key] = int(val)            # ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ìƒíƒœ
        st.session_state[txt_key] = f"{int(val):,}"  # ìœ„ì ¯ í‘œì‹œ í…ìŠ¤íŠ¸(ì½¤ë§ˆ)

    # ìœ„ì ¯ ìƒì„±
    raw = st.text_input(
        label,
        value=st.session_state[txt_key],
        key=txt_key,
        help=help_text,
        placeholder="ì˜ˆ: 500,000,000",
        on_change=_on_blur_format,
    )

    # ë¼ì´ë¸Œ íƒ€ì´í•‘ ë™ì•ˆì—ë„ ê·¸ë˜í”„ê°€ ì¦‰ì‹œ ë°˜ì˜ë˜ë„ë¡ ì •ìˆ˜ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸(ìœ„ì ¯ í‚¤ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    digits_live = re.sub(r"[^\d]", "", str(raw or ""))
    live_val = int(digits_live) if digits_live else 0
    if live_val < 0:
        live_val = 0
    st.session_state[key] = int(live_val)

    return int(st.session_state[key])


# --- 3. UI ë¶€ë¶„ ---
st.set_page_config(page_title="AI ë¶„ì„ ì‹œìŠ¤í…œ v0.18", layout="wide")
st.title("í›ˆ's GLë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("---")

for key in ['mapping_confirmed', 'analysis_done']:
    if key not in st.session_state:
        st.session_state[key] = False

# --- NEW: ëª¨ë“ˆ ê²°ê³¼ ìˆ˜ì§‘ìš© ì»¨í…Œì´ë„ˆ ---
if 'modules' not in st.session_state:
    st.session_state['modules'] = {}

def _push_module(mod: ModuleResult):
    """ModuleResultë¥¼ ì„¸ì…˜ì— ìˆ˜ì§‘(ë™ëª… ëª¨ë“ˆì€ ìµœì‹ ìœ¼ë¡œ êµì²´)."""
    try:
        if mod and getattr(mod, "name", None):
            st.session_state['modules'][str(mod.name)] = mod
    except Exception:
        pass


# (removed) number_input ê¸°ë°˜ ëŒ€ì²´ êµ¬í˜„: ì‰¼í‘œ ë¯¸í‘œì‹œÂ·í‚¤ ì¶©ëŒ ìœ ë°œ ê°€ëŠ¥ì„± â†’ ë‹¨ì¼ êµ¬í˜„ìœ¼ë¡œ í†µì¼

with st.sidebar:
    st.header("1. ë°ì´í„° ì¤€ë¹„")
    uploaded_file = st.file_uploader("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.", type=["xlsx", "xlsm"])
    if 'last_file' not in st.session_state or st.session_state.last_file != uploaded_file:
        st.session_state.mapping_confirmed = False
        st.session_state.analysis_done = False
        st.session_state.last_file = uploaded_file

    st.markdown("---")
    st.header("2. ë¶„ì„ ê¸°ê°„")
    default_scope = st.session_state.get("period_scope", "ë‹¹ê¸°")
    st.session_state.period_scope = st.radio(
        "ë¶„ì„ ìŠ¤ì½”í”„(íŠ¸ë Œë“œ ì œì™¸):",
        options=["ë‹¹ê¸°", "ë‹¹ê¸°+ì „ê¸°"],
        index=["ë‹¹ê¸°","ë‹¹ê¸°+ì „ê¸°"].index(default_scope),
        horizontal=True,
        help="ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ ëª¨ë“ˆì— ì ìš©ë©ë‹ˆë‹¤. íŠ¸ë Œë“œëŠ” ì„¤ê³„ìƒ CY vs PY ë¹„êµ ìœ ì§€."
    )
    st.markdown("---")
    st.header("3. Embedding / Clustering")
    st.session_state.use_large_embedding = st.toggle(
        "Use Large Embedding (cost â†‘)",
        value=st.session_state.get("use_large_embedding", EMB_USE_LARGE_DEFAULT),
        help="Large model improves semantics but is slower and more expensive."
    )
    st.session_state.rescue_tau = st.slider(
        "Noise rescue Ï„ (cosine)",
        min_value=0.60, max_value=0.90, step=0.01,
        value=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
        help="Reassign -1 (noise) to nearest cluster if similarity â‰¥ Ï„."
    )
    st.markdown("---")
    st.header("4. Materiality")
    pm_val = _krw_input(
        "Performance Materiality (KRW)",
        key="pm_value",
        default_value=PM_DEFAULT,
        help_text="Used for KIT (PM exceed) and integrated risk scoring."
    )
    st.caption("â“˜ The PM threshold is drawn as a red dotted line on applicable charts. "
               "Y-axis scaling may change to accommodate this line.")

    # ğŸ§¹ ìºì‹œ ê´€ë¦¬
    with st.expander("ğŸ§¹ ìºì‹œ ê´€ë¦¬", expanded=False):
        if st.button("ì„ë² ë”© ìºì‹œ ë¹„ìš°ê¸°"):
            import shutil
            from services.cache import _model_dir
            for m in ["text-embedding-3-small", "text-embedding-3-large"]:
                try:
                    shutil.rmtree(_model_dir(m), ignore_errors=True)
                except Exception as e:
                    st.warning(f"{m} ì‚­ì œ ì‹¤íŒ¨: {e}")
            st.success("ì„ë² ë”© ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ë°ì´í„° ìºì‹œ ë¹„ìš°ê¸°"):
            st.cache_data.clear()
            st.success("Streamlit ë°ì´í„° ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ìºì‹œ ì •ë³´ ë³´ê¸°"):
            from services.cache import get_cache_info
            try:
                st.write(get_cache_info("text-embedding-3-small"))
                st.write(get_cache_info("text-embedding-3-large"))
            except Exception as e:
                st.info(f"ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")


@st.cache_data(show_spinner=False)
def _read_excel(_file, sheet_name=None):
    return pd.read_excel(_file, sheet_name=sheet_name)


@st.cache_data(show_spinner=False)
def _read_xls(_file):
    # pickle ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ë§Œ ìºì‹œ â†’ ì‹œíŠ¸ëª… ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    return pd.ExcelFile(_file).sheet_names

# (removed duplicated definition) _krw_input â€” ìœ„ì˜ ë‹¨ì¼ ë²„ì „ë§Œ ìœ ì§€

def _apply_scope(df: pd.DataFrame, scope: str) -> pd.DataFrame:
    """ìŠ¤ì½”í”„ ì ìš© ì‹œ ê²°ì¸¡ ì»¬ëŸ¼ ë°©ì–´: 'period_tag' ë¯¸ì¡´ì¬ë©´ ì›ë³¸ ë°˜í™˜.
    df.get('period_tag','')ê°€ ë¬¸ìì—´ì„ ë°˜í™˜í•  ê²½ìš° .eq í˜¸ì¶œ AttributeErrorë¥¼ ë°©ì§€í•œë‹¤.
    """
    if df is None or df.empty or 'period_tag' not in df.columns:
        return df
    if scope == "ë‹¹ê¸°":
        return df[df['period_tag'].eq('CY')]
    if scope == "ë‹¹ê¸°+ì „ê¸°":
        return df[df['period_tag'].isin(['CY', 'PY'])]
    return df

def _lf_by_scope() -> LedgerFrame:
    """ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì½”í”„ ì ìš© LedgerFrame."""
    hist = st.session_state.get('lf_hist')
    scope = st.session_state.get('period_scope', 'ë‹¹ê¸°')
    if hist is None:
        return None
    return LedgerFrame(df=_apply_scope(hist.df, scope), meta=hist.meta)

# (removed) êµ¬ë²„ì „ í…ìŠ¤íŠ¸ì…ë ¥ + Â±step / âœ–reset ë³€í˜•ë“¤ â€” ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ë²„íŠ¼ë¥˜ ì‚­ì œ ë° ë‹¨ì¼í™”


if uploaded_file is not None:
    if not st.session_state.mapping_confirmed:
        # ... ì»¬ëŸ¼ ë§¤í•‘ UI ...
        try:
            st.info("2ë‹¨ê³„: ì—‘ì…€ì˜ ì»¬ëŸ¼ì„ ë¶„ì„ í‘œì¤€ í•„ë“œì— ë§ê²Œ ì§€ì •í•´ì£¼ì„¸ìš”.")
            sheet_names = _read_xls(uploaded_file)
            first_ledger_sheet = next((s for s in sheet_names if 'ledger' in s.lower()), None)
            if first_ledger_sheet is None:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            ledger_cols = _read_excel(uploaded_file, sheet_name=first_ledger_sheet).columns.tolist()
            ledger_map = {}
            st.markdown("#### **Ledger ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            cols = st.columns(6)
            ledger_fields = {'íšŒê³„ì¼ì': 'ì¼ì', 'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê±°ë˜ì²˜': 'ê±°ë˜ì²˜', 'ì ìš”': 'ì ìš”', 'ì°¨ë³€': 'ì°¨ë³€', 'ëŒ€ë³€': 'ëŒ€ë³€'}
            for i, (key, keyword) in enumerate(ledger_fields.items()):
                with cols[i]:
                    is_optional = key == 'ê±°ë˜ì²˜'
                    default_col = find_column_by_keyword(ledger_cols, keyword)
                    options = ['ì„ íƒ ì•ˆ í•¨'] + ledger_cols if is_optional else ledger_cols
                    default_index = options.index(default_col) if default_col in options else 0
                    ledger_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", options, index=default_index, key=f"map_ledger_{key}")
            st.markdown("---")
            st.markdown("#### **Master ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            master_cols = _read_excel(uploaded_file, sheet_name='Master').columns.tolist()
            master_map = {}
            cols = st.columns(7)
            master_fields = {'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…': 'ê³„ì •ëª…', 'BS/PL': 'BS/PL', 'ì°¨ë³€/ëŒ€ë³€': 'ì°¨ë³€/ëŒ€ë³€', 'ë‹¹ê¸°ë§ì”ì•¡': 'ë‹¹ê¸°ë§', 'ì „ê¸°ë§ì”ì•¡': 'ì „ê¸°ë§', 'ì „ì „ê¸°ë§ì”ì•¡': 'ì „ì „ê¸°ë§'}
            for i, (key, keyword) in enumerate(master_fields.items()):
                with cols[i]:
                    default_col = find_column_by_keyword(master_cols, keyword)
                    default_index = master_cols.index(default_col) if default_col in master_cols else 0
                    master_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", master_cols, index=default_index, key=f"map_master_{key}")
            if st.button("âœ… ë§¤í•‘ í™•ì¸ ë° ë°ì´í„° ì²˜ë¦¬", type="primary"):
                st.session_state.ledger_map = ledger_map
                st.session_state.master_map = master_map
                st.session_state.mapping_confirmed = True
                st.rerun()
        except Exception as e:
            st.error(f"ì—‘ì…€ íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    else:  # ë§¤í•‘ í™•ì¸ í›„
        try:
            ledger_map, master_map = st.session_state.ledger_map, st.session_state.master_map
            master_df = _read_excel(uploaded_file, sheet_name='Master')
            sheet_names = _read_xls(uploaded_file)
            ledger_sheets = [s for s in sheet_names if 'ledger' in s.lower()]
            if not ledger_sheets:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            all_parts = []
            for s in ledger_sheets:
                part = _read_excel(uploaded_file, sheet_name=s)
                part['source_sheet'] = s
                part = add_provenance_columns(part)
                all_parts.append(part)
            ledger_df = pd.concat(all_parts, ignore_index=True)
            # row_id: íŒŒì¼ëª…|ì‹œíŠ¸:í–‰  (ì„¸ì…˜/ì¬ì‹¤í–‰ì—ë„ ì•ˆì •)
            try:
                base = Path(getattr(uploaded_file, "name", "uploaded.xlsx")).stem
                if 'row_id' in ledger_df.columns:
                    ledger_df['row_id'] = base + "|" + ledger_df['row_id'].astype(str)
            except Exception:
                pass
            ledger_df.rename(columns={v: k for k, v in ledger_map.items() if v != 'ì„ íƒ ì•ˆ í•¨'}, inplace=True)
            master_df.rename(columns={v: k for k, v in master_map.items()}, inplace=True)

            # ğŸ”§ ë³‘í•© ì „ì— íƒ€ì…/í¬ë§·ì„ ë¨¼ì € í†µì¼
            for df_ in [ledger_df, master_df]:
                if 'ê³„ì •ì½”ë“œ' in df_.columns:
                    df_['ê³„ì •ì½”ë“œ'] = (
                        df_['ê³„ì •ì½”ë“œ']
                        .astype(str)
                        .str.replace(r'\.0$', '', regex=True)
                        .str.strip()
                    )

            master_essentials = master_df[['ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…']].drop_duplicates()
            ledger_df = pd.merge(ledger_df, master_essentials, on='ê³„ì •ì½”ë“œ', how='left')
            ledger_df['ê³„ì •ëª…'] = ledger_df['ê³„ì •ëª…'].fillna('ë¯¸ì§€ì • ê³„ì •')

            ledger_df['íšŒê³„ì¼ì'] = pd.to_datetime(ledger_df['íšŒê³„ì¼ì'], errors='coerce')
            ledger_df.dropna(subset=['íšŒê³„ì¼ì'], inplace=True)
            for col in ['ì°¨ë³€', 'ëŒ€ë³€']:
                ledger_df[col] = pd.to_numeric(ledger_df[col], errors='coerce').fillna(0)
            for col in ['ë‹¹ê¸°ë§ì”ì•¡', 'ì „ê¸°ë§ì”ì•¡', 'ì „ì „ê¸°ë§ì”ì•¡']:
                if col in master_df.columns:
                    master_df[col] = pd.to_numeric(master_df[col], errors='coerce').fillna(0)
                else:
                    master_df[col] = 0
            ledger_df['ê±°ë˜ê¸ˆì•¡'] = ledger_df['ì°¨ë³€'] - ledger_df['ëŒ€ë³€']
            ledger_df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] = abs(ledger_df['ê±°ë˜ê¸ˆì•¡'])
            ledger_df['ì—°ë„'] = ledger_df['íšŒê³„ì¼ì'].dt.year
            ledger_df['ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.month
            # âœ… ë¶„ì„ ê·œì¹™: ê³„ì • ì„œë¸Œì…‹ ë¶„ì„ ì‹œì—ë„ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•œ í¸ì˜ íŒŒìƒ
            ledger_df['ì—°ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
            # âœ… period_tag ì¶”ê°€(CY/PY/Other)
            ledger_df = add_period_tag(ledger_df)
            if 'ê±°ë˜ì²˜' not in ledger_df.columns:
                ledger_df['ê±°ë˜ì²˜'] = 'ì •ë³´ ì—†ìŒ'
            ledger_df['ê±°ë˜ì²˜'] = ledger_df['ê±°ë˜ì²˜'].fillna('ì •ë³´ ì—†ìŒ').astype(str)

            if st.button("ğŸš€ ì „ì²´ ë¶„ì„ ì‹¤í–‰", type="primary"):
                with st.spinner('ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                    # âœ… ì •í•©ì„±ì€ ì‚¬ìš©ì ê¸°ê°„ ì„ íƒê³¼ ë¬´ê´€í•˜ê²Œ ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
                    st.session_state.recon_status, st.session_state.recon_df = analyze_reconciliation(ledger_df, master_df)
                    # âœ… í‘œì¤€ LedgerFrame êµ¬ì„±(ì •í•©ì„±ì€ í•­ìƒ ì „ì²´ ê¸°ì¤€: DF_hist)
                    lf_hist = LedgerFrame(df=ledger_df, meta={
                        "file_name": getattr(uploaded_file, "name", "uploaded.xlsx"),
                        "master_df": master_df,
                    })
                    # ì´ˆê¸°ì—” focus=hist (í›„ì† ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì í•„í„° ì—°ê²°)
                    lf_focus = lf_hist

                    st.session_state.master_df = master_df
                    st.session_state.ledger_df = ledger_df
                    st.session_state.lf_hist = lf_hist
                    st.session_state.lf_focus = lf_focus
                    st.session_state.analysis_done = True
                st.rerun()

            if st.session_state.analysis_done:
                st.success("âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                with st.expander("ğŸ” ë¹ ë¥¸ ì§„ë‹¨(ë°ì´í„° í’ˆì§ˆ ì²´í¬)", expanded=False):
                    df = st.session_state.ledger_df.copy()
                    issues = []

                    invalid_date = int(df['íšŒê³„ì¼ì'].isna().sum())
                    if invalid_date > 0:
                        issues.append(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ(NaT): {invalid_date:,}ê±´")

                    if 'ê±°ë˜ì²˜' in df.columns:
                        missing_vendor = int((df['ê±°ë˜ì²˜'].isna() | (df['ê±°ë˜ì²˜'] == 'ì •ë³´ ì—†ìŒ')).sum())
                        if missing_vendor > 0:
                            issues.append(f"â„¹ï¸ ê±°ë˜ì²˜ ì •ë³´ ì—†ìŒ/ê²°ì¸¡: {missing_vendor:,}ê±´")

                    zero_abs = int((df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] == 0).sum())
                    issues.append(f"â„¹ï¸ ê¸ˆì•¡ ì ˆëŒ€ê°’ì´ 0ì¸ ì „í‘œ: {zero_abs:,}ê±´")

                    unlinked = int(df['ê³„ì •ëª…'].eq('ë¯¸ì§€ì • ê³„ì •').sum())
                    if unlinked > 0:
                        issues.append(f"â— Masterì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì „í‘œ(ê³„ì •ëª… ë¯¸ì§€ì •): {unlinked:,}ê±´")

                    st.write("**ì²´í¬ ê²°ê³¼**")
                    if issues:
                        for line in issues:
                            st.write("- " + line)
                    else:
                        st.success("ë¬¸ì œ ì—†ì´ ê¹”ë”í•©ë‹ˆë‹¤!")
                tab_integrity, tab_vendor, tab_anomaly, tab_ts, tab_report = st.tabs(["ğŸŒŠ ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„", "ğŸ¢ ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„", "ğŸ”¬ ì´ìƒ íŒ¨í„´ íƒì§€", "ğŸ“‰ ì‹œê³„ì—´ ì˜ˆì¸¡", "ğŸ§  ë¶„ì„ ì¢…í•© ëŒ€ì‹œë³´ë“œ"])

                # (ì´ì „ ë²„ì „) ëŒ€ì‹œë³´ë“œ íƒ­ì€ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì œê±°ë¨
                with tab_integrity:  # ...
                    st.header("ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    st.subheader("1. ë°ì´í„° ì •í•©ì„± ê²€ì¦ ê²°ê³¼")
                    mod = st.session_state.get('modules', {}).get('integrity')
                    status = (getattr(mod, 'summary', {}) or {}).get('overall_status', 'Pass') if mod else 'Pass'
                    result_df = (getattr(mod, 'tables', {}) or {}).get('reconciliation') if mod else st.session_state.get('recon_df')
                    if status == "Pass":
                        st.success("âœ… ëª¨ë“  ê³„ì •ì˜ ë°ì´í„°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤.")
                    elif status == "Warning":
                        st.warning("âš ï¸ ì¼ë¶€ ê³„ì •ì—ì„œ ì‚¬ì†Œí•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.error("ğŸš¨ ì¼ë¶€ ê³„ì •ì—ì„œ ì¤‘ëŒ€í•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    def highlight_status(row):
                        if row.ìƒíƒœ == 'Fail':
                            return ['background-color: #ffcccc'] * len(row)
                        elif row.ìƒíƒœ == 'Warning':
                            return ['background-color: #fff0cc'] * len(row)
                        return [''] * len(row)

                    format_dict = {col: '{:,.0f}' for col in result_df.select_dtypes(include=np.number).columns}
                    st.dataframe(result_df.style.apply(highlight_status, axis=1).format(format_dict), use_container_width=True)
                    st.markdown("---")
                    st.subheader("2. ê³„ì •ë³„ ì›”ë³„ ì¶”ì´ (PY vs CY)")
                    # âœ… ìë™ ì¶”ì²œ ì œê±°: ì‚¬ìš©ìê°€ ê³„ì •ì„ ì„ íƒí•œ ê²½ìš°ì—ë§Œ ê·¸ë˜í”„ ë Œë”
                    account_list = st.session_state.master_df['ê³„ì •ëª…'].unique()
                    selected_accounts = st.multiselect(
                        "ë¶„ì„í•  ê³„ì •ì„ ì„ íƒí•˜ì„¸ìš” (1ê°œ ì´ìƒ í•„ìˆ˜)",
                        account_list, default=[]
                    )
                    if not selected_accounts:
                        st.info("ê³„ì •ì„ 1ê°œ ì´ìƒ ì„ íƒí•˜ë©´ ì›”ë³„ ì¶”ì´ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        lf_use = st.session_state.get('lf_focus') or st.session_state.get('lf_hist')
                        # ì„ íƒëœ ê³„ì •ëª…ì„ ê³„ì •ì½”ë“œë¡œ ë³€í™˜
                        mdf = st.session_state.master_df
                        accounts_codes = (
                            mdf[mdf['ê³„ì •ëª…'].isin(selected_accounts)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        mod = run_trend_module(lf_use, accounts=accounts_codes)
                        _push_module(mod)
                        for w in mod.warnings:
                            st.warning(w)
                        if mod.figures:
                            for title, fig in mod.figures.items():
                                # PM ì„ê³„ì„ (í•­ìƒ í‘œì‹œ; ë²”ìœ„ ë°–ì´ë©´ ìë™ í™•ì¥)
                                st.plotly_chart(
                                    add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT))),
                                    use_container_width=True,
                                    key=f"trend_{title}"
                                )
                        else:
                            st.info("í‘œì‹œí•  ì¶”ì´ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("3. ê³„ì • ê°„ ìƒê´€ íˆíŠ¸ë§µ")
                    # âœ… ë²„íŠ¼ ì—†ì´ ì¦‰ì‹œ ë Œë”: ê³„ì • 2ê°œ ì´ìƒ ì„ íƒ + ì„ê³„ì¹˜ ìŠ¬ë¼ì´ë” ì œê³µ
                    corr_accounts = st.multiselect(
                        "ìƒê´€ ë¶„ì„ ëŒ€ìƒ ê³„ì •(2ê°œ ì´ìƒ ì„ íƒ)",
                        account_list,
                        default=selected_accounts,
                        help="ì„ íƒí•œ ê³„ì •ë“¤ ê°„ ì›”ë³„ íë¦„ì˜ í”¼ì–´ìŠ¨ ìƒê´€ì„ ê³„ì‚°í•©ë‹ˆë‹¤."
                    )
                    corr_thr = st.slider(
                        "ìƒê´€ ì„ê³„ì¹˜(ê°•í•œ ìƒê´€ìŒ í‘œ ì „ìš©)",
                        min_value=0.50, max_value=0.95, step=0.05, value=0.70,
                        help="ì ˆëŒ€ê°’ ê¸°ì¤€ ì„ê³„ì¹˜ ì´ìƒì¸ ê³„ì •ìŒë§Œ í‘œì— í‘œì‹œí•©ë‹ˆë‹¤."
                    )
                    if len(corr_accounts) < 2:
                        st.info("ê³„ì •ì„ **2ê°œ ì´ìƒ** ì„ íƒí•˜ë©´ íˆíŠ¸ë§µì´ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        lf_use = _lf_by_scope()
                        mdf = st.session_state.master_df
                        codes = mdf[mdf['ê³„ì •ëª…'].isin(corr_accounts)]['ê³„ì •ì½”ë“œ'].astype(str).tolist()
                        cmod = run_correlation_module(
                            lf_use,
                            accounts=codes,
                            corr_threshold=float(corr_thr),
                            cycles_map=get_effective_cycles(),
                        )
                        _push_module(cmod)
                        for w in cmod.warnings:
                            st.warning(w)
                        if cmod.figures:
                            stable_codes = "_".join(map(str, codes)) or "all"
                            stable_thr = str(int(corr_thr*100))
                            st.plotly_chart(
                                cmod.figures['heatmap'],
                                use_container_width=True,
                                key=f"corr_heatmap_{stable_codes}_{stable_thr}"
                            )
                        if 'strong_pairs' in cmod.tables and not cmod.tables['strong_pairs'].empty:
                            st.markdown("**ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ**")
                            st.dataframe(cmod.tables['strong_pairs'], use_container_width=True)
                        if 'excluded_accounts' in cmod.tables and not cmod.tables['excluded_accounts'].empty:
                            with st.expander("ì œì™¸ëœ ê³„ì • ë³´ê¸°(ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)", expanded=False):
                                st.dataframe(cmod.tables['excluded_accounts'], use_container_width=True)

                with tab_vendor:
                    st.header("ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")

                    st.subheader("ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± (ê³„ì •ë³„)")
                    master_df_res = st.session_state.master_df
                    account_list_vendor = master_df_res['ê³„ì •ëª…'].unique()
                    selected_accounts_vendor = st.multiselect("ë¶„ì„í•  ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”.", account_list_vendor, default=[])

                    # ğŸ”§ ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„° â€” KRW ì…ë ¥(ì»¤ë°‹ ì‹œ ì‰¼í‘œ ì •ê·œí™”)
                    min_amount_vendor = _krw_input(
                        "ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„°",
                        key="vendor_min_amount",
                        default_value=0,
                        help_text="CY ê¸°ì¤€ ê±°ë˜ê¸ˆì•¡ í•©ê³„ê°€ ì´ ê°’ ë¯¸ë§Œì¸ ê±°ë˜ì²˜ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°ë©ë‹ˆë‹¤."
                    )
                    include_others_vendor = st.checkbox("ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°", value=True)

                    if selected_accounts_vendor:
                        selected_codes = (
                            master_df_res[master_df_res['ê³„ì •ëª…'].isin(selected_accounts_vendor)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        lf_use = _lf_by_scope()
                        vmod = run_vendor_module(
                            lf_use,
                            account_codes=selected_codes,
                            min_amount=float(min_amount_vendor),
                            include_others=bool(include_others_vendor),
                        )
                        _push_module(vmod)
                        if vmod.figures:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'pareto' in vmod.figures:
                                    figp = vmod.figures['pareto']
                                    figp = add_materiality_threshold(figp, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figp, use_container_width=True, key=f"vendor_pareto_{'_'.join(selected_accounts_vendor) or 'all'}")
                            with col2:
                                if 'heatmap' in vmod.figures:
                                    figh = add_pm_badge(vmod.figures['heatmap'], float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figh, use_container_width=True, key=f"vendor_heatmap_{'_'.join(selected_accounts_vendor) or 'all'}")
                        else:
                            st.warning("ì„ íƒí•˜ì‹  ê³„ì •ì—ëŠ” ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        for w in vmod.warnings:
                            st.warning(w)
                    else:
                        st.info("ê³„ì •ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ê³„ì •ì˜ ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± ë¶„ì„ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("ê±°ë˜ì²˜ë³„ ì„¸ë¶€ ë¶„ì„ (ì „ì²´ ê³„ì •)")
                    full_ledger_df = st.session_state.ledger_df
                    vendor_list = sorted(full_ledger_df[full_ledger_df['ê±°ë˜ì²˜'] != 'ì •ë³´ ì—†ìŒ']['ê±°ë˜ì²˜'].unique())

                    if len(vendor_list) > 0:
                        options = ['ì„ íƒí•˜ì„¸ìš”...'] + vendor_list
                        selected_vendor = st.selectbox("ìƒì„¸ ë¶„ì„í•  ê±°ë˜ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.", options, index=0)

                        if selected_vendor != 'ì„ íƒí•˜ì„¸ìš”...':
                            all_months_in_data = pd.period_range(
                                start=full_ledger_df['íšŒê³„ì¼ì'].min(),
                                end=full_ledger_df['íšŒê³„ì¼ì'].max(),
                                freq='M'
                            ).strftime('%Y-%m').tolist()
                            detail_fig = create_vendor_detail_figure(full_ledger_df, selected_vendor, all_months_in_data)
                            # PM line on vendor detail (stacked bars)
                            try:
                                detail_fig = add_materiality_threshold(detail_fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                            except Exception:
                                pass
                            if detail_fig:
                                st.plotly_chart(detail_fig, use_container_width=True, key=f"vendor_detail_{selected_vendor}")
                    else:
                        st.info("ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                with tab_anomaly:
                    st.header("ì´ìƒ íŒ¨í„´ íƒì§€")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    mdf = st.session_state.master_df
                    acct_names = mdf['ê³„ì •ëª…'].unique()
                    pick = st.multiselect("ëŒ€ìƒ ê³„ì • ì„ íƒ(ë¯¸ì„ íƒ ì‹œ ìë™ ì¶”ì²œ)", acct_names, default=[])
                    topn = st.slider("í‘œì‹œ ê°œìˆ˜(ìƒìœ„ |Z|)", min_value=10, max_value=500, value=20, step=10)
                    if st.button("ì´ìƒì¹˜ ë¶„ì„ ì‹¤í–‰"):
                        lf_use = _lf_by_scope()
                        codes = None
                        if pick:
                            codes = mdf[mdf['ê³„ì •ëª…'].isin(pick)]['ê³„ì •ì½”ë“œ'].astype(str).tolist()
                        amod = run_anomaly_module(lf_use, target_accounts=codes, topn=topn, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                        _push_module(amod)
                        for w in amod.warnings: st.warning(w)
                        if 'anomaly_top' in amod.tables:
                            _tbl = amod.tables['anomaly_top'].copy()
                            fmt = {}
                            if 'ë°œìƒì•¡' in _tbl.columns: fmt['ë°œìƒì•¡'] = '{:,.0f}'
                            if 'Z-Score' in _tbl.columns: fmt['Z-Score'] = '{:.2f}'
                            st.dataframe(_tbl.style.format(fmt), use_container_width=True)
                        if 'zscore_hist' in amod.figures:
                            st.plotly_chart(amod.figures['zscore_hist'], use_container_width=True, key="anomaly_hist")

                with tab_ts:
                    st.header("ì‹œê³„ì—´ ì˜ˆì¸¡")
                    with st.expander("ğŸ§­ í•´ì„ ê°€ì´ë“œ", expanded=False):
                        st.markdown(
                            """
### ìš©ì–´
- **z(í‘œì¤€í™” ì§€ìˆ˜)**: `z = (ì‹¤ì¸¡ âˆ’ ì˜ˆì¸¡) / Ïƒ`  
  - ì›”ë³„ ì˜ˆì¸¡ ì”ì°¨(ì‹¤ì¸¡-ì˜ˆì¸¡)ë¥¼ í‘œì¤€í™”í•œ ì§€ìˆ˜ì…ë‹ˆë‹¤. **ì´ìƒ íŒ¨í„´ íƒì§€ì˜ Z-Scoreì™€ ë‹¤ë¥¸ ê°œë…ì…ë‹ˆë‹¤.**
  - |z|â‰ˆ2ëŠ” **ì´ë¡€ì **, |z|â‰ˆ3ì€ **ë§¤ìš° ì´ë¡€ì **ì…ë‹ˆë‹¤.  
- **Ïƒ(í‘œì¤€í¸ì°¨) ì§‘ê³„**: ìµœê·¼ *k=6ê°œì›”* ì”ì°¨ì˜ í‘œì¤€í¸ì°¨ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ì§§ìœ¼ë©´ ì‹œì‘~í˜„ì¬ê¹Œì§€ì˜ **expanding Ïƒ**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
- **ìœ„í—˜ë„(0~1)** = `min(1, 0.5Â·|z|/3 + 0.3Â·PMëŒ€ë¹„ + 0.2Â·KIT)`  
  - PMëŒ€ë¹„ = `min(1, |ì‹¤ì¸¡âˆ’ì˜ˆì¸¡| / PM)`,  **KIT** = PM ì´ˆê³¼ ì—¬ë¶€(True/False)
- **Flow / Balance**: *Flow*ëŠ” **ì›” ë°œìƒì•¡(Î”ì”ì•¡)**, *Balance*ëŠ” **ì›”ë§ ì”ì•¡**ì…ë‹ˆë‹¤. *(BS ê³„ì •ì€ Balance ê¸°ì¤€ë„ ë³‘í–‰ ê³„ì‚°í•©ë‹ˆë‹¤.)*
- **ì •ìƒì„±**: ì‹œê³„ì—´ì˜ í‰ê· /ë¶„ì‚°ì´ ì‹œê°„ì— ë”°ë¼ **ì•ˆ ë³€í•¨**(ARIMAê°€ íŠ¹íˆ ì„ í˜¸).
- **MAE**: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨(ì› ë‹¨ìœ„). **ì‘ì„ìˆ˜ë¡ ì •í™•**.
- **MAPE**: ìƒëŒ€ ì˜¤ì°¨(%). **ê·œëª¨ ë‹¤ë¥¸ ê³„ì • ë¹„êµ**ì— ìœ ìš©.
- **AIC/BIC**: ëª¨ë¸ ë³µì¡ë„ê¹Œì§€ ê³ ë ¤í•œ **ì •ë³´ëŸ‰ ì§€í‘œ**. **ì‘ì„ìˆ˜ë¡ ìš°ìˆ˜**.

### ì°¨íŠ¸ ì½ê¸°
- ì‹¤ì„ =ì‹¤ì¸¡, ì ì„ =ì˜ˆì¸¡(**MoR**: EMA/MA/ARIMA/Prophet ì¤‘ ìë™ ì„ íƒ)  
- íšŒìƒ‰ ì ì„ : **ì—°(êµµê²Œ)** / **ë¶„ê¸°(ì–‡ê²Œ)** ê²½ê³„ì„ , ë¶‰ì€ ì ì„ : **PM ê¸°ì¤€ì„ **

### ì‚¬ìš©í•œ ì˜ˆì¸¡ëª¨ë¸
- **MA(ì´ë™í‰ê· )**: ìµœê·¼ *n*ê°œì›” **ë‹¨ìˆœ í‰ê· **. **ì§§ì€ ë°ì´í„°/ë³€ë™ ì™„ë§Œ**í•  ë•Œ ì•ˆì •ì .
- **EMA(ì§€ìˆ˜ì´ë™í‰ê· )**: **ìµœê·¼ê°’ ê°€ì¤‘** í‰ê· . **ìµœê·¼ ì¶”ì„¸ ë°˜ì˜**ì´ í•„ìš”í•  ë•Œ ìœ ë¦¬.
- **ARIMA(p,d,q)**: **ìê¸°ìƒê´€** ê¸°ë°˜. **ê³„ì ˆì„±ì´ ì•½(ë˜ëŠ” ì œê±° ê°€ëŠ¥)**í•˜ê³  **ë°ì´í„°ê°€ ì¶©ë¶„**í•  ë•Œ ê°•í•¨.
- **Prophet**: **ì—°/ë¶„ê¸° ê³„ì ˆì„±Â·íœ´ì¼íš¨ê³¼**ê°€ ëšœë ·í•  ë•Œ ì í•©(ì´ìƒì¹˜ì— ë¹„êµì  ê²¬ê³ ).

> :blue[**ëª¨ë¸ì€ ê³„ì •Ã—ê¸°ì¤€(Flow/Balance)ë³„ë¡œ êµì°¨ê²€ì¦ ì˜¤ì°¨(MAPE/MAE)ì™€ (ê°€ëŠ¥í•˜ë©´) ì •ë³´ëŸ‰(AIC/BIC)ì„ ì¢…í•©í•´ ìë™ ì„ íƒë©ë‹ˆë‹¤.**]
"""
                        )
                    # ëª¨ë¸ ê°€ìš© ë°°ì§€(ë””ë²„ê¹… ê²¸ ì‚¬ìš©ì ì•ˆë‚´)
                    try:
                        # model_registry is imported at the top
                        _reg = model_registry()
                        st.caption(f"ì§€ì› ëª¨ë¸: EMA âœ“ Â· MA âœ“ Â· ARIMA {'âœ“' if _reg['arima'] else 'â€”'} Â· Prophet {'âœ“' if _reg['prophet'] else 'â€”'}")
                    except Exception:
                        pass
                    # (ì¤‘ë³µ ê°€ì´ë“œ ì œê±°ë¨)
                    lf_use = _lf_by_scope()
                    mdf = st.session_state.master_df
                    dfm = lf_use.df.copy()
                    dfm['ì—°ì›”'] = dfm['íšŒê³„ì¼ì'].dt.to_period('M').dt.to_timestamp('M')
                    agg = (dfm.groupby(['ê³„ì •ëª…','ì—°ì›”'])['ê±°ë˜ê¸ˆì•¡'].sum()
                               .reset_index().rename(columns={'ê³„ì •ëª…':'account','ì—°ì›”':'date','ê±°ë˜ê¸ˆì•¡':'amount'}))
                    pick_accounts_ts = st.multiselect("ëŒ€ìƒ ê³„ì •", sorted(agg['account'].unique()), default=[], key="ts_accounts")
                    use_ts = agg if not pick_accounts_ts else agg[agg['account'].isin(pick_accounts_ts)]
                    # BS ì—¬ë¶€ë¥¼ ë°˜ì˜í•´ balance ê¸°ì¤€ë„ ë³‘í–‰ ê³„ì‚°
                    try:
                        bs_map = st.session_state.master_df[['ê³„ì •ëª…','BS/PL']].drop_duplicates()
                        _bs_flag = bs_map.set_index('ê³„ì •ëª…')['BS/PL'].map(lambda x: str(x).upper()== 'BS').to_dict()
                    except Exception:
                        _bs_flag = {}
                    work_ts = use_ts.copy()
                    work_ts['is_bs'] = work_ts['account'].map(lambda name: bool(_bs_flag.get(str(name), False)))

                    res = run_timeseries_module_with_flag(work_ts,
                                       account_col='account', date_col='date', amount_col='amount', is_bs_col='is_bs',
                                       pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                    if not res.empty:
                        out = res.copy()
                        out = out.rename(columns={'account':'ê³„ì •'})
                        for c in ['actual','predicted','error','z','risk']:
                            out[c] = pd.to_numeric(out[c], errors='coerce')
                        # ì‚¬ìš©ì ì¹œí™”ì  í‘œê¸°(ê¸°ì¤€): ë°œìƒì•¡/ì”ì•¡
                        try:
                            out['measure'] = out['measure'].map(lambda m: 'ë°œìƒì•¡(flow)' if str(m)=='flow' else ('ì”ì•¡(balance)' if str(m)=='balance' else str(m)))
                        except Exception:
                            pass
                        _disp = out[['date','ê³„ì •','measure','model','actual','predicted','error','z','risk']].rename(columns={
                            'date': 'ì›”',
                            'measure': 'ê¸°ì¤€(Measure)',
                            'model': 'ëª¨ë¸(MoR)',
                            'actual': 'ì‹¤ì œ(ì›” í•©ê³„)',
                            'predicted': 'ì˜ˆì¸¡(ì›” í•©ê³„)',
                            'error': 'ì°¨ì´(ì‹¤ì œ-ì˜ˆì¸¡)',
                            'z': 'í‘œì¤€í™”ì§€ìˆ˜(z)',
                            'risk': 'ìœ„í—˜ë„(0~1)'
                        })
                        st.caption("MoR(ìµœì  ëª¨ë¸) ê¸°ì¤€. BS ê³„ì •ì€ balance ê¸°ì¤€ë„ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.")
                        st.dataframe(_disp.style.format({
                            'ì‹¤ì œ(ì›” í•©ê³„)':'{:,.0f}', 'ì˜ˆì¸¡(ì›” í•©ê³„)':'{:,.0f}', 'ì°¨ì´(ì‹¤ì œ-ì˜ˆì¸¡)':'{:,.0f}', 'í‘œì¤€í™”ì§€ìˆ˜(z)':'{:+.2f}', 'ìœ„í—˜ë„(0~1)':'{:.2f}'
                        }), use_container_width=True)

                        # === ë¼ì¸ì°¨íŠ¸ ===
                        st.markdown("#### ë¼ì¸ì°¨íŠ¸")
                        # ì›”ë³„ ì§‘ê³„ì—ì„œ flow/balance íˆìŠ¤í† ë¦¬ êµ¬ì„±
                        hist_base = use_ts.rename(columns={'amount':'flow'}).sort_values('date').copy()
                        hist_base['balance'] = hist_base['flow']
                        # ê³„ì •ë³„ opening (=ì „ê¸°ë§ì”ì•¡) ë§µ
                        _open = st.session_state.master_df[['ê³„ì •ëª…','ì „ê¸°ë§ì”ì•¡']].drop_duplicates()
                        opening_map = _open.set_index('ê³„ì •ëª…')['ì „ê¸°ë§ì”ì•¡'].to_dict()

                        def _apply_opening(g):
                            acc_name = str(g['account'].iloc[0])
                            opn = float(opening_map.get(acc_name, 0.0))
                            g = g.copy()
                            g['balance'] = opn + g['flow'].astype(float).cumsum()
                            return g

                        hist_base = hist_base.groupby('account', group_keys=False).apply(_apply_opening)

                        # ê³„ì • ì„ íƒ
                        sel_acc = st.selectbox("ê³„ì • ì„ íƒ(ë¼ì¸ì°¨íŠ¸)", sorted(hist_base['account'].unique()), key="ts_plot_acc_main")

                        # BS/PL íŒë‹¨
                        _mdf = st.session_state.master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','BS/PL','ì°¨ë³€/ëŒ€ë³€']].drop_duplicates()
                        is_bs = bool(_mdf[_mdf['ê³„ì •ëª…'] == sel_acc]['BS/PL'].astype(str).str.upper().eq('BS').any())

                        cur_hist = hist_base[hist_base['account'] == sel_acc].copy()
                        if cur_hist.empty:
                            st.info("ì„ íƒ ê³„ì •ì˜ ì›”ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        else:
                            # (ê¸°ì¡´ í•™ìŠµ/ëª¨ë¸ í‘œê¸° ë¡œì§ ì œê±° â€” create_timeseries_figureì—ì„œ ë©”íƒ€ì™€ ì§€í‘œ ì œê³µ)

                            # ëŒ€ë³€ê³„ì •(ë¶€ì±„Â·ìë³¸Â·ìˆ˜ìµ)ì¸ ê²½ìš° ê·¸ë˜í”„ ë¶€í˜¸ ë°˜ì „
                            try:
                                from utils.helpers import is_credit_account
                                # Masterì—ì„œ í•´ë‹¹ ê³„ì •ì˜ ì†ì„± ì¡°íšŒ
                                _row = _mdf[_mdf['ê³„ì •ëª…'] == sel_acc].iloc[0] if not _mdf[_mdf['ê³„ì •ëª…'] == sel_acc].empty else None
                                acc_type = _row.get('BS/PL', 'PL') if _row is not None else 'PL'
                                dc_flag = _row.get('ì°¨ë³€/ëŒ€ë³€') if _row is not None else None
                                if is_credit_account(acc_type if acc_type in ['ë¶€ì±„','ìë³¸','ìˆ˜ìµ'] else None, dc_flag):
                                    cur_hist = cur_hist.copy()
                                    cur_hist['flow'] = -cur_hist['flow']
                                    cur_hist['balance'] = -cur_hist['balance']
                            except Exception:
                                pass

                            # UI: ê³µí†µ ì˜µì…˜ ì„¤ì •
                            show_dividers = st.toggle("ì—°/ë¶„ê¸° êµ¬ë¶„ì„  í‘œì‹œ", value=True, key=f"ts_dividers_toggle_{sel_acc}")
                            pm_val_current = float(st.session_state.get("pm_value", PM_DEFAULT))

                            # Helper to render figure and stats
                            def _render_fig_and_stats(fig, stats, key_suffix):
                                if fig and stats:
                                    try:
                                        diag = stats.get("diagnostics", {})
                                        pval = diag.get("p_value")
                                        b1, b2, b3 = st.columns(3)
                                        b1.caption(f"ê³„ì ˆì„±: {'ê°•í•¨' if diag.get('seasonality') else 'ì•½í•¨'}")
                                        ptxt = "" if pval is None or (isinstance(pval, float) and np.isnan(pval)) else f" (p={pval:.3f})"
                                        b2.caption(f"ì •ìƒì„±: {'í™•ë³´' if diag.get('stationary') else 'ë¯¸í™•ë³´'}" + ptxt)
                                        b3.caption(f"ë°ì´í„°: {diag.get('n_months')}ê°œì›” â€” {'ì¶©ë¶„' if not diag.get('is_short') else 'ì§§ìŒ'}")
                                    except Exception:
                                        pass
                                    st.plotly_chart(fig, use_container_width=True, key=f"ts_line_{sel_acc}_{key_suffix}")
                                    try:
                                        meta = stats.get("metadata", {})
                                        metrics = stats.get("metrics", {})
                                        mae, mape = metrics.get('mae'), metrics.get('mape')
                                        aic, bic = metrics.get('aic'), metrics.get('bic')
                                        st.caption(
                                            f"ì„ íƒëª¨ë¸: **{meta.get('model')}** Â· í•™ìŠµê¸°ê°„: {meta.get('data_span')} ({meta.get('train_months')}ê°œì›”) Â· "
                                            f"Ïƒìœˆë„ìš°: {meta.get('sigma_window')}ê°œì›” Â· MAE: {mae:,.0f}ì› Â· MAPE: {mape:.1f}% Â· "
                                            f"AIC: {aic if isinstance(aic, float) and np.isfinite(aic) else 'â€”'} Â· BIC: {bic if isinstance(bic, float) and np.isfinite(bic) else 'â€”'}"
                                        )
                                        if meta.get('reasoning'):
                                            st.info(meta.get('reasoning'))
                                    except Exception:
                                        pass
                                    # 4. Detailed Stats Expander
                                    with st.expander("ì´ ì°¨íŠ¸ì˜ í†µê³„ ì„¤ì • ë³´ê¸°", expanded=False):
                                        st.write(stats.get("details"))
                                elif stats and "error" in stats:
                                    st.warning(stats["error"])
                                else:
                                    st.info("ì°¨íŠ¸ë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                            if is_bs:
                                pair = st.toggle("ìŒì°¨íŠ¸ ë³´ê¸°(Flow+Balance)", value=True)
                                if pair:
                                    c1, c2 = st.columns(2)
                                    with c1:
                                        f1, st1 = create_timeseries_figure(
                                            cur_hist, 'flow', f"{sel_acc} â€” Flow (actual vs MoR)",
                                            pm_value=pm_val_current,
                                            show_dividers=show_dividers
                                        )
                                        _render_fig_and_stats(f1, st1, "flow")
                                    with c2:
                                        f2, st2 = create_timeseries_figure(
                                            cur_hist, 'balance', f"{sel_acc} â€” Balance (actual vs MoR)",
                                            pm_value=pm_val_current,
                                            show_dividers=show_dividers
                                        )
                                        _render_fig_and_stats(f2, st2, "balance")
                                else:
                                    fig, stx = create_timeseries_figure(
                                        cur_hist, 'flow', f"{sel_acc} â€” Flow (actual vs MoR)",
                                        pm_value=pm_val_current,
                                        show_dividers=show_dividers
                                    )
                                    _render_fig_and_stats(fig, stx, "flow_single")
                            else:
                                fig, stx = create_timeseries_figure(
                                    cur_hist, 'flow', f"{sel_acc} â€” Flow (actual vs MoR)",
                                    pm_value=pm_val_current,
                                    show_dividers=show_dividers
                                )
                                _render_fig_and_stats(fig, stx, "flow_only")

                        # (ì‚­ì œë¨) ë§‰ëŒ€ ëŒ€ì¡° UI â€” ì˜¤ë¥˜ ì›ì¸ ê²½ë¡œ ì°¨ë‹¨

                    else:
                        st.info("ì˜ˆì¸¡ì„ í‘œì‹œí•  ì¶©ë¶„í•œ ì›”ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                # âš ï¸ ê¸°ì¡´ tab5(ìœ„í—˜í‰ê°€) ë¸”ë¡ ì „ì²´ ì‚­ì œë¨
                
                with tab_report:
                    st.header("ğŸ§  ë¶„ì„ ì¢…í•© ëŒ€ì‹œë³´ë“œ")
                    # --- Preview: modules session quick view ---
                    modules_list_preview = list(st.session_state.get('modules', {}).values())
                    with st.expander("ğŸ” ëª¨ë“ˆë³„ ìš”ì•½/ì¦ê±° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        if not modules_list_preview:
                            st.info("ëª¨ë“ˆ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ê° ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                        else:
                            for mr in modules_list_preview:
                                try:
                                    st.subheader(f"â€¢ {getattr(mr, 'name', 'module')}")
                                    if getattr(mr, 'summary', None):
                                        st.json(mr.summary)
                                    evs = list(getattr(mr, 'evidences', []))
                                    if evs:
                                        st.write("Evidence ìƒ˜í”Œ (ìƒìœ„ 3)")
                                        for ev in evs[:3]:
                                            try:
                                                st.write(f"- reason={ev.reason} | risk={float(ev.risk_score):.2f} | amount={float(ev.financial_impact):,.0f}")
                                            except Exception:
                                                st.write("- (í‘œì‹œ ì‹¤íŒ¨)")
                                    if getattr(mr, 'tables', None):
                                        try: st.caption(f"tables: {list(mr.tables.keys())}")
                                        except Exception: pass
                                    if getattr(mr, 'figures', None):
                                        try: st.caption(f"figures: {list(mr.figures.keys())}")
                                        except Exception: pass
                                except Exception:
                                    st.caption("(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨)")
                    # LLM í‚¤ ë¯¸ê°€ìš©ì´ì–´ë„ ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„± ê°€ëŠ¥
                    LLM_OK = False
                    try:
                        from services.llm import openai_available
                        LLM_OK = bool(openai_available())
                    except Exception:
                        LLM_OK = False
                    if not LLM_OK:
                        st.info("ğŸ”Œ OpenAI Key ì—†ìŒ: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ë¯¸ì‚¬ìš©)")
                    rendered_report = False

                    # === ëª¨ë¸/í† í°/ì»¨í…ìŠ¤íŠ¸ ì˜µì…˜ UI ===
                    colm1, colm2, colm3 = st.columns([1,1,1])
                    with colm1:
                        llm_model_choice = st.selectbox(
                            "LLM ëª¨ë¸", options=["gpt-5", "gpt-4o"], index=1,
                            help="gpt-5 ë¯¸ê°€ìš© ì‹œ ìë™ìœ¼ë¡œ gpt-4oë¡œ ëŒ€ì²´í•˜ì„¸ìš”(ì½”ë“œì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬)."
                        )
                    with colm2:
                        desired_tokens = st.number_input(
                            "ë³´ê³ ì„œ ìµœëŒ€ ì¶œë ¥ í† í°", min_value=512, max_value=32000, value=16000, step=512,
                            help="ì‹¤ì œ ì „ì†¡ê°’ì€ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ì™€ ì…ë ¥ í† í°ì„ ê³ ë ¤í•´ ì•ˆì „ í´ë¨í”„ë©ë‹ˆë‹¤."
                        )
                    with colm3:
                        ctx_topk = st.number_input("ì»¨í…ìŠ¤íŠ¸ Evidence Top-K(ëª¨ë“ˆë³„)", min_value=5, max_value=100, value=20, step=5)
                        st.caption("ìš”ì•½/ë„í‘œëŠ” ìµœì†Œí™”í•˜ê³  ì¦ê±°ëŠ” ìƒìœ„ Top-Kë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        st.session_state['ctx_topk'] = int(ctx_topk)

                    # ì„ íƒí•œ ëª¨ë¸/í† í°ì„ ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ í•˜ë‹¨ í˜¸ì¶œë¶€ì—ì„œ ì‹¤ì œ ì‚¬ìš©
                    st.session_state['llm_model'] = llm_model_choice
                    st.session_state['llm_max_tokens'] = int(desired_tokens)

                    # --- ì…ë ¥ ì˜ì—­ ---
                    mdf = st.session_state.master_df
                    ldf = st.session_state.ledger_df

                    # â‘  ê³„ì • ì„ íƒ(í•„ìˆ˜) â€” ìë™ ì¶”ì²œ ì œê±°
                    acct_names_all = sorted(mdf['ê³„ì •ëª…'].dropna().unique().tolist())
                    pick_accounts = st.multiselect(
                        "ë³´ê³ ì„œ ëŒ€ìƒ ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”. (ìµœì†Œ 1ê°œ)",
                        options=acct_names_all,
                        default=[]
                    )
                    # â‘¡ ì˜µì…˜ ì œê±°: í•­ìƒ ìˆ˜í–‰ í”Œë˜ê·¸
                    opt_knn_evidence = True
                    opt_patterns = True
                    opt_patterns_py = True

                    # â‘¢ ì‚¬ìš©ì ë©”ëª¨(ì„ íƒ)
                    manual_ctx = st.text_area(
                        "ë³´ê³ ì„œì— ì¶”ê°€í•  ë©”ëª¨/ì£¼ì˜ì‚¬í•­(ì„ íƒ)",
                        placeholder="ì˜ˆ: 5~7ì›” ëŒ€í˜• ìº í˜ì¸ ì§‘í–‰ ì˜í–¥, 3ë¶„ê¸°ë¶€í„° ë‹¨ê°€ ì¸ìƒ ì˜ˆì • ë“±"
                    )

                    # â‘£ ì„ íƒ ê³„ì •ì½”ë“œ ë§¤í•‘
                    pick_codes = (
                        mdf[mdf['ê³„ì •ëª…'].isin(pick_accounts)]['ê³„ì •ì½”ë“œ']
                        .astype(str).tolist()
                    )

                    colA, colB, colC = st.columns([1,1,1])
                    with colA: st.write("ì„ íƒ ê³„ì •ì½”ë“œ:", ", ".join(pick_codes) if pick_codes else "-")
                    with colB: st.write("ê¸°ì¤€ ì—°ë„(CY):", int(ldf['ì—°ë„'].max()))
                    with colC: st.write("ë³´ê³ ì„œ ê¸°ì¤€:", "Current Year GL")

                    # ë²„íŠ¼ì€ ê³„ì • ë¯¸ì„ íƒ ì‹œ ë¹„í™œì„±í™”
                    btn = st.button("ğŸ“ ë³´ê³ ì„œ ìƒì„±", type="primary", disabled=(len(pick_codes) == 0))
                    if len(pick_codes) == 0:
                        st.info("ê³„ì • 1ê°œ ì´ìƒ ì„ íƒ ì‹œ ë²„íŠ¼ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

                    if btn:
                        import time
                        from analysis.anomaly import compute_amount_columns
                        from analysis.embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
                        from analysis.report import build_report_context, run_final_analysis, build_methodology_note, run_offline_fallback_report
                        from services.llm import LLMClient
                        from analysis.anomaly import ensure_zscore

                        t0 = time.perf_counter()
                        with st.status("ë³´ê³ ì„œ ì¤€ë¹„ ì¤‘...", expanded=True) as s:
                            # Step 1) ë°ì´í„° ìŠ¬ë¼ì´ì‹±
                            s.write("â‘  ìŠ¤ì½”í”„ ì ìš© ë° ë°ì´í„° ìŠ¬ë¼ì´ì‹±(CY/PY)â€¦")
                            cur_year = ldf['ì—°ë„'].max()
                            df_cy = ldf[(ldf['period_tag'] == 'CY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            df_py = ldf[(ldf['period_tag'] == 'PY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            s.write(f"    â”” CY {len(df_cy):,}ê±´ / PY {len(df_py):,}ê±´")

                            # Step 2) í•„ìˆ˜ íŒŒìƒ(ë°œìƒì•¡/ìˆœì•¡)
                            s.write("â‘¡ ê¸ˆì•¡ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±(ë°œìƒì•¡/ìˆœì•¡)â€¦")
                            df_cy = compute_amount_columns(df_cy)
                            df_py = compute_amount_columns(df_py)

                            # Step 3) (ì„ íƒ) íŒ¨í„´ìš”ì•½: ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ (LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œì—ë§Œ)
                            cl_ok = False
                            if LLM_OK and opt_patterns and not df_cy.empty:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰(ì„ íƒ)â€¦")
                                # ì…ë ¥ í…ìŠ¤íŠ¸ í’ë¶€í™” + ì„ë² ë”© + HDBSCAN (ìµœëŒ€ N ì œí•œìœ¼ë¡œ ì•ˆì „ê°€ë“œ)
                                df_cy_small = df_cy.copy()
                                max_rows = 8000
                                if len(df_cy_small) > max_rows:
                                    df_cy_small = df_cy_small.sample(max_rows, random_state=42)
                                    s.write(f"    â”” ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                df_cy_small = ensure_rich_embedding_text(df_cy_small)
                                try:
                                    emb_client = LLMClient(model=st.session_state.get('llm_model')).client  # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´
                                    # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë„¤ì´ë°ì„ í•„ìˆ˜ë¡œ ìš”êµ¬
                                    df_clu, ok = perform_embedding_and_clustering(
                                        df_cy_small, emb_client,
                                        name_with_llm=True, must_name_with_llm=True,
                                        use_large=bool(st.session_state.get("use_large_embedding", False)),
                                        rescue_tau=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
                                        llm_model=st.session_state.get('llm_model', 'gpt-4o'),
                                        embed_texts_fn=get_or_embed_texts,
                                    )
                                    if ok:
                                        # ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì´ë¦„ì„ LLMìœ¼ë¡œ í†µí•©
                                        from analysis.embedding import unify_cluster_names_with_llm, unify_cluster_labels_llm
                                        df_clu, name_map = unify_cluster_names_with_llm(
                                            df_clu, emb_client,
                                            llm_model=st.session_state.get('llm_model', 'gpt-4o'),
                                            embed_texts_fn=get_or_embed_texts
                                        )
                                        # ì¶”ê°€ LLM ë¼ë²¨ í†µí•©(JSON ë§¤í•‘ ë°©ì‹) â€” CYì˜ cluster_groupì€ ìœ ì§€
                                        try:
                                            raw_map = unify_cluster_labels_llm(df_clu['cluster_name'].dropna().unique().tolist(), emb_client)
                                            if raw_map:
                                                df_clu['cluster_name'] = df_clu['cluster_name'].map(lambda x: raw_map.get(str(x), x))
                                                # â— cluster_groupëŠ” unify_cluster_names_with_llm()ì´ ì •í•œ canonicalì„ ìœ ì§€
                                        except Exception:
                                            pass
                                        # ê°„ë‹¨ ìš”ì•½(ìƒìœ„ 5ê°œ)
                                        topc = (df_clu.groupby('cluster_group')['ë°œìƒì•¡']
                                                .agg(['count','sum']).sort_values('sum', ascending=False).head(5))
                                        s.write("    â”” í´ëŸ¬ìŠ¤í„° ìƒìœ„ 5ê°œ ìš”ì•½:")
                                        st.dataframe(
                                            topc.rename(columns={'count':'ê±´ìˆ˜','sum':'ë°œìƒì•¡í•©ê³„'})
                                                .style.format({'ë°œìƒì•¡í•©ê³„':'{:,.0f}'}),
                                            use_container_width=True
                                        )
                                        # í’ˆì§ˆ ì§€í‘œ(ë…¸ì´ì¦ˆìœ¨Â·í´ëŸ¬ìŠ¤í„° ìˆ˜ ë“±) ê¸°ë¡
                                        try:
                                            n = int(len(df_clu))
                                            noise_rate = float((df_clu['cluster_id'] == -1).mean()) if n else 0.0
                                            n_clusters = int(df_clu.loc[df_clu['cluster_id'] != -1, 'cluster_id'].nunique())
                                            if n_clusters > 0:
                                                avg_size = float(df_clu[df_clu['cluster_id'] != -1].groupby('cluster_id').size().mean())
                                            else:
                                                avg_size = 0.0
                                            rescue_rate = float(df_clu.get('rescued', False).mean()) if 'rescued' in df_clu.columns else 0.0
                                            model_used = df_clu.attrs.get('embedding_model', 'unknown')
                                            umap_on = bool(df_clu.attrs.get('umap_used', False))
                                            s.write(
                                                f"    â”” Quality: N={n:,}, noise={noise_rate*100:.1f}%, "
                                                f"clusters={n_clusters}, avg_size={avg_size:.1f}, rescued={rescue_rate*100:.1f}%"
                                            )
                                            s.write(
                                                f"    â”” Model/UMAP: {model_used} | UMAP={'on' if umap_on else 'off'} | Ï„={float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)):.2f}"
                                            )
                                            # ëŒ€ì‹œë³´ë“œ ì¹´ë“œìš© í’ˆì§ˆ ì§€í‘œ ì €ì¥
                                            st.session_state['cluster_quality'] = {
                                                "N": n,
                                                "noise_rate": noise_rate,
                                                "n_clusters": n_clusters,
                                                "avg_size": avg_size,
                                                "rescued_rate": rescue_rate,
                                                "model": model_used,
                                                "umap": umap_on,
                                                "tau": float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)),
                                            }
                                        except Exception:
                                            pass
                                        # ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸ì— ë°˜ì˜: group/label ë™ì‹œ ë¶€ì°©
                                        df_cy = df_cy.merge(
                                            df_clu[['row_id','cluster_id','cluster_name','cluster_group']],
                                            on='row_id', how='left'
                                        )
                                        # í•„ìš” ì‹œ vectorë„ í•¨ê»˜ ë³‘í•© ê°€ëŠ¥:
                                        # df_cy = df_cy.merge(df_clu[['row_id','vector']], on='row_id', how='left')
                                        # (í˜„ì¬ëŠ” perform_embedding_only ë‹¨ê³„ì—ì„œ CY/PY dfì— vectorê°€ ì§ì ‘ ë¶€ì—¬ë¨)
                                        # --- PY clustering and alignment (optional) ---
                                        if opt_patterns_py and not df_py.empty:
                                            try:
                                                from analysis.embedding import cluster_year, align_yearly_clusters, unify_cluster_labels_llm
                                                # sampling guard similar to CY
                                                df_py_small = df_py.copy()
                                                max_rows = 8000
                                                if len(df_py_small) > max_rows:
                                                    df_py_small = df_py_small.sample(max_rows, random_state=42)
                                                    s.write(f"    â”” PY ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                                df_py_clu = cluster_year(
                                                    df_py_small, emb_client, embed_texts_fn=get_or_embed_texts
                                                )
                                                # ê°€ëŠ¥í•œ ê²½ìš° row_id ê¸°ì¤€ìœ¼ë¡œ PY ê²°ê³¼ ì»¬ëŸ¼ì„ df_pyì— ë³‘í•©
                                                if not df_py_clu.empty and 'row_id' in df_py.columns:
                                                    df_py = df_py.merge(df_py_clu, on='row_id', how='left', suffixes=("", "_pyclu"))
                                                # ì •ë ¬: PY í´ëŸ¬ìŠ¤í„°ë¥¼ CY í´ëŸ¬ìŠ¤í„°ì— ë§¤í•‘
                                                if 'cluster_id' in df_py_clu.columns:
                                                    mapping = align_yearly_clusters(df_clu, df_py_clu, sim_threshold=0.70)
                                                    # cluster_id â†’ (aligned_cy_cluster, aligned_sim)
                                                    cy_id_to_name = df_clu.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()
                                                    def _get_pair(cid):
                                                        try:
                                                            if pd.isna(cid):
                                                                return (np.nan, np.nan)
                                                            cid_int = int(cid)
                                                            return mapping.get(cid_int, (np.nan, np.nan))
                                                        except Exception:
                                                            return (np.nan, np.nan)
                                                    if 'cluster_id' in df_py.columns:
                                                        pairs = df_py['cluster_id'].map(_get_pair)
                                                        df_py[['aligned_cy_cluster', 'aligned_sim']] = pd.DataFrame(pairs.tolist(), index=df_py.index)
                                                        # ì´ë¦„ì€ CYì˜ ì´ë¦„ìœ¼ë¡œ ì •ë ¬(ê°€ëŠ¥í•œ ê²½ìš°)
                                                        df_py['cluster_name'] = df_py['aligned_cy_cluster'].map(cy_id_to_name).fillna(df_py.get('cluster_name'))
                                                # ìµœì¢… ë¼ë²¨ ì •í•©: ì „ì²´ ì´ë¦„ ì§‘í•© ê¸°ì¤€ìœ¼ë¡œ í†µí•©; CYì˜ cluster_groupì€ ìœ ì§€, PYëŠ” canonicalë¡œ ì •ë ¬
                                                try:
                                                    all_names = pd.Series([], dtype=object)
                                                    if 'cluster_name' in df_cy.columns:
                                                        all_names = pd.concat([all_names, df_cy['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    if 'cluster_name' in df_py.columns:
                                                        all_names = pd.concat([all_names, df_py['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    all_names = all_names.dropna().unique().tolist()
                                                    canon = unify_cluster_labels_llm(all_names, emb_client)
                                                    if canon:
                                                        if 'cluster_name' in df_cy.columns:
                                                            df_cy['cluster_name'] = df_cy['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_name' in df_py.columns:
                                                            df_py['cluster_name'] = df_py['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_group' in df_py.columns:
                                                            df_py['cluster_group'] = df_py['cluster_name']
                                                except Exception:
                                                    pass
                                            except Exception as e:
                                                s.write(f"    â”” PY í´ëŸ¬ìŠ¤í„°ë§/ì •ë ¬ ìŠ¤í‚µ: {e}")
                                        # ì»¨í…ìŠ¤íŠ¸ì— ë³„ë„ ë…¸íŠ¸ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                                        cl_ok = True
                                    else:
                                        s.write("    â”” LLM í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ â†’ ë³´ê³ ì„œ ìƒì„± ìš”ê±´ ë¯¸ì¶©ì¡±")
                                except Exception as e:
                                    s.write(f"    â”” ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
                            else:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§: LLM ë¯¸ê°€ìš© ë˜ëŠ” ì˜µì…˜ ë¹„í™œì„± â†’ ìŠ¤í‚µ")

                            # Step 3-1) (ì˜µì…˜ A) ê·¼ê±° ì¸ìš©(KNN)ìš© ì„ë² ë”©ë§Œ ìˆ˜í–‰ (LLM ê°€ëŠ¥ ì‹œ)
                            if LLM_OK and opt_knn_evidence:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš©ìš© ì„ë² ë”©(CY/PY)â€¦")
                                from analysis.embedding import perform_embedding_only, ensure_rich_embedding_text
                                emb_client2 = LLMClient().client
                                df_cy = ensure_rich_embedding_text(df_cy)
                                df_py = ensure_rich_embedding_text(df_py)
                                df_cy = perform_embedding_only(
                                    df_cy, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                                df_py = perform_embedding_only(
                                    df_py, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                            elif not LLM_OK:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš© ì„ë² ë”©: LLM ë¯¸ê°€ìš© â†’ ìŠ¤í‚µ")

                            # Step 3-2) Z-Score: ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ í•¨
                            s.write("â‘¢-2 Z-Score ê³„ì‚°/ê²€ì¦â€¦")
                            df_cy, z_ok = ensure_zscore(df_cy, pick_codes)
                            df_py, _    = ensure_zscore(df_py, pick_codes)  # ì „ê¸°ì—ë„ Z-Score ê³„ì‚°(ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©)
                            if not z_ok:
                                s.write("    â”” Z-Score ë¯¸ê³„ì‚° ë˜ëŠ” ì „ë¶€ ê²°ì¸¡")

                            # âœ… ê²Œì´íŠ¸ ì™„í™”: Z-Scoreë§Œ í™•ë³´ë˜ë©´ ë³´ê³ ì„œ ì§„í–‰.
                            #    (í´ëŸ¬ìŠ¤í„° ì‹¤íŒ¨ ì‹œ ê´€ë ¨ ì„¹ì…˜ì€ ìë™ ì¶•ì•½/ìƒëµ)
                            if not z_ok:
                                st.error("ë³´ê³ ì„œ ìƒì„± ì¤‘ë‹¨: Z-Score ì—†ìŒ.")
                                s.update(label="ë³´ê³ ì„œ ìš”ê±´ ë¯¸ì¶©ì¡±", state="error")
                                st.stop()
                            if not cl_ok:
                                s.write("    â”” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì—†ìŒ â†’ ë¦¬í¬íŠ¸ì—ì„œ í´ëŸ¬ìŠ¤í„° ì„¹ì…˜ì€ ìƒëµ/ì¶•ì•½ë©ë‹ˆë‹¤.")

                            # Step 4) ì»¨í…ìŠ¤íŠ¸ ìƒì„±(ì „ ëª¨ë“ˆ í¬í•¨) + ë°©ë²•ë¡  ë…¸íŠ¸
                            s.write("â‘£ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±(ì „ ëª¨ë“ˆ)â€¦")
                            from analysis.report_adapter import wrap_dfs_as_module_result
                            from analysis.report import generate_rag_context_from_modules
                            from analysis.integrity import run_integrity_module
                            from analysis.timeseries import run_timeseries_module

                            # (1) ì„¸ì…˜ ì´ˆê¸°í™” ë° ê³µí†µ ê°’ ì¤€ë¹„
                            st.session_state['modules'] = {}
                            lf_use = _lf_by_scope()
                            pm_use = float(st.session_state.get('pm_value', PM_DEFAULT))

                            # (2) ì£¼ìš” ëª¨ë“ˆ ì‹¤í–‰ ë° ìˆ˜ì§‘
                            if lf_use is not None:
                                # ì´ìƒì¹˜
                                try:
                                    amod = run_anomaly_module(lf_use, target_accounts=pick_codes or None,
                                                              topn=int(st.session_state.get('ctx_topk', 20)), pm_value=pm_use)
                                    _push_module(amod)
                                except Exception as _e:
                                    st.warning(f"anomaly ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ì¶”ì„¸(ì„ íƒ ê³„ì • í•„ìš”)
                                try:
                                    if pick_codes:
                                        _push_module(run_trend_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"trend ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ê±°ë˜ì²˜
                                try:
                                    if pick_codes:
                                        _push_module(run_vendor_module(lf_use, account_codes=pick_codes,
                                                                       min_amount=0.0, include_others=True))
                                except Exception as _e:
                                    st.warning(f"vendor ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ìƒê´€(2ê°œ ì´ìƒì¼ ë•Œë§Œ)
                                try:
                                    if len(pick_codes) >= 2:
                                        _push_module(run_correlation_module(lf_use, accounts=pick_codes,
                                                                            corr_threshold=0.70,
                                                                            cycles_map=get_effective_cycles()))
                                except Exception as _e:
                                    st.warning(f"correlation ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ì •í•©ì„±(ë ˆê±°ì‹œâ†’DTO)
                                try:
                                    _push_module(run_integrity_module(ldf, mdf))
                                except Exception as _e:
                                    st.warning(f"integrity ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # NEW: ì‹œê³„ì—´ í¬í•¨(ì§‘ê³„â†’DTO ë˜í•‘)
                                try:
                                    if not df_cy.empty:
                                        ts = df_cy.copy()
                                        ts["date"] = pd.to_datetime(ts["íšŒê³„ì¼ì"], errors="coerce").dt.to_period("M").dt.to_timestamp()
                                        ts["account"] = ts["ê³„ì •ì½”ë“œ"].astype(str)
                                        ts["amount"] = ts.get("ë°œìƒì•¡", 0.0).astype(float)
                                        ts_in = ts.groupby(["account","date"], as_index=False)["amount"].sum()
                                        df_ts = run_timeseries_module(ts_in, account_col="account", date_col="date", amount_col="amount",
                                                                      pm_value=pm_use, output="flow", make_balance=False)
                                        summ_ts = {
                                            "n_series": int(df_ts["account"].nunique()) if not df_ts.empty else 0,
                                            "n_points": int(len(df_ts)),
                                            "max_abs_z": float(df_ts["z"].abs().max()) if ("z" in df_ts.columns and not df_ts.empty) else 0.0,
                                        }
                                        _push_module(ModuleResult(name="timeseries", summary=summ_ts,
                                                                  tables={"ts": df_ts}, figures={}, evidences=[], warnings=[]))
                                except Exception as _e:
                                    st.warning(f"timeseries ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")

                            # (3) ë ˆê±°ì‹œ DFë„ ì–´ëŒ‘í„°ë¡œ í•¨ê»˜ í¬í•¨(ê²½ëŸ‰ ì»¨í…ìŠ¤íŠ¸ìš©)
                            mr_ctx = wrap_dfs_as_module_result(df_cy, df_py, name="report_ctx")
                            modules_list = list(st.session_state.get('modules', {}).values()) + [mr_ctx]
                            # (4) ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„±(Top-K ì ìš©) â€” ì‹ ê·œ ê²½ë¡œë§Œ ì‚¬ìš© + ë©”ëª¨ ì£¼ì…
                            ctx = generate_rag_context_from_modules(
                                modules_list,
                                pm_value=pm_use,
                                topk=int(st.session_state.get('ctx_topk', 20)),
                                manual_note=(manual_ctx or "")
                            )

                            # (ìƒë‹¨ ê³µí†µ ë¯¸ë¦¬ë³´ê¸°ë¡œ ëŒ€ì²´)
                            note = build_methodology_note(report_accounts=pick_codes)

                            # Step 5) LLM í˜¸ì¶œ ì „ ì ê²€(ê¸¸ì´/í† í°)
                            s.write("â‘¤ LLM í”„ë¡¬í”„íŠ¸ ì ê²€â€¦")
                            prompt_len = len(ctx) + len(note)
                            s.write(f"    â”” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {prompt_len:,} chars")
                            try:
                                import tiktoken
                                enc = tiktoken.get_encoding("cl100k_base")
                                est_tokens = len(enc.encode(ctx)) + len(enc.encode(note))
                                s.write(f"    â”” ì˜ˆìƒ í† í° ìˆ˜: ~{est_tokens:,} tokens")
                            except Exception:
                                s.write("    â”” tiktoken ë¯¸ì„¤ì¹˜: í† í° ì¶”ì • ìƒëµ")

                            # Step 6) ë³´ê³ ì„œ ìƒì„±: LLM ê°€ëŠ¥í•˜ë©´ ì‹œë„, ì‹¤íŒ¨/ë¶ˆê°€ ì‹œ ì˜¤í”„ë¼ì¸ í´ë°±
                            final_report = None
                            if LLM_OK:
                                s.write("â‘¥ LLM ìš”ì•½ ìƒì„± í˜¸ì¶œâ€¦")
                                try:
                                    t_llm0 = time.perf_counter()
                                    llm_client = LLMClient(model=st.session_state.get('llm_model'))
                                    final_report = run_final_analysis(
                                        context=ctx + "\n" + note,
                                        account_codes=pick_codes,
                                        model=st.session_state.get('llm_model'),
                                        max_tokens=int(st.session_state.get('llm_max_tokens', 16000)),
                                        generate_fn=llm_client.generate,
                                    )
                                    s.write(f"    â”” LLM ì™„ë£Œ (ê²½ê³¼ {time.perf_counter()-t_llm0:.1f}s)")
                                except Exception as e:
                                    s.write(f"    â”” LLM ì‹¤íŒ¨: {e} â†’ ì˜¤í”„ë¼ì¸ í´ë°±ìœ¼ë¡œ ì „í™˜")

                            if final_report is None:
                                s.write("â‘¥-í´ë°±: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±â€¦")
                                final_report = run_offline_fallback_report(
                                    current_df=df_cy,
                                    previous_df=df_py,
                                    account_codes=pick_codes,
                                    pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                                )

                            s.update(label="ë³´ê³ ì„œ ì¤€ë¹„ ì™„ë£Œ", state="complete")

                            # ê²°ê³¼ ì¶œë ¥ ë° ì„¸ì…˜ ë³´ì¡´
                            st.session_state['last_report'] = final_report
                            st.session_state['last_context'] = ctx + "\n" + note
                            st.session_state['last_dfcy'] = df_cy
                            st.session_state['last_dfpy'] = df_py

                            st.success("ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                            st.markdown(final_report)

                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])

                        # ZIP ë‹¨ì¼ ë‹¤ìš´ë¡œë“œ + RAW ë¯¸ë¦¬ë³´ê¸°
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()

                        raw_df = _build_raw_evidence(st.session_state['last_dfcy'])
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")

                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_current"  # ê³ ìœ  í‚¤(í˜„ì¬ ê²°ê³¼)
                        )

                        st.caption(f"â± ì´ ì†Œìš”: {time.perf_counter()-t0:.1f}s")
                        rendered_report = True

                    # === ìºì‹œëœ ì´ì „ ê²°ê³¼ ë Œë”(ë²„íŠ¼ ë¯¸í´ë¦­ ì‹œì—ë§Œ) ===
                    if st.session_state.get('last_report') and not btn:
                        st.success("ë³´ê³ ì„œê°€ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                        st.markdown(st.session_state['last_report'])
                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])
                        # RAW ë¯¸ë¦¬ë³´ê¸° + ZIP ë²„íŠ¼ ì¬ì¶œë ¥
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()
                        raw_df = _build_raw_evidence(st.session_state.get('last_dfcy', pd.DataFrame()))
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")
                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_cached"  # ê³ ìœ  í‚¤(ìºì‹œ ê²°ê³¼)
                        )
                        # (ê°€ëŠ¥ ì‹œ) í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì¹´ë“œ í‘œì‹œ
                        cq = st.session_state.get("cluster_quality")
                        if cq:
                            st.markdown("---")
                            st.subheader("í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ìš”ì•½")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("Noise rate", f"{cq['noise_rate']*100:.1f}%")
                            c2.metric("#Clusters", f"{cq['n_clusters']}")
                            c3.metric("Avg size", f"{cq['avg_size']:.1f}")
                            c4.metric("Rescued", f"{cq['rescued_rate']*100:.1f}%")
                            st.caption(f"Model: {cq['model']} | UMAP: {'on' if cq['umap'] else 'off'} | Ï„={cq['tau']:.2f} | N={cq['N']:,}")
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            if st.button("ë§¤í•‘ ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸°"):
                st.session_state.mapping_confirmed = False
                st.rerun()
else:
    st.info("â¬…ï¸ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")


