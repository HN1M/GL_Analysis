
==============================
📄 FILE: app.py
==============================

# app_v0.17.py (거래처 상세 분석 오류 수정)
# --- BEGIN: LLM 키 부팅 보장 ---
try:
    from infra.env_loader import boot as _llm_boot
    _llm_boot()  # 키 로드 + 상태 로그
except Exception as _e:
    # 최악의 경우에도 앱은 뜨게 하고, 상태를 stderr로만 알림
    import sys
    print(f"[env_loader] 초기화 실패: {_e}", file=sys.stderr)
# --- END: LLM 키 부팅 보장 ---

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
import plotly.graph_objects as go
from utils.helpers import find_column_by_keyword, add_provenance_columns, add_period_tag
from analysis.integrity import analyze_reconciliation, run_integrity_module
from analysis.contracts import LedgerFrame, ModuleResult
from analysis.trend import create_monthly_trend_figure, run_trend_module
from analysis.timeseries import (
    run_timeseries_module,          # ← 보고서 탭에서 계속 사용
    create_timeseries_figure        # ← 그래프 렌더 그대로 사용
)
from analysis.ts_v2 import (
    run_timeseries_minimal,
    compute_series_stats,     # ← NEW
    build_anomaly_table,      # ← NEW
    add_future_shading        # ← NEW (시각 음영)
)
from analysis.aggregation import aggregate_monthly, month_end_00
from analysis.anomaly import run_anomaly_module, compute_amount_columns
from analysis.correlation import run_correlation_module
from analysis.vendor import (
    create_pareto_figure,
    create_vendor_heatmap,
    create_vendor_detail_figure,
    run_vendor_module,
)
from analysis.report import run_final_analysis, build_methodology_note
from analysis.embedding import (
    ensure_rich_embedding_text,
    perform_embedding_and_clustering,
    perform_embedding_only,
    unify_cluster_names_with_llm,
)
from analysis.anomaly import calculate_grouped_stats_and_zscore
from services.llm import LLMClient
from services.cache import get_or_embed_texts
import services.cycles_store as cyc
from config import EMB_USE_LARGE_DEFAULT, HDBSCAN_RESCUE_TAU, EMB_MODEL_SMALL
try:
    from config import PM_DEFAULT
except Exception:
    PM_DEFAULT = 500_000_000
from utils.viz import add_materiality_threshold, add_pm_badge
from services.cluster_naming import (
    make_synonym_confirm_fn,
    unify_cluster_labels_llm,
)

# --- KRW 입력(천단위 콤마) 유틸: 콜백 기반으로 안정화 ---
def _krw_input(label: str, key: str, *, default_value: int, help_text: str = "") -> int:
    """
    한국 원화 입력 위젯(천단위 콤마). 핵심 규칙:
    1) 위젯 키(pm_value__txt 등)를 런 루프에서 직접 대입하지 않는다.
    2) 콤마 재포맷은 on_change 콜백 안에서만 수행한다.
    3) 분석에 쓰는 정수 값은 st.session_state[key]에 보관한다.
    """
    txt_key = f"{key}__txt"  # 실제 text_input 위젯이 바인딩되는 키

    # 초기 셋업: 숫자/문자 상태를 위젯 생성 전에 준비
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if txt_key not in st.session_state:
        st.session_state[txt_key] = f"{int(st.session_state[key]):,}"

    # 콜백: 포커스 아웃/Enter 시 콤마 포맷을 적용하고 숫자 상태를 동기화
    def _on_blur_format():
        raw_now = st.session_state.get(txt_key, "")
        digits = re.sub(r"[^\d]", "", str(raw_now or ""))
        val = int(digits) if digits else 0
        if val < 0:
            val = 0
        st.session_state[key] = int(val)            # 분석에 쓰는 정수 상태
        st.session_state[txt_key] = f"{int(val):,}"  # 위젯 표시 텍스트(콤마)

    # 위젯 생성
    raw = st.text_input(
        label,
        value=st.session_state[txt_key],
        key=txt_key,
        help=help_text,
        placeholder="예: 500,000,000",
        on_change=_on_blur_format,
    )

    # 라이브 타이핑 동안에도 그래프가 즉시 반영되도록 정수 상태만 업데이트(위젯 키는 건드리지 않음)
    digits_live = re.sub(r"[^\d]", "", str(raw or ""))
    live_val = int(digits_live) if digits_live else 0
    if live_val < 0:
        live_val = 0
    st.session_state[key] = int(live_val)

    return int(st.session_state[key])


# 사이클 프리셋을 계정 선택기로 주입하는 헬퍼
def _apply_cycles_to_picker(*, upload_id: str, cycles_state_key: str, accounts_state_key: str, master_df: pd.DataFrame):
    """선택된 사이클의 계정들을 계정 멀티셀렉트에 합쳐 넣어준다."""
    cycles_map = cyc.get_effective_cycles(upload_id)
    chosen_cycles = st.session_state.get(cycles_state_key, []) or []
    # 지원: KO 라벨 또는 코드 라벨 — 공식 KO 라벨 집합 기준으로 판별
    KO_LABELS = set(cyc.CYCLE_KO.values())
    if chosen_cycles and all(lbl in KO_LABELS for lbl in chosen_cycles):
        codes = cyc.accounts_for_cycles_ko(cycles_map, chosen_cycles)
    else:
        codes = cyc.accounts_for_cycles(cycles_map, chosen_cycles)
    names = (master_df[master_df['계정코드'].astype(str).isin(codes)]['계정명']
                .dropna().astype(str).unique().tolist())
    cur = set(st.session_state.get(accounts_state_key, []) or [])
    st.session_state[accounts_state_key] = sorted(cur.union(names))


# --- NEW: Correlation UI helpers (DRY) ---
from analysis.correlation import (
    run_correlation_module,
    run_correlation_focus_module as run_corr_focus,
    friendly_correlation_explainer,
    suggest_anchor_accounts,
)
from config import CORR_THRESHOLD_DEFAULT, CORR_MIN_ACTIVE_MONTHS_DEFAULT

def _render_corr_basic_tab(*, upload_id: str):
    """
    기본 상관관계 분석(히트맵/강한 상관쌍/제외계정)을 렌더합니다.
    - 기존 '데이터 무결성 및 흐름' 탭의 구현을 그대로 옮겨, 상관 탭의 '기본' 서브탭에서 사용.
    - state key는 'corr_basic_*' 네임스페이스로 충돌 방지.
    """
    import services.cycles_store as cyc
    mdf = st.session_state.master_df
    acct_names = sorted(mdf['계정명'].dropna().astype(str).unique().tolist())
    st.subheader("계정 간 상관 히트맵(기본)")
    # 버퍼 적용(위젯 생성 전)
    if st.session_state.get("corr_basic_accounts_needs_update") and st.session_state.get("corr_basic_accounts_buf"):
        st.session_state["corr_basic_accounts"] = list(st.session_state["corr_basic_accounts_buf"])  
        st.session_state["corr_basic_accounts_needs_update"] = False

    # 대상 계정 선택 위젯
    picked_accounts = st.multiselect(
        "상관 분석 대상 계정(2개 이상 선택)",
        acct_names,
        default=[],
        help="선택한 계정들 간 월별 흐름의 피어슨 상관을 계산합니다.",
        key="corr_basic_accounts"
    )

    # 사이클 프리셋(대상 계정 선택 하단)
    cycles_map_now = cyc.get_effective_cycles(upload_id)
    if cycles_map_now:
        picked_cycles = st.multiselect(
            "사이클 프리셋 선택", list(cyc.CYCLE_KO.values()),
            default=[], key="corr_basic_cycles"
        )
        if st.button("➕ 프리셋 적용", key="btn_apply_cycles_corr_basic"):
            mapping = cyc.get_effective_cycles(upload_id)
            codes = cyc.accounts_for_cycles_ko(mapping, picked_cycles)
            code_to_name = (
                mdf[['계정코드','계정명']].assign(계정코드=lambda d: d['계정코드'].astype(str)).drop_duplicates()
                  .set_index('계정코드')['계정명'].astype(str).to_dict()
            )
            cur_set = set(st.session_state.get("corr_basic_accounts", []))
            cur_set.update({code_to_name.get(c, c) for c in codes})
            st.session_state["corr_basic_accounts_buf"] = sorted(cur_set)
            st.session_state["corr_basic_accounts_needs_update"] = True
            st.rerun()
    corr_thr = st.slider(
        "상관 임계치(강한 상관쌍 표 전용)",
        min_value=0.50, max_value=0.95, step=0.05, value=0.70,
        help="절대값 기준 임계치 이상인 계정쌍만 표에 표시합니다.",
        key="corr_basic_thr"
    )

    if len(picked_accounts) < 2:
        st.info("계정을 **2개 이상** 선택하면 히트맵이 표시됩니다.")
        return

    # 스코프 적용된 LedgerFrame을 재사용
    lf_use = _lf_by_scope()

    # 계정명 → 코드
    codes = (
        mdf[mdf['계정명'].isin(picked_accounts)]['계정코드']
        .astype(str).tolist()
    )
    cmod = run_correlation_module(
        lf_use,
        accounts=codes,
        corr_threshold=float(corr_thr),
        cycles_map=cyc.get_effective_cycles(upload_id),
    )
    _push_module(cmod)
    for w in cmod.warnings:
        st.warning(w)

    # 히트맵(+호버 계정명)
    if 'heatmap' in cmod.figures:
        fig = cmod.figures['heatmap']
        try:
            name_map = dict(zip(
                mdf["계정코드"].astype(str),
                mdf["계정명"].astype(str)
            ))
            tr = fig.data[0]
            x_codes = list(map(str, getattr(tr, 'x', [])))
            y_codes = list(map(str, getattr(tr, 'y', [])))
            x_names = [name_map.get(c, c) for c in x_codes]
            y_names = [name_map.get(c, c) for c in y_codes]
            tr.update(x=x_names, y=y_names)
            fig.update_traces(hovertemplate="계정: %{y} × %{x}<br>상관계수: %{z:.3f}<extra></extra>")
        except Exception:
            pass
        st.plotly_chart(fig, use_container_width=True, key=f"corr_basic_heatmap_{'_'.join(codes)}_{int(corr_thr*100)}")

    # 임계치 이상 상관쌍
    if 'strong_pairs' in cmod.tables and not cmod.tables['strong_pairs'].empty:
        st.markdown("**임계치 이상 상관쌍**")
        st.dataframe(cmod.tables['strong_pairs'], use_container_width=True, height=320)

    # 제외된 계정
    if 'excluded_accounts' in cmod.tables and not cmod.tables['excluded_accounts'].empty:
        with st.expander("제외된 계정 보기(변동없음/활동월 부족)", expanded=False):
            exc = cmod.tables['excluded_accounts'].copy()
            if '계정코드' in exc.columns:
                name_map = dict(zip(
                    mdf["계정코드"].astype(str),
                    mdf["계정명"].astype(str)
                ))
                exc['계정코드'] = exc['계정코드'].astype(str)
                exc['계정명'] = exc['계정코드'].map(name_map)
                cols = ['계정명', '계정코드'] + [c for c in exc.columns if c not in ('계정명','계정코드')]
                exc = exc[cols]
            st.dataframe(exc, use_container_width=True)


def _render_corr_advanced_tab(*, upload_id: str):
    """
    고급 상관관계 분석(방법/시차/롤링 안정성 등)을 렌더합니다.
    - 기존 '상관관계(고급)' 탭의 코드를 서브탭용 함수로 모듈화.
    - state key는 기존 'corr_adv_*' 유지(호환).
    """
    import services.cycles_store as cyc
    st.subheader("고급 상관관계")
    lf_adv = _lf_by_scope()  # 스코프 일관성 유지
    if lf_adv is None:
        st.info("원장을 먼저 업로드해 주세요.")
        return

    mdf_adv = st.session_state.master_df
    acct_names_adv = sorted(mdf_adv['계정명'].dropna().astype(str).unique().tolist())
    # 버퍼 적용(위젯 생성 전)
    if st.session_state.get("corr_adv_accounts_needs_update") and st.session_state.get("corr_adv_accounts_buf"):
        st.session_state["corr_adv_accounts"] = list(st.session_state["corr_adv_accounts_buf"])  
        st.session_state["corr_adv_accounts_needs_update"] = False

    # 대상 계정 선택
    picked_accounts_adv = st.multiselect("분석 계정(다중 선택)", options=acct_names_adv, key="corr_adv_accounts")
    
    # 사이클 프리셋(대상 계정 선택 하단)
    picked_cycles_adv = st.multiselect("사이클 프리셋(선택 시 계정 자동 반영)", options=list(cyc.CYCLE_KO.values()), key="corr_adv_cycles")
    if st.button("프리셋 적용", key="btn_apply_preset_corr_adv"):
        mapping = cyc.get_effective_cycles(upload_id)
        codes = cyc.accounts_for_cycles_ko(mapping, picked_cycles_adv)
        code_to_name = (
            mdf_adv[['계정코드','계정명']].assign(계정코드=lambda d: d['계정코드'].astype(str)).drop_duplicates()
                .set_index('계정코드')['계정명'].astype(str).to_dict()
        )
        cur_set = set(st.session_state.get("corr_adv_accounts", []))
        cur_set.update({code_to_name.get(c, c) for c in codes})
        st.session_state["corr_adv_accounts_buf"] = sorted(cur_set)
        st.session_state["corr_adv_accounts_needs_update"] = True
        st.rerun()

    method = st.selectbox("상관 방식", ["pearson", "spearman", "kendall"], index=0, key="corr_adv_method")
    corr_threshold = st.slider("임계치(|r|)", 0.1, 0.95, 0.70, 0.05, key="corr_adv_thr")
    c1, c2 = st.columns(2)
    with c1:
        max_lag = st.slider("최대 시차(개월)", 0, 12, 6, 1, key="corr_adv_maxlag")
    with c2:
        rolling_window = st.slider("롤링 윈도우(개월)", 3, 24, 6, 1, key="corr_adv_rollwin")

    if st.button("분석 실행", key="run_corr_adv"):
        try:
            from analysis.corr_advanced import run_corr_advanced as run_corr_adv
            # ✅ UI(계정명) → 코드 변환
            _names = st.session_state.get("corr_adv_accounts", picked_accounts_adv) or []
            _codes = (
                mdf_adv[mdf_adv['계정명'].isin(_names)]['계정코드']
                .astype(str).drop_duplicates().tolist()
            )
            mr = run_corr_adv(
                lf_adv,
                _codes,
                method=st.session_state.get("corr_adv_method", "pearson"),
                corr_threshold=float(st.session_state.get("corr_adv_thr", 0.70)),
                max_lag=int(st.session_state.get("corr_adv_maxlag", 6)),
                rolling_window=int(st.session_state.get("corr_adv_rollwin", 6)),
            )
            st.subheader("히트맵")
            if "heatmap" in mr.figures:
                st.plotly_chart(mr.figures["heatmap"], use_container_width=True)
            if "strong_pairs" in mr.tables:
                st.subheader("임계치 이상 상관쌍")
                st.dataframe(mr.tables["strong_pairs"], use_container_width=True)
            if "lagged_pairs" in mr.tables:
                st.subheader("최적 시차 상관(Top)")
                st.dataframe(mr.tables["lagged_pairs"], use_container_width=True)
            if "rolling_stability" in mr.tables:
                st.subheader("롤링 안정성(변동성 낮은 순)")
                st.dataframe(mr.tables["rolling_stability"], use_container_width=True)
        except Exception as _e:
            st.warning(f"고급 상관 분석 실패: {_e}")

# --- NEW: Focus Tab (단일계정) ---
def _render_corr_focus_tab(*, upload_id: str):
    import services.cycles_store as cyc
    from analysis.correlation import run_correlation_focus_module
    st.subheader("단일 계정(포커스) 상관")
    lf_use = _lf_by_scope()
    if lf_use is None or lf_use.df.empty:
        st.info("분석할 데이터가 없습니다."); return
    mdf = st.session_state.master_df
    acct_names = sorted(mdf['계정명'].dropna().astype(str).unique().tolist())
    col1, col2 = st.columns([2,1])
    with col1:
        focus_name = st.selectbox("포커스 계정(1개)", options=["선택하세요..."]+acct_names, index=0, key="corr_focus_name")
    with col2:
        within = st.checkbox("동일 사이클 내에서만", value=True, help="선택 시 같은 사이클에 속한 계정들만 비교합니다.", key="corr_focus_within")
    if focus_name and focus_name != "선택하세요...":
        code = (mdf[mdf['계정명']==focus_name]['계정코드'].astype(str).head(1).tolist() or [""])[0]
        mapping = cyc.get_effective_cycles(upload_id)
        mr = run_correlation_focus_module(lf_use, focus_account=focus_name, cycles_map=mapping, within_same_cycle=bool(within))
        _push_module(mr)
        for w in mr.warnings: st.warning(w)
        if "bar" in mr.figures: st.plotly_chart(mr.figures["bar"], use_container_width=True)
        if "focus_corr" in mr.tables and not mr.tables["focus_corr"].empty:
            st.dataframe(mr.tables["focus_corr"], use_container_width=True, height=_auto_table_height(mr.tables["focus_corr"]))


# --- 3. UI 부분 ---
st.set_page_config(page_title="AI 분석 시스템 v0.18", layout="wide")
st.title("훈's GL분석 시스템")
st.markdown("---")

for key in ['mapping_confirmed', 'analysis_done']:
    if key not in st.session_state:
        st.session_state[key] = False

# --- NEW: 모듈 결과 수집용 컨테이너 ---
if 'modules' not in st.session_state:
    st.session_state['modules'] = {}

def _push_module(mod: ModuleResult):
    """ModuleResult를 세션에 수집(동명 모듈은 최신으로 교체)."""
    try:
        if mod and getattr(mod, "name", None):
            st.session_state['modules'][str(mod.name)] = mod
    except Exception:
        pass


# (removed) number_input 기반 대체 구현: 쉼표 미표시·키 충돌 유발 가능성 → 단일 구현으로 통일

with st.sidebar:
    st.header("1. 데이터 준비")
    uploaded_file = st.file_uploader("분석할 엑셀 파일을 올려주세요.", type=["xlsx", "xlsm"])
    if 'last_file' not in st.session_state or st.session_state.last_file != uploaded_file:
        st.session_state.mapping_confirmed = False
        st.session_state.analysis_done = False
        st.session_state.last_file = uploaded_file

    st.markdown("---")
    st.header("2. 분석 기간")
    default_scope = st.session_state.get("period_scope", "당기")
    st.session_state.period_scope = st.radio(
        "분석 스코프(트렌드 제외):",
        options=["당기", "당기+전기"],
        index=["당기","당기+전기"].index(default_scope),
        horizontal=True,
        help="상관/거래처/이상치 모듈에 적용됩니다. 트렌드는 설계상 CY vs PY 비교 유지."
    )
    st.markdown("---")
    st.header("3. Embedding / Clustering")
    st.session_state.use_large_embedding = st.toggle(
        "Use Large Embedding (cost ↑)",
        value=st.session_state.get("use_large_embedding", EMB_USE_LARGE_DEFAULT),
        help="Large model improves semantics but is slower and more expensive."
    )
    st.session_state.rescue_tau = st.slider(
        "Noise rescue τ (cosine)",
        min_value=0.60, max_value=0.90, step=0.01,
        value=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
        help="Reassign -1 (noise) to nearest cluster if similarity ≥ τ."
    )
    st.markdown("---")
    st.header("4. Materiality")
    pm_val = _krw_input(
        "Performance Materiality (KRW)",
        key="pm_value",
        default_value=PM_DEFAULT,
        help_text="Used for KIT (PM exceed) and integrated risk scoring."
    )
    st.caption("ⓘ The PM threshold is drawn as a red dotted line on applicable charts. "
               "Y-axis scaling may change to accommodate this line.")

    # 🧹 캐시 관리
    with st.expander("🧹 캐시 관리", expanded=False):
        if st.button("임베딩 캐시 비우기"):
            import shutil
            from services.cache import _model_dir
            for m in ["text-embedding-3-small", "text-embedding-3-large"]:
                try:
                    shutil.rmtree(_model_dir(m), ignore_errors=True)
                except Exception as e:
                    st.warning(f"{m} 삭제 실패: {e}")
            st.success("임베딩 캐시 삭제 완료")

        if st.button("데이터 캐시 비우기"):
            st.cache_data.clear()
            st.success("Streamlit 데이터 캐시 삭제 완료")

        if st.button("캐시 정보 보기"):
            from services.cache import get_cache_info
            try:
                st.write(get_cache_info("text-embedding-3-small"))
                st.write(get_cache_info("text-embedding-3-large"))
            except Exception as e:
                st.info(f"정보 조회 실패: {e}")


@st.cache_data(show_spinner=False)
def _read_excel(_file, sheet_name=None):
    return pd.read_excel(_file, sheet_name=sheet_name)


@st.cache_data(show_spinner=False)
def _read_xls(_file):
    # pickle 직렬화 가능한 타입만 캐시 → 시트명 리스트로 반환
    return pd.ExcelFile(_file).sheet_names

# (removed duplicated definition) _krw_input — 위의 단일 버전만 유지

def _apply_scope(df: pd.DataFrame, scope: str) -> pd.DataFrame:
    """스코프 적용 시 결측 컬럼 방어: 'period_tag' 미존재면 원본 반환.
    df.get('period_tag','')가 문자열을 반환할 경우 .eq 호출 AttributeError를 방지한다.
    """
    if df is None or df.empty or 'period_tag' not in df.columns:
        return df
    if scope == "당기":
        return df[df['period_tag'].eq('CY')]
    if scope == "당기+전기":
        return df[df['period_tag'].isin(['CY', 'PY'])]
    return df

# === 공용: 표 높이 자동 제한(행 수 기반) ===
def _auto_table_height(df: pd.DataFrame, max_rows: int = 8,
                       row_px: int = 28, header_px: int = 38, pad_px: int = 12) -> int:
    """
    표 높이를 '표시 행 수' 기준으로 계산해 넘기기 위한 유틸.
    - max_rows: 최대 표시 행수
    - 실패 시 300px로 폴백
    """
    try:
        n = int(min(max(len(df), 1), max_rows))
        return int(header_px + n * row_px + pad_px)
    except Exception:
        return 300

def _lf_by_scope() -> LedgerFrame:
    """상관/거래처/이상치에서 사용할 스코프 적용 LedgerFrame."""
    hist = st.session_state.get('lf_hist')
    scope = st.session_state.get('period_scope', '당기')
    if hist is None:
        return None
    return LedgerFrame(df=_apply_scope(hist.df, scope), meta=hist.meta)

# (removed) 구버전 텍스트입력 + ±step / ✖reset 변형들 — 사용자 요청으로 버튼류 삭제 및 단일화


if uploaded_file is not None:
    if not st.session_state.mapping_confirmed:
        # ... 컬럼 매핑 UI ...
        try:
            st.info("2단계: 엑셀의 컬럼을 분석 표준 필드에 맞게 지정해주세요.")
            sheet_names = _read_xls(uploaded_file)
            first_ledger_sheet = next((s for s in sheet_names if 'ledger' in s.lower()), None)
            if first_ledger_sheet is None:
                st.error("오류: 'Ledger' 시트를 찾을 수 없습니다.")
                st.stop()
            ledger_cols = _read_excel(uploaded_file, sheet_name=first_ledger_sheet).columns.tolist()
            ledger_map = {}
            st.markdown("#### **Ledger 시트** 항목 매핑")
            cols = st.columns(6)
            ledger_fields = {'회계일자': '일자', '계정코드': '계정코드', '거래처': '거래처', '적요': '적요', '차변': '차변', '대변': '대변'}
            for i, (key, keyword) in enumerate(ledger_fields.items()):
                with cols[i]:
                    is_optional = key == '거래처'
                    default_col = find_column_by_keyword(ledger_cols, keyword)
                    options = ['선택 안 함'] + ledger_cols if is_optional else ledger_cols
                    default_index = options.index(default_col) if default_col in options else 0
                    ledger_map[key] = st.selectbox(f"**'{key}'** 필드 선택", options, index=default_index, key=f"map_ledger_{key}")
            st.markdown("---")
            st.markdown("#### **Master 시트** 항목 매핑")
            master_cols = _read_excel(uploaded_file, sheet_name='Master').columns.tolist()
            master_map = {}
            cols = st.columns(7)
            master_fields = {'계정코드': '계정코드', '계정명': '계정명', 'BS/PL': 'BS/PL', '차변/대변': '차변/대변', '당기말잔액': '당기말', '전기말잔액': '전기말', '전전기말잔액': '전전기말'}
            for i, (key, keyword) in enumerate(master_fields.items()):
                with cols[i]:
                    default_col = find_column_by_keyword(master_cols, keyword)
                    default_index = master_cols.index(default_col) if default_col in master_cols else 0
                    master_map[key] = st.selectbox(f"**'{key}'** 필드 선택", master_cols, index=default_index, key=f"map_master_{key}")
            if st.button("✅ 매핑 확인 및 데이터 처리", type="primary"):
                st.session_state.ledger_map = ledger_map
                st.session_state.master_map = master_map
                st.session_state.mapping_confirmed = True
                st.rerun()
        except Exception as e:
            st.error(f"엑셀 파일의 컬럼을 읽는 중 오류가 발생했습니다: {e}")

    else:  # 매핑 확인 후
        try:
            ledger_map, master_map = st.session_state.ledger_map, st.session_state.master_map
            master_df = _read_excel(uploaded_file, sheet_name='Master')
            sheet_names = _read_xls(uploaded_file)
            ledger_sheets = [s for s in sheet_names if 'ledger' in s.lower()]
            if not ledger_sheets:
                st.error("오류: 'Ledger' 시트를 찾을 수 없습니다.")
                st.stop()
            all_parts = []
            for s in ledger_sheets:
                part = _read_excel(uploaded_file, sheet_name=s)
                part['source_sheet'] = s
                part = add_provenance_columns(part)
                all_parts.append(part)
            ledger_df = pd.concat(all_parts, ignore_index=True)
            # row_id: 파일명|시트:행  (세션/재실행에도 안정)
            try:
                base = Path(getattr(uploaded_file, "name", "uploaded.xlsx")).stem
                if 'row_id' in ledger_df.columns:
                    ledger_df['row_id'] = base + "|" + ledger_df['row_id'].astype(str)
            except Exception:
                pass
            ledger_df.rename(columns={v: k for k, v in ledger_map.items() if v != '선택 안 함'}, inplace=True)
            master_df.rename(columns={v: k for k, v in master_map.items()}, inplace=True)

            # 🔧 병합 전에 타입/포맷을 먼저 통일
            for df_ in [ledger_df, master_df]:
                if '계정코드' in df_.columns:
                    df_['계정코드'] = (
                        df_['계정코드']
                        .astype(str)
                        .str.replace(r'\.0$', '', regex=True)
                        .str.strip()
                    )

            master_essentials = master_df[['계정코드', '계정명']].drop_duplicates()
            ledger_df = pd.merge(ledger_df, master_essentials, on='계정코드', how='left')
            ledger_df['계정명'] = ledger_df['계정명'].fillna('미지정 계정')

            ledger_df['회계일자'] = pd.to_datetime(ledger_df['회계일자'], errors='coerce')
            ledger_df.dropna(subset=['회계일자'], inplace=True)
            for col in ['차변', '대변']:
                ledger_df[col] = pd.to_numeric(ledger_df[col], errors='coerce').fillna(0)
            for col in ['당기말잔액', '전기말잔액', '전전기말잔액']:
                if col in master_df.columns:
                    master_df[col] = pd.to_numeric(master_df[col], errors='coerce').fillna(0)
                else:
                    master_df[col] = 0
            ledger_df['거래금액'] = ledger_df['차변'] - ledger_df['대변']
            ledger_df['거래금액_절대값'] = abs(ledger_df['거래금액'])
            ledger_df['연도'] = ledger_df['회계일자'].dt.year
            ledger_df['월'] = ledger_df['회계일자'].dt.month
            # ✅ 분석 규칙: 계정 서브셋 분석 시에도 전체 히스토리를 확보하기 위한 편의 파생
            ledger_df['연월'] = ledger_df['회계일자'].dt.to_period('M').astype(str)
            # ✅ period_tag 추가(CY/PY/Other)
            ledger_df = add_period_tag(ledger_df)
            if '거래처' not in ledger_df.columns:
                ledger_df['거래처'] = '정보 없음'
            ledger_df['거래처'] = ledger_df['거래처'].fillna('정보 없음').astype(str)

            if st.button("🚀 전체 분석 실행", type="primary"):
                with st.spinner('데이터를 분석 중입니다...'):
                    # ✅ 정합성은 사용자 기간 선택과 무관하게 전체 기준으로 계산
                    st.session_state.recon_status, st.session_state.recon_df = analyze_reconciliation(ledger_df, master_df)
                    # ✅ 표준 LedgerFrame 구성(정합성은 항상 전체 기준: DF_hist)
                    lf_hist = LedgerFrame(df=ledger_df, meta={
                        "file_name": getattr(uploaded_file, "name", "uploaded.xlsx"),
                        "master_df": master_df,
                    })
                    # 초기엔 focus=hist (후속 단계에서 사용자 필터 연결)
                    lf_focus = lf_hist

                    st.session_state.master_df = master_df
                    st.session_state.ledger_df = ledger_df
                    st.session_state.lf_hist = lf_hist
                    st.session_state.lf_focus = lf_focus
                    st.session_state.analysis_done = True
                st.rerun()

            if st.session_state.analysis_done:
                st.success("✅ 분석이 완료되었습니다. 아래 탭에서 결과를 확인하세요.")
                # --- 계정→사이클 매핑 검토/수정 ---
                upload_id = getattr(uploaded_file, "name", "uploaded.xlsx")
                # 업로드 직후 1회: 프리셋 없으면 룰베이스 자동 생성
                names_dict = (
                    master_df[['계정코드','계정명']]
                        .drop_duplicates()
                        .assign(계정코드=lambda d: d['계정코드'].astype(str))
                        .set_index('계정코드')['계정명'].astype(str).to_dict()
                )
                if not cyc.get_effective_cycles(upload_id):
                    cyc.build_cycles_preset(upload_id, names_dict, use_llm=False)

                with st.expander("🧭 계정 → 사이클 매핑 검토/수정", expanded=False):
                    cur_map = cyc.get_effective_cycles(upload_id)
                    map_df = master_df[['계정코드','계정명']].drop_duplicates().copy()
                    map_df['계정코드'] = map_df['계정코드'].astype(str)
                    map_df['사이클(표시)'] = map_df['계정코드'].map(lambda c: cyc.code_to_ko(cur_map.get(c, 'Other')))

                    st.caption("사이클 라벨을 수정한 뒤 저장을 누르세요. (표시는 한글, 내부는 코드로 저장됩니다)")
                    edited = st.data_editor(
                        map_df, hide_index=True, use_container_width=True,
                        column_config={
                            "사이클(표시)": st.column_config.SelectboxColumn(
                                options=list(cyc.CYCLE_KO.values()), required=True
                            )
                        }
                    )

                    c1, c2, c3 = st.columns(3)
                    with c1:
                        if st.button("💾 매핑 저장", type="primary", key="btn_save_cycles_map"):
                            new_map_codes = {
                                str(r['계정코드']): cyc.ko_to_code(r['사이클(표시)'])
                                for _, r in edited.iterrows()
                            }
                            cyc.set_cycles_map(upload_id, new_map_codes)
                            st.success(f"저장됨: {len(new_map_codes):,}개 계정")
                    with c2:
                        if st.button("🤖 LLM 추천 병합", help="룰베이스 결과 위에 LLM 제안을 덮어씌웁니다", key="btn_merge_llm_cycles"):
                            cyc.build_cycles_preset(upload_id, names_dict, use_llm=True)
                            st.success("LLM 추천을 병합했습니다.")
                            st.rerun()
                    with c3:
                        if st.button("↺ 룰베이스로 초기화", key="btn_reset_rule_cycles"):
                            cyc.build_cycles_preset(upload_id, names_dict, use_llm=False)
                            st.success("룰베이스로 초기화했습니다.")
                            st.rerun()
                with st.expander("🔍 빠른 진단(데이터 품질 체크)", expanded=False):
                    df = st.session_state.ledger_df.copy()
                    issues = []

                    invalid_date = int(df['회계일자'].isna().sum())
                    if invalid_date > 0:
                        issues.append(f"❗ 유효하지 않은 날짜(NaT): {invalid_date:,}건")

                    if '거래처' in df.columns:
                        missing_vendor = int((df['거래처'].isna() | (df['거래처'] == '정보 없음')).sum())
                        if missing_vendor > 0:
                            issues.append(f"ℹ️ 거래처 정보 없음/결측: {missing_vendor:,}건")

                    zero_abs = int((df['거래금액_절대값'] == 0).sum())
                    issues.append(f"ℹ️ 금액 절대값이 0인 전표: {zero_abs:,}건")

                    unlinked = int(df['계정명'].eq('미지정 계정').sum())
                    if unlinked > 0:
                        issues.append(f"❗ Master와 매칭되지 않은 전표(계정명 미지정): {unlinked:,}건")

                    st.write("**체크 결과**")
                    if issues:
                        for line in issues:
                            st.write("- " + line)
                    else:
                        st.success("문제 없이 깔끔합니다!")
                tab_integrity, tab_vendor, tab_anomaly, tab_ts, tab_report, tab_corr = st.tabs(["🌊 데이터 무결성 및 흐름", "🏢 거래처 심층 분석", "🔬 이상 패턴 탐지", "📉 시계열 예측", "🧠 분석 종합 대시보드", "📊 상관관계"])

                # (이전 버전) 대시보드 탭은 사용자 요청으로 제거됨
                with tab_integrity:  # ...
                    st.header("데이터 무결성 및 흐름")
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')}")
                    st.subheader("1. 데이터 정합성 검증 결과")
                    mod = st.session_state.get('modules', {}).get('integrity')
                    result_df = (getattr(mod, 'tables', {}) or {}).get('reconciliation') if mod else st.session_state.get('recon_df')
                    
                    # === 표 데이터와 직접 연동된 배너 로직 ===
                    def render_integrity_banner(df):
                        if df is None or df.empty:
                            st.info("검증할 데이터가 없습니다.")
                            return
                        
                        # 1) '차이'를 안전하게 수치화
                        if "차이" in df.columns:
                            diff = pd.to_numeric(df["차이"].astype(str).str.replace(",", ""), errors="coerce").fillna(0)
                        else:
                            diff = pd.Series([0] * len(df))
                        
                        # 2) 허용 오차(1원) 이하를 0으로 간주
                        tol = 1.0
                        fails_mask = diff.abs() > tol
                        
                        # 3) '상태'가 있으면 교차 확인(방어적)
                        if "상태" in df.columns:
                            fails_mask = fails_mask | df["상태"].astype(str).str.lower().eq("fail")
                        
                        n_fail = int(fails_mask.sum())
                        
                        if n_fail > 0:
                            max_gap = float(diff.abs().max())
                            st.warning(f"❌ 불일치 계정 {n_fail}건 발견 · 최대 차이: {max_gap:,.0f}")
                        else:
                            st.success("✅ 모든 계정의 데이터가 일치합니다.")
                    
                    render_integrity_banner(result_df)

                    def highlight_status(row):
                        if row.상태 == 'Fail':
                            return ['background-color: #ffcccc'] * len(row)
                        elif row.상태 == 'Warning':
                            return ['background-color: #fff0cc'] * len(row)
                        return [''] * len(row)

                    format_dict = {col: '{:,.0f}' for col in result_df.select_dtypes(include=np.number).columns}
                    st.dataframe(result_df.style.apply(highlight_status, axis=1).format(format_dict), use_container_width=True)
                    st.markdown("---")
                    st.subheader("2. 계정별 월별 추이 (PY vs CY)")
                    # ✅ 자동 추천 제거: 사용자가 계정을 선택한 경우에만 그래프 렌더
                    account_list = st.session_state.master_df['계정명'].unique()
                    selected_accounts = st.multiselect(
                        "분석할 계정을 선택하세요 (1개 이상 필수)",
                        account_list, default=[],
                        key="trend_accounts_pick"
                    )
                    # ▼ 사이클 프리셋(선택 시 위 멀티셀렉트에 계정 자동 반영)
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles = st.multiselect(
                            "사이클 프리셋 선택(선택하면 위 계정 목록에 자동 반영)",
                            list(cyc.CYCLE_KO.values()),
                            default=[], key="trend_cycles_pick"
                        )
                        st.button("➕ 프리셋 적용", key="btn_apply_cycles_trend", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="trend_cycles_pick",
                                              accounts_state_key="trend_accounts_pick",
                                              master_df=st.session_state.master_df))
                    if not selected_accounts:
                        st.info("계정을 1개 이상 선택하면 월별 추이 그래프가 표시됩니다.")
                    else:
                        lf_use = st.session_state.get('lf_focus') or st.session_state.get('lf_hist')
                        # 선택된 계정명을 계정코드로 변환
                        mdf = st.session_state.master_df
                        accounts_codes = (
                            mdf[mdf['계정명'].isin(selected_accounts)]['계정코드']
                            .astype(str)
                            .tolist()
                        )
                        mod = run_trend_module(lf_use, accounts=accounts_codes)
                        _push_module(mod)
                        for w in mod.warnings:
                            st.warning(w)
                        if mod.figures:
                            for title, fig in mod.figures.items():
                                # PM 임계선(항상 표시; 범위 밖이면 자동 확장)
                                st.plotly_chart(
                                    add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT))),
                                    use_container_width=True,
                                    key=f"trend_{title}"
                                )
                        else:
                            st.info("표시할 추이 그래프가 없습니다.")



                with tab_vendor:
                    st.header("거래처 심층 분석")
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')}")

                    st.subheader("거래처 집중도 및 활동성 (계정별)")
                    master_df_res = st.session_state.master_df
                    account_list_vendor = master_df_res['계정명'].unique()
                    selected_accounts_vendor = st.multiselect("분석할 계정(들)을 선택하세요.", account_list_vendor, default=[], key="vendor_accounts_pick")
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles_vendor = st.multiselect(
                            "사이클 프리셋 선택", list(cyc.CYCLE_KO.values()),
                            default=[], key="vendor_cycles_pick"
                        )
                        st.button("➕ 프리셋 적용", key="btn_apply_cycles_vendor", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="vendor_cycles_pick",
                                              accounts_state_key="vendor_accounts_pick",
                                              master_df=st.session_state.master_df))

                    # 🔧 최소 거래금액(연간, CY) 필터 — KRW 입력(커밋 시 쉼표 정규화)
                    min_amount_vendor = _krw_input(
                        "최소 거래금액(연간, CY) 필터",
                        key="vendor_min_amount",
                        default_value=0,
                        help_text="CY 기준 거래금액 합계가 이 값 미만인 거래처는 '기타'로 합산됩니다."
                    )
                    include_others_vendor = st.checkbox("나머지는 '기타'로 합산", value=True)

                    if selected_accounts_vendor:
                        selected_codes = (
                            master_df_res[master_df_res['계정명'].isin(selected_accounts_vendor)]['계정코드']
                            .astype(str)
                            .tolist()
                        )
                        lf_use = _lf_by_scope()
                        vmod = run_vendor_module(
                            lf_use,
                            account_codes=selected_codes,
                            min_amount=float(min_amount_vendor),
                            include_others=bool(include_others_vendor),
                        )
                        _push_module(vmod)
                        if vmod.figures:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'pareto' in vmod.figures:
                                    figp = vmod.figures['pareto']
                                    figp = add_materiality_threshold(figp, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figp, use_container_width=True, key=f"vendor_pareto_{'_'.join(selected_accounts_vendor) or 'all'}")
                            with col2:
                                if 'heatmap' in vmod.figures:
                                    figh = add_pm_badge(vmod.figures['heatmap'], float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figh, use_container_width=True, key=f"vendor_heatmap_{'_'.join(selected_accounts_vendor) or 'all'}")
                        else:
                            st.warning("선택하신 계정에는 분석할 거래처 데이터가 부족합니다.")
                        for w in vmod.warnings:
                            st.warning(w)
                    else:
                        st.info("계정을 선택하면 해당 계정의 거래처 집중도 및 활동성 분석을 볼 수 있습니다.")

                    st.markdown("---")
                    st.subheader("거래처별 세부 분석 (전체 계정)")
                    full_ledger_df = st.session_state.ledger_df
                    vendor_list = sorted(full_ledger_df[full_ledger_df['거래처'] != '정보 없음']['거래처'].unique())

                    if len(vendor_list) > 0:
                        options = ['선택하세요...'] + vendor_list
                        selected_vendor = st.selectbox("상세 분석할 거래처를 선택하세요.", options, index=0)

                        if selected_vendor != '선택하세요...':
                            all_months_in_data = pd.period_range(
                                start=full_ledger_df['회계일자'].min(),
                                end=full_ledger_df['회계일자'].max(),
                                freq='M'
                            ).strftime('%Y-%m').tolist()
                            detail_fig = create_vendor_detail_figure(full_ledger_df, selected_vendor, all_months_in_data)
                            # PM line on vendor detail (stacked bars)
                            try:
                                detail_fig = add_materiality_threshold(detail_fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                            except Exception:
                                pass
                            if detail_fig:
                                st.plotly_chart(detail_fig, use_container_width=True, key=f"vendor_detail_{selected_vendor}")
                    else:
                        st.info("분석할 거래처 데이터가 없습니다.")

                with tab_anomaly:
                    st.header("이상 패턴 탐지")
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')}")
                    mdf = st.session_state.master_df
                    acct_names = mdf['계정명'].unique()
                    pick = st.multiselect("대상 계정 선택(미선택 시 자동 추천)", acct_names, default=[])
                    topn = st.slider("표시 개수(상위 |Z|)", min_value=10, max_value=500, value=20, step=10)
                    if st.button("이상치 분석 실행"):
                        lf_use = _lf_by_scope()
                        codes = None
                        if pick:
                            codes = mdf[mdf['계정명'].isin(pick)]['계정코드'].astype(str).tolist()
                        amod = run_anomaly_module(lf_use, target_accounts=codes, topn=topn, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                        _push_module(amod)
                        for w in amod.warnings: st.warning(w)
                        if 'anomaly_top' in amod.tables:
                            _tbl = amod.tables['anomaly_top'].copy()
                            fmt = {}
                            if '발생액' in _tbl.columns: fmt['발생액'] = '{:,.0f}'
                            if 'Z-Score' in _tbl.columns: fmt['Z-Score'] = '{:.2f}'
                            st.dataframe(_tbl.style.format(fmt), use_container_width=True)
                        if 'zscore_hist' in amod.figures:
                            st.plotly_chart(amod.figures['zscore_hist'], use_container_width=True, key="anomaly_hist")

                with tab_ts:
                    st.header("시계열 예측")
                    with st.expander("🧭 해석 가이드", expanded=False, icon=":material/help:"):
                        st.markdown(
                            """
### 용어
- **z(표준화 지수)**: `z = (실측 − 예측) / σ`  
  - 월별 예측 잔차(실측-예측)를 표준화한 지수입니다. **이상 패턴 탐지의 Z-Score와 다른 개념입니다.**
  - |z|≈2는 **이례적**, |z|≈3은 **매우 이례적**입니다.  
- **σ(표준편차) 집계**: 최근 *k=6개월* 잔차의 표준편차로 표준화합니다. 데이터가 짧으면 시작~현재까지의 **expanding σ**를 사용합니다.  
- **위험도(0~1)** = `min(1, 0.5·|z|/3 + 0.3·PM대비 + 0.2·KIT)`  
  - PM대비 = `min(1, |실측−예측| / PM)`,  **KIT** = PM 초과 여부(True/False)
- **Flow / Balance**: *Flow*는 **월 발생액(Δ잔액)**, *Balance*는 **월말 잔액**입니다. *(BS 계정은 Balance 기준도 병행 계산합니다.)*
- **정상성**: 시계열의 평균/분산이 시간에 따라 **안 변함**(ARIMA가 특히 선호).
- **MAE**: 평균 절대 오차(원 단위). **작을수록 정확**.
- **MAPE**: 상대 오차(%). **규모 다른 계정 비교**에 유용.
- **AIC/BIC**: 모델 복잡도까지 고려한 **정보량 지표**. **작을수록 우수**.

### 차트 읽기
- 실선=실측, 점선=예측(**MoR**: EMA/MA/ARIMA/Prophet 중 자동 선택)  
- 회색 점선: **연(굵게)** / **분기(얇게)** 경계선, 붉은 점선: **PM 기준선**

### 사용한 예측모델
- **MA(이동평균)**: 최근 *n*개월 **단순 평균**. **짧은 데이터/변동 완만**할 때 안정적.
- **EMA(지수이동평균)**: **최근값 가중** 평균. **최근 추세 반영**이 필요할 때 유리.
- **ARIMA(p,d,q)**: **자기상관** 기반. **계절성이 약(또는 제거 가능)**하고 **데이터가 충분**할 때 강함.
- **Prophet**: **연/분기 계절성·휴일효과**가 뚜렷할 때 적합(이상치에 비교적 견고).

> :blue[**모델은 계정×기준(Flow/Balance)별로 교차검증 오차(MAPE/MAE)와 (가능하면) 정보량(AIC/BIC)을 종합해 자동 선택됩니다.**]
"""
                        )

                    # 0) 원장/세션 확보
                    master_df: pd.DataFrame = st.session_state.get("master_df", pd.DataFrame())
                    if master_df.empty:
                        st.info("원장 데이터가 없습니다.")
                        st.stop()

                    # --- state bootstrap --- (위젯 생성 전에 실행)
                    st.session_state.setdefault("ts_accounts_names", [])
                    st.session_state.setdefault("ts_cycles_ko", [])
                    st.session_state.setdefault("ts_acc_buffer", None)
                    st.session_state.setdefault("ts_acc_needs_update", False)

                    # --- preset 주입 훅: rerun 직후, 멀티셀렉트 생성 '이전'에 1회 주입 ---
                    if st.session_state.ts_acc_needs_update and st.session_state.ts_acc_buffer is not None:
                        st.session_state.ts_accounts_names = st.session_state.ts_acc_buffer
                        st.session_state.ts_acc_needs_update = False

                    upload_id = getattr(uploaded_file, 'name', '_default')

                    # 두 컨테이너로 시각 순서는 유지(계정 위, 프리셋 아래) + 코드 순서 제어
                    box_accounts = st.container()
                    box_preset = st.container()

                    # Helper: 프리셋(KO 라벨) → 계정명 리스트로 확장
                    def expand_cycles_to_account_names(*, upload_id: str, cycles_ko: list[str], master_df: pd.DataFrame) -> list[str]:
                        try:
                            mapping = cyc.get_effective_cycles(upload_id)
                            codes = cyc.accounts_for_cycles_ko(mapping, cycles_ko)
                            df_map = master_df[["계정코드","계정명"]].dropna().copy()
                            df_map["계정코드"] = df_map["계정코드"].astype(str)
                            code_to_name = df_map.drop_duplicates("계정코드").set_index("계정코드")["계정명"].astype(str).to_dict()
                            names = [code_to_name.get(str(c), str(c)) for c in codes]
                            # 유니크+순서보존
                            return list(dict.fromkeys([n for n in names if n]))
                        except Exception:
                            return []

                    # (아래) 프리셋 영역: 버튼으로 버퍼만 갱신
                    with box_preset:
                        st.markdown("#### 사이클 프리셋 선택(선택 시 위 계정 목록에 **적용 버튼**으로 반영)")
                        chosen_cycles = st.multiselect(
                            "사이클 프리셋",
                            options=list(cyc.CYCLE_KO.values()),
                            key="ts_cycles_ko",
                        )
                        if st.button("➕ 프리셋 적용", key="ts_apply_preset"):
                            names_from_cycles = expand_cycles_to_account_names(
                                upload_id=upload_id,
                                cycles_ko=st.session_state.ts_cycles_ko,
                                master_df=master_df,
                            )
                            merged = list(dict.fromkeys([
                                *st.session_state.ts_accounts_names,
                                *names_from_cycles,
                            ]))
                            st.session_state.ts_acc_buffer = merged
                            st.session_state.ts_acc_needs_update = True
                            st.rerun()

                    # (위) 계정 영역: 멀티셀렉트 그리기(값 주입은 상단 훅이 담당)
                    with box_accounts:
                        all_account_names = (
                            master_df["계정명"].dropna().astype(str).sort_values().unique().tolist()
                        )
                        picked_names = st.multiselect(
                            "대상 계정(복수 선택 가능)",
                            options=all_account_names,
                            key="ts_accounts_names",
                            help="선택한 계정에 대해서만 예측 테이블/그래프를 생성합니다."
                        )

                    # 미래 예측 개월 수 슬라이더 추가
                    forecast_horizon = st.slider(
                        "미래 예측 개월 수(시각화용)", min_value=0, max_value=12, value=0, step=1,
                        help="표본 N<6이면 자동으로 0으로 비활성화됩니다."
                        )

                    if not picked_names:
                        st.info("시계열 결과가 없습니다. (선택한 계정/기간에 데이터 없음)")
                        st.stop()

                    # 2) 계정명 → 계정코드
                    name_to_code = (
                        master_df.dropna(subset=["계정명","계정코드"]).astype({"계정명":"string","계정코드":"string"})
                                 .drop_duplicates(subset=["계정명"]).set_index("계정명")["계정코드"].to_dict()
                    )
                    want_codes = [name_to_code.get(n) for n in picked_names if n in name_to_code]

                    # 3) 정식 시계열 파이프라인: ledger → 월별집계(flow) → balance(opening+누적) → 예측/진단/그림
                    lf_use = st.session_state.get('lf_hist')
                    st.caption("ⓘ 시계열 분석은 좌측 스코프 설정과 무관하게 전체 히스토리를 사용합니다.")
                    if lf_use is None:
                        st.info("원장을 먼저 업로드해 주세요.")
                        st.stop()

                    # (1) 분석 대상 슬라이스
                    ldf = lf_use.df.copy()
                    ldf = ldf[ldf['계정코드'].astype(str).isin([str(x) for x in want_codes])]

                    # (2) 필수 파생: 발생액/순액 보장
                    from analysis.anomaly import compute_amount_columns
                    ldf = compute_amount_columns(ldf)

                    # (3) 날짜/금액 컬럼 픽업(없으면 안전 종료)
                    from analysis.timeseries import DATE_CANDIDATES, AMT_CANDIDATES
                    date_col = next((c for c in DATE_CANDIDATES if c in ldf.columns), None)
                    amount_col = next((c for c in AMT_CANDIDATES if c in ldf.columns), None)
                    if not date_col or not amount_col:
                        st.error(
                            "필수 컬럼을 찾지 못했습니다.\n"
                            f"- 날짜 후보: {DATE_CANDIDATES}\n- 금액 후보: {AMT_CANDIDATES}\n\n"
                            f"현재 컬럼: {list(ldf.columns)}"
                        )
                        st.stop()

                    # (4) opening(전기초) 맵 구성 — BS Balance용: Master '전전기말잔액' 우선 사용
                    opening_map = {}
                    if "계정코드" in master_df.columns:
                        # 어떤 컬럼을 opening으로 쓸지 결정: 전전기말 → 전기말 → 없으면 0
                        src_col = None
                        if "전전기말잔액" in master_df.columns:
                            src_col = "전전기말잔액"
                        elif "전기말잔액" in master_df.columns:
                            src_col = "전기말잔액"

                        if src_col:
                            _m = master_df[["계정코드", src_col]].dropna(subset=["계정코드"]).copy()
                            _m["계정코드"] = _m["계정코드"].astype(str)
                            _m[src_col] = pd.to_numeric(_m[src_col], errors="coerce").fillna(0.0)
                            # 내부 키는 통일해서 사용
                            opening_map = (
                                _m.rename(columns={src_col: "opening_bs"})
                                  .groupby("계정코드")["opening_bs"].first()
                                  .to_dict()
                            )
                        else:
                            opening_map = {}  # 모든 계정 0.0으로 처리(아래에서 기본값 적용)

                    # (5) BS/PL 플래그
                    is_bs_map = {}
                    if "BS/PL" in master_df.columns and "계정코드" in master_df.columns:
                        is_bs_map = (
                            master_df.dropna(subset=["계정코드","BS/PL"])
                            .astype({"계정코드":"string","BS/PL":"string"})
                            .drop_duplicates(subset=["계정코드"])
                            .assign(is_bs=lambda d: d["BS/PL"].str.upper().eq("BS"))
                            .set_index("계정코드")["is_bs"].to_dict()
                        )

                    # (5.5) 차변/대변 플래그(대변 계정은 부호 반전)
                    is_credit_map = {}
                    if "차변/대변" in master_df.columns and "계정코드" in master_df.columns:
                        is_credit_map = (
                            master_df.dropna(subset=["계정코드","차변/대변"])
                                     .assign(계정코드=lambda d: d["계정코드"].astype(str),
                                             credit=lambda d: d["차변/대변"].astype(str).str.contains("대변"))
                                     .drop_duplicates(subset=["계정코드"])
                                     .set_index("계정코드")["credit"].to_dict()
                        )

                    # (6) 모델 선택(레지스트리)
                    st.caption("모형: EMA(고정). 복잡 러너는 비활성화되었습니다.")
                    backend = "ema"

                    PM = float(st.session_state.get("pm_value", PM_DEFAULT))
                    # (7) 계정별 실행: 결과 수집용 버퍼
                    gathered_flow = []
                    gathered_balance = []
                    results_per_account = {}

                    for code in want_codes:
                        sub = ldf[ldf["계정코드"].astype(str) == str(code)].copy()
                        if sub.empty:
                            continue
                        acc_name = (master_df[master_df["계정코드"].astype(str)==str(code)]["계정명"].dropna().astype(str).head(1).tolist() or [str(code)])[0]
                        is_bs = bool(is_bs_map.get(str(code), False))

                        PM = float(st.session_state.get("pm_value", PM_DEFAULT))
                        # ✨ 대변계정이면 부호 반전
                        sign = -1.0 if bool(is_credit_map.get(str(code), False)) else 1.0
                        # 모델 입력 컬럼을 수치화 + 반전
                        try:
                            sub[amount_col] = pd.to_numeric(sub[amount_col], errors="coerce").fillna(0.0) * float(sign)
                        except Exception:
                            sub[amount_col] = pd.to_numeric(sub.get(amount_col, 0.0), errors="coerce").fillna(0.0) * float(sign)
                        # BS 잔액용 opening도 동일 기준으로 반전
                        opening = 0.0
                        if isinstance(opening_map, dict):
                            opening = float(opening_map.get(str(code), 0.0)) * float(sign)

                        out = run_timeseries_minimal(
                            sub,
                            account_name=acc_name,
                            date_col=date_col,
                            amount_col=amount_col,
                            is_bs=bool(is_bs),
                            opening=opening,
                            pm_value=PM
                        )

                        # (수집) 통합 요약표(1행) + 그래프(다포인트) 분리
                        if not out.empty:
                            tmp = out.copy()
                            tmp.insert(0, "계정", acc_name)

                            # 그래프/진단용(전 구간)
                            results_per_account[acc_name] = tmp

                            # === 요약행(마지막 1행) + 통계열 추가 ===
                            for ms in ("flow", "balance"):
                                dfm = tmp[tmp["measure"].eq(ms)]
                                if dfm.empty:
                                    continue
                                stats = compute_series_stats(dfm)
                                last_row = dfm.tail(1).copy()
                                last_row["MAE"]  = stats["MAE"]
                                last_row["MAPE"] = stats["MAPE"]
                                last_row["RMSE"] = stats["RMSE"]
                                last_row["N"]    = stats["N"]

                                if ms == "flow":
                                    gathered_flow.append(last_row)
                                else:
                                    gathered_balance.append(last_row)

                    # === 공용: 표 높이 자동 계산 ===
                    def _auto_table_height(df: pd.DataFrame, *, min_rows=3, max_rows=10, row_px=32, header_px=40, padding_px=16) -> int:
                        n = 0 if df is None else int(len(df))
                        n = max(min_rows, min(max_rows, n))
                        return header_px + n * row_px + padding_px

                    # === NEW: 통합 테이블(그래프보다 위에 한 번만) ===
                    def _render_table(blocks, title):
                        if not blocks:
                            return
                        tbl = pd.concat(blocks, ignore_index=True)

                        show_cols = ["계정","date","actual","predicted","error","z","risk","model",
                                     "MAE","MAPE","RMSE","N"]  # ← 통계 열 추가
                        for c in show_cols:
                            if c not in tbl.columns:
                                tbl[c] = np.nan

                        # 사용자 친화 라벨/정렬
                        tbl = (tbl.rename(columns={
                            "date":"일자","actual":"실측","predicted":"예측",
                            "error":"잔차","risk":"위험도","model":"모델(MoR)"
                        })
                            .sort_values(["계정","일자"])
                        )

                        st.subheader(title)
                        fmt = {}
                        for c in ["실측","예측","잔차","MAE","RMSE"]:
                            if c in tbl.columns: fmt[c] = "{:,.0f}"
                        if "MAPE" in tbl.columns: fmt["MAPE"] = "{:.2f}%"
                        if "z" in tbl.columns: fmt["z"] = "{:.2f}"
                        if "위험도" in tbl.columns: fmt["위험도"] = "{:.2f}"

                        # 표 높이: 행수 기반으로 자동(최대 420px)
                        rows = max(1, len(tbl))
                        height = min(420, 42 + rows * 28)

                        st.dataframe(tbl[["계정","일자","실측","예측","잔차","z","위험도","모델(MoR)","MAE","MAPE","RMSE","N"]]
                                     .style.format(fmt),
                                     use_container_width=True, height=height)

                        st.download_button(
                            "CSV 다운로드", data=tbl.to_csv(index=False).encode("utf-8-sig"),
                            file_name=f"timeseries_summary_{'flow' if 'Flow' in title else 'balance'}.csv",
                            mime="text/csv"
                        )

                    def _auto_height(df: pd.DataFrame, max_rows: int = 12) -> int:
                        rows = int(min(len(df), max_rows))
                        base_row = 34  # 체감값
                        header = 38
                        pad = 8
                        return header + rows * base_row + pad

                    def _render_table_combined(flow_blocks, balance_blocks, title="선택계정 요약 (Flow+Balance)"):
                        import pandas as pd
                        import numpy as np
                        blocks = []

                        def _prep(df, label):
                            if df is None or len(df) == 0:
                                return None
                            x = pd.concat(df, ignore_index=True)
                            
                            # 먼저 rename을 해서 컬럼명을 통일
                            rename_map = {
                                "account": "계정", "date": "일자",
                                "actual": "실측", "predicted": "예측",
                                "error": "잔차", "risk": "위험도",
                                "model": "모델(MoR)"
                            }
                            for k, v in rename_map.items():
                                if k in x.columns and v not in x.columns:  # 중복 방지
                                    x.rename(columns={k: v}, inplace=True)
                            
                            # 그 다음 '기준' 컬럼 추가
                            x.insert(0, "기준", label)
                            return x

                        f = _prep(flow_blocks, "발생액(Flow)")
                        b = _prep(balance_blocks, "잔액(Balance)")
                        if f is not None: blocks.append(f)
                        if b is not None: blocks.append(b)
                        if not blocks:
                                return

                        tbl = pd.concat(blocks, ignore_index=True)

                        # 중복 컬럼 제거 (혹시 있다면)
                        tbl = tbl.loc[:, ~tbl.columns.duplicated()]

                        # ✅ z 라벨 변경(표에서만)
                        col_map = {
                            "date":"일자","actual":"실측","predicted":"예측","error":"잔차",
                            "z":"z(시계열)","risk":"위험도","model":"모델(MoR)"
                        }
                        for k, v in col_map.items():
                            if k in tbl.columns:
                                tbl.rename(columns={k: v}, inplace=True)

                        want_cols = ["기준", "계정", "일자", "실측", "예측", "잔차", "z(시계열)", "위험도", "모델(MoR)"]
                        show_cols = [c for c in want_cols if c in tbl.columns]
                        tbl = tbl[show_cols].copy()

                        # 포맷
                        fmt = {"실측":"{:,.0f}","예측":"{:,.0f}","잔차":"{:,.0f}","위험도":"{:.2f}","z(시계열)":"{:.2f}"}

                        st.subheader(title)

                        # 인덱스 숨김 + 통일된 높이
                        tbl = tbl.reset_index(drop=True)
                        try:
                            st.dataframe(
                                tbl.style.format(fmt),
                                use_container_width=True,
                                hide_index=True,
                                height=_auto_height(tbl)
                            )
                        except TypeError:
                            st.dataframe(
                                tbl.style.format(fmt),
                                use_container_width=True,
                                height=_auto_height(tbl)
                            )

                        # CSV
                            st.download_button(
                            "CSV 다운로드",
                            data=tbl.to_csv(index=False).encode("utf-8-sig"),
                            file_name="timeseries_summary_all.csv",
                                mime="text/csv"
                            )

                    def _render_outlier_alert(results_per_account: dict, *, topn: int = 10, z_thr: float = 2.0):
                        """
                        results_per_account: {계정명 -> DataFrame}, DataFrame은 최소 컬럼
                          ['date','actual','predicted','error','z','risk','model','measure'] 가정
                          measure ∈ {'flow','balance'}
                        """
                        import pandas as pd
                        import numpy as np
                        rows = []
                        for acc_name, df in (results_per_account or {}).items():
                            if df is None or df.empty or "z" not in df.columns:
                                continue
                            dfx = df.copy()
                            # 계정명 보강 (혹시 누락 대비)
                            if "계정" not in dfx.columns:
                                dfx["계정"] = acc_name
                            # 기준 라벨(발생액/잔액)
                            dfx["기준"] = dfx.get("measure", "").map(
                                {"flow": "발생액(Flow)", "balance": "잔액(Balance)"}
                            ).fillna("발생액(Flow)")
                            # |z| 필터
                            dfx = dfx[dfx["z"].abs() >= float(z_thr)]
                            if not dfx.empty:
                                rows.append(dfx)

                        # 헤더 + 테이블
                        import streamlit as st
                        st.subheader(f"이상월 알림 (상위 {topn}건, 기준 |z| ≥ {z_thr:.1f})")

                        if not rows:
                            st.info(f"이상월 없음(기준 |z| ≥ {z_thr:.1f})")
                            return

                        out = pd.concat(rows, ignore_index=True)

                        # 정렬: |z| 내림차순 → |잔차| 보조
                        out = out.sort_values(
                            by=["z", "error"],
                            key=lambda s: s.abs() if s.name in ("z", "error") else s,
                            ascending=[False, False]
                        ).head(int(topn))

                        # 표시 컬럼/한글명 + z 라벨 변경
                        rename = {"date": "일자", "actual": "실측", "predicted": "예측",
                                  "error": "잔차", "z": "z(시계열)", "risk": "위험도", "model": "모델"}
                        for k, v in rename.items():
                            if k in out.columns:
                                out.rename(columns={k: v}, inplace=True)

                        show_cols = [c for c in ["계정", "일자", "실측", "예측", "잔차", "z(시계열)", "위험도", "모델", "기준"] if c in out.columns]
                        out = out[show_cols]

                        fmt = {"실측":"{:,.0f}","예측":"{:,.0f}","잔차":"{:,.0f}","위험도":"{:.2f}","z(시계열)":"{:.2f}"}

                        try:
                            st.dataframe(
                                out.style.format(fmt),
                                use_container_width=True,
                                hide_index=True,
                                height=_auto_height(out)
                            )
                        except TypeError:
                            # Streamlit 구버전 호환
                            st.dataframe(
                                out.reset_index(drop=True).style.format(fmt),
                                use_container_width=True,
                                height=_auto_height(out)
                            )

                    # === NEW: 선택계정 통계 및 이상월 리스트 렌더링 함수 정의 ===
                    def _safe_div(a, b):
                        try:
                            b = np.where(np.abs(b) < 1e-9, 1.0, b)
                            return a / b
                        except Exception:
                            return np.nan

                    _render_table_combined(gathered_flow, gathered_balance, title="선택계정 요약 (Flow+Balance)")

                    # 통합 이상월 알림 (|z| ≥ 2.0 고정)
                    _render_outlier_alert(results_per_account, topn=10, z_thr=2.0)

                    # ============ 🔎 시계열 파이프라인 진단(현황판) ============ #
                    with st.expander("🔎 시계열 파이프라인 진단(현황판)", expanded=False):
                        st.caption("각 단계별로 포인트 수/타입/정규화 상태를 집계합니다. 그래프가 안 뜨면 어디서 끊겼는지 여기서 확인하세요.")
                        # 0) 원본 슬라이스 요약
                        st.markdown("**0) 입력(원장 슬라이스) 요약**")
                        try:
                            st.write({
                                "선택계정 수": len(want_codes),
                                "원장 행수(선택계정)": int(len(ldf)),
                                "date_col": date_col,
                                "amount_col": amount_col,
                                "date_dtype": str(ldf[date_col].dtype),
                                "amount_dtype": str(ldf[amount_col].dtype),
                                "NaT(날짜)": int(pd.to_datetime(ldf[date_col], errors="coerce").isna().sum()),
                                "NaN(금액)": int(pd.to_numeric(ldf[amount_col], errors="coerce").isna().sum()),
                                "기간": f"{pd.to_datetime(ldf[date_col], errors='coerce').min()} ~ {pd.to_datetime(ldf[date_col], errors='coerce').max()}",
                            })
                            st.dataframe(
                                ldf[[date_col, "계정코드", "계정명", amount_col]].head(5),
                                use_container_width=True,
                                height=_auto_table_height(ldf.head(5))
                            )
                        except Exception as _e:
                            st.warning(f"입력 요약 실패: {_e}")

                        # 1) 계정×월 집계 확인
                        st.markdown("**1) 월별 집계 상태**")
                        try:
                            _tmp = ldf[[date_col, amount_col]].copy()
                            _tmp = _tmp.rename(columns={date_col: '회계일자', amount_col: '거래금액'})
                            _grp = aggregate_monthly(_tmp, date_col='회계일자', amount_col='거래금액').rename(columns={"amount":"flow"})
                            _grp["date"] = pd.to_datetime(_grp["date"], errors="coerce")
                            _norm_ok = int((_grp["date"].dt.hour.eq(0) & _grp["date"].dt.minute.eq(0)).sum())
                            st.write({
                                "집계 포인트 수": int(len(_grp)),
                                "월말 00:00:00 비율": f"{_norm_ok}/{len(_grp)}",
                                "예: 첫 3행": None
                            })
                            st.dataframe(
                                _grp.head(3),
                                use_container_width=True,
                                height=_auto_table_height(_grp.head(3))
                            )
                            # (보너스) 경계선 예상 개수
                            try:
                                rng = pd.date_range(pd.to_datetime(ldf[date_col]).min(), pd.to_datetime(ldf[date_col]).max(), freq="M")
                                q_ends = [m for m in rng if m.month in (3,6,9,12)]
                                y_ends = [m for m in rng if m.month == 12]
                                st.write({"경계선(분기말) 예상 개수": len(q_ends), "경계선(연말) 예상 개수": len(y_ends)})
                            except Exception as _ee:
                                st.info(f"경계선 개수 계산 실패: {_ee}")
                        except Exception as _e:
                            st.warning(f"월별 집계 상태 계산 실패: {_e}")

                        # 2) 러너 결과 요약
                        st.markdown("**2) 모델 입력/출력 상태(run_timeseries_minimal · EMA)**")
                        try:
                            if not (gathered_flow or gathered_balance):
                                st.warning("러너 출력(gathered_*)가 비어 있습니다. 상단 입력/집계 단계 확인 필요.")
                            else:
                                parts = []
                                if gathered_flow: parts += gathered_flow
                                if gathered_balance: parts += gathered_balance
                                parts = [p for p in parts if isinstance(p, pd.DataFrame) and not p.empty]
                                if not parts:
                                    st.warning("러너 출력이 비어 있습니다.(유효한 DataFrame 없음)")
                                else:
                                    _all = pd.concat(parts, ignore_index=True)
                                    st.write({
                                        "계정×기준(measure) 개수": int(_all[["계정","measure"]].drop_duplicates().shape[0]) if set(["계정","measure"]).issubset(_all.columns) else 0,
                                        "actual 존재": bool("actual" in _all.columns),
                                        "predicted 존재": bool("predicted" in _all.columns),
                                        "flow 포인트": int(_all[_all.get("measure","flow").eq("flow")].shape[0]) if "measure" in _all.columns else int(_all.shape[0]),
                                        "balance 포인트": int(_all[_all.get("measure","flow").eq("balance")].shape[0]) if "measure" in _all.columns else 0,
                                    })
                                    st.dataframe(
                                        _all.head(5),
                                        use_container_width=True,
                                        height=_auto_table_height(_all.head(5))
                                    )
                        except Exception as _e:
                            st.warning(f"러너 출력 요약 실패: {type(_e).__name__}: {_e}")

                        # === Opening 소스/적용 여부 표시 ===
                        st.markdown("**Opening 소스 및 적용 현황**")
                        try:
                            src_used = ("전전기말잔액" if "전전기말잔액" in master_df.columns else
                                        "전기말잔액" if "전기말잔액" in master_df.columns else "N/A")
                            st.write({"opening_source_for_BS_balance": src_used})
                            # 표본 1~2개 계정에 대해 opening 적용값 미리보기
                            _peek = []
                            for k, v in list(opening_map.items())[:2]:
                                _peek.append({"계정코드": k, "opening_raw": v})
                            if _peek:
                                st.dataframe(pd.DataFrame(_peek), use_container_width=True, height=120)
                        except Exception:
                            pass

                        # 부호 보정 가드 표시
                        st.markdown("**부호 보정 가드**")
                        try:
                            pipeline_norm = any(c in ldf.columns for c in ["발생액_norm", "amount_norm", "__sign", "sign"])
                            plot_sign_flip = False  # 플롯 레벨 반전은 하지 않음
                            st.write({
                                "pipeline_norm": bool(pipeline_norm),
                                "plot_sign_flip": False,   # 플롯 레벨 반전 금지
                                "opening_sign_applied_in_pipeline": True,
                                "guard_ok": bool(pipeline_norm and not plot_sign_flip)
                            })
                            if pipeline_norm and plot_sign_flip:
                                st.warning("경고: 파이프라인과 플롯에서 모두 부호를 만지면 이중 반전 위험이 있습니다.")
                        except Exception as _e:
                            st.info(f"부호 보정 가드 점검 실패: {_e}")

                        # 3) 그림 입력 전 점검(계정별)
                        st.markdown("**3) 그림 입력 사전 점검(create_timeseries_figure 직전)**")
                        try:
                            for acc_name, df_all in results_per_account.items():
                                for ms in (["flow","balance"] if df_all["measure"].eq("balance").any() else ["flow"]):
                                    dfx = df_all[df_all["measure"].eq(ms)]
                                    st.write(f"- {acc_name} / {ms}: N={len(dfx)} · 컬럼={list(dfx.columns)} · 날짜범위={pd.to_datetime(dfx['date']).min()}~{pd.to_datetime(dfx['date']).max()}")
                                    st.dataframe(
                                        dfx[["date","actual","predicted"]].head(3),
                                        use_container_width=True,
                                        height=_auto_table_height(dfx.head(3))
                                    )
                        except Exception as _e:
                            st.warning(f"그림 입력 점검 실패: {_e}")

                    # === 연/분기 경계선 기본 표시 ===
                    show_dividers = True

                    # === 계정×기준 통계 요약 ===
                    def _safe_stats_block(df_in: pd.DataFrame) -> dict:
                        s = {}
                        try:
                            a = pd.to_numeric(df_in.get("actual"), errors="coerce")
                            p = pd.to_numeric(df_in.get("predicted"), errors="coerce")
                            e = a - p
                            s["N"] = int(len(df_in))
                            s["MAE"] = float(np.nanmean(np.abs(e)))
                            denom = a.replace(0, np.nan)
                            s["MAPE(%)"] = float(np.nanmean(np.abs(e / denom)) * 100.0)
                            s["RMSE"] = float(np.sqrt(np.nanmean((e**2))))
                            z = pd.to_numeric(df_in.get("z"), errors="coerce")
                            s["|z|_max"] = float(np.nanmax(np.abs(z))) if z is not None else np.nan
                            s["last_z"] = float(z.iloc[-1]) if (z is not None and len(z) > 0) else np.nan
                            s["last_err"] = float(e.iloc[-1]) if len(e) > 0 else np.nan
                            s["model"] = str(df_in.get("model").iloc[-1]) if "model" in df_in.columns and len(df_in) > 0 else ""
                        except Exception:
                            pass
                        return s

                    stats_rows = []
                    for acc_name, df_all in results_per_account.items():
                        for ms in (["flow","balance"] if df_all["measure"].eq("balance").any() else ["flow"]):
                            d = df_all[df_all["measure"].eq(ms)]
                            if d.empty:
                                continue
                            row = {"계정": acc_name, "기준": ("발생액(Flow)" if ms=="flow" else "잔액(Balance)")}
                            row.update(_safe_stats_block(d))
                            stats_rows.append(row)

                    if stats_rows:
                        stats_df = pd.DataFrame(stats_rows)[
                            ["계정","기준","N","MAE","MAPE(%)","RMSE","|z|_max","last_z","last_err","model"]
                        ]
                        fmt = {"MAE":"{:,.0f}","MAPE(%)":"{:.2f}","RMSE":"{:,.0f}","|z|_max":"{:.2f}","last_z":"{:.2f}","last_err":"{:,.0f}"}
                        st.subheader("계정별 통계 요약")
                        st.dataframe(
                            stats_df.style.format(fmt),
                            use_container_width=True,
                            height=_auto_table_height(stats_df)
                        )

                    # === 그래프 렌더(아래): 계정별로 표시 ===
                    for acc_name, df_all in results_per_account.items():
                        for measure in (["flow","balance"] if (df_all["measure"].eq("balance").any()) else ["flow"]):
                            dfm = df_all[df_all["measure"]==measure].rename(columns={"account":"계정"})
                            title = f"{acc_name} — {'발생액(Flow)' if measure=='flow' else '잔액(Balance)'}"

                            # 표본수 게이트: N<6이면 미래 음영 비활성화
                            stats = compute_series_stats(dfm)
                            _hz = int(forecast_horizon or 0)
                            if stats["N"] < 6:
                                _hz = 0

                            fig, stats_d = create_timeseries_figure(
                                dfm, measure=measure, title=title,
                                pm_value=PM, show_dividers=True   # ← 분기/연 경계선 켜기
                            )

                            # 미래 구간 음영(시각화 전용)
                            if _hz > 0 and not dfm.empty:
                                last_date = pd.to_datetime(dfm["date"]).max()
                                fig = add_future_shading(fig, last_date, horizon_months=_hz)

                            st.subheader(title)
                            if fig is not None:
                                st.plotly_chart(fig, use_container_width=True)

                            # MoR 로그 표시
                            log = (results_per_account.get(acc_name, pd.DataFrame())).attrs.get("mor_log", {})
                            if stats_d or log:
                                st.caption(
                                    f"모형:{dfm['model'].iloc[-1] if not dfm.empty else '-'} · "
                                    f"선정근거:{log.get('metric','-')} "
                                    f"(MAPE={log.get('mape_best',''):g}%, MAE={log.get('mae_best',''):,.0f}) · "
                                    f"표본월:{log.get('n_months','-')}"
                                    + ("" if _hz == forecast_horizon else " · (표본 부족으로 미래음영 비활성)")
                                )
                # ⚠️ 기존 tab5(위험평가) 블록 전체 삭제됨
                
                with tab_corr:
                    st.header("상관관계")
                    if not uploaded_file:
                        st.info("📁 원장 파일을 업로드하면 상관관계 분석이 가능합니다.")
                    else:
                        upload_id = getattr(uploaded_file, 'name', '_default')
                        tb1, tb2, tb3 = st.tabs(["기본", "고급", "포커스(1계정)"])
                        with tb1: _render_corr_basic_tab(upload_id=upload_id)
                        with tb2: _render_corr_advanced_tab(upload_id=upload_id)
                        with tb3: _render_corr_focus_tab(upload_id=upload_id)
                with tab_report:
                    st.header("🧠 분석 종합 대시보드")
                    
                    # === 앱 초기화 시 빈 화면 방지 ===
                    if not uploaded_file:
                        st.info("📁 원장 파일을 업로드하면 종합 대시보드가 표시됩니다.")
                    elif not st.session_state.get('modules', {}):
                        st.info("📊 다른 분석 탭(시계열, 이상 패턴 등)을 실행하면 결과가 여기에 집계됩니다.")
                        st.caption("💡 **시계열 예측** 탭에서 계정을 선택하고 분석을 실행해보세요.")
                    else:
                        # --- Preview: modules session quick view ---
                        modules_list_preview = list(st.session_state.get('modules', {}).values())
                        with st.expander("🔎 모듈별 요약/증거 미리보기", expanded=False):
                            if not modules_list_preview:
                                st.info("모듈 결과가 비어 있습니다. 먼저 각 모듈을 실행하세요.")
                            else:
                                for mr in modules_list_preview:
                                    try:
                                        st.subheader(f"• {getattr(mr, 'name', 'module')}")
                                        if getattr(mr, 'summary', None):
                                            st.json(mr.summary)
                                        evs = list(getattr(mr, 'evidences', []))
                                        if evs:
                                            st.write("Evidence 샘플 (상위 3)")
                                            for ev in evs[:3]:
                                                try:
                                                    st.write(f"- reason={ev.reason} | risk={float(ev.risk_score):.2f} | amount={float(ev.financial_impact):,.0f}")
                                                except Exception:
                                                    st.write("- (표시 실패)")
                                        if getattr(mr, 'tables', None):
                                            try: st.caption(f"tables: {list(mr.tables.keys())}")
                                            except Exception: pass
                                        if getattr(mr, 'figures', None):
                                            try: st.caption(f"figures: {list(mr.figures.keys())}")
                                            except Exception: pass
                                    except Exception:
                                        st.caption("(미리보기 실패)")
                    # LLM 키 미가용이어도 오프라인 리포트 모드로 생성 가능
                    LLM_OK = False
                    try:
                        from services.llm import openai_available
                        LLM_OK = bool(openai_available())
                    except Exception:
                        LLM_OK = False
                    if not LLM_OK:
                        st.info("🔌 OpenAI Key 없음: 오프라인 리포트 모드로 생성합니다. (클러스터/요약 LLM 미사용)")
                    rendered_report = False

                    # === 모델/토큰/컨텍스트 옵션 UI ===
                    colm1, colm2, colm3 = st.columns([1,1,1])
                    with colm1:
                        llm_model_choice = st.selectbox(
                            "LLM 모델", options=["gpt-5", "gpt-4o"], index=1,
                            help="gpt-5 미가용 시 자동으로 gpt-4o로 대체하세요(코드에서 예외 처리)."
                        )
                    with colm2:
                        desired_tokens = st.number_input(
                            "보고서 최대 출력 토큰", min_value=512, max_value=32000, value=16000, step=512,
                            help="실제 전송값은 모델 컨텍스트와 입력 토큰을 고려해 안전 클램프됩니다."
                        )
                    with colm3:
                        ctx_topk = st.number_input("컨텍스트 Evidence Top-K(모듈별)", min_value=5, max_value=100, value=20, step=5)
                        st.caption("요약/도표는 최소화하고 증거는 상위 Top-K만 사용합니다.")
                        st.session_state['ctx_topk'] = int(ctx_topk)

                    # 선택한 모델/토큰을 세션에 저장하여 하단 호출부에서 실제 사용
                    st.session_state['llm_model'] = llm_model_choice
                    st.session_state['llm_max_tokens'] = int(desired_tokens)

                    # --- 입력 영역 ---
                    mdf = st.session_state.master_df
                    ldf = st.session_state.ledger_df

                    # ① 계정 선택(필수) — 자동 추천 제거
                    acct_names_all = sorted(mdf['계정명'].dropna().unique().tolist())
                    pick_accounts = st.multiselect(
                        "보고서 대상 계정(들)을 선택하세요. (최소 1개)",
                        options=acct_names_all,
                        default=[],
                        key="report_accounts_pick"
                    )
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles_report = st.multiselect(
                            "사이클 프리셋 선택", list(cyc.CYCLE_KO.values()),
                            default=[], key="report_cycles_pick"
                        )
                        st.button("➕ 프리셋 적용", key="btn_apply_cycles_report", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="report_cycles_pick",
                                              accounts_state_key="report_accounts_pick",
                                              master_df=st.session_state.master_df))
                    # ② 옵션 제거: 항상 수행 플래그
                    opt_knn_evidence = True
                    opt_patterns = True
                    opt_patterns_py = True

                    # ③ 사용자 메모(선택)
                    manual_ctx = st.text_area(
                        "보고서에 추가할 메모/주의사항(선택)",
                        placeholder="예: 5~7월 대형 캠페인 집행 영향, 3분기부터 단가 인상 예정 등"
                    )

                    # ④ 선택 계정코드 매핑
                    pick_codes = (
                        mdf[mdf['계정명'].isin(st.session_state['report_accounts_pick'])]['계정코드']
                        .astype(str).tolist()
                    )

                    colA, colB, colC = st.columns([1,1,1])
                    with colA: st.write("선택 계정코드:", ", ".join(pick_codes) if pick_codes else "-")
                    with colB: st.write("기준 연도(CY):", int(ldf['연도'].max()))
                    with colC: st.write("보고서 기준:", "Current Year GL")

                    # 버튼은 계정 미선택 시 비활성화
                    btn = st.button("📝 보고서 생성", type="primary", disabled=(len(pick_codes) == 0))
                    if len(pick_codes) == 0:
                        st.info("계정 1개 이상 선택 시 버튼이 활성화됩니다.")

                    if btn:
                        import time
                        from analysis.anomaly import compute_amount_columns
                        from analysis.embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
                        from analysis.report import build_report_context, run_final_analysis, build_methodology_note, run_offline_fallback_report
                        from services.llm import LLMClient
                        from analysis.anomaly import ensure_zscore

                        t0 = time.perf_counter()
                        with st.status("보고서 준비 중...", expanded=True) as s:
                            # Step 1) 데이터 슬라이싱
                            s.write("① 스코프 적용 및 데이터 슬라이싱(CY/PY)…")
                            cur_year = ldf['연도'].max()
                            df_cy = ldf[(ldf['period_tag'] == 'CY') & (ldf['계정코드'].astype(str).isin(pick_codes))].copy()
                            df_py = ldf[(ldf['period_tag'] == 'PY') & (ldf['계정코드'].astype(str).isin(pick_codes))].copy()
                            s.write(f"    └ CY {len(df_cy):,}건 / PY {len(df_py):,}건")

                            # Step 2) 필수 파생(발생액/순액)
                            s.write("② 금액 파생 컬럼 생성(발생액/순액)…")
                            df_cy = compute_amount_columns(df_cy)
                            df_py = compute_amount_columns(df_py)

                            # Step 3) (선택) 패턴요약: 임베딩/클러스터링 (LLM 사용 가능 시에만)
                            cl_ok = False
                            if LLM_OK and opt_patterns and not df_cy.empty:
                                s.write("③ 임베딩·클러스터링 실행(선택)…")
                                # 입력 텍스트 풍부화 + 임베딩 + HDBSCAN (최대 N 제한으로 안전가드)
                                df_cy_small = df_cy.copy()
                                max_rows = 8000
                                if len(df_cy_small) > max_rows:
                                    df_cy_small = df_cy_small.sample(max_rows, random_state=42)
                                    s.write(f"    └ 데이터가 많아 {max_rows:,}건으로 샘플링")
                                df_cy_small = ensure_rich_embedding_text(df_cy_small)
                                try:
                                    llm_service = LLMClient(model=st.session_state.get('llm_model', 'gpt-4o'))
                                    emb_client = llm_service.client  # OpenAI 클라이언트 객체
                                    naming_function = llm_service.name_cluster
                                    # 보고서 생성을 위해 LLM 기반 클러스터 네이밍을 필수로 요구
                                    df_clu, ok = perform_embedding_and_clustering(
                                        df_cy_small, emb_client,
                                        name_with_llm=True, must_name_with_llm=True,
                                        use_large=bool(st.session_state.get("use_large_embedding", False)),
                                        rescue_tau=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
                                        embed_texts_fn=get_or_embed_texts,
                                        naming_fn=naming_function,
                                    )
                                    if ok:
                                        # 유사한 클러스터 이름을 LLM으로 통합
                                        df_clu, name_map = unify_cluster_names_with_llm(
                                            df_clu,
                                            sim_threshold=0.90,
                                            emb_model=st.session_state.get('embedding_model', None) or EMB_MODEL_SMALL,
                                            embed_texts_fn=get_or_embed_texts,
                                            confirm_pair_fn=make_synonym_confirm_fn(emb_client, st.session_state.get('llm_model', 'gpt-4o')),
                                        )
                                        # 추가 LLM 라벨 통합(JSON 매핑 방식) — CY의 cluster_group은 유지
                                        try:
                                            raw_map = unify_cluster_labels_llm(df_clu['cluster_name'].dropna().unique().tolist(), emb_client)
                                            if raw_map:
                                                df_clu['cluster_name'] = df_clu['cluster_name'].map(lambda x: raw_map.get(str(x), x))
                                                # ❗ cluster_group는 unify_cluster_names_with_llm()이 정한 canonical을 유지
                                        except Exception:
                                            pass
                                        # 간단 요약(상위 5개)
                                        topc = (df_clu.groupby('cluster_group')['발생액']
                                                .agg(['count','sum']).sort_values('sum', ascending=False).head(5))
                                        s.write("    └ 클러스터 상위 5개 요약:")
                                        st.dataframe(
                                            topc.rename(columns={'count':'건수','sum':'발생액합계'})
                                                .style.format({'발생액합계':'{:,.0f}'}),
                                            use_container_width=True
                                        )
                                        # 품질 지표(노이즈율·클러스터 수 등) 기록
                                        try:
                                            n = int(len(df_clu))
                                            noise_rate = float((df_clu['cluster_id'] == -1).mean()) if n else 0.0
                                            n_clusters = int(df_clu.loc[df_clu['cluster_id'] != -1, 'cluster_id'].nunique())
                                            if n_clusters > 0:
                                                avg_size = float(df_clu[df_clu['cluster_id'] != -1].groupby('cluster_id').size().mean())
                                            else:
                                                avg_size = 0.0
                                            rescue_rate = float(df_clu.get('rescued', False).mean()) if 'rescued' in df_clu.columns else 0.0
                                            model_used = df_clu.attrs.get('embedding_model', 'unknown')
                                            umap_on = bool(df_clu.attrs.get('umap_used', False))
                                            s.write(
                                                f"    └ Quality: N={n:,}, noise={noise_rate*100:.1f}%, "
                                                f"clusters={n_clusters}, avg_size={avg_size:.1f}, rescued={rescue_rate*100:.1f}%"
                                            )
                                            s.write(
                                                f"    └ Model/UMAP: {model_used} | UMAP={'on' if umap_on else 'off'} | τ={float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)):.2f}"
                                            )
                                            # 대시보드 카드용 품질 지표 저장
                                            st.session_state['cluster_quality'] = {
                                                "N": n,
                                                "noise_rate": noise_rate,
                                                "n_clusters": n_clusters,
                                                "avg_size": avg_size,
                                                "rescued_rate": rescue_rate,
                                                "model": model_used,
                                                "umap": umap_on,
                                                "tau": float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)),
                                            }
                                        except Exception:
                                            pass
                                        # 보고서 컨텍스트에 반영: group/label 동시 부착
                                        df_cy = df_cy.merge(
                                            df_clu[['row_id','cluster_id','cluster_name','cluster_group']],
                                            on='row_id', how='left'
                                        )
                                        # 필요 시 vector도 함께 병합 가능:
                                        # df_cy = df_cy.merge(df_clu[['row_id','vector']], on='row_id', how='left')
                                        # (현재는 perform_embedding_only 단계에서 CY/PY df에 vector가 직접 부여됨)
                                        # --- PY clustering and alignment (optional) ---
                                        if opt_patterns_py and not df_py.empty:
                                            try:
                                                from analysis.embedding import cluster_year, align_yearly_clusters, unify_cluster_labels_llm
                                                # sampling guard similar to CY
                                                df_py_small = df_py.copy()
                                                max_rows = 8000
                                                if len(df_py_small) > max_rows:
                                                    df_py_small = df_py_small.sample(max_rows, random_state=42)
                                                    s.write(f"    └ PY 데이터가 많아 {max_rows:,}건으로 샘플링")
                                                df_py_clu = cluster_year(
                                                    df_py_small, emb_client, embed_texts_fn=get_or_embed_texts
                                                )
                                                # 가능한 경우 row_id 기준으로 PY 결과 컬럼을 df_py에 병합
                                                if not df_py_clu.empty and 'row_id' in df_py.columns:
                                                    df_py = df_py.merge(df_py_clu, on='row_id', how='left', suffixes=("", "_pyclu"))
                                                # 정렬: PY 클러스터를 CY 클러스터에 매핑
                                                if 'cluster_id' in df_py_clu.columns:
                                                    mapping = align_yearly_clusters(df_clu, df_py_clu, sim_threshold=0.70)
                                                    # cluster_id → (aligned_cy_cluster, aligned_sim)
                                                    cy_id_to_name = df_clu.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()
                                                    def _get_pair(cid):
                                                        try:
                                                            if pd.isna(cid):
                                                                return (np.nan, np.nan)
                                                            cid_int = int(cid)
                                                            return mapping.get(cid_int, (np.nan, np.nan))
                                                        except Exception:
                                                            return (np.nan, np.nan)
                                                    if 'cluster_id' in df_py.columns:
                                                        pairs = df_py['cluster_id'].map(_get_pair)
                                                        df_py[['aligned_cy_cluster', 'aligned_sim']] = pd.DataFrame(pairs.tolist(), index=df_py.index)
                                                        # 이름은 CY의 이름으로 정렬(가능한 경우)
                                                        df_py['cluster_name'] = df_py['aligned_cy_cluster'].map(cy_id_to_name).fillna(df_py.get('cluster_name'))
                                                # 최종 라벨 정합: 전체 이름 집합 기준으로 통합; CY의 cluster_group은 유지, PY는 canonical로 정렬
                                                try:
                                                    all_names = pd.Series([], dtype=object)
                                                    if 'cluster_name' in df_cy.columns:
                                                        all_names = pd.concat([all_names, df_cy['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    if 'cluster_name' in df_py.columns:
                                                        all_names = pd.concat([all_names, df_py['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    all_names = all_names.dropna().unique().tolist()
                                                    canon = unify_cluster_labels_llm(all_names, emb_client)
                                                    if canon:
                                                        if 'cluster_name' in df_cy.columns:
                                                            df_cy['cluster_name'] = df_cy['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_name' in df_py.columns:
                                                            df_py['cluster_name'] = df_py['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_group' in df_py.columns:
                                                            df_py['cluster_group'] = df_py['cluster_name']
                                                except Exception:
                                                    pass
                                            except Exception as e:
                                                s.write(f"    └ PY 클러스터링/정렬 스킵: {e}")
                                        # 컨텍스트에 별도 노트는 추가하지 않음
                                        cl_ok = True
                                    else:
                                        s.write("    └ LLM 클러스터 이름 생성 실패 또는 결과 없음 → 보고서 생성 요건 미충족")
                                except Exception as e:
                                    s.write(f"    └ 임베딩/클러스터링 실패: {e}")
                            else:
                                s.write("③ 임베딩·클러스터링: LLM 미가용 또는 옵션 비활성 → 스킵")

                            # Step 3-1) (옵션 A) 근거 인용(KNN)용 임베딩만 수행 (LLM 가능 시)
                            if LLM_OK and opt_knn_evidence:
                                s.write("③-1 근거 인용용 임베딩(CY/PY)…")
                                from analysis.embedding import perform_embedding_only, ensure_rich_embedding_text
                                emb_client2 = LLMClient().client
                                df_cy = ensure_rich_embedding_text(df_cy)
                                df_py = ensure_rich_embedding_text(df_py)
                                df_cy = perform_embedding_only(
                                    df_cy, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                                df_py = perform_embedding_only(
                                    df_py, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                            elif not LLM_OK:
                                s.write("③-1 근거 인용 임베딩: LLM 미가용 → 스킵")

                            # Step 3-2) Z-Score: 반드시 존재해야 함
                            s.write("③-2 Z-Score 계산/검증…")
                            df_cy, z_ok = ensure_zscore(df_cy, pick_codes)
                            df_py, _    = ensure_zscore(df_py, pick_codes)  # 전기에도 Z-Score 계산(컨텍스트에서 사용)
                            if not z_ok:
                                s.write("    └ Z-Score 미계산 또는 전부 결측")

                            # ✅ 게이트 완화: Z-Score만 확보되면 보고서 진행.
                            #    (클러스터 실패 시 관련 섹션은 자동 축약/생략)
                            if not z_ok:
                                st.error("보고서 생성 중단: Z-Score 없음.")
                                s.update(label="보고서 요건 미충족", state="error")
                                st.stop()
                            if not cl_ok:
                                s.write("    └ 클러스터링 결과 없음 → 리포트에서 클러스터 섹션은 생략/축약됩니다.")

                            # Step 4) 컨텍스트 생성(전 모듈 포함) + 방법론 노트
                            s.write("④ 컨텍스트 텍스트 구성(전 모듈)…")
                            from analysis.report_adapter import wrap_dfs_as_module_result
                            from analysis.report import generate_rag_context_from_modules
                            from analysis.integrity import run_integrity_module
                            from analysis.timeseries import run_timeseries_module

                            # (1) 세션 초기화 및 공통 값 준비
                            st.session_state['modules'] = {}
                            lf_use = _lf_by_scope()
                            pm_use = float(st.session_state.get('pm_value', PM_DEFAULT))

                            # (2) 주요 모듈 실행 및 수집
                            if lf_use is not None:
                                # 이상치
                                try:
                                    amod = run_anomaly_module(lf_use, target_accounts=pick_codes or None,
                                                              topn=int(st.session_state.get('ctx_topk', 20)), pm_value=pm_use)
                                    _push_module(amod)
                                except Exception as _e:
                                    st.warning(f"anomaly 모듈 실패: {_e}")
                                # 추세(선택 계정 필요)
                                try:
                                    if pick_codes:
                                        _push_module(run_trend_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"trend 모듈 실패: {_e}")
                                # 거래처
                                try:
                                    if pick_codes:
                                        _push_module(run_vendor_module(lf_use, account_codes=pick_codes,
                                                                       min_amount=0.0, include_others=True))
                                except Exception as _e:
                                    st.warning(f"vendor 모듈 실패: {_e}")
                                # 상관(2개 이상일 때만)
                                try:
                                    if len(pick_codes) >= 2:
                                        _push_module(run_correlation_module(
                                            lf_use, accounts=pick_codes,
                                            corr_threshold=float(CORR_THRESHOLD_DEFAULT),
                                            cycles_map=cyc.get_effective_cycles(upload_id),
                                            emit_evidences=True,
                                        ))
                                except Exception as _e:
                                    st.warning(f"correlation 모듈 실패: {_e}")
                                # 정합성(ModuleResult) — 선택 계정 필터 적용
                                try:
                                    _push_module(run_integrity_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"integrity 모듈 실패: {_e}")
                                # NEW: 시계열 포함(집계→DTO 래핑)
                                try:
                                    if not df_cy.empty:
                                        ts = pd.concat([df_cy, df_py], ignore_index=True)
                                        ts["date"] = month_end_00(ts["회계일자"])  # 월말 00:00:00 정규화
                                        ts["account"] = ts["계정코드"].astype(str)
                                        ts["amount"] = ts.get("발생액", 0.0).astype(float)
                                        ts_in = ts.groupby(["account","date"], as_index=False)["amount"].sum()
                                        df_ts = run_timeseries_module(ts_in, account_col="account", date_col="date", amount_col="amount",
                                                                      pm_value=pm_use, output="flow", make_balance=False)
                                        summ_ts = {
                                            "n_series": int(df_ts["account"].nunique()) if not df_ts.empty else 0,
                                            "n_points": int(len(df_ts)),
                                            "max_abs_z": float(df_ts["z"].abs().max()) if ("z" in df_ts.columns and not df_ts.empty) else 0.0,
                                        }
                                        _push_module(ModuleResult(name="timeseries", summary=summ_ts,
                                                                  tables={"ts": df_ts}, figures={}, evidences=[], warnings=([] if not df_ts.empty else ["insufficient_points"])))
                                except Exception as _e:
                                    st.warning(f"timeseries 모듈 실패: {_e}")

                            # (3) 레거시 DF도 어댑터로 함께 포함(경량 컨텍스트용)
                            mr_ctx = wrap_dfs_as_module_result(df_cy, df_py, name="report_ctx")
                            modules_list = list(st.session_state.get('modules', {}).values()) + [mr_ctx]
                            # (4) 최종 컨텍스트 생성(Top-K 적용) — 신규 경로만 사용 + 메모 주입
                            ctx = generate_rag_context_from_modules(
                                modules_list,
                                pm_value=pm_use,
                                topk=int(st.session_state.get('ctx_topk', 20)),
                                manual_note=(manual_ctx or "")
                            )

                            # (상단 공통 미리보기로 대체)
                            note = build_methodology_note(report_accounts=pick_codes)

                            # Step 5) LLM 호출 전 점검(길이/토큰)
                            s.write("⑤ LLM 프롬프트 점검…")
                            prompt_len = len(ctx) + len(note)
                            s.write(f"    └ 컨텍스트 길이: {prompt_len:,} chars")
                            try:
                                import tiktoken
                                enc = tiktoken.get_encoding("cl100k_base")
                                est_tokens = len(enc.encode(ctx)) + len(enc.encode(note))
                                s.write(f"    └ 예상 토큰 수: ~{est_tokens:,} tokens")
                            except Exception:
                                s.write("    └ tiktoken 미설치: 토큰 추정 생략")

                            # Step 6) 보고서 생성: LLM 가능하면 시도, 실패/불가 시 오프라인 폴백
                            final_report = None
                            if LLM_OK:
                                s.write("⑥ LLM 요약 생성 호출…")
                                try:
                                    t_llm0 = time.perf_counter()
                                    llm_client = LLMClient(model=st.session_state.get('llm_model'))
                                    final_report = run_final_analysis(
                                        context=ctx + "\n" + note,
                                        account_codes=pick_codes,
                                        model=st.session_state.get('llm_model'),
                                        max_tokens=int(st.session_state.get('llm_max_tokens', 16000)),
                                        generate_fn=llm_client.generate,
                                    )
                                    s.write(f"    └ LLM 완료 (경과 {time.perf_counter()-t_llm0:.1f}s)")
                                except Exception as e:
                                    s.write(f"    └ LLM 실패: {e} → 오프라인 폴백으로 전환")

                            if final_report is None:
                                s.write("⑥-폴백: 오프라인 리포트 생성…")
                                final_report = run_offline_fallback_report(
                                    current_df=df_cy,
                                    previous_df=df_py,
                                    account_codes=pick_codes,
                                    pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                                )

                            s.update(label="보고서 준비 완료", state="complete")

                            # 결과 출력 및 세션 보존
                            st.session_state['last_report'] = final_report
                            st.session_state['last_context'] = ctx + "\n" + note
                            st.session_state['last_dfcy'] = df_cy
                            st.session_state['last_dfpy'] = df_py

                            st.success("보고서가 생성되었습니다.")
                            st.markdown("### 📄 AI 요약 보고서")
                            st.markdown(final_report)

                        with st.expander("🔎 근거 컨텍스트(LLM 입력)", expanded=False):
                            st.text(st.session_state['last_context'])

                        # ZIP 단일 다운로드 + RAW 미리보기
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['회계일자','계정코드','계정명','거래처','적요','발생액','순액','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()

                        raw_df = _build_raw_evidence(st.session_state['last_dfcy'])
                        st.markdown("#### 📑 근거: 선택 계정 원장(RAW) + 클러스터")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'발생액':'{:,.0f}','순액':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("표시할 RAW가 없습니다.")

                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "📥 보고서+근거 다운로드(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_current"  # 고유 키(현재 결과)
                        )

                        st.caption(f"⏱ 총 소요: {time.perf_counter()-t0:.1f}s")
                        rendered_report = True

                    # === 캐시된 이전 결과 렌더(버튼 미클릭 시에만) ===
                    if st.session_state.get('last_report') and not btn:
                        st.success("보고서가 준비되어 있습니다.")
                        st.markdown("### 📄 AI 요약 보고서")
                        st.markdown(st.session_state['last_report'])
                        with st.expander("🔎 근거 컨텍스트(LLM 입력)", expanded=False):
                            st.text(st.session_state['last_context'])
                        # RAW 미리보기 + ZIP 버튼 재출력
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['회계일자','계정코드','계정명','거래처','적요','발생액','순액','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()
                        raw_df = _build_raw_evidence(st.session_state.get('last_dfcy', pd.DataFrame()))
                        st.markdown("#### 📑 근거: 선택 계정 원장(RAW) + 클러스터")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'발생액':'{:,.0f}','순액':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("표시할 RAW가 없습니다.")
                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "📥 보고서+근거 다운로드(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_cached"  # 고유 키(캐시 결과)
                        )
                        # (가능 시) 클러스터 품질 카드 표시
                        cq = st.session_state.get("cluster_quality")
                        if cq:
                            st.markdown("---")
                            st.subheader("클러스터 품질 요약")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("Noise rate", f"{cq['noise_rate']*100:.1f}%")
                            c2.metric("#Clusters", f"{cq['n_clusters']}")
                            c3.metric("Avg size", f"{cq['avg_size']:.1f}")
                            c4.metric("Rescued", f"{cq['rescued_rate']*100:.1f}%")
                            st.caption(f"Model: {cq['model']} | UMAP: {'on' if cq['umap'] else 'off'} | τ={cq['tau']:.2f} | N={cq['N']:,}")
        except Exception as e:
            st.error(f"데이터 처리 중 오류가 발생했습니다: {e}")
            if st.button("매핑 단계로 돌아가기"):
                st.session_state.mapping_confirmed = False
                st.rerun()
else:
    st.info("⬅️ 왼쪽 사이드바에서 분석할 엑셀 파일을 업로드해주세요.")





==============================
📄 FILE: config.py
==============================

LLM_MODEL = "gpt-4o"
LLM_TEMPERATURE = 0.2
LLM_JSON_MODE = True
PM_DEFAULT = 500_000_000  # Project-wide Performance Materiality (KRW)

EMBED_BATCH_SIZE = 256
EMBED_CACHE_DIR = ".cache/embeddings"

# 훈님 결정 반영 ✅
SHAP_TOP_N_PER_ACCOUNT_DEFAULT = 25   # 사용자 UI에서 20~30 범위 선택 가능
CYCLE_RECOMMENDER = "llm_only"        # LLM 100% 자동 추천
PM_DEFAULT = PM_DEFAULT              # (kept above; single source of truth)

# --- Correlation defaults ---
# New canonical names
CORR_DEFAULT_METHOD = "pearson"
CORR_THRESHOLD_DEFAULT = 0.70
CORR_MIN_ACTIVE_MONTHS_DEFAULT = 6
CORR_MAX_LAG_DEFAULT = 6
CORR_ROLLING_WINDOW_DEFAULT = 6

# Backward-compatible aliases (kept for existing imports)
CORR_METHOD_DEFAULT = CORR_DEFAULT_METHOD
CORR_MIN_ACTIVE_MONTHS = CORR_MIN_ACTIVE_MONTHS_DEFAULT
CORR_ROLLWIN_DEFAULT = CORR_ROLLING_WINDOW_DEFAULT

# ---- NEW: Embedding & Clustering defaults ----
# Embedding model switch (Small by default; Large improves semantics at higher cost)
EMB_MODEL_SMALL = "text-embedding-3-small"
EMB_MODEL_LARGE = "text-embedding-3-large"
EMB_USE_LARGE_DEFAULT = False           # UI/auto-upscale can override per run

# UMAP threshold (apply UMAP → HDBSCAN only when N is large)
UMAP_APPLY_THRESHOLD = 8000             # set 0/None to disable
UMAP_N_COMPONENTS = 20
UMAP_N_NEIGHBORS = 30
UMAP_MIN_DIST = 0.0

# HDBSCAN noise-rescue cosine threshold
HDBSCAN_RESCUE_TAU = 0.75               # 0.72~0.78 usually works well

# Adaptive clustering knobs (computed per run, not static)
# min_cluster_size = max(8, int(sqrt(N))); min_samples = max(2, int(0.5 * min_cluster_size))

# ===== NEW: Materiality & Risk Weights =====
# === 통합 리스크 가중치(고정값; v2.0-RC 동결) ===
# 점수 = 0.5*A(|Z|정규화) + 0.4*F(PM 대비 비율 capped 1) + 0.1*K(PM 초과=1)
RISK_WEIGHT_A = 0.5
RISK_WEIGHT_F = 0.4
RISK_WEIGHT_K = 0.1

# --- NEW: Z-Score → sigmoid 스케일 조정 (로드맵 호환)
# anomaly_score = sigmoid(|Z| / Z_SIGMOID_DIVISOR)
# 로드맵 권고: 3.0 (과도 포화 완화)
Z_SIGMOID_DIVISOR = 3.0
Z_SIGMOID_SCALE = Z_SIGMOID_DIVISOR  # 하위호환

# --- 표준 회계 사이클 (STANDARD_ACCOUNTING_CYCLES) ---
# 키: 사이클 식별자, 값: 해당 사이클에 매핑될 가능성이 높은 계정명 키워드(부분일치)
# *한국어/영문 혼용. 필요 시 프로젝트 도메인에 맞춰 보강하세요.
STANDARD_ACCOUNTING_CYCLES = {
    "Cash": ["현금", "예금", "단기금융", "Cash", "Bank"],
    "Revenue": ["매출", "판매수익", "Sales", "Revenue"],
    "Receivables": ["매출채권", "외상매출금", "미수금", "Receivable", "A/R"],
    "Inventory": ["재고", "상품", "제품", "원재료", "재공품", "Inventory"],
    "Payables": ["매입채무", "외상매입금", "미지급금", "Payable", "A/P"],
    "Expenses": ["복리후생비", "급여", "임차료", "접대비", "감가상각비", "비용", "Expense"],
    "FixedAssets": ["유형자산", "감가상각누계", "기계장치", "건물", "비품", "PPE", "Fixed Asset"],
    "Equity": ["자본금", "이익잉여금", "자본잉여금", "Equity", "Capital"],
}

# --- NEW: Anomaly (Semantic & Isolation Forest) defaults ---
IFOREST_ENABLED_DEFAULT = True
IFOREST_N_ESTIMATORS = 256
IFOREST_MAX_SAMPLES = "auto"
IFOREST_CONTAM_DEFAULT = 0.03
IFOREST_RANDOM_STATE = 42

# Semantic outlier thresholds
SEMANTIC_Z_THRESHOLD = 2.5
SEMANTIC_MIN_RECORDS = 12
ANOMALY_IFOREST_SCORE_THRESHOLD = 0.70

# --- Provisional rule naming (도메인 합의 전) ---
PROVISIONAL_RULE_VERSION = "v1.0"
PROVISIONAL_RULE_NAME = f"잠정 기준({PROVISIONAL_RULE_VERSION})"

def provisional_risk_formula_str() -> str:
    """UI/리포트 안내문에 쓰일 가중치 요약 문자열을 동적으로 생성"""
    a = int(RISK_WEIGHT_A * 100)
    f = int(RISK_WEIGHT_F * 100)
    k = int(RISK_WEIGHT_K * 100)
    return f"통계적 이상({a}%) + 재무적 영향({f}%) + KIT 여부({k}%)"

# 리포트(최종본) 포함 조건 노브 (기본: 포함 안 함)
INCLUDE_RISK_MATRIX_SUMMARY_IN_FINAL = False
# ‘상위 N’ 결과가 이 값 미만이면 최종본에 생략 (근거 컨텍스트엔 유지)
RISK_MATRIX_SECTION_MIN_ITEMS = 3

# --- TimeSeries forecast knobs ---
FORECAST_MIN_POINTS = 8         # Prophet/ARIMA 사용 권장 최소 길이(권고치)
ARIMA_DEFAULT_ORDER = (1,1,1)

# --- User overrides for STANDARD_ACCOUNTING_CYCLES ---
CYCLES_USER_OVERRIDES_PATH = ".cache/cycles_overrides.json"



==============================
📄 FILE: analysis/aggregation.py
==============================

import pandas as pd


def month_end_00(date_series: pd.Series) -> pd.Series:
    """
    월말을 '해당월 말일 00:00:00'로 정규화 (반올림/올림 없이 고정)
    예: 2025-06-30 00:00:00
    """
    ds = pd.to_datetime(date_series, errors="coerce")
    return ds.dt.to_period("M").dt.to_timestamp("M")  # 월말 00:00:00


def aggregate_monthly(df: pd.DataFrame, date_col: str, amount_col: str) -> pd.DataFrame:
    """
    월별 발생액 합계를 반환.
    - 날짜 컬럼은 반드시 month_end_00로 정규화
    """
    work = df[[date_col, amount_col]].copy()
    work[date_col] = month_end_00(work[date_col])
    out = (
        work.groupby(date_col, dropna=True, as_index=False)[amount_col]
        .sum()
        .sort_values(date_col)
    )
    out = out.rename(columns={date_col: "date", amount_col: "amount"})
    return out





==============================
📄 FILE: analysis/anomaly.py
==============================

from __future__ import annotations
import numpy as np, math
import pandas as pd
from typing import List, Optional, Dict, Any, Tuple
from utils.helpers import find_column_by_keyword
from analysis.embedding import ensure_rich_embedding_text, perform_embedding_only  # ← services 주입식 임베딩 사용
from config import (
    IFOREST_ENABLED_DEFAULT, IFOREST_N_ESTIMATORS, IFOREST_MAX_SAMPLES,
    IFOREST_CONTAM_DEFAULT, IFOREST_RANDOM_STATE,
    SEMANTIC_Z_THRESHOLD, SEMANTIC_MIN_RECORDS, ANOMALY_IFOREST_SCORE_THRESHOLD
)


def compute_amount_columns(df: pd.DataFrame) -> pd.DataFrame:
    """발생액(절대 규모) / 순액(차-대) 계산."""
    dcol = find_column_by_keyword(df.columns, '차변')
    ccol = find_column_by_keyword(df.columns, '대변')
    df = df.copy()
    if not dcol or not ccol:
        df['발생액'] = 0.0; df['순액'] = 0.0; df['거래금액'] = 0.0
        return df
    d = pd.to_numeric(df[dcol], errors='coerce').fillna(0.0)
    c = pd.to_numeric(df[ccol], errors='coerce').fillna(0.0)
    row_amt = np.where((d > 0) & (c == 0), d,
              np.where((c > 0) & (d == 0), c,
              np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
    df['발생액'] = row_amt
    df['순액']  = d - c
    df['거래금액'] = df['순액']
    return df


def calculate_grouped_stats_and_zscore(df: pd.DataFrame, target_accounts: List[str], data_type: str = "당기") -> pd.DataFrame:
    """선택 계정 그룹의 발생액 분포 기준 Z-Score 산출."""
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    df = compute_amount_columns(df.copy())
    if not acct_col:
        df['Z-Score'] = 0.0
        return df
    is_target = df[acct_col].astype(str).isin([str(x) for x in target_accounts])
    tgt = df.loc[is_target, '발생액'].astype(float)
    df['Z-Score'] = 0.0
    if tgt.empty:
        return df
    mu = float(tgt.mean()); std = float(tgt.std(ddof=1))
    if std and std > 0:
        df.loc[is_target, 'Z-Score'] = (df.loc[is_target, '발생액'] - mu) / std
    else:
        med = float(tgt.median()); mad = float((np.abs(tgt - med)).median())
        df.loc[is_target, 'Z-Score'] = 0.0 if mad == 0 else 0.6745 * (df.loc[is_target, '발생액'] - med) / mad
    return df

# ---------------------- NEW: Semantic features -------------------------
def _cosine(a: np.ndarray, b: np.ndarray) -> float:
    da = float(np.linalg.norm(a)); db = float(np.linalg.norm(b))
    if da == 0.0 or db == 0.0: return 0.0
    return float(np.dot(a, b) / (da * db))

def _zseries(x: pd.Series) -> pd.Series:
    x = x.astype(float)
    mu, sd = float(x.mean()), float(x.std(ddof=1))
    if sd and sd > 0: return (x - mu) / sd
    # MAD fallback
    med = float(x.median()); mad = float((x.sub(med).abs()).median())
    return pd.Series(0.0, index=x.index) if mad == 0 else 0.6745 * (x - med) / mad

def _maybe_subcluster_vectors(X: np.ndarray) -> np.ndarray:
    """Return labels for vectors (auto-k KMeans via KDMeans shim)."""
    try:
        from analysis.kdmeans_shim import HDBSCAN
        model = HDBSCAN(n_clusters=None, random_state=42)
        return model.fit_predict(X).astype(int)
    except Exception:
        return np.zeros(len(X), dtype=int)

def _add_semantic_features(
    df: pd.DataFrame,
    *,
    acct_col: str,
    embed_client: Any,
    embed_texts_fn,              # injected (e.g., services.cache.get_or_embed_texts)
    use_large: Optional[bool] = None,
    subcluster: bool = False
) -> pd.DataFrame:
    """임베딩 벡터, 계정/클러스터 센트로이드, semantic_z(코사인 거리 z) 생성."""
    if df is None or df.empty: return df
    base = ensure_rich_embedding_text(df.copy())  # desc+vendor+월+규모+성격 조합 텍스트 생성
    base = perform_embedding_only(
        base, client=embed_client, text_col="embedding_text",
        use_large=use_large, embed_texts_fn=embed_texts_fn
    )
    if 'vector' not in base.columns or base['vector'].isna().any():
        return base
    # 벡터 행렬
    V = np.vstack(base['vector'].values).astype(float)
    # 선택: 계정 내 서브클러스터
    if subcluster:
        labels = pd.Series(index=base.index, dtype=int)
        for code, sub in base.groupby(base[acct_col].astype(str)):
            idx = sub.index
            Xi = np.vstack(sub['vector'].values)
            if len(Xi) < max(SEMANTIC_MIN_RECORDS, 4):
                labels.loc[idx] = 0
            else:
                labels.loc[idx] = _maybe_subcluster_vectors(Xi)
        base['cluster_id'] = labels.astype(int)
    else:
        base['cluster_id'] = 0
    # 그룹(계정×클러스터) 센트로이드 & 코사인 거리
    dists = []
    for (acct, cid), sub in base.groupby([base[acct_col].astype(str), 'cluster_id']):
        vecs = np.vstack(sub['vector'].values)
        c = vecs.mean(axis=0)
        # 1 - cosine sim → semantic distance
        dd = [1.0 - _cosine(v, c) for v in vecs]
        dists.append(pd.Series(dd, index=sub.index))
    base['semantic_dist'] = pd.concat(dists).sort_index()
    # z-표준화(계정×클러스터별)
    base['semantic_z'] = (
        base.groupby([base[acct_col].astype(str), 'cluster_id'])['semantic_dist']
            .transform(_zseries)
            .astype(float)
    )
    return base

# ---------------------- NEW: Isolation Forest --------------------------
def _fit_iforest_and_score(F: pd.DataFrame, *, contamination: float) -> np.ndarray:
    """Return anomaly scores in [0,1]."""
    try:
        from sklearn.ensemble import IsolationForest
    except Exception:
        return np.zeros(len(F), dtype=float)
    # NaN 방어 및 스케일링 간단 적용
    X = F.fillna(0.0).astype(float).values
    iso = IsolationForest(
        n_estimators=int(IFOREST_N_ESTIMATORS),
        max_samples=IFOREST_MAX_SAMPLES,
        contamination=float(contamination),
        random_state=int(IFOREST_RANDOM_STATE),
        n_jobs=-1
    ).fit(X)
    raw = -iso.score_samples(X)              # 더 클수록 이상
    lo, hi = float(np.min(raw)), float(np.max(raw))
    s = (raw - lo) / (hi - lo + 1e-12)      # [0,1]
    return s

# --- NEW: ensure_zscore ---
def ensure_zscore(df: pd.DataFrame, account_codes: List[str]):
    """
    Recompute Z-Score for the given account subset and return (df, ok).
    ok=True only if Z-Score column exists and has at least one non-null value.
    """
    df2 = calculate_grouped_stats_and_zscore(df.copy(), target_accounts=[str(x) for x in account_codes] if account_codes else [])
    z = df2.get('Z-Score')
    ok = (z is not None) and (z.notna().any())
    return df2, bool(ok)




# === (ADD) v0.18: ModuleResult 러너 ===
from analysis.contracts import ModuleResult, EvidenceDetail
from config import PM_DEFAULT, RISK_WEIGHT_A, RISK_WEIGHT_F, RISK_WEIGHT_K, Z_SIGMOID_SCALE, Z_SIGMOID_DIVISOR
import plotly.express as px
import numpy as np
import pandas as pd


def _z_bins_025_sigma(series: pd.Series):
    """0.25σ 간격 bin (±3σ 테일 포함)."""
    # 경계에 +3.0 포함(+inf 테일) → 총 bin 수 = 24(코어) + 2(테일) = 26
    edges = [-np.inf] + [round(x, 2) for x in np.arange(-3.0, 3.0 + 0.25, 0.25)] + [np.inf]
    core_lefts = [x for x in np.arange(-3.0, 3.0, 0.25)]  # 24개
    labels_mid = [f"{a:.2f}~{a+0.25:.2f}σ" for a in core_lefts]
    labels = ["≤-3σ"] + labels_mid + ["≥3σ"]               # 26개
    cats = pd.cut(
        series.astype(float),
        bins=edges,
        labels=labels,
        right=False,
        include_lowest=True,
    )
    # 빈 구간도 0으로 채워 순서 유지
    counts = cats.value_counts(sort=False).reindex(labels, fill_value=0)
    out = pd.DataFrame({"구간": labels, "건수": counts.values})
    order = labels
    return out, order


def _sigmoid(x: float) -> float:
    import math
    return 1.0 / (1.0 + math.exp(-x))


def _risk_from(z_abs: float, amount: float, pm: float):
    """리스크 점수 구성요소 계산.
    - a: 시그모이드 정규화된 이탈 강도(|Z|/scale). scale은 설정값.
    - f: PM 대비 금액비율(0~1로 캡). PM이 0/음수면 0으로 강제.
    - k: Key Item 플래그(PM 초과시 1). PM이 0/음수면 0으로 강제.
    """
    # 우선순위: Z_SIGMOID_DIVISOR(신규 노브) > Z_SIGMOID_SCALE(구명). 기본 1.0
    div = None
    try:
        div = float(Z_SIGMOID_DIVISOR)
    except Exception:
        div = None
    if not div or div <= 0:
        try:
            div = float(Z_SIGMOID_SCALE)
        except Exception:
            div = 1.0
    if not div or div <= 0:
        div = 1.0
    a = _sigmoid(float(abs(z_abs)) / float(div))      # anomaly_score
    # PM 가드: pm<=0이면 f=0, k=0
    if pm is None or float(pm) <= 0:
        f = 0.0
        k = 0.0
    else:
        f = min(1.0, abs(float(amount)) / float(pm))  # PM ratio (capped at 1)
        k = 1.0 if abs(float(amount)) >= float(pm) else 0.0
    score = RISK_WEIGHT_A * a + RISK_WEIGHT_F * f + RISK_WEIGHT_K * k
    return a, f, k, score


def _assertions_for_row(z_val: float) -> List[str]:
    # 기본 규칙: A는 항상 포함. 음의 큰 이탈(C), 양의 큰 이탈(E)을 보강.
    out = {"A"}
    try:
        if float(z_val) <= -2.0:
            out.add("C")
        if float(z_val) >=  2.0:
            out.add("E")
    except Exception:
        pass
    return sorted(out)


def run_anomaly_module(
    lf,
    target_accounts=None,
    topn=20,
    pm_value: Optional[float] = None,
    *,
    # --- NEW: injection knobs (analysis 레이어는 services에 직접 의존 금지) ---
    embed_client: Any = None,
    embed_texts_fn=None,
    use_large_embedding: Optional[bool] = None,
    semantic_enabled: bool = True,
    subcluster_enabled: bool = False,
    iforest_enabled: Optional[bool] = None,
    iforest_contamination: Optional[float] = None,
):
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    if not acct_col:
        return ModuleResult("anomaly", {}, {}, {}, [], ["계정코드 컬럼을 찾지 못했습니다."])

    # 대상 계정 서브셋
    if target_accounts:
        codes = [str(x) for x in target_accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    # Z-Score 계산
    df = calculate_grouped_stats_and_zscore(df, target_accounts=df[acct_col].astype(str).unique().tolist())
    if '회계일자' in df.columns:
        df['연월'] = df['회계일자'].dt.to_period('M').astype(str)

    # 이상치 플래그 (±3σ)
    df['is_outlier'] = df['Z-Score'].abs() >= 3

    # === (NEW) 의미피처/IForest 생성 ===
    if semantic_enabled and (embed_client is not None) and (embed_texts_fn is not None):
        try:
            df = _add_semantic_features(
                df, acct_col=acct_col, embed_client=embed_client,
                embed_texts_fn=embed_texts_fn, use_large=use_large_embedding,
                subcluster=subcluster_enabled
            )
        except Exception:
            # 의미피처 실패해도 기본 Z-Score 흐름은 유지
            pass
    # Isolation Forest (의미피처가 있든 없든 수치특징만으로도 동작)
    if iforest_enabled is None:
        iforest_enabled = bool(IFOREST_ENABLED_DEFAULT)
    if iforest_enabled:
        try:
            feats: Dict[str, Any] = {}
            feats['amt']      = pd.to_numeric(df.get('발생액', 0.0), errors='coerce').abs()
            feats['amt_log']  = np.log1p(feats['amt'])
            feats['z_abs']    = df.get('Z-Score', 0.0).abs()
            feats['sem_abs']  = df.get('semantic_z', 0.0).abs() if 'semantic_z' in df.columns else 0.0
            if '연월' in df.columns:
                # 간단 월 인덱스(모델의 시퀀스 surrogate)
                feats['month_idx'] = pd.Categorical(df['연월']).codes.astype(float)
            F = pd.DataFrame(feats, index=df.index)
            contam = float(iforest_contamination) if iforest_contamination is not None else float(IFOREST_CONTAM_DEFAULT)
            df['iforest_score'] = _fit_iforest_and_score(F, contamination=contam)
        except Exception:
            pass

    # 이상치 후보 테이블 (절댓값 기준 상위)
    out_cols = [c for c in ['row_id','회계일자','연월','계정코드','계정명','거래처','적요','발생액','Z-Score'] if c in df.columns]
    # (NEW) 테이블에 신호 컬럼 노출
    for extra in ['semantic_z','iforest_score','cluster_id']:
        if extra in df.columns and extra not in out_cols:
            out_cols.append(extra)
    cand = (df.assign(absz=df['Z-Score'].abs())
              .sort_values('absz', ascending=False)
              .drop(columns=['absz'])
              .head(int(topn)))
    table = cand[out_cols + (['is_outlier'] if 'is_outlier' in cand.columns and 'is_outlier' not in out_cols else [])] if out_cols else cand

    # === EvidenceDetail 생성 (KIT + |Z| 기준) ===
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    ev_rows: List[EvidenceDetail] = []
    # 증거 채집 대상: (1) PM 초과 or (2) |Z|>=2.5 or (3) 상위 topn
    #               + (4) semantic_z 과대 or (5) iforest_score 과대
    mask_key = df['발생액'].abs() >= pm if '발생액' in df.columns else pd.Series(False, index=df.index)
    mask_z   = df['Z-Score'].abs() >= 2.5 if 'Z-Score' in df.columns else pd.Series(False, index=df.index)
    mask_sem = df['semantic_z'].abs() >= float(SEMANTIC_Z_THRESHOLD) if 'semantic_z' in df.columns else pd.Series(False, index=df.index)
    thr_ifo  = float(ANOMALY_IFOREST_SCORE_THRESHOLD)
    mask_ifo = df['iforest_score'] >= thr_ifo if 'iforest_score' in df.columns else pd.Series(False, index=df.index)
    idx_sel  = set(df.index[mask_key | mask_z | mask_sem | mask_ifo].tolist()) | set(table.index.tolist())
    sub = df.loc[sorted(idx_sel)].copy() if len(idx_sel)>0 else df.head(0).copy()
    for _, r in sub.iterrows():
        z  = float(r.get('Z-Score', 0.0)) if pd.notna(r.get('Z-Score', np.nan)) else 0.0
        za = abs(z)
        amt = float(r.get('발생액', 0.0))
        a, f, k, score = _risk_from(za, amt, pm)   # (기존) 통합 위험 점수는 PM/|Z| 기반 유지
        # (NEW) anomaly_score에 의미/IForest 신호를 반영해 탐색 우선순위 개선
        semz = float(abs(r.get('semantic_z', 0.0))) if pd.notna(r.get('semantic_z', np.nan)) else 0.0
        ifo  = float(r.get('iforest_score', 0.0)) if pd.notna(r.get('iforest_score', np.nan)) else 0.0
        try:
            div = float(Z_SIGMOID_DIVISOR) if float(Z_SIGMOID_DIVISOR) > 0 else 3.0
        except Exception:
            div = 3.0
        sem_a = 1.0 / (1.0 + math.exp(-(semz/div))) if semz > 0 else 0.0
        anomaly_score = float(max(a, sem_a, ifo))
        ev_rows.append(EvidenceDetail(
            row_id=str(r.get('row_id','')),
            reason="; ".join(filter(None, [
                f"amt_z={z:+.2f}",
                (f"sem_z={r.get('semantic_z'):+.2f}" if 'semantic_z' in r and pd.notna(r['semantic_z']) else ""),
                (f"iforest={ifo:.2f}" if 'iforest_score' in r and pd.notna(r['iforest_score']) else "")
            ])),
            anomaly_score=anomaly_score,
            financial_impact=abs(amt),
            risk_score=float(score),
            is_key_item=bool(abs(amt) >= pm),
            impacted_assertions=_assertions_for_row(z),
            links={
                "account_code": str(r.get('계정코드','')),
                "account_name": str(r.get('계정명','')),
                "vendor":      str(r.get('거래처','')),
                "narration":   str(r.get('적요','')),
                "cluster_name": str(r.get('cluster_name','')) if 'cluster_name' in r.index else "",
                "cluster_group": str(r.get('cluster_group','')) if 'cluster_group' in r.index else "",
                "month":       str(r.get('연월','')) if '연월' in r.index else "",
                "period_tag": str(r.get('period_tag','')),
            }
        ))

    # step-σ bin 분포 막대
    figures = {}
    try:
        dist_df, order = _z_bins_025_sigma(df['Z-Score'])
        total_n = int(len(df))
        outlier_rate = float((df['Z-Score'].abs() >= 3).mean() * 100) if total_n else 0.0
        title = f"Z-Score 분포 (0.25σ bin, ±3σ 집계) — N={total_n:,}, outlier≈{outlier_rate:.1f}%"
        fig = px.bar(dist_df, x='구간', y='건수', title=title)
        fig.update_yaxes(separatethousands=True)
        fig.update_layout(bargap=0.10)
        figures = {"zscore_hist": fig}
    except Exception:
        pass

    summary = {
        "n_rows": int(len(df)),
        "n_candidates": int(len(table)),
        "accounts": sorted(df[acct_col].astype(str).unique().tolist()),
        "period_tag_coverage": dict(df.get('period_tag', pd.Series(dtype=str)).value_counts()) if 'period_tag' in df.columns else {}
    }
    # Evidence 미리보기 테이블(선택)
    try:
        import pandas as _pd
        ev_tbl = _pd.DataFrame([{
            "row_id": e.row_id,
            "계정코드": e.links.get("account_code",""),
            "계정명":   e.links.get("account_name",""),
            "risk_score": e.risk_score,
            "is_key_item": e.is_key_item,
            "impacted": ",".join(e.impacted_assertions),
            "reason": e.reason,
        } for e in ev_rows]).sort_values("risk_score", ascending=False).head(100)
    except Exception:
        ev_tbl = None

    return ModuleResult(
        name="anomaly",
        summary=summary,
        tables={"anomaly_top": table, **({"evidence_preview": ev_tbl} if ev_tbl is not None else {})},
        figures=figures,
        evidences=ev_rows,
        warnings=[]
    )


==============================
📄 FILE: analysis/assertion_risk.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from analysis.contracts import ModuleResult, EvidenceDetail, ASSERTIONS


HEATMAP_BS_RISK = "max"  # or "balance_only" / "weighted"


def _agg_bs_risk(rows: pd.DataFrame) -> float:
    """BS 셀 위험도 집계 규칙(기본 max).
    - EvidenceDetail.measure가 제공되는 경우에만 적용 가능.
    - 'weighted'는 balance 0.6, flow 0.4 가중.
    """
    if rows is None or rows.empty:
        return 0.0
    if HEATMAP_BS_RISK == "balance_only":
        r = rows.loc[rows.get("measure", pd.Series()).eq("balance"), "risk_score"]
        return float(r.max() if not r.empty else rows["risk_score"].max())
    if HEATMAP_BS_RISK == "weighted":
        w = rows.get("measure", pd.Series(index=rows.index)).map({"balance": 0.6, "flow": 0.4}).fillna(0.5)
        try:
            return float(np.average(rows["risk_score"].astype(float), weights=w))
        except Exception:
            return float(rows["risk_score"].max())
    return float(rows["risk_score"].max())


def build_matrix(modules: List[ModuleResult]):
    """
    모듈 EvidenceDetail → (계정 × 주장) 최대 risk_score 매트릭스 + 드릴다운 맵
    반환: (matrix_df[account_name x ASSERTIONS], evidence_map[(acct, asrt)] -> [row_id...])
    """
    bucket_rows: Dict[Tuple[str,str], List[Dict]] = {}
    emap: Dict[Tuple[str,str], List[str]] = {}
    accts: set[str] = set()

    for mod in modules:
        for ev in (mod.evidences or []):
            acct = ev.links.get("account_name") or ev.links.get("account_code") or "UNMAPPED"
            accts.add(acct)
            for a in (ev.impacted_assertions or []):
                key = (acct, a)
                bucket_rows.setdefault(key, []).append({
                    "risk_score": float(ev.risk_score),
                    "measure": getattr(ev, "measure", None)
                })
                emap.setdefault(key, []).append(str(ev.row_id))

    idx = sorted(accts)
    mat = pd.DataFrame(index=idx, columns=ASSERTIONS, data=0.0)
    for (acct, asrt), rows in bucket_rows.items():
        df = pd.DataFrame(rows)
        mat.loc[acct, asrt] = _agg_bs_risk(df) if not df.empty else 0.0
    return mat.fillna(0.0), emap





==============================
📄 FILE: analysis/contracts.py
==============================

from dataclasses import dataclass, field
import pandas as pd
from typing import Dict, List, Any, Optional, Literal
# --- New: Measure 타입 힌트("flow" 또는 "balance") ---
Measure = Literal["flow", "balance"]


@dataclass(frozen=True)
class LedgerFrame:
    df: pd.DataFrame
    meta: Dict[str, Any]  # 예: {"company": "...", "file_name": "...", "uploaded_at": ...}

# CEAVOP assertions
ASSERTIONS = ["C","E","A","V","O","P"]

@dataclass(frozen=True)
class EvidenceDetail:
    row_id: str
    reason: str                  # e.g., "|Z|=3.1 (CY group mean-based)"
    anomaly_score: float         # 0~1 normalized
    financial_impact: float      # KRW absolute amount
    risk_score: float            # integrated score
    is_key_item: bool            # PM exceed flag
    # --- NEW: measurement basis and sign rule ---
    measure: Measure = "flow"     # "flow"(월별 발생액, Δ잔액/순액) 또는 "balance"
    sign_rule: str = "assets/expenses↑=+, liabilities/equity↑=-"
    # --- NEW: 시계열 예측 메타 (옵셔널) ---
    model: Optional[str] = None           # 사용된 모델명 (예: EMA/MA/ARIMA/Prophet)
    window_policy: Optional[str] = None   # 예: "PY+CY"
    data_span: Optional[str] = None       # 예: "YYYY-MM ~ YYYY-MM"
    train_months: Optional[int] = None    # 학습 월 수
    horizon: Optional[int] = None         # 예측 수평(월)
    basis_note: Optional[str] = None      # 예: "BS는 잔액·발생액 병렬 계산"
    extra: Optional[Dict[str, Any]] = field(default_factory=dict)
    impacted_assertions: List[str] = field(default_factory=list)  # e.g., ["A","C"]
    links: Dict[str, Any] = field(default_factory=dict)           # e.g., {"account_code": "...", "account_name": "..."}

@dataclass(frozen=True)
class ModuleResult:
    name: str
    summary: Dict[str, Any]             # LLM 입력용 핵심 수치/지표
    tables: Dict[str, pd.DataFrame]
    figures: Dict[str, Any]             # plotly Figure
    evidences: List[EvidenceDetail]     # structured evidences
    warnings: List[str]


# 공개 API 명시(스키마 고정에 도움)
__all__ = [
    "LedgerFrame", "EvidenceDetail", "ModuleResult", "ASSERTIONS"
]




==============================
📄 FILE: analysis/correlation.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
import plotly.express as px
from typing import List, Dict, Any, Tuple, Optional, Mapping, Sequence
import re
from analysis.contracts import LedgerFrame, ModuleResult, EvidenceDetail
from utils.helpers import find_column_by_keyword
from config import (
    CORR_DEFAULT_METHOD, CORR_THRESHOLD_DEFAULT, CORR_MIN_ACTIVE_MONTHS_DEFAULT
)


def _monthly_pivot(df: pd.DataFrame, acct_col: str) -> pd.DataFrame:
    """계정코드×연월 피벗(거래금액 합계). PL/BS 모두 월 흐름 기준."""
    if '회계일자' not in df.columns:
        raise ValueError("회계일자 필요")
    g = (df.assign(연월=df['회계일자'].dt.to_period('M').astype(str))
           .groupby([acct_col, '연월'])['거래금액'].sum()
           .unstack('연월', fill_value=0.0)
           .sort_index())
    return g
 
def _filter_accounts_for_corr(piv: pd.DataFrame, min_active_months: int = 6) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    - Drop accounts with zero variance across months (std == 0) OR
      with insufficient active months (abs(value)>0 in fewer than min_active_months months).
    - Return filtered pivot and an exclusions dataframe with reasons.
    """
    if piv.empty:
        return piv, pd.DataFrame(columns=['계정코드','사유','활동월수','표준편차'])
    std = piv.std(axis=1)
    active = (piv.abs() > 0).sum(axis=1)
    reason = []
    idx = piv.index.astype(str)
    keep = (std > 0) & (active >= int(min_active_months))
    for code, s, a, k in zip(idx, std, active, keep):
        if k:
            continue
        r = []
        if s == 0:
            r.append("변동없음(표준편차 0)")
        if a < int(min_active_months):
            r.append(f"활동 월 부족(<{int(min_active_months)})")
        reason.append((code, " & ".join(r) if r else "제외", int(a), float(s)))
    excluded = pd.DataFrame(reason, columns=['계정코드','사유','활동월수','표준편차'])
    return piv.loc[keep], excluded


def _infer_cycle(account_name: str, cycles_map: Mapping[str, Sequence[str]]) -> Optional[str]:
    """
    STANDARD_ACCOUNTING_CYCLES 기반의 간단한 키워드 매핑.
    가장 먼저 매칭되는 사이클을 반환(우선순위: dict 정의 순서).
    """
    name = str(account_name or "").lower()
    for cycle, keywords in cycles_map.items():
        for kw in keywords:
            if kw and re.search(re.escape(str(kw).lower()), name):
                return cycle
    return None


def map_accounts_to_cycles(accounts: List[str], *, cycles_map: Mapping[str, Sequence[str]]) -> Dict[str, Optional[str]]:
    """배치 매핑: 계정명 리스트 → {계정명: 사이클(or None)}.
    cycles_map은 상위 레이어(app/services)에서 주입합니다.
    """
    return {acc: _infer_cycle(acc, cycles_map) for acc in accounts}


def _normalize_cycles_map(df: pd.DataFrame, cycles_map):
    """
    cycles_map 입력 유연화:
    - {계정코드 -> 사이클코드} 형태면 그대로 사용
    - {사이클코드 -> [키워드]} 형태면 계정명 기반으로 추정 매핑 생성
    - None이면 빈 dict
    """
    if not cycles_map:
        return {}
    # code->cycle 형태 판별
    # 값이 문자열이면 사이클 코드라고 가정
    if isinstance(next(iter(cycles_map.values())), str):
        return {str(k): str(v) for k, v in cycles_map.items()}
    # cycle->keywords 형태면 계정명으로 유추
    try:
        name_map = (df.drop_duplicates("계정코드")
                      .assign(계정코드=lambda d: d["계정코드"].astype(str))
                      .set_index("계정코드")["계정명"].astype(str).to_dict())
    except Exception:
        name_map = {}
    out = {}
    for code, nm in name_map.items():
        cyc = _infer_cycle(nm, cycles_map)
        if cyc: out[code] = cyc
    return out


def friendly_correlation_explainer() -> str:
    return (
        "### 해석 가이드(요약)\n"
        "- **상관 ≠ 인과**: 함께 움직인다고 원인/결과는 아닙니다.\n"
        "- **표본 길이**와 **활동월 수**가 짧으면 수치가 흔들립니다.\n"
        "- **음(-)의 상관**은 한쪽이 오르면 다른 쪽이 내리는 동행입니다.\n"
        "- 고급 탭의 **시차** 결과가 크면, ‘선후’ 관계 단서가 될 수 있으나 인과 입증은 아닙니다.\n"
        "- **롤링 안정성**이 낮으면(변동성↑) 일시적 상관일 가능성이 큽니다.\n"
    )


def suggest_anchor_accounts(lf: LedgerFrame, *, cycles_codes: list[str] | None = None,
                            corr_threshold: float = CORR_THRESHOLD_DEFAULT, topn: int = 5) -> pd.DataFrame:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    if not acct_col or df.empty:
        return pd.DataFrame(columns=['계정코드','계정명','규모합계','표준편차','degree','score'])
    piv = _monthly_pivot(df, acct_col)
    if piv.empty:
        return pd.DataFrame()
    if cycles_codes:
        idx_keep = piv.index.astype(str).isin([str(x) for x in cycles_codes])
        piv = piv.loc[idx_keep]
    if piv.shape[0] < 1:
        return pd.DataFrame()
    abs_piv = piv.abs()
    size = abs_piv.sum(axis=1)
    vol  = abs_piv.std(axis=1)
    corr = piv.T.corr('pearson').fillna(0.0)
    deg  = (corr.abs() >= float(corr_threshold)).sum(axis=1) - 1
    nz = lambda s: (s - s.min()) / (s.max() - s.min() + 1e-12)
    score = 0.4*nz(size) + 0.4*nz(vol) + 0.2*nz(deg)
    try:
        name_map = (
            df.drop_duplicates('계정코드')
              .assign(계정코드=lambda d: d['계정코드'].astype(str))
              .set_index('계정코드')['계정명'].astype(str).to_dict()
        )
    except Exception:
        name_map = {}
    out = (
        pd.DataFrame({
            '계정코드': piv.index.astype(str),
            '계정명':  piv.index.astype(str).map(name_map),
            '규모합계': size.values, '표준편차': vol.values,
            'degree': deg.reindex(piv.index).values, 'score': score.values
        })
        .sort_values('score', ascending=False)
        .head(int(topn))
    )
    return out


def run_correlation_module(
    lf: LedgerFrame,
    accounts: List[str] | None = None,
    *,
    method: str = CORR_DEFAULT_METHOD,
    corr_threshold: float = CORR_THRESHOLD_DEFAULT,
    min_active_months: int = CORR_MIN_ACTIVE_MONTHS_DEFAULT,
    cycles_map: Mapping[str, Sequence[str]] | Mapping[str, str] | None = None,
    within_same_cycle: bool | None = None,
    emit_evidences: bool = False,
) -> ModuleResult:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    if not acct_col:
        return ModuleResult("correlation", {}, {}, {}, [], ["계정코드 컬럼을 찾지 못했습니다."])

    # 대상 계정 필터
    if accounts:
        codes = [str(a) for a in accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    if df.empty:
        return ModuleResult("correlation", {}, {}, {}, [], ["선택된 데이터가 없습니다."])

    piv = _monthly_pivot(df, acct_col)
    piv_f, excluded = _filter_accounts_for_corr(piv, min_active_months=min_active_months)
    if piv_f.shape[0] < 2:
        warn = "상관을 계산할 계정이 2개 미만입니다."
        if not excluded.empty:
            warn += f" (제외된 계정 {len(excluded)}개: 변동없음/활동월 부족)"
        return ModuleResult("correlation", {}, {"excluded_accounts": excluded}, {}, [], [warn])

    corr = piv_f.T.corr(method=method)  # 계정×계정
    # 계정코드 → 계정명 매핑
    try:
        name_map = (
            df.drop_duplicates('계정코드')
              .assign(계정코드=lambda d: d['계정코드'].astype(str))
              .set_index('계정코드')['계정명']
              .astype(str).to_dict()
        )
    except Exception:
        name_map = {}
    xn = [name_map.get(str(c), str(c)) for c in corr.columns]
    yn = [name_map.get(str(r), str(r)) for r in corr.index]
    fig = px.imshow(
        corr,
        text_auto=False,
        title="계정 간 월별 상관 히트맵",
        labels=dict(x="계정", y="계정", color="상관계수"),
        aspect='auto',
        x=xn,
        y=yn,
    )
    fig.update_traces(hovertemplate="계정: %{y} × %{x}<br>상관계수: %{z:.3f}<extra></extra>")
    fig.update_coloraxes(cmin=-1, cmax=1)
    fig.update_xaxes(type='category')
    fig.update_yaxes(type='category')

    # 임계 상관쌍 테이블 (idempotent-safe)
    def build_strong_pairs(corr_matrix: pd.DataFrame, code_to_name: dict, threshold: float = 0.7) -> pd.DataFrame:
        import numpy as _np
        import pandas as _pd
        mask = _np.triu(_np.ones_like(corr_matrix, dtype=bool), k=1)
        cm = corr_matrix.copy().mask(mask)
        rows = []
        abs_vals = cm.abs().values
        idx_i, idx_j = _np.where(abs_vals >= threshold)
        for i, j in zip(idx_i, idx_j):
            rows.append({
                "계정코드_A": corr_matrix.index[i],
                "계정코드_B": corr_matrix.columns[j],
                "상관계수": float(cm.values[i, j]),
            })
        pairs_df = _pd.DataFrame(rows)
        if pairs_df.empty:
            return pairs_df
        pairs_df = pairs_df.assign(
            계정명_A=pairs_df["계정코드_A"].map(code_to_name),
            계정명_B=pairs_df["계정코드_B"].map(code_to_name),
        )
        base_cols = ["계정명_A", "계정코드_A", "계정명_B", "계정코드_B", "상관계수"]
        pairs_df = pairs_df[base_cols]
        pairs_df = pairs_df.reindex(
            pairs_df["상관계수"].abs().sort_values(ascending=False).index
        )
        return pairs_df

    pairs_df = build_strong_pairs(corr, name_map, threshold=float(corr_threshold))

    # === Evidence 생성: |r|≥thr 쌍을 구조화 (risk_score = |r|, financial_impact = min(두 계정의 월별 절대합))
    evidences = []
    try:
        # 계정별 규모(절대 흐름) 합계
        abs_sum = piv_f.abs().sum(axis=1).astype(float)  # index: 계정코드
        cyc_map_norm = _normalize_cycles_map(df, cycles_map)
        for _, row in pairs_df.iterrows():
            code_a = str(row["계정코드_A"]); code_b = str(row["계정코드_B"])
            same_cyc = None
            if cyc_map_norm:
                same_cyc = str(cyc_map_norm.get(code_a,"")) == str(cyc_map_norm.get(code_b,""))
            if within_same_cycle is True and same_cyc is not True:
                continue  # 동일 사이클만 남김
            r = float(row["상관계수"])
            fin = float(min(abs_sum.get(code_a, 0.0), abs_sum.get(code_b, 0.0)))
            evidences.append(EvidenceDetail(
                row_id=f"{code_a}|{code_b}",
                reason=f"corr={r:+.2f}" + (f" · same_cycle={bool(same_cyc)}" if same_cyc is not None else ""),
                anomaly_score=abs(r),           # 정규화(0~1)
                financial_impact=fin,           # 잠재 공동변동 규모의 보수적 근사
                risk_score=abs(r),              # r의 크기가 해석 복잡도/추적 필요도를 대변
                is_key_item=False,
                impacted_assertions=[],         # Assertions 비활성(훈님 방침)
                links={
                    "account_code_a": code_a, "account_code_b": code_b,
                    "account_name_a": row.get("계정명_A",""), "account_name_b": row.get("계정명_B",""),
                    "corr": r, "same_cycle": bool(same_cyc) if same_cyc is not None else None
                }
            ))
    except Exception:
        evidences = []

    # 사이클 매핑 요약(계정명 필요하므로 별도 표에서는 계정명 매핑 필요 시 upstream에서 처리)
    summary = {
        "n_accounts": int(corr.shape[0]),
        "n_pairs_over_threshold": int(len(pairs_df)),
        "corr_threshold": float(corr_threshold),
        "method": str(method)
    }
    return ModuleResult(
        name="correlation",
        summary=summary,
        tables={"strong_pairs": pairs_df, "corr_matrix": corr, "excluded_accounts": excluded},
        figures={"heatmap": fig},
        evidences=evidences,
        warnings=([f"제외된 계정 {len(excluded)}개(변동없음/활동월 부족)."] if not excluded.empty else [])
    )


# --- NEW: Focus 모듈 (단일 계정 vs 나머지) ---
def run_correlation_focus_module(
    lf: LedgerFrame,
    focus_account: str,                 # 계정코드 또는 계정명
    *,
    cycles_map: Mapping[str, Sequence[str]] | Mapping[str, str] | None = None,
    method: str = CORR_DEFAULT_METHOD,
    min_active_months: int = CORR_MIN_ACTIVE_MONTHS_DEFAULT,
    within_same_cycle: bool = True,
    corr_threshold: float = CORR_THRESHOLD_DEFAULT,
) -> ModuleResult:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    if not acct_col:
        return ModuleResult("correlation_focus", {}, {}, {}, [], ["계정코드 컬럼을 찾지 못했습니다."])
    # 코드/이름 방어
    fc = str(focus_account)
    mask = (df[acct_col].astype(str) == fc) | (df.get("계정명","").astype(str) == fc)
    if not mask.any():
        return ModuleResult("correlation_focus", {}, {}, {}, [], ["선택한 계정을 찾을 수 없습니다."])
    piv = _monthly_pivot(df, acct_col)
    piv_f, excluded = _filter_accounts_for_corr(piv, min_active_months=min_active_months)
    # focus 존재 보장
    # 이름→코드 매핑
    name_to_code = (
        df.drop_duplicates("계정명")
          .assign(계정코드=lambda d: d["계정코드"].astype(str))
          .set_index("계정명")["계정코드"].astype(str).to_dict()
    )
    code = fc if fc in piv_f.index else name_to_code.get(fc)
    if code not in piv_f.index:
        return ModuleResult("correlation_focus", {}, {"excluded_accounts": excluded}, {}, [], ["포커스 계정에 유효한 월별 변동이 없습니다."])
    # within_same_cycle 필터
    cyc_map_norm = _normalize_cycles_map(df, cycles_map)
    if within_same_cycle and cyc_map_norm:
        my_cycle = str(cyc_map_norm.get(str(code), ""))
        keep = [ix for ix in piv_f.index if str(cyc_map_norm.get(str(ix), "")) == my_cycle]
        piv_f = piv_f.loc[keep] if len(keep) >= 2 else piv_f
    if piv_f.shape[0] < 2:
        return ModuleResult("correlation_focus", {}, {"excluded_accounts": excluded}, {}, [], ["상관을 계산할 타 계정이 부족합니다."])
    rvec = piv_f.T.corr(method=method)[str(code)].drop(labels=[str(code)], errors="ignore").sort_values(key=lambda s: s.abs(), ascending=False)
    # 코드→이름 맵
    name_map = (df.drop_duplicates('계정코드').assign(계정코드=lambda d: d['계정코드'].astype(str))
                   .set_index('계정코드')['계정명'].astype(str).to_dict())
    tbl = pd.DataFrame({
        "상대계정코드": rvec.index.astype(str),
        "상대계정명": [name_map.get(c, c) for c in rvec.index.astype(str)],
        "상관계수": rvec.values
    })
    # 시각화(바 차트)
    fig = px.bar(tbl.head(30), x="상대계정명", y="상관계수", title=f"포커스: {name_map.get(str(code), str(code))} vs 타 계정")
    fig.update_yaxes(range=[-1,1])
    # evidence (임계 이상 Top-N)
    evid = []
    abs_sum = piv_f.abs().sum(axis=1).astype(float)
    for _, r in tbl.iterrows():
        v = float(r["상관계수"])
        if abs(v) < float(corr_threshold): break
        c2 = str(r["상대계정코드"])
        fin = float(min(abs_sum.get(str(code),0.0), abs_sum.get(c2,0.0)))
        same_cyc = None
        if cyc_map_norm:
            same_cyc = str(cyc_map_norm.get(str(code),"")) == str(cyc_map_norm.get(c2,""))
            if within_same_cycle and not same_cyc: 
                continue
        evid.append(EvidenceDetail(
            row_id=f"{code}|{c2}",
            reason=f"focus_corr={v:+.2f}" + (f" · same_cycle={bool(same_cyc)}" if same_cyc is not None else ""),
            anomaly_score=abs(v),
            financial_impact=fin,
            risk_score=abs(v),
            is_key_item=False,
            impacted_assertions=[],
            links={"focus_code": str(code), "other_code": c2, "focus_name": name_map.get(str(code), str(code)), "other_name": r["상대계정명"], "corr": v}
        ))
    summ = {"focus_code": str(code), "n_candidates": int(len(tbl))}
    return ModuleResult("correlation_focus", summ, {"focus_corr": tbl, "excluded_accounts": excluded}, {"bar": fig}, evid, [])



==============================
📄 FILE: analysis/corr_advanced.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import List, Dict
from analysis.contracts import ModuleResult, EvidenceDetail, LedgerFrame
from config import CORR_THRESHOLD_DEFAULT, CORR_MAX_LAG_DEFAULT, CORR_ROLLWIN_DEFAULT
import plotly.express as px


def _pivot_monthly_flow(lf: LedgerFrame, accounts: List[str]) -> pd.DataFrame:
    """
    입력 accounts 가 '계정코드' 또는 '계정명' 어느 쪽이든 동작하도록 방어.
    (UI에서 계정명을 전달했을 때 빈 피벗이 되던 문제 수정)
    """
    df = lf.df.copy()
    accs = {str(a) for a in (accounts or [])}
    if not accs:
        return pd.DataFrame()
    code_mask = df["계정코드"].astype(str).isin(accs)
    name_mask = df["계정명"].astype(str).isin(accs) if "계정명" in df.columns else False
    df = df[code_mask | name_mask].copy()
    if df.empty:
        return pd.DataFrame()
    df["월"] = pd.to_datetime(df["회계일자"], errors="coerce").dt.to_period("M").astype(str)
    # 월 기준 발생액(절대값) 합계 피벗
    pivot = df.pivot_table(index="월", columns="계정명", values="거래금액_절대값", aggfunc="sum").fillna(0.0)
    return pivot.sort_index()


def _corr_with_lag(a: pd.Series, b: pd.Series, lag: int) -> float:
    if lag > 0:
        return a.iloc[lag:].corr(b.iloc[:-lag])
    elif lag < 0:
        return a.iloc[:lag].corr(b.iloc[-lag:])
    else:
        return a.corr(b)


def _best_lag_pair(pivot: pd.DataFrame, max_lag: int) -> List[Dict[str, object]]:
    cols = list(pivot.columns)
    out: List[Dict[str, object]] = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            s1, s2 = pivot[cols[i]], pivot[cols[j]]
            best_lag, best_val = 0, np.nan
            for lag in range(-max_lag, max_lag + 1):
                v = _corr_with_lag(s1, s2, lag)
                if not np.isnan(v) and (np.isnan(best_val) or abs(v) > abs(best_val)):
                    best_lag, best_val = lag, v
            if not np.isnan(best_val):
                out.append({"계정A": cols[i], "계정B": cols[j], "최적시차": best_lag, "상관계수": best_val})
    out.sort(key=lambda x: abs(x["상관계수"]), reverse=True)
    return out


def _rolling_stability(pivot: pd.DataFrame, window: int = 6) -> List[Dict[str, object]]:
    cols = list(pivot.columns)
    out: List[Dict[str, object]] = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            r = pivot[cols[i]].rolling(window).corr(pivot[cols[j]])
            if len(r.dropna()) == 0:
                continue
            vol = float(r.std(skipna=True))
            mean = float(r.mean(skipna=True))
            out.append({"계정A": cols[i], "계정B": cols[j], "롤링평균": mean, "롤링변동성": vol})
    out.sort(key=lambda x: x["롤링변동성"])  # 낮은 변동성 우선
    return out


def run_corr_advanced(
    lf: LedgerFrame,
    accounts: List[str],
    *,
    method: str = "pearson",
    corr_threshold: float = CORR_THRESHOLD_DEFAULT,
    max_lag: int = CORR_MAX_LAG_DEFAULT,
    rolling_window: int = CORR_ROLLWIN_DEFAULT,
    cycles_map=None,
    within_same_cycle: bool=False,
) -> ModuleResult:
    name = "corr_advanced"
    if lf is None or getattr(lf, "df", None) is None:
        return ModuleResult(name=name, summary={}, tables={}, figures={}, evidences=[], warnings=["LedgerFrame 없음"])
    if not accounts:
        return ModuleResult(name=name, summary={"n_accounts": 0}, tables={}, figures={}, evidences=[], warnings=["선택 계정 없음"])

    pivot = _pivot_monthly_flow(lf, accounts)
    if pivot.empty or len(pivot.columns) < 2:
        return ModuleResult(name=name, summary={"n_accounts": len(accounts)}, tables={}, figures={}, evidences=[], warnings=["데이터 부족"])

    corr = pivot.corr(method=method).replace([np.inf, -np.inf], np.nan).fillna(0.0)

    # 히트맵 (계정명으로)
    fig_heat = px.imshow(
        corr,
        text_auto=False,
        color_continuous_scale="Blues",
        labels=dict(color="상관계수"),
        x=corr.columns,
        y=corr.index,
        title="계정 간 월별 상관 히트맵",
    )

    # 임계치 이상 쌍
    strong: List[Dict[str, object]] = []
    cols = list(corr.columns)
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            v = float(corr.iloc[i, j])
            if abs(v) >= float(corr_threshold):
                strong.append({"계정A": cols[i], "계정B": cols[j], "상관계수": v})
    # 동일 사이클 필터(선택)
    if within_same_cycle and cycles_map and "계정명" in lf.df.columns:
        # 이름->코드 역매핑
        nm2cd = (lf.df.drop_duplicates("계정명")
                    .assign(계정코드=lambda d: d["계정코드"].astype(str))
                    .set_index("계정명")["계정코드"].astype(str).to_dict())
        def _same(a,b):
            ca, cb = nm2cd.get(a), nm2cd.get(b)
            return (cycles_map.get(str(ca)) == cycles_map.get(str(cb))) if (ca and cb and isinstance(cycles_map, dict)) else True
        strong = [r for r in strong if _same(r["계정A"], r["계정B"])]
    strong_df = pd.DataFrame(strong)

    # 최적 시차 상관
    lag_pairs = pd.DataFrame(_best_lag_pair(pivot, int(max_lag)))

    # 롤링 안정성(낮은 변동성 우선)
    roll = pd.DataFrame(_rolling_stability(pivot, int(rolling_window)))

    # Evidence 샘플
    evid: List[EvidenceDetail] = []
    for row in strong[: min(10, len(strong))]:
        evid.append(EvidenceDetail(
            row_id=f"{row['계정A']}|{row['계정B']}",
            reason=f"corr={row['상관계수']:+.2f} (|r|≥{corr_threshold})",
            risk_score=min(1.0, abs(float(row["상관계수"]))),
            financial_impact=0.0,
            is_key_item=False,
            impacted_assertions=[],
            links={"account_a": row["계정A"], "account_b": row["계정B"], "type": "corr_strong"},
        ))

    summary = {
        "n_accounts": int(len(accounts)),
        "n_pairs_over_threshold": int(len(strong)),
        "corr_threshold": float(corr_threshold),
        "max_lag": int(max_lag),
        "rolling_window": int(rolling_window),
    }

    tables = {
        "corr_matrix": corr,
        "strong_pairs": strong_df,
        "lagged_pairs": lag_pairs,
        "rolling_stability": roll,
    }
    figures = {"heatmap": fig_heat}

    return ModuleResult(name=name, summary=summary, tables=tables, figures=figures, evidences=evid, warnings=[])





==============================
📄 FILE: analysis/embedding.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
import time
from typing import Dict, List, Optional, Tuple, Callable, Sequence, Any
# --- KDMeans 기반 HDBSCAN 대체 사용(의미상 HDBSCAN과 유사 동작) ---
from analysis.kdmeans_shim import HDBSCAN   # (주의) 내부적으로 KMeans 기반 구현
_HAS_HDBSCAN = True
# ---------------------------------------

from utils.helpers import find_column_by_keyword
from config import (
    EMB_MODEL_SMALL, EMB_MODEL_LARGE, EMB_USE_LARGE_DEFAULT,
    UMAP_APPLY_THRESHOLD, UMAP_N_COMPONENTS, UMAP_N_NEIGHBORS, UMAP_MIN_DIST,
    HDBSCAN_RESCUE_TAU,
)

# Embedding call defaults (can be overridden via pick_emb_model / params)
EMB_BATCH_SIZE = 128
EMB_TIMEOUT = 60
EMB_MAX_RETRY = 4
EMB_TRUNC_CHARS = 2000


def embed_texts_batched(
    texts: Sequence[str],
    *,
    embed_texts_fn: Callable[..., Any],
    client,
    model: str,
    batch_size: int = EMB_BATCH_SIZE,
    timeout: int = EMB_TIMEOUT,
    max_retry: int = EMB_MAX_RETRY,
    trunc_chars: int = EMB_TRUNC_CHARS,
) -> Dict[str, List[float]]:
    """배치 임베딩 유틸. {원본문자열: 벡터} 반환.
    services 레이어에 직접 의존하지 않고, 호출자가 임베딩 함수(embed_texts_fn)를 주입한다.
    """
    if not texts:
        return {}
    san: List[str] = []
    for t in texts:
        s = t if isinstance(t, str) else str(t)
        san.append(s[:trunc_chars] if trunc_chars and len(s) > trunc_chars else s)

    # 호출자로부터 주입받은 함수 사용(예: services.cache.get_or_embed_texts)
    return embed_texts_fn(
        san, client=client, model=model, batch_size=batch_size, timeout=timeout, max_retry=max_retry
    )


def _clean_text_series(s: pd.Series) -> pd.Series:
    """Lightweight denoising: collapse long numbers, squeeze spaces, trim."""
    s = s.astype(str)
    s = s.str.replace(r"\d{8,}", "#NUM", regex=True)
    s = s.str.replace(r"\s+", " ", regex=True).str.strip()
    return s

def ensure_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure df['embedding_text'] exists (desc+vendor) and is cleaned."""
    if 'embedding_text' not in df.columns:
        desc = df['적요'].fillna('').astype(str) if '적요' in df.columns else ''
        cp   = df['거래처'].fillna('').astype(str) if '거래처' in df.columns else ''
        df['embedding_text'] = desc + " (거래처: " + cp + ")"
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def _amount_bucket(a: float) -> str:
    a = float(abs(a))
    if a < 1_000_000:   return "1백만 미만"
    if a < 10_000_000:  return "1천만 미만"
    if a < 100_000_000: return "1억원 미만"
    if a < 500_000_000: return "5억원 미만"
    if a < 1_000_000_000:return "10억원 미만"
    if a < 5_000_000_000:return "50억원 미만"
    return "50억원 이상"


def ensure_rich_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """적요+거래처+월+금액구간+차/대 성격을 조합해 임베딩 텍스트 생성."""
    # 발생액/순액은 anomaly.compute_amount_columns를 쓰면 순환 import가 생김 → 최소 필드만 계산
    def _compute_amount_cols(_df: pd.DataFrame) -> pd.DataFrame:
        dcol = find_column_by_keyword(_df.columns, '차변')
        ccol = find_column_by_keyword(_df.columns, '대변')
        if not dcol or not ccol:
            _df['발생액'] = 0.0; _df['순액'] = 0.0
            return _df
        d = pd.to_numeric(_df[dcol], errors='coerce').fillna(0.0)
        c = pd.to_numeric(_df[ccol], errors='coerce').fillna(0.0)
        row_amt = np.where((d > 0) & (c == 0), d,
                  np.where((c > 0) & (d == 0), c,
                  np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
        _df['발생액'] = row_amt
        _df['순액']  = d - c
        return _df

    df = _compute_amount_cols(df.copy())
    month = df['회계일자'].dt.month.fillna(0).astype(int).astype(str).str.zfill(2) if '회계일자' in df.columns else "00"
    amtbin = df['발생액'].apply(_amount_bucket)
    sign   = np.where(df['순액'] >= 0, "차변성", "대변성")
    desc = df['적요'].fillna('').astype(str) if '적요' in df.columns else ''
    cp   = df['거래처'].fillna('').astype(str) if '거래처' in df.columns else ''
    df['embedding_text'] = desc + " | 거래처:" + cp + " | 월:" + month + " | 금액구간:" + amtbin + " | 성격:" + sign
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def perform_embedding_only(
    df: pd.DataFrame,
    client,
    text_col: str = 'embedding_text',
    *,
    use_large: bool|None=None,
    embed_texts_fn: Callable[..., Any],
) -> pd.DataFrame:
    """df[text_col]을 배치 임베딩해서 df['vector'] 추가"""
    if df.empty: return df
    if text_col not in df.columns:
        raise ValueError(f"임베딩 텍스트 컬럼 '{text_col}'이 없습니다.")
    uniq = df[text_col].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(
        uniq,
        embed_texts_fn=embed_texts_fn,
        client=client,
        model=model,
    )
    df = df.copy()
    df['vector'] = df[text_col].astype(str).map(mapping)
    # Guard embedding failures
    if df is None or df.empty:
        return df
    # 누락 보강 시도
    if df['vector'].isna().any():
        miss = df.loc[df['vector'].isna(), text_col].astype(str).unique().tolist()
        if miss:
            fb = embed_texts_batched(
                miss,
                embed_texts_fn=embed_texts_fn,
                client=client,
                model=model,
            )
            df.loc[df['vector'].isna(), 'vector'] = df.loc[df['vector'].isna(), text_col].astype(str).map(fb)
    return df


def _l2_normalize(X: np.ndarray) -> np.ndarray:
    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)

def _adaptive_hdbscan(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    n = int(X.shape[0])
    model = HDBSCAN(
        n_clusters=None,           # 자동 k 선택(실루엣 기반)
        min_cluster_size=max(8, int(np.sqrt(max(2, n)))),  # 너무 많은 군집 방지
        max_k=None,                # 필요시 상한 지정 가능
        k_search="silhouette",     # 휴리스틱 대신 실루엣 기반
        sample_size=2000,
        random_state=42,
        n_init="auto",
    )
    model.fit(X)
    labels = model.labels_.astype(int)
    try:
        probs = model.probabilities_.astype(float)
    except Exception:
        probs = np.ones(shape=(n,), dtype=float)
    return labels, probs

def _optional_umap(X: np.ndarray, enabled: Optional[bool] = None) -> Tuple[np.ndarray, bool]:
    """Dimensionality reduction control.
    - enabled=True: always try UMAP; on failure return (X, False)
    - enabled=False: skip → (X, False)
    - enabled=None: apply only if dataset size >= UMAP_APPLY_THRESHOLD
    Returns (X_or_reduced, used_flag).
    """
    if enabled is False:
        return X, False
    force = enabled is True
    try:
        thr = int(UMAP_APPLY_THRESHOLD) if UMAP_APPLY_THRESHOLD else None
    except Exception:
        thr = None
    if force or (thr and X.shape[0] >= thr):
        try:
            import umap
            reducer = umap.UMAP(
                n_components=int(UMAP_N_COMPONENTS),
                n_neighbors=int(UMAP_N_NEIGHBORS),
                min_dist=float(UMAP_MIN_DIST),
                random_state=42,
                metric="euclidean",
            )
            return reducer.fit_transform(X), True
        except Exception:
            return X, False
    return X, False

def _rescue_noise(df: pd.DataFrame, tau: float = HDBSCAN_RESCUE_TAU) -> pd.DataFrame:
    # KDMeans는 노이즈(-1) 라벨이 없으므로 구조적 리스큐 불필요
    return df

def pick_emb_model(use_large: bool|None=None) -> str:
    """Select embedding model (small/large)."""
    flag = EMB_USE_LARGE_DEFAULT if use_large is None else bool(use_large)
    return EMB_MODEL_LARGE if flag else EMB_MODEL_SMALL


def postprocess_cluster_names(df: pd.DataFrame) -> pd.DataFrame:
    """(간소화) LLM이 준 cluster_name을 그대로 유지한다. 태그/접미사 미부여."""
    return df


def perform_embedding_and_clustering(
    df: pd.DataFrame,
    client,
    *,
    name_with_llm: bool = True,
    must_name_with_llm: bool = False,
    naming_fn: Optional[Callable[[list[str], list[str]], Optional[str]]] = None,
    use_large: bool|None = None,
    rescue_tau: float = HDBSCAN_RESCUE_TAU,
    umap_enabled: bool|None = None,   # None => use config threshold
    embed_texts_fn: Callable[..., Any],
):
    """
    Embedding + (optional UMAP) + L2-normalized Euclidean HDBSCAN + noise rescue + (LLM naming).
    Returns: (df, ok)
    ok=False if: no vectors, or LLM naming required but missing/failed.
    """
    df = ensure_embedding_text(df.copy())
    uniq = df['embedding_text'].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(
        uniq,
        embed_texts_fn=embed_texts_fn,
        client=client,
        model=model,
    )
    df['vector'] = df['embedding_text'].astype(str).map(mapping)
    # Guard: embedding may fail and return None vectors
    if df is None or df.empty:
        return None, False
    # keep only valid vectors
    mask = df['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)
    df = df.loc[mask].copy()
    if df.empty:
        return None, False

    X = np.vstack(df['vector'].values).astype(float)
    # Optional UMAP if dataset large (threshold controlled by config)
    X, umap_used = _optional_umap(X, enabled=umap_enabled)
    # L2 normalize and cluster with Euclidean (≈ cosine)
    Xn = _l2_normalize(X)
    labels, probs = _adaptive_hdbscan(Xn)
    df['cluster_id'] = labels
    df['cluster_prob'] = probs
    # telemetry attrs
    try:
        df.attrs['embedding_model'] = model
        df.attrs['umap_used'] = bool(umap_used)
        df.attrs['rescue_tau'] = float(rescue_tau) if rescue_tau is not None else None
    except Exception:
        pass

    # --- Cluster naming via injected LLM callback (with graceful fallback) ---
    labels_uniq = sorted(pd.Series(labels).unique())
    names = {}
    if name_with_llm and (naming_fn is not None):
        for cid in labels_uniq:
            if cid == -1:
                names[cid] = "클러스터 노이즈(-1)"
                continue
            sub = df[df['cluster_id'] == cid]
            descs = sub['적요'].dropna().astype(str).unique().tolist()[:5] if '적요' in sub.columns else []
            vendors = sub['거래처'].dropna().astype(str).unique().tolist()[:5] if '거래처' in sub.columns else []
            # 콜백 사용(services.cluster_naming에서 생성)
            try:
                cand = naming_fn(descs, vendors)
            except Exception:
                cand = None
            # fallback rule-based name if LLM failed
            if not cand or cand == "이름 생성 실패":
                # heuristic: frequent vendor or keyword + amount tag
                amt_tag = "규모 중간"
                try:
                    abs_amt = sub.get('발생액', pd.Series(dtype=float)).abs().median()
                    if float(abs_amt) >= 1e8: amt_tag = "1억원 이상"
                    elif float(abs_amt) >= 1e7: amt_tag = "1천만~1억"
                except Exception:
                    pass
                top_vendor = sub.get('거래처', pd.Series(dtype=str)).value_counts().index.tolist()
                vname = top_vendor[0] if top_vendor else "일반"
                cand = f"{vname} 중심({amt_tag})"
            names[cid] = cand
    else:
        for cid in labels_uniq:
            names[cid] = "클러스터 노이즈(-1)" if cid == -1 else "이름 생성 실패"

    df['cluster_name'] = df['cluster_id'].map(names)
    df = postprocess_cluster_names(df)

    # --- Noise rescue: reassign -1 to nearest centroid if cosine >= tau ---
    if rescue_tau and float(rescue_tau) > 0:
        df = _rescue_noise(df, tau=float(rescue_tau))

    # gate: if must_name_with_llm, all non-noise clusters must have valid names
    if must_name_with_llm:
        non_noise = df[df['cluster_id'] != -1]
        has_any = not non_noise.empty
        invalid = non_noise['cluster_name'].isna() | non_noise['cluster_name'].astype(str).str.contains("^이름 생성 실패|^클러스터\\s", regex=True)
        if (not has_any) or bool(invalid.any()):
            return df, False

    # default reporting group equals the (validated) cluster_name; may be unified later
    df['cluster_group'] = df['cluster_name']
    return df, True



# --- NEW: LLM synonym grouping for cluster names ---
def _cosine_sim_matrix(vecs: list[list[float]]):
    import numpy as np
    V = np.asarray(vecs, dtype=float)
    if V.ndim != 2 or V.shape[0] == 0:
        return np.zeros((0, 0))
    Vn = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    return Vn @ Vn.T


def unify_cluster_names_with_llm(
    df: pd.DataFrame,
    sim_threshold: float = 0.90,
    emb_model: str = EMB_MODEL_SMALL,
    *,
    embed_texts_fn: Callable[..., Any],
    confirm_pair_fn: Optional[Callable[[str, str], bool]] = None,
):
    """
    Collapse clusters with effectively identical names.
    Strategy:
      1) Embed unique names (excluding noise), preselect candidate pairs via cosine >= sim_threshold.
      2) Ask LLM YES/NO if two names are synonyms for accounting transaction categories.
      3) Union-Find merge; choose canonical = most frequent name in df (fallback shortest).
    Returns: (df_with_cluster_group, mapping{name->canonical})
    """
    import numpy as np
    import itertools
    base = df.copy()
    if 'cluster_name' not in base.columns:
        base['cluster_group'] = base.get('cluster_name', None)
        return base, {}
    names = (
        base.loc[base['cluster_id'] != -1, 'cluster_name']
        .dropna().astype(str).unique().tolist()
    )
    if not names:
        base['cluster_group'] = base['cluster_name']
        return base, {}

    # Embedding prefilter
    name2vec = embed_texts_batched(
        names,
        embed_texts_fn=embed_texts_fn,
        client=None,
        model=emb_model,
    )
    ordered = [n for n in names if n in name2vec]
    vecs = [name2vec[n] for n in ordered]
    S = _cosine_sim_matrix(vecs)

    # Union-Find
    parent = {n: n for n in ordered}
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    # Pair confirmation (LLM or 다른 정책) — 반드시 주입 받은 confirm_pair_fn을 사용
    for i, j in itertools.combinations(range(len(ordered)), 2):
        if S[i, j] < float(sim_threshold):
            continue
        a, b = ordered[i], ordered[j]
        if confirm_pair_fn is None:
            # 콜백이 없으면 보수적으로 merge 생략(아키텍처 준수)
            continue
        try:
            if confirm_pair_fn(a, b):
                union(a, b)
        except Exception:
            continue

    # Build groups
    groups = {}
    for n in ordered:
        r = find(n)
        groups.setdefault(r, []).append(n)

    # Choose canonical per group
    freq = base['cluster_name'].value_counts().to_dict()
    mapping = {}
    for root, members in groups.items():
        cand = sorted(members, key=lambda x: (-freq.get(x, 0), len(x)))[0]
        for m in members:
            mapping[m] = cand

    base['cluster_group'] = base['cluster_name'].map(lambda x: mapping.get(x, x))
    return base, mapping


# --- NEW: Utilities for PY→CY mapping and label unification ---
def _cosine(a, b):
    import numpy as np
    if a is None or b is None:
        return np.nan
    a = np.asarray(a)
    b = np.asarray(b)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    return float(np.dot(a, b) / denom) if denom else np.nan


def map_previous_to_current_clusters(df_cur: pd.DataFrame, df_prev: pd.DataFrame) -> pd.DataFrame:
    """
    전기 전표를 당기 클러스터 센트로이드에 최근접 배정하여 (mapped_cluster_id/name, mapped_sim) 부여.
    - 노이즈(-1) 센트로이드는 제외
    - 반환: prev_df(with mapped_cluster_id, mapped_cluster_name, mapped_sim)
    """
    import numpy as np
    import pandas as pd
    need_cols = ['cluster_id', 'cluster_name', 'vector']
    if any(c not in df_cur.columns for c in need_cols) or 'vector' not in df_prev.columns:
        return df_prev.copy()
    cur = df_cur[df_cur['cluster_id'] != -1].copy()
    if cur.empty:
        return df_prev.copy()
    # 센트로이드 계산
    cents = (
        cur.groupby(['cluster_id', 'cluster_name'])['vector']
           .apply(lambda s: np.mean(np.vstack(list(s)), axis=0))
           .reset_index()
    )
    prev = df_prev.copy()

    def _pick(row: pd.Series) -> pd.Series:
        v = row.get('vector', None)
        if v is None:
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        sims = cents['vector'].apply(lambda c: _cosine(v, c))
        if len(sims) == 0 or sims.isna().all():
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        idx = int(sims.idxmax())
        return pd.Series({
            'mapped_cluster_id': int(cents.loc[idx, 'cluster_id']),
            'mapped_cluster_name': cents.loc[idx, 'cluster_name'],
            'mapped_sim': float(sims.max()) if not np.isnan(sims.max()) else np.nan,
        })

    prev[['mapped_cluster_id', 'mapped_cluster_name', 'mapped_sim']] = prev.apply(_pick, axis=1)
    return prev


def unify_cluster_labels_llm(*_args, **_kwargs) -> dict:
    """Deprecated in analysis layer. Use services.cluster_naming.unify_cluster_labels_llm instead."""
    return {}


# --- NEW: Yearly clustering helpers and alignment ---
def cluster_year(df: pd.DataFrame, client, *, embed_texts_fn: Callable[..., Any]) -> pd.DataFrame:
    """
    당기/전기 등 입력 df에 대해 풍부 임베딩 텍스트를 보장하고 HDBSCAN+LLM 네이밍을 실행.
    반환: ['row_id','cluster_id','cluster_name','cluster_prob','vector']가 포함된 DataFrame(부분집합 가능).
    입력이 비어있으면 빈 DataFrame 반환.
    """
    if df is None or df.empty:
        return pd.DataFrame()
    from .embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
    df_in = ensure_rich_embedding_text(df.copy())
    df_out, ok = perform_embedding_and_clustering(
        df_in,
        client,
        name_with_llm=True,
        must_name_with_llm=False,
        embed_texts_fn=embed_texts_fn,
    )
    if not ok or df_out is None:
        return pd.DataFrame()
    keep = [c for c in ['row_id','cluster_id','cluster_name','cluster_prob','vector'] if c in df_out.columns]
    return df_out[keep].copy()


def compute_centroids(df: pd.DataFrame) -> pd.DataFrame:
    """
    ['cluster_id','vector']를 갖는 df에서 클러스터별 센트로이드 계산(-1 제외).
    'cluster_name'이 있으면 함께 유지.
    반환: columns=['cluster_id','cluster_name','vector']
    """
    import numpy as np
    import pandas as pd
    need = ['cluster_id','vector']
    if df is None or df.empty or any(c not in df.columns for c in need):
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    base = df[df['cluster_id'] != -1].copy()
    if base.empty:
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    def _mean_stack(s):
        try:
            return np.mean(np.vstack(list(s)), axis=0)
        except Exception:
            return None
    cents = base.groupby('cluster_id')['vector'].apply(_mean_stack).reset_index()
    if 'cluster_name' in base.columns:
        name_map = base.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name']
        cents['cluster_name'] = cents['cluster_id'].map(name_map)
    else:
        cents['cluster_name'] = None
    # re-order columns
    cents = cents[['cluster_id','cluster_name','vector']]
    # drop rows with invalid vectors
    cents = cents[cents['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)]
    return cents.reset_index(drop=True)


def align_yearly_clusters(df_cy: pd.DataFrame, df_py: pd.DataFrame, sim_threshold: float = 0.70) -> dict:
    """
    CY/PY 센트로이드 코사인 유사도 행렬 기반 Hungarian 매칭(cost=1-sim).
    반환: {py_cluster_id: (cy_cluster_id, sim)} (임계치 미만은 값 None)
    """
    import numpy as np
    py_c = compute_centroids(df_py)
    cy_c = compute_centroids(df_cy)
    if py_c.empty or cy_c.empty:
        return {}
    # build similarity matrix
    py_vecs = list(py_c['vector'].values)
    cy_vecs = list(cy_c['vector'].values)
    S_py = _cosine_sim_matrix(py_vecs)
    S_cy = _cosine_sim_matrix(cy_vecs)
    # We need PY x CY sims; compute directly
    # Efficient: normalize and dot
    import numpy as np
    def _norm(V):
        V = np.asarray([np.asarray(v, dtype=float) for v in V], dtype=float)
        return V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    Npy = _norm(py_vecs)
    Ncy = _norm(cy_vecs)
    sim = Npy @ Ncy.T  # shape: [n_py, n_cy]
    # Hungarian matching on cost = 1 - sim
    try:
        from scipy.optimize import linear_sum_assignment
        cost = 1.0 - sim
        row_ind, col_ind = linear_sum_assignment(cost)
    except Exception:
        # Fallback: greedy matching by highest sim without replacement
        pairs = []
        used_py = set(); used_cy = set()
        # flatten and sort
        flat = [
            (i, j, float(sim[i, j]))
            for i in range(sim.shape[0])
            for j in range(sim.shape[1])
        ]
        flat.sort(key=lambda x: x[2], reverse=True)
        for i, j, s in flat:
            if i in used_py or j in used_cy:
                continue
            pairs.append((i, j))
            used_py.add(i); used_cy.add(j)
        row_ind = np.array([p[0] for p in pairs], dtype=int)
        col_ind = np.array([p[1] for p in pairs], dtype=int)
    mapping: dict[int, tuple[int, float] | None] = {}
    for k in range(len(row_ind)):
        i = int(row_ind[k]); j = int(col_ind[k])
        s = float(sim[i, j])
        py_id = int(py_c.loc[i, 'cluster_id'])
        cy_id = int(cy_c.loc[j, 'cluster_id'])
        if s >= float(sim_threshold):
            mapping[py_id] = (cy_id, s)
        else:
            mapping[py_id] = None
    # Ensure all PY clusters are present in mapping
    for py_id in py_c['cluster_id'].tolist():
        if py_id not in mapping:
            mapping[py_id] = None
    return mapping




==============================
📄 FILE: analysis/evidence.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import Optional


def build_knn_index(prev_df: pd.DataFrame):
    """prev_df['vector']로 KNN 생성."""
    if 'vector' not in prev_df.columns or prev_df['vector'].isna().any():
        raise ValueError("build_knn_index: prev_df에 'vector' 필요")
    from sklearn.neighbors import NearestNeighbors
    X = np.vstack(prev_df['vector'].values)
    knn = NearestNeighbors(metric='cosine', n_neighbors=min(10, len(X))).fit(X)
    return knn, X


def cluster_centroid_vector(cluster_df: pd.DataFrame):
    if 'vector' not in cluster_df.columns or cluster_df.empty:
        return None
    return np.mean(np.vstack(cluster_df['vector'].values), axis=0)


def retrieve_similar_from_previous(prev_df, prev_knn, prev_X, query_vec, topk=5, dedup_by_vendor=True, min_sim=0.7):
    if query_vec is None or prev_X is None or len(prev_X) == 0:
        return pd.DataFrame()
    dist, idx = prev_knn.kneighbors([query_vec], n_neighbors=min(max(10, topk*3), len(prev_X)))
    cands = prev_df.iloc[idx[0]].copy()
    cands['similarity'] = (1 - dist[0])
    cands = cands[cands['similarity'] >= min_sim]
    if dedup_by_vendor and '거래처' in cands.columns:
        cands = cands.sort_values('similarity', ascending=False).drop_duplicates('거래처', keep='first')
    cands = cands.sort_values('similarity', ascending=False).head(topk)
    cols = ['회계일자','계정코드','거래처','적요','발생액','similarity']
    for c in cols:
        if c not in cands.columns: cands[c] = np.nan
    return cands[cols]


def build_cluster_evidence_block(current_df: pd.DataFrame, previous_df: pd.DataFrame,
                                 topk: int = 3, restrict_same_months: bool = True, min_sim: float = 0.7,
                                 dedup_by_vendor: bool = True) -> str:
    if any(col not in current_df.columns for col in ['cluster_id','vector']):
        return "\n\n## 근거 인용(전기 유사 거래)\n- 현재 데이터에 클러스터/벡터가 없어 근거를 생성할 수 없습니다."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## 근거 인용(전기 유사 거래)\n- 전기 데이터 임베딩이 없어 근거를 생성할 수 없습니다."
    def _ok_vec(v):
        return v is not None and isinstance(v, (list, tuple, np.ndarray)) and len(v) > 0
    lines = ["\n\n## 근거 인용(전기 유사 거래)"]
    for cid in sorted(current_df['cluster_id'].unique()):
        cur_c = current_df[current_df['cluster_id'] == cid]
        if cur_c.empty: continue
        cname = cur_c['cluster_name'].iloc[0] if 'cluster_name' in cur_c.columns else str(cid)
        lines.append(f"[클러스터 #{cid} | {cname}]")
        prev_subset = previous_df.copy()
        if restrict_same_months and '회계일자' in cur_c.columns and cur_c['회계일자'].notna().any():
            months = set(cur_c['회계일자'].dt.month.dropna().unique().tolist())
            filtered = previous_df[previous_df['회계일자'].dt.month.isin(months)]
            prev_subset = filtered if not filtered.empty else previous_df
        if 'vector' in prev_subset.columns:
            prev_subset = prev_subset[prev_subset['vector'].apply(_ok_vec)].copy()
        if prev_subset.empty:
            lines.append("    └ 전기 유사 벡터 없음"); continue
        try:
            knn, X = build_knn_index(prev_subset)
        except Exception as e:
            lines.append(f"    └ 인덱스 생성 실패: {e}"); continue
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_subset, knn, X, qv, topk=topk, dedup_by_vendor=dedup_by_vendor, min_sim=min_sim)
        if ev.empty:
            lines.append("    └ 유사 전표: 없음")
        else:
            def _fmt_date(x): 
                try: return x.strftime('%Y-%m-%d') if pd.notna(x) else ""
                except: return ""
            def _fmt_money(x):
                try: return f"{int(x):,}원"
                except: return str(x)
            def _fmt_sim(s):
                try: return f"{float(s):.2f}"
                except: return "N/A"
            for rank, (_, r) in enumerate(ev.sort_values('similarity', ascending=False).iterrows(), 1):
                lines.append(f"    {rank}) {_fmt_date(r['회계일자'])} | {str(r['거래처'])} | {_fmt_money(r['발생액'])} | sim {_fmt_sim(r['similarity'])}")
    return "\n".join(lines)



def build_transaction_evidence_block(current_df, previous_df, topn=10, per_tx_topk=3, min_sim=0.8):
    import numpy as np, pandas as pd
    def _ok_vec(v): return isinstance(v, (list, tuple, np.ndarray)) and len(v)>0
    if current_df.empty or 'vector' not in current_df.columns: 
        return "\n\n## 거래별 근거\n- 현재 데이터에 벡터가 없어 근거를 생성할 수 없습니다."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## 거래별 근거\n- 전기 데이터 임베딩이 없어 근거를 생성할 수 없습니다."

    cur = current_df.copy()
    if 'Z-Score' in cur.columns and cur['Z-Score'].notna().any():
        order_idx = cur['Z-Score'].abs().sort_values(ascending=False).index
    else:
        # Z-Score 미시행 시 발생액 상위
        amt = cur.get('발생액', pd.Series(dtype=float))
        order_idx = amt.sort_values(ascending=False).index
    cur = cur.reindex(order_idx).head(int(topn))

    # 전기 벡터 유효성 필터
    prev = previous_df.copy()
    prev = prev[prev['vector'].apply(_ok_vec)]
    if prev.empty:
        return "\n\n## 거래별 근거\n- 전기 데이터 벡터가 유효하지 않습니다."

    from .evidence import build_knn_index, retrieve_similar_from_previous
    try:
        knn, X = build_knn_index(prev)
    except Exception:
        return "\n\n## 거래별 근거\n- 전기 KNN 인덱스 생성 실패."

    lines = [f"\n\n## 거래별 근거 (상위 {len(cur)}건)"]
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        if qv is None: continue
        # 동월 우선
        psub = prev
        if '회계일자' in r and pd.notna(r['회계일자']):
            m = r['회계일자'].month
            cand = prev[prev['회계일자'].dt.month == m]
            if not cand.empty: psub = cand
            knn, X = build_knn_index(psub)
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=int(per_tx_topk), dedup_by_vendor=True, min_sim=float(min_sim))
        dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
        amt = r.get('발생액', 0.0); z = r.get('Z-Score', np.nan)
        ztxt = f" | Z={z:+.2f}" if not pd.isna(z) else ""
        lines.append(f"[{i}] {dt} | 거래처:{r.get('거래처','')} | 금액:{int(amt):,}원{ztxt}")
        if ev.empty:
            lines.append("    └ 유사 전표: 없음")
        else:
            lines.append(f"    └ 전기 유사 Top {len(ev)}")
            for _, rr in ev.iterrows():
                d2 = rr['회계일자'].strftime('%Y-%m-%d') if pd.notna(rr['회계일자']) else ''
                lines.append(f"       • {d2} | {rr['거래처']} | {int(rr['발생액']):,}원 | sim {rr['similarity']:.2f}")
    return "\n".join(lines)

# --- NEW: Structured evidence blocks for the redesigned context ---
def build_current_cluster_block(current_df: pd.DataFrame) -> str:
    """
    ## 당기 클러스터 및 금액
    - One bullet per cluster_group: total absolute amount, count, and ONE example voucher.
    """
    import pandas as pd
    if current_df.empty or 'cluster_group' not in current_df.columns:
        return "\n\n## 당기 클러스터 및 금액\n- (클러스터 결과 없음)"
    lines = ["\n\n## 당기 클러스터 및 금액"]
    grp = current_df.copy()
    grp['abs_amt'] = grp.get('발생액', pd.Series(dtype=float)).abs()
    for name, cdf in grp.groupby('cluster_group', dropna=False):
        tot = cdf['abs_amt'].sum()
        cnt = len(cdf)
        ex = cdf.sort_values('abs_amt', ascending=False).head(1).iloc[0]
        dt = ex['회계일자'].strftime('%Y-%m-%d') if '회계일자' in ex and pd.notna(ex['회계일자']) else ''
        vend = ex.get('거래처', '')
        amt = int(ex.get('발생액', 0.0))
        lines.append(f"- [{name}] 건수 {cnt}건, 규모(절대값) {tot:,.0f}원")
        lines.append(f"  · 예시: {dt} | {vend} | {amt:,.0f}원")
    return "\n".join(lines)

def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float = 0.70) -> str:
    """
    ## 전기 클러스터 및 금액
    Project PY vouchers onto CY cluster centroids; report total abs amount, avg similarity, and ONE example.
    """
    import pandas as pd
    import numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous, cluster_centroid_vector
    if current_df.empty or previous_df.empty or 'vector' not in previous_df.columns or 'cluster_group' not in current_df.columns:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 데이터/벡터/클러스터 정보 없음)"
    lines = ["\n\n## 전기 클러스터 및 금액"]
    prev_ok = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)]
    if prev_ok.empty:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 유효 벡터 없음)"
    try:
        knn, X = build_knn_index(prev_ok)
    except Exception:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 KNN 인덱스 생성 실패)"
    for name, cur_c in current_df.groupby('cluster_group', dropna=False):
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_ok, knn, X, qv, topk=10, dedup_by_vendor=True, min_sim=float(min_sim))
        if ev.empty:
            lines.append(f"- [{name}] 유사 전표 없음")
            continue
        ev['abs_amt'] = ev.get('발생액', pd.Series(dtype=float)).abs()
        tot = ev['abs_amt'].sum()
        avg_sim = ev['similarity'].mean()
        ex = ev.sort_values('similarity', ascending=False).head(1).iloc[0]
        dt = ex['회계일자'].strftime('%Y-%m-%d') if pd.notna(ex['회계일자']) else ''
        lines.append(f"- [{name}] 규모(절대값) {tot:,.0f}원, 평균 유사도 {avg_sim:.2f}")
        lines.append(f"  · 예시: {dt} | {ex['거래처']} | {int(ex['발생액']):,}원 | sim {ex['similarity']:.2f}")
    return "\n".join(lines)

def build_zscore_top5_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## Z-score 기준 TOP5 전표
    List top |Z| vouchers with one counterpart from PY (same-month preferred), no row-id.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous
    if current_df.empty or 'Z-Score' not in current_df.columns:
        return "\n\n## Z-score 기준 TOP5 전표\n- (Z-Score 미계산)"
    cur = current_df.copy()
    order = cur['Z-Score'].abs().sort_values(ascending=False).index
    cur = cur.reindex(order).head(int(topn))
    lines = [f"\n\n## Z-score 기준 TOP5 전표"]
    if previous_df.empty or 'vector' not in previous_df.columns:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    # KNN on PY (same-month preferred)
    prev = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if prev.empty:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    knn_all, X_all = build_knn_index(prev)
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
        head = f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}"
        if qv is None:
            lines.append(head)
            continue
        psub = prev
        if '회계일자' in r and pd.notna(r['회계일자']):
            m = r['회계일자'].month
            cand = prev[prev['회계일자'].dt.month == m]
            if not cand.empty:
                psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  · 전기 대응: 없음")
        else:
            rr = ev.iloc[0]
            d2 = rr['회계일자'].strftime('%Y-%m-%d') if pd.notna(rr['회계일자']) else ''
            lines.append(f"  · 전기 대응: {d2} | {rr['거래처']} | {int(rr['발생액']):,}원 | sim {rr['similarity']:.2f}")
    return "\n".join(lines)


# --- NEW: 전기 기준 TOP5 블록 ---
def build_zscore_top5_block_for_py(previous_df: pd.DataFrame, current_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## 전기 Z-score 기준 TOP5 전표
    전기 데이터를 기준으로 |Z| 상위 5건을 나열하고, 가능한 경우 당기 대응 1건을 함께 표시.
    previous_df에 Z-Score가 있어야 한다.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous

    if previous_df.empty or 'Z-Score' not in previous_df.columns:
        return "\n\n## 전기 Z-score 기준 TOP5 전표\n- (전기 Z-Score 미계산)"

    prev = previous_df.copy()
    order = prev['Z-Score'].abs().sort_values(ascending=False).index
    prev = prev.reindex(order).head(int(topn))

    lines = [f"\n\n## 전기 Z-score 기준 TOP5 전표"]

    if current_df.empty or 'vector' not in current_df.columns:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    cur_ok = current_df[current_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if cur_ok.empty:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    knn_all, X_all = build_knn_index(cur_ok)

    for i, (_, r) in enumerate(prev.iterrows(), 1):
        dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
        head = f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}"

        qv = r.get('vector', None)
        if qv is None:
            lines.append(head); continue

        psub = cur_ok
        if '회계일자' in r and pd.notna(r['회계일자']):
            m = r['회계일자'].month
            cand = cur_ok[cur_ok['회계일자'].dt.month == m]
            if not cand.empty: psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all

        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  · 당기 대응: 없음")
        else:
            rr = ev.iloc[0]
            d2 = rr['회계일자'].strftime('%Y-%m-%d') if pd.notna(rr['회계일자']) else ''
            lines.append(f"  · 당기 대응: {d2} | {rr['거래처']} | {int(rr['발생액']):,}원 | sim {rr['similarity']:.2f}")

    return "\n".join(lines)


==============================
📄 FILE: analysis/integrity.py
==============================

import pandas as pd
from typing import List, Dict, Any, Optional
try:
    from analysis.contracts import ModuleResult, LedgerFrame
except Exception:
    ModuleResult = None  # 타입 힌트/런타임 가드
    LedgerFrame = None   # 타입 힌트/런타임 가드


def analyze_reconciliation(ledger_df: pd.DataFrame, master_df: pd.DataFrame):
    """Master와 Ledger 데이터 간의 정합성을 검증합니다.

    반환값:
    - overall_status: "Pass" | "Warning" | "Fail"
    - 결과 DataFrame
    """
    results, overall_status = [], "Pass"
    cy_ledger_df = ledger_df[ledger_df['연도'] == ledger_df['연도'].max()]
    for _, master_row in master_df.iterrows():
        account_code = master_row['계정코드']
        bspl = master_row.get('BS/PL', 'PL').upper()
        bop = master_row.get('전기말잔액', 0)
        eop_master = master_row.get('당기말잔액', 0)

        net_change_gl = cy_ledger_df[cy_ledger_df['계정코드'] == account_code]['거래금액'].sum()
        eop_gl = (bop + net_change_gl) if bspl == 'BS' else net_change_gl

        difference = eop_master - eop_gl
        diff_pct = abs(difference) / max(abs(eop_master), 1)
        status = "Fail" if diff_pct > 0.001 else "Warning" if abs(difference) > 0 else "Pass"
        if status == "Fail":
            overall_status = "Fail"
        elif status == "Warning" and overall_status == "Pass":
            overall_status = "Warning"

        results.append({
            '계정코드': account_code,
            '계정명': master_row.get('계정명', ''),
            '구분': bspl,
            '기초잔액(Master)': bop,
            '당기증감액(Ledger)': net_change_gl,
            '계산된 기말잔액(GL)': eop_gl,
            '기말잔액(Master)': eop_master,
            '차이': difference,
            '상태': status
        })
    return overall_status, pd.DataFrame(results)


# NEW: 표준 DTO(ModuleResult) 반환 래퍼
def run_integrity_module(lf: LedgerFrame, accounts: Optional[List[str]] = None):
    """
    기존 analyze_reconciliation 결과를 표준 ModuleResult로 감쌉니다.
    - summary: 상태/계정 수/Fail·Warning 건수/최대 차이
    - tables: {"reconciliation": 결과 DF}
    - evidences: (MVP 단계) 비움
    """
    ledger_df = lf.df if hasattr(lf, 'df') else lf
    master_df = (lf.meta or {}).get('master_df') if hasattr(lf, 'meta') else None
    status, df = analyze_reconciliation(ledger_df, master_df)
    if accounts:
        try:
            accs = [str(a) for a in accounts]
            df = df[df['계정코드'].astype(str).isin(accs)].copy()
        except Exception:
            pass
    summary: Dict[str, Any] = {
        "status": str(status),
        "n_accounts": int(df["계정코드"].nunique()) if (df is not None and not df.empty and "계정코드" in df.columns) else 0,
        "n_fail": int((df["상태"] == "Fail").sum()) if (df is not None and "상태" in df.columns) else 0,
        "n_warn": int((df["상태"] == "Warning").sum()) if (df is not None and "상태" in df.columns) else 0,
        "max_abs_diff": float(df["차이"].abs().max()) if (df is not None and not df.empty and "차이" in df.columns) else 0.0,
    }
    warnings: List[str] = [] if status == "Pass" else [f"정합성 상태: {status}"]
    if ModuleResult is None:
        # contracts 미가용 환경 안전가드
        from typing import NamedTuple
        class _MR(NamedTuple):
            name: str; summary: Dict[str, Any]; tables: Dict[str, pd.DataFrame]; figures: Dict; evidences: List; warnings: List[str]
        return _MR("integrity", summary, {"reconciliation": df}, {}, [], warnings)
    return ModuleResult(
        name="integrity",
        summary=summary,
        tables={"reconciliation": df},
        figures={},
        evidences=[],
        warnings=warnings,
    )



==============================
📄 FILE: analysis/kdmeans_shim.py
==============================

from __future__ import annotations
from typing import Optional, Sequence
import numpy as np

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
except Exception as e:
    raise ImportError("scikit-learn이 필요합니다. `pip install scikit-learn`") from e


class HDBSCAN:
    """
    KDMeans: KMeans를 사용하되 HDBSCAN의 최소 속성 인터페이스를 흉내냄.
    - fit(X): labels_, probabilities_ 설정
    - labels_: np.ndarray[int], [0..k-1]
    - probabilities_: np.ndarray[float], 0~1 (KDMeans에서는 전부 1.0로 설정)
    매개변수:
      - n_clusters: 고정 k (None이면 자동 선택)
      - min_cluster_size: k 상한을 계산하기 위한 힌트(너무 많은 군집 방지)
      - max_k: 자동 선택 시 k 상한(기본: 데이터 크기와 min_cluster_size로 유도)
      - k_search: "silhouette" | "heuristic"
      - sample_size: 자동 선택 시 실루엣 계산에 사용할 샘플 크기(기본 2000)
      - random_state: 재현성
      - n_init: KMeans 초기화 횟수(또는 "auto")
    """
    def __init__(
        self,
        n_clusters: Optional[int] = None,
        min_cluster_size: int = 8,
        max_k: Optional[int] = None,
        k_search: str = "silhouette",
        sample_size: int = 2000,
        random_state: int = 42,
        n_init: str | int = "auto",
    ):
        self.n_clusters = n_clusters
        self.min_cluster_size = max(2, int(min_cluster_size))
        self.max_k = max_k
        self.k_search = k_search
        self.sample_size = int(sample_size)
        self.random_state = int(random_state)
        self.n_init = n_init

        # 학습 후 속성(HDBSCAN 호환)
        self.labels_: Optional[np.ndarray] = None
        self.probabilities_: Optional[np.ndarray] = None
        # 추가 텔레메트리
        self.chosen_k_: Optional[int] = None
        self.silhouette_: Optional[float] = None

    # --- 내부: k 후보 산정 ---
    def _candidate_ks(self, n: int) -> Sequence[int]:
        if n < 2:
            return [1]
        base = max(2, int(np.sqrt(n)))
        # 최소 크기 제약 기반 상한
        max_by_min = max(2, n // self.min_cluster_size)
        # 외부 상한 적용
        if self.max_k is not None:
            max_by_min = min(max_by_min, int(self.max_k))
        # 지나치게 큰 k는 계산 비용 이슈 → 실무적으로 캡
        hard_cap = 24 if n >= 1200 else 12
        k_hi = max(2, min(max_by_min, hard_cap))

        ks = {2, 3, 5, base - 1, base, base + 1, int(np.log2(n)) + 1, k_hi}
        ks = {int(k) for k in ks if 2 <= int(k) <= k_hi}
        return sorted(ks)

    # --- 내부: 샘플링 ---
    def _sample(self, X: np.ndarray) -> np.ndarray:
        n = X.shape[0]
        if n <= self.sample_size:
            return X
        rng = np.random.default_rng(self.random_state)
        idx = rng.choice(n, size=self.sample_size, replace=False)
        return X[idx]

    # --- 내부: k 자동 선택 (실루엣) ---
    def _choose_k(self, X: np.ndarray) -> int:
        n = X.shape[0]
        if n < 2:
            return 1
        if self.n_clusters is not None:
            return max(1, int(self.n_clusters))

        # 후보 목록
        ks = self._candidate_ks(n)
        if len(ks) == 0:
            return max(2, int(np.sqrt(n)))

        if self.k_search != "silhouette":
            # 휴리스틱: √n에 가장 가까운 값
            base = max(2, int(np.sqrt(n)))
            return min(ks, key=lambda k: abs(k - base))

        Xs = self._sample(X)
        best_k, best_s = None, -1.0

        for k in ks:
            if k >= len(Xs):   # 샘플보다 큰 k 불가
                continue
            try:
                km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
                labels = km.fit_predict(Xs)
                # 모든 라벨이 하나면 실루엣 계산 불가
                if len(set(labels)) < 2:
                    continue
                s = silhouette_score(Xs, labels, metric="euclidean")
                if s > best_s:
                    best_k, best_s = int(k), float(s)
            except Exception:
                continue

        if best_k is None:
            # 폴백: √n 인근
            base = max(2, int(np.sqrt(n)))
            best_k = min(ks, key=lambda k: abs(k - base))
            best_s = float("nan")

        self.silhouette_ = best_s
        return int(best_k)

    # --- 공개 API ---
    def fit(self, X: np.ndarray):
        X = np.asarray(X, dtype=float)
        if X.ndim != 2 or X.shape[0] < 1:
            raise ValueError("X must be 2D array with at least 1 row")

        k = self._choose_k(X)
        self.chosen_k_ = k

        km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
        labels = km.fit_predict(X)

        # HDBSCAN 호환 속성 부여
        self.labels_ = labels.astype(int)
        self.probabilities_ = np.ones(shape=(X.shape[0],), dtype=float)
        return self

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        return self.fit(X).labels_





==============================
📄 FILE: analysis/report.py
==============================

from __future__ import annotations
import pandas as pd
from typing import List, Callable, Optional, Any
from .evidence import (
    build_current_cluster_block,
    # build_previous_projection_block,  # 파일 하단 로컬 정의 사용
    build_zscore_top5_block,
    build_zscore_top5_block_for_py,
)
from .timeseries import run_timeseries_module
from .embedding import map_previous_to_current_clusters
import numpy as np
from config import PM_DEFAULT
import re
import json
import time


def _fmt_money(x):
    try:
        return f"{float(x):,.0f}원"
    except Exception:
        return str(x)


# --- 단위 강제 후처리: 억/만 → 원 단위 ---
_NUM = r'(?:\d{1,3}(?:,\d{3})*|\d+)'


def _to_int(s):
    return int(str(s).replace(',', ''))


def _replace_korean_units(m):
    # 케이스: "3억 5,072만 원" / "54억 1,444만 원" / "2억 원" / "370만 원"
    eok = m.group('eok')
    man = m.group('man')
    won = m.group('won')
    total = 0
    if eok:
        total += _to_int(eok) * 100_000_000
    if man:
        total += _to_int(man) * 10_000
    if won:
        total += _to_int(won)
    return f"{total:,.0f}원"


def _enforce_won_units(text: str) -> str:
    # 1) 억/만/원 혼합을 원 단위로 치환
    pat = re.compile(
        rf'(?:(?P<eok>{_NUM})\s*억)?\s*(?:(?P<man>{_NUM})\s*만)?\s*(?:(?P<won>{_NUM})\s*원)?'
        r'(?!\s*단위)', flags=re.IGNORECASE)

    def _smart_sub(s):
        out = []
        last = 0
        for m in pat.finditer(s):
            # 의미 없는 빈 매칭 방지: 억/만이 없으면 스킵(이미 원 단위일 가능성)
            if not any(m.group(g) for g in ('eok', 'man')):
                continue
            out.append(s[last:m.start()])
            out.append(_replace_korean_units(m))
            last = m.end()
        out.append(s[last:])
        return ''.join(out)

    return _smart_sub(text)


def _boldify_bracket_headers(text: str) -> str:
    # [요약], [주요 거래], [결론], [용어 설명] → **[...]**\n
    text = re.sub(r'^\[(요약|주요 거래|결론|용어 설명)\]\s*', r'**[\1]**\n', text, flags=re.MULTILINE)
    return text
def _strip_control(s: str) -> str:
    # 탭/개행 제외 모든 제어문자 제거 (0x00-0x1F, 0x7F)
    return re.sub(r"[\x00-\x08\x0b-\x1f\x7f]", "", s or "")


# --- ModuleResult 기반 컨텍스트(경량): 정렬·금액·메모 지원 + Top-K 일관화 ---
def build_report_context_from_modules(
    modules: List["ModuleResult"],
    pm_value: float,
    topk: int = 20,
    manual_note: str = ""
) -> str:
    """
    여러 ModuleResult에서 summary/상위 evidences를 뽑아 간단한 텍스트 컨텍스트를 생성.
    - Evidence 정렬: risk_score → financial_impact 내림차순
    - 금액 표기: financial_impact를 금액으로 표기
    - 감사 메모(manual_note) 주입
    - Top-K 일관 적용
    """
    lines: List[str] = []
    lines.append(f"[PM] {pm_value:,.0f} KRW")
    for m in modules or []:
        try:
            lines.append(f"\n## Module: {getattr(m, 'name', 'module')}")
            summ = getattr(m, "summary", None)
            if summ:
                lines.append(f"- summary: {summ}")

            # Evidence 정렬(내림차순): 위험도 → 금액
            evs = list(getattr(m, "evidences", []))
            def _key(e):
                try:
                    return (
                        float(getattr(e, "risk_score", 0.0)),
                        float(getattr(e, "financial_impact", 0.0)),
                    )
                except Exception:
                    return (0.0, 0.0)
            evs = sorted(evs, key=_key, reverse=True)
            k = max(0, int(topk))

            if evs and k > 0:
                lines.append(f"- evidences(top{k}):")
                for e in evs[:k]:
                    try:
                        lnk = getattr(e, "links", {}) or {}
                        acct_nm = lnk.get("account_name", "")
                        acct_cd = lnk.get("account_code", "")
                        risk = float(getattr(e, "risk_score", 0.0))
                        kit  = bool(getattr(e, "is_key_item", False))
                        amt  = getattr(e, "financial_impact", None)
                        amt_txt = f" amount={amt:,.0f}" if isinstance(amt, (int, float)) else ""
                        measure = getattr(e, "measure", None)
                        model = getattr(e, "model", None)
                        tag = ""
                        if measure: tag += f"[{measure}]"
                        if model:   tag += f"[{model}]"
                        rsn  = str(getattr(e, "reason", ""))
                        desc = []
                        for k2 in ["vendor","narration","cluster_name","cluster_group","month"]:
                            v = lnk.get(k2)
                            if v: desc.append(f"{k2}={str(v)[:60]}")
                        desc_txt = (" " + "; ".join(desc)) if desc else ""
                        lines.append(
                            _strip_control(
                                f"  - {tag} {acct_nm}({acct_cd}) risk={risk:.2f} KIT={kit}{amt_txt} reason={rsn}{desc_txt}"
                            )
                        )
                    except Exception:
                        continue

            # 표 크기 요약(유지)
            tbls = getattr(m, "tables", None)
            if tbls:
                for nm, df in (tbls or {}).items():
                    try:
                        lines.append(f"- table[{nm}]: rows={len(df)} cols={len(df.columns)}")
                    except Exception:
                        pass
        except Exception:
            continue

    if manual_note:
        lines.append("\n## Auditor Note\n" + manual_note.strip())

    return _strip_control("\n".join(lines)).strip()


# Backward-compatible alias with explicit name used in app layer
def generate_rag_context_from_modules(
    modules: List["ModuleResult"],
    pm_value: float,
    topk: int = 20,
    manual_note: str = ""
) -> str:
    return build_report_context_from_modules(modules, pm_value, topk=topk, manual_note=manual_note)


def _safe_load(s: str):
    """엄격한 JSON 로더: 코드 펜스 제거 후 strict json.loads.
    주변 텍스트/마크다운 허용하지 않음.
    """
    text = (s or "").strip()
    # 시작 펜스 제거
    text = re.sub(r"^\s*```(?:json|JSON)?\s*\n", "", text)
    # 끝 펜스 제거
    text = re.sub(r"\n\s*```\s*$", "", text)
    text = text.strip()
    return json.loads(text)


def build_report_context(master_df: pd.DataFrame, current_df: pd.DataFrame, previous_df: pd.DataFrame,
                         account_codes: List[str], manual_context: str = "",
                         include_risk_summary: bool = False, pm_value: float | None = None) -> str:
    acc_info = master_df[master_df['계정코드'].astype(str).isin(account_codes)]
    acc_names = ", ".join(acc_info['계정명'].unique().tolist())
    master_summary = f"- 분석 대상 계정 그룹: {acc_names} ({', '.join(account_codes)})"
    if not acc_info.empty:
        acct_type = acc_info.iloc[0].get('BS/PL', 'PL')
        has_dates_cur = ('회계일자' in current_df.columns) and current_df['회계일자'].notna().any()
        has_dates_prev = ('회계일자' in previous_df.columns) and previous_df['회계일자'].notna().any()
        if str(acct_type).upper() == 'PL' and has_dates_cur:
            min_date = current_df['회계일자'].min(); max_date = current_df['회계일자'].max()
            if has_dates_prev:
                # 래핑 구간(예: 11~2월) 오판 방지: 연-월 Period로 비교
                cur_months = current_df['회계일자'].dt.to_period('M')
                prev_months = previous_df['회계일자'].dt.to_period('M')
                mask = prev_months.isin(cur_months.unique())
                prev_f = previous_df.loc[mask]
            else:
                prev_f = previous_df.copy()
            # Net first (순액: 차-대), absolute as reference (규모(절대값))
            cur_net = current_df.get('순액', pd.Series(dtype=float)).sum()
            prev_net = prev_f.get('순액', pd.Series(dtype=float)).sum()
            cur_abs = current_df.get('발생액', pd.Series(dtype=float)).sum()
            prev_abs = prev_f.get('발생액', pd.Series(dtype=float)).sum()
            var = cur_net - prev_net
            var_pct = (var / prev_net * 100) if prev_net not in (0, 0.0) else float('inf')
            period = f"{min_date.strftime('%m월')}~{max_date.strftime('%m월')}"
            master_summary += (
                f"\n- 당기 **순액(차-대)** 합계 ({period}): {cur_net:,.0f}원"
                f" | 전기 동기간 순액: {prev_net:,.0f}원 | 순액 증감: {var:,.0f}원 ({var_pct:+.2f}%)"
                f"\n- (참고) **규모(절대값)** 발생액: 당기 {cur_abs:,.0f}원 | 전기 {prev_abs:,.0f}원"
                f" | 차이: {cur_abs - prev_abs:,.0f}원"
            )
        else:
            cur_bal = acc_info.get('당기말잔액', pd.Series(dtype=float)).sum()
            prior_bal = acc_info.get('전기말잔액', pd.Series(dtype=float)).sum()
            var = cur_bal - prior_bal
            var_pct = (var / prior_bal * 100) if prior_bal not in (0, 0.0) else float('inf')
            master_summary += f"\n- 당기말 잔액(합산): {cur_bal:,.0f}원 | 전기말 잔액(합산): {prior_bal:,.0f}원 | 증감: {var:,.0f}원 ({var_pct:+.2f}%)"

    manual_summary = f"\n\n## 사용자 제공 추가 정보\n{manual_context}" if manual_context and not manual_context.isspace() else ""

    # --- New context layout ---
    sec_info = f"## 분석대상 계정정보\n{master_summary}{manual_summary}"
    sec_cur = build_current_cluster_block(current_df)
    # Prior-year: 전표 전체를 CY 센트로이드에 최근접 매핑하여 합산하는 증거 블록 사용
    sec_prev = build_previous_projection_block(current_df, previous_df)
    sec_top5_cy = build_zscore_top5_block(current_df, previous_df, topn=5)
    sec_top5_py = build_zscore_top5_block_for_py(previous_df, current_df, topn=5)
    sec_ts = build_timeseries_summary_block(current_df)

    # --- 위험 매트릭스 요약 제거(경영진주장/매트릭스 비활성화) ---
    # sec_risk = ""
    # if include_risk_summary:
    #     try:
    #         sec_risk = _build_risk_matrix_section(current_df, pm_value=pm_value)
    #     except Exception:
    #         sec_risk = ""

    parts = [sec_info, sec_cur, sec_prev, sec_ts, sec_top5_cy, sec_top5_py]
    # if sec_risk:
    #     parts.insert(2, sec_risk)  # 정보→(위험)→클러스터 순
    return "\n".join(parts)


def build_timeseries_summary_block(current_df: pd.DataFrame, topn: int = 5) -> str:
    """
    ## 예측 이탈 요약
    계정별 월별 합계를 기반으로 마지막 포인트의 예측 대비 이탈을 요약.
    """
    if current_df is None or current_df.empty or '회계일자' not in current_df.columns:
        return "\n\n## 예측 이탈 요약\n- (데이터 없음)"
    df = current_df.copy()
    # 날짜/계정 가드
    try:
        df['회계일자'] = pd.to_datetime(df['회계일자'], errors='coerce')
    except Exception:
        return "\n\n## 예측 이탈 요약\n- (날짜 형식 오류)"
    if '계정명' not in df.columns:
        return "\n\n## 예측 이탈 요약\n- (계정명이 필요합니다)"
    df['연월'] = df['회계일자'].dt.to_period('M')
    # 금액 컬럼 유연 인식
    cand = ['거래금액', '발생액', '거래금액_절대값', 'amount', '금액']
    val_col = next((c for c in cand if c in df.columns), None)
    if not val_col:
        return "\n\n## 예측 이탈 요약\n- (금액 컬럼을 찾지 못했습니다)"

    m = (df.groupby(['계정명','연월'], as_index=False)[val_col].sum())
    m['account'] = m['계정명']
    # Flow는 월 ‘집계’ 개념이므로 내부 앵커는 월초(start)로 통일
    m['date'] = m['연월'].dt.to_timestamp(how='start')
    m['amount'] = m[val_col]

    rows = run_timeseries_module(m[['account','date','amount']])
    if rows is None or rows.empty:
        return "\n\n## 예측 이탈 요약\n- (유의미한 이탈 없음)"

    rows = rows.sort_values('risk', ascending=False).head(int(topn))
    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime('%Y-%m-%d') if _pd.notna(x) else ""
        except:
            return ""
    lines = [
        "\n\n## 예측 이탈 요약",
        "※ 기본은 '월별 발생액(Δ잔액/flow)'. BS 계정은 **balance** 기준도 내부 평가하며, 아래 표기는 MoR과 z·risk를 함께 보여줍니다."
    ]
    for _, r in rows.iterrows():
        _m = str(r.get('measure','flow'))
        _when = "월합계" if _m == "flow" else "월말"
        lines.append(
            f"- [{_fmt_dt(r['date'])}·{_when}] {r['account']} ({_m}, MoR={r.get('model','-')})"
            f" | 실제 {r['actual']:,.0f}원 vs 예측 {r['predicted']:,.0f}원"
            f" → {'상회' if float(r['error'])>0 else '하회'} | z={float(r['z']):+.2f} | risk={float(r['risk']):.2f}"
        )
    return "\n".join(lines)


def _build_risk_matrix_section(*_args, **_kwargs) -> str:
    # [Removed] CEAVOP/위험 매트릭스 섹션은 2025-08-30 기준 비활성화.
    # 재도입 전까지 빈 문자열을 반환하여 호출부를 깨지 않음.
    return ""


def build_methodology_note(report_accounts=None) -> str:
    lines = [
        "\n\n## 분석 기준(알림)",
        "- 이번 분석은 UI에서 선택된 계정 기준으로 산출되었습니다.",
        "- 요약 수치: **순액(차-대)** 기준. (발생액=규모(절대값)은 참고용)",
        "- Z-Score: 선택 계정들의 **발생액(절대값)** 분포 기준.",
        "- 유사도/근거: **적요+거래처** 임베딩 후 코사인 유사도(전기 동월 우선).",
        "- '클러스터 노이즈(-1)'는 의미가 충분히 모이지 않아 자동으로 묶이지 않은 산발적 거래 묶음입니다.",
    ]
    return "\n".join(lines)


def _format_from_json(obj: dict) -> str:
    """
    단순 스키마(JSON) → 최종 마크다운.
    - key_transactions: LLM이 작성한 전체 섹션 마크다운을 그대로 사용
    - glossary: 필수 항목 보강(없을 경우 기본 정의 추가)
    """
    summary = (obj.get("summary") or "").strip()
    kt_val = obj.get("key_transactions")
    # 과거 호환(오브젝트가 오면 텍스트로 변환 시도)
    if isinstance(kt_val, dict):
        parts = []
        for k, v in kt_val.items():
            if isinstance(v, str):
                parts.append(v.strip())
        key_tx_md = "\n\n".join(p for p in parts if p)
    else:
        key_tx_md = (kt_val or "").strip()

    conclusion = (obj.get("conclusion") or "").strip()

    md = (
        f"**[요약]**\n{summary}\n\n"
        f"**[주요 거래]**\n{key_tx_md}\n\n"
        f"**[결론]**\n{conclusion}"
    )
    return md


# --- 전기 전체 매핑 합산 방식으로 교체: 이전 전표를 CY 클러스터에 최근접 매핑 후 합산 ---
def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float | None = None) -> str:
    """
    Project all PY vouchers onto CY cluster centroids and aggregate absolute amounts by the CY cluster_group.
    - No similarity computation is shown or used for filtering.
    - Output contains only total absolute amount and ONE example voucher (no sim).
    """
    import pandas as pd
    if current_df is None or previous_df is None or current_df.empty or previous_df.empty:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 데이터 없음)"
    need_cur = {'cluster_id','cluster_name','vector'}
    if not need_cur.issubset(current_df.columns) or 'vector' not in previous_df.columns:
        return "\n\n## 전기 클러스터 및 금액\n- (클러스터/벡터 정보 부족)"

    prev_m = map_previous_to_current_clusters(current_df, previous_df)
    if prev_m is None or prev_m.empty or 'mapped_cluster_id' not in prev_m.columns:
        return "\n\n## 전기 클러스터 및 금액\n- (매핑 실패)"

    if 'cluster_group' in current_df.columns:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_group'].to_dict()
    else:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()

    prev_m = prev_m.copy()
    prev_m['mapped_group'] = prev_m['mapped_cluster_id'].map(id2group)
    prev_m['abs_amt'] = prev_m.get('발생액', pd.Series(dtype=float)).abs()

    agg = (
        prev_m.groupby('mapped_group', dropna=False)
              .agg(규모=('abs_amt','sum'))
              .reset_index()
              .sort_values('규모', ascending=False)
    )

    lines = ["\n\n## 전기 클러스터 및 금액"]
    for _, row in agg.iterrows():
        g = row['mapped_group'] if pd.notna(row['mapped_group']) else '(미매핑)'
        tot = row['규모']
        sub = prev_m[prev_m['mapped_group'] == row['mapped_group']]
        if not sub.empty:
            ex = sub.sort_values('abs_amt', ascending=False).head(1).iloc[0]
            raw_dt = ex.get('회계일자', None)
            if pd.notna(raw_dt):
                try:
                    _dt = pd.to_datetime(raw_dt, errors='coerce')
                    dt = _dt.strftime('%Y-%m-%d') if pd.notna(_dt) else ''
                except Exception:
                    dt = ''
            else:
                dt = ''
            lines.append(f"- [{g}] 규모(절대값) {tot:,.0f}원")
            lines.append(f"  · 예시: {dt} | {ex.get('거래처','')} | {int(ex.get('발생액',0)):,.0f}원")
        else:
            lines.append(f"- [{g}] 규모(절대값) {tot:,.0f}원")
    return "\n".join(lines)


def run_final_analysis(
    context: str,
    account_codes: list[str],
    *,
    model: str | None = None,
    max_tokens: int | None = 16000,
    generate_fn: Optional[Callable[..., str]] = None,
) -> str:
    system = (
        "You are a CPA. Do all hidden reasoning internally and output ONLY the JSON object in the EXACT schema below. "
        "Language: Korean (ko-KR) for every natural-language value.\n"
        "Schema: {"
        '"summary": str,'
        '"key_transactions": str,'
        '"conclusion": str,'
        '"glossary": [str]'
        "}\n"
        "Authoring rules:\n"
        "• Monetary values MUST be formatted in KRW like '1,234원' (never 억/만원).\n"
        "• [요약]은 계정군 수준의 변동과 규모를 한 문장으로 명료히.\n"
        "• [주요 거래]는 전체 서술을 네가 설계하되, 컨텍스트(CY/PY 클러스터, 매핑, Z-score 상위 항목)를 근거로 구성 비중/이상치/전기 대응관계를 자연스럽게 녹여라. 필요하면 불릿·소제목을 임의로 사용해 가독성을 높여라.\n"
        "• [결론]은 원인·리스크·통제·액션아이템 중심으로 실무적 제안 위주로 작성한다.\n"
        "• [용어 설명]에는 반드시 다음 두 항목이 포함되도록 한다:   1) '클러스터 노이즈(-1)' 정의, 2) 'Z-Score'가 ‘평균에서 몇 표준편차’인지의 직관적 의미.\n"
        "Compliance: Output MUST be the JSON object itself, with no markdown/code-fences or extra text."
    )
    user = (
        f"Target accounts: {', '.join(account_codes)}\n"
        f"{context}\n"
        "Return ONLY the JSON per schema via function call."
    )

    tool_schema = {
        "type": "function",
        "function": {
            "name": "emit_report",
            "description": "Return the report strictly in the fixed JSON schema.",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "key_transactions": {"type": "string"},
                    "conclusion": {"type": "string"},
                    "glossary": {"type": "array","items":{"type":"string"}}
                },
                "required": ["summary","key_transactions","conclusion","glossary"]
            }
        }
    }

    max_retries = 2
    last_err = None

    for attempt in range(max_retries + 1):
        try:
            if generate_fn is None:
                raise RuntimeError("generate_fn not provided (LLM dependency must be injected)")
            raw = generate_fn(
                system=system, user=user, model=model,
                max_tokens=max_tokens, tools=[tool_schema], force_json=False
            )
            obj = _safe_load(raw)
            text = _format_from_json(obj)
            return _enforce_won_units(text)
        except Exception as e:
            last_err = e
            if attempt < max_retries:
                time.sleep(1.0)
            continue

    raise ValueError(f"LLM failed to produce valid JSON report after retries. Details: {last_err}")


# --- NEW: LLM 미사용/실패 시 폴백 리포트 (순수 로컬 계산) ---
def run_offline_fallback_report(current_df: pd.DataFrame,
                                previous_df: pd.DataFrame,
                                account_codes: list[str],
                                pm_value: float | None = None) -> str:
    """
    외부 LLM을 전혀 사용하지 않고 간단 보고서를 생성한다.
    - 요약: CY/PY 순액/규모 비교
    - 주요 거래: |Z| Top 5 (가능하면 Z 기준, 없으면 발생액 상위)
    - 결론: KIT(≥PM) 건수/비중 및 리스크 주의
    - 용어: Z-Score, Key Item 기본 정의
    """
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    cur = current_df.copy()
    prev = previous_df.copy()

    def _safe_sum(df, col): 
        return float(df.get(col, pd.Series(dtype=float)).sum()) if not df.empty else 0.0

    cur_net  = _safe_sum(cur, "순액")
    prev_net = _safe_sum(prev, "순액")
    cur_abs  = _safe_sum(cur, "발생액")
    prev_abs = _safe_sum(prev, "발생액")

    var_net = cur_net - prev_net
    var_abs = cur_abs - prev_abs
    var_pct = (var_net / prev_net * 100.0) if prev_net not in (0, 0.0) else float("inf")

    # Top 5: Z-Score 우선, 없으면 발생액 상위
    top_df = cur.copy()
    if "Z-Score" in top_df.columns and top_df["Z-Score"].notna().any():
        top_df = top_df.reindex(top_df["Z-Score"].abs().sort_values(ascending=False).index)
    else:
        top_df = top_df.reindex(top_df.get("발생액", pd.Series(dtype=float)).abs().sort_values(ascending=False).index)
    top_df = top_df.head(5)

    # KIT 집계(절대발생액 기준)
    kit_mask = top_df.get("발생액", pd.Series(dtype=float)).abs() >= pm if not top_df.empty else pd.Series([], dtype=bool)
    kit_cnt  = int(kit_mask.sum()) if not top_df.empty else 0

    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime("%Y-%m-%d") if _pd.notna(x) else ""
        except Exception:
            return ""

    # Compose sections (간단 Markdown)
    summary = (
        f"선택 계정({', '.join(account_codes)}) 기준으로 당기 **순액** {cur_net:,.0f}원,"
        f" 전기 {prev_net:,.0f}원 → 증감 {var_net:,.0f}원 ({var_pct:+.2f}%).\n"
        f"(참고) **규모(발생액 절대값)** 당기 {cur_abs:,.0f}원, 전기 {prev_abs:,.0f}원 → 차이 {var_abs:,.0f}원."
    )

    kt_lines = []
    if not top_df.empty:
        for i, (_, r) in enumerate(top_df.iterrows(), 1):
            dt = _fmt_dt(r.get("회계일자"))
            vend = str(r.get("거래처", "") or "")
            amt = float(r.get("발생액", 0.0))
            z   = r.get("Z-Score", np.nan)
            ztxt = f" | Z={float(z):+.2f}" if not pd.isna(z) else ""
            kt_lines.append(f"- [{i}] {dt} | {vend} | {amt:,.0f}원{ztxt}")
    key_tx = "\n".join(kt_lines) if kt_lines else "- 상위 항목을 산출할 데이터가 없습니다."

    conclusion = (
        f"PM {pm:,.0f}원 기준 **Key Item(KIT)** 후보는 상위 리스트 중 {kit_cnt}건입니다. "
        "Z-Score가 큰 항목은 적요·거래처 등 근거 확인과 원인 파악이 필요합니다. "
        "주요 변동은 월별 추이/상관 분석과 함께 교차검토하는 것을 권장합니다."
    )
    return (
        f"**[요약]**\n{summary}\n\n"
        f"**[주요 거래]**\n{key_tx}\n\n"
        f"**[결론]**\n{conclusion}"
    )




==============================
📄 FILE: analysis/report_adapter.py
==============================

from __future__ import annotations
from typing import Any, Dict
import pandas as pd
# Assuming contracts are available in the environment
try:
    from analysis.contracts import ModuleResult
except ImportError:
    # Define a simple fallback if contracts are missing
    from typing import NamedTuple, List, Dict
    class ModuleResult(NamedTuple):
        name: str; summary: Dict; tables: Dict; figures: Dict; evidences: List; warnings: List

def wrap_dfs_as_module_result(df_cy: pd.DataFrame, df_py: pd.DataFrame, name: str = "report_ctx") -> ModuleResult:
    """Wraps legacy df_cy/df_py into a standard ModuleResult for unified processing."""
    df_cy = (df_cy.copy() if df_cy is not None else pd.DataFrame())
    df_py = (df_py.copy() if df_py is not None else pd.DataFrame())
    summary: Dict[str, Any] = {
        "rows_cy": int(len(df_cy)),
        "rows_py": int(len(df_py)),
        "accounts_cy": int(df_cy["계정코드"].nunique()) if "계정코드" in df_cy.columns else 0,
        "accounts_py": int(df_py["계정코드"].nunique()) if "계정코드" in df_py.columns else 0,
    }
    return ModuleResult(
        name=name,
        summary=summary,
        tables={"current": df_cy, "previous": df_py},
        figures={},
        evidences=[],   # Adapter only wraps DFs, does not generate new evidences
        warnings=[]
    )



==============================
📄 FILE: analysis/summarization.py
==============================




==============================
📄 FILE: analysis/timeseries.py
==============================

# timeseries.py
# v3 — Compact TS module with PY+CY window, MoR(EMA/MA/ARIMA/Prophet), dual-basis(flow/balance)
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple, Callable, Mapping
import math
import numpy as np
STANDARD_COLS = ["date","account","measure","model","actual","predicted","error","z","risk"]

def _ensure_ts_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    시계열 결과 DF를 표준 스키마로 정규화한다.
    누락된 컬럼은 NaN으로 추가하고, date는 datetime으로 강제.
    """
    if df is None or len(df) == 0:
        return pd.DataFrame(columns=STANDARD_COLS)
    out = df.copy()
    if "date" in out.columns:
        out["date"] = pd.to_datetime(out["date"], errors="coerce")
    for c in STANDARD_COLS:
        if c not in out.columns:
            out[c] = np.nan
    # 불필요 컬럼은 보존하되, 기준 컬럼 우선 반환
    extra = [c for c in out.columns if c not in STANDARD_COLS]
    return out[STANDARD_COLS + extra]
import pandas as pd
import plotly.graph_objects as go
from pandas.tseries.offsets import MonthEnd
def to_month_end_index(idx) -> pd.DatetimeIndex:
    """Convert a datetime-like or period-like index to month-end DatetimeIndex.
    NEW: Always normalize to month-end 00:00:00 (floor to day) for stable axes.
    """
    pidx = pd.PeriodIndex(idx, freq="M")
    _end = pidx.to_timestamp(how="end").floor("D")
    return pd.DatetimeIndex(_end)

# Attempt to import visualization and helper utilities (assuming they exist in the project structure)
try:
    from utils.viz import add_period_guides, add_materiality_threshold
except ImportError:
    # Fallbacks if utils.viz is not found
    def add_period_guides(fig, dates):
        return fig
    def add_materiality_threshold(fig, threshold):
        return fig
try:
    from utils.helpers import model_reason_text
except ImportError:
    # Fallback if utils.helpers is not found
    def model_reason_text(model_name, diagnostics):
        return f"Model {model_name} was selected based on cross-validation metrics."

# Optional contracts import for DTO outputs
try:
    from analysis.contracts import LedgerFrame, ModuleResult, EvidenceDetail
except Exception:  # pragma: no cover - keep loose coupling for tests
    LedgerFrame = None  # type: ignore
    ModuleResult = None  # type: ignore
    EvidenceDetail = None  # type: ignore

# -------- Optional config / anomaly imports with safe fallbacks --------
try:
    from config import PM_DEFAULT as _PM_DEFAULT
except Exception:
    # 실운영 기본값: 5억 (사용자 입력 미제공 시 최후의 안전값)
    _PM_DEFAULT = 500_000_000
try:
    from config import FORECAST_MIN_POINTS as _FORECAST_MIN_POINTS
except Exception:
    _FORECAST_MIN_POINTS = 8
try:
    from config import ARIMA_DEFAULT_ORDER as _ARIMA_DEFAULT_ORDER
except Exception:
    _ARIMA_DEFAULT_ORDER = (1, 1, 1)

def _risk_from_fallback(z_abs: float, amount: float, pm: float) -> float:
    # simple logistic mapping as a fallback
    return float(1.0 / (1.0 + math.exp(-abs(z_abs))))

try:
    from analysis.anomaly import _risk_from as _RISK_EXTERNAL  # type: ignore
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        try:
            r = _RISK_EXTERNAL(z_abs, amount=amount, pm=float(pm))
            return float(r[-1] if isinstance(r, (list, tuple)) else r)
        except Exception:
            return _risk_from_fallback(z_abs, amount, pm)
except Exception:
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        return _risk_from_fallback(z_abs, amount, pm)

# ----------------------------- Utilities ------------------------------
DATE_CANDIDATES = ['회계일자','전표일자','거래일자','일자','date','Date']
AMT_CANDIDATES  = ['거래금액','발생액','금액','금액(원)','거래금액_절대값','발생액_절대값','순액','순액(원)']

def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    for c in candidates:
        if c in df.columns:
            return c
    return None
def _to_month_period_index(dates: pd.Series) -> pd.PeriodIndex:
    return pd.to_datetime(dates).dt.to_period("M")

def _to_month_end(ts: pd.Series) -> pd.Series:
    """Normalize datetimes to month-end and floor to seconds for clean display."""
    ts = pd.to_datetime(ts, errors="coerce")
    return (ts + MonthEnd(0)).dt.floor("S")

def _monthly_flow_and_balance(
    df: pd.DataFrame,
    date_col: str,
    amount_col: str,
    opening: float = 0.0,
) -> Tuple[pd.Series, pd.Series]:
    """월별 발생액 합계(flow)와 기초+누적발생액(balance) 반환.
    반환 Series는 month-end DatetimeIndex를 가지며 index.name="date"로 설정된다.
    """
    if df is None or df.empty:
        idx = pd.DatetimeIndex([], name="date")
        return pd.Series(dtype=float, index=idx), pd.Series(dtype=float, index=idx)
    p = pd.to_datetime(df[date_col], errors="coerce").dt.to_period("M")
    amt = pd.to_numeric(df[amount_col], errors="coerce").fillna(0.0)
    flow = amt.groupby(p).sum().astype(float)
    balance = (flow.cumsum() + float(opening)).astype(float)
    month_end = flow.index.to_timestamp(how="end").floor("D")
    flow_s = pd.Series(flow.values, index=month_end)
    bal_s = pd.Series(balance.values, index=month_end)
    flow_s.index.name = bal_s.index.name = "date"
    return flow_s, bal_s

def _longest_contiguous_month_run(periods: pd.PeriodIndex) -> pd.PeriodIndex:
    if len(periods) <= 1: return periods
    p = pd.PeriodIndex(np.unique(np.asarray(periods)), freq="M")
    best_s = best_e = cur_s = 0
    for i in range(1, len(p)):
        if (p[i] - p[i-1]).n != 1:
            if i-1 - cur_s > best_e - best_s:
                best_s, best_e = cur_s, i-1
            cur_s = i
    if len(p)-1 - cur_s > best_e - best_s:
        best_s, best_e = cur_s, len(p)-1
    return p[best_s:best_e+1]

def _smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    denom = np.maximum(1e-12, np.abs(yt) + np.abs(yp))
    return float(np.mean(200.0 * np.abs(yt - yp) / denom))

def _std_last(x: np.ndarray, w: int = 6) -> float:
    if len(x) < 2: return 0.0
    s = np.std(x[-min(w, len(x)):], ddof=1) if len(x) > 1 else 0.0
    return float(s if math.isfinite(s) else 0.0)

def z_and_risk(residuals: np.ndarray, pm: float = _PM_DEFAULT) -> Tuple[np.ndarray, np.ndarray]:
    """잔차 시퀀스에 대해 표준화 z와 위험도 배열을 반환.
    테스트 호환을 위해 간단한 정규화와 |z|→risk 매핑을 사용.
    """
    r = np.asarray(residuals, dtype=float)
    if r.size <= 1:
        z = np.zeros_like(r)
    else:
        sd = float(np.std(r, ddof=1))
        z = (r / sd) if sd > 0 else np.zeros_like(r)
    risk_vals = np.array([_risk_score(abs(float(zi)), amount=1.0, pm=float(pm)) for zi in z], dtype=float)
    return z, risk_vals

def _has_seasonality(y: pd.Series) -> bool:
    y = pd.Series(y, dtype=float)
    if len(y) < 12: return False
    ac = np.abs(np.fft.rfft((y - y.mean()).values))
    core = ac[2:] if len(ac) > 2 else ac
    return bool(core.size and (core.max() / (core.mean() + 1e-9) > 5.0))

# --------------------------- Model backends ---------------------------
def _model_registry() -> Dict[str, bool]:
    ok_arima = ok_prophet = False
    try:
        import statsmodels.api as _  # noqa
        ok_arima = True
    except Exception:
        pass
    try:
        from prophet import Prophet as _  # noqa
        ok_prophet = True
    except Exception:
        pass
    return {"ema": True, "ma": True, "arima": ok_arima, "prophet": ok_prophet}

def model_registry() -> Dict[str, bool]:
    """공개 API: 사용 가능한 백엔드 레지스트리 반환."""
    return _model_registry()

# EMA
def _fit_ema(y: pd.Series, alpha: float = 0.3) -> Dict[str, Any]:
    return {"alpha": float(alpha), "y": y}

def _pred_ema(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]
    alpha = float(m["alpha"])
    pred = y.ewm(alpha=alpha, adjust=False).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# MA
def _fit_ma(y: pd.Series, window: int = 6) -> Dict[str, Any]:
    return {"window": int(window), "y": y}

def _pred_ma(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]; w = int(m["window"])
    pred = y.rolling(w, min_periods=1).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# ARIMA
def _fit_arima(y: pd.Series, order: Tuple[int,int,int] = _ARIMA_DEFAULT_ORDER):
    import statsmodels.api as sm
    return sm.tsa.ARIMA(y, order=tuple(order)).fit()

def _pred_arima(m, steps: Optional[int] = None) -> np.ndarray:
    if steps is None:
        fv = pd.Series(m.fittedvalues).shift(1).fillna(method="bfill")
        return fv.values
    return np.asarray(m.forecast(steps=int(steps)))

# Prophet
def _fit_prophet(y: pd.Series):
    from prophet import Prophet
    df = pd.DataFrame({"ds": y.index.to_timestamp(), "y": y.values})
    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    m.fit(df)
    return {"m": m, "idx": y.index}

def _pred_prophet(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    model = m["m"]; idx: pd.PeriodIndex = m["idx"]
    if steps is None:
        fit = model.predict(pd.DataFrame({"ds": idx.to_timestamp()}))["yhat"].values
        return np.roll(fit, 1)  # 1-step ahead approx.
    last = idx[-1].to_timestamp()
    future = pd.date_range(last + pd.offsets.MonthBegin(1), periods=int(steps), freq="MS")
    return model.predict(pd.DataFrame({"ds": future}))["yhat"].values

# -------------------- Model selection (MoR) via rolling CV -------------
def _rolling_origin_cv(
    y: pd.Series,
    fit_fn, pred_fn,
    k: int = 3, min_train: int = 6
) -> float:
    y = y.dropna(); n = len(y)
    if n < max(min_train + k, 8):
        try:
            m = fit_fn(y); yhat = pred_fn(m)
        except Exception:
            return 999.0
        yhat = np.asarray(yhat)[:n] if yhat is not None else np.repeat(y.iloc[:1].values, n)
        return _smape(y.values, yhat)
    step = max((n - min_train) // (k + 1), 1)
    scores = []
    for i in range(min_train, n, step):
        tr = y.iloc[:i]; te = y.iloc[i:i+step]
        if te.empty: break
        try:
            m = fit_fn(tr); yh = pred_fn(m, steps=len(te))
            scores.append(_smape(te.values, np.asarray(yh)[:len(te)]))
        except Exception:
            scores.append(999.0)
    return float(np.mean(scores)) if scores else 999.0

def _choose_model(y: pd.Series, measure: str) -> Tuple[str, np.ndarray]:
    reg = _model_registry()
    cands: List[Tuple[str, np.ndarray, Any, Any]] = []
    # always EMA/MA
    m_ema = _fit_ema(y); yhat_ema = _pred_ema(m_ema); cands.append(("EMA", yhat_ema, _fit_ema, _pred_ema))
    m_ma  = _fit_ma(y);  yhat_ma  = _pred_ma(m_ma);   cands.append(("MA",  yhat_ma,  _fit_ma,  _pred_ma))
    # ARIMA
    if reg["arima"]:
        try:
            m = _fit_arima(y); yhat = _pred_arima(m); cands.append(("ARIMA", yhat, _fit_arima, _pred_arima))
        except Exception:
            pass
    # Prophet: only for flow, enough data & seasonal
    if measure == "flow" and reg["prophet"] and len(y) >= 12 and _has_seasonality(y):
        try:
            m = _fit_prophet(y); yhat = _pred_prophet(m); cands.append(("Prophet", yhat, _fit_prophet, _pred_prophet))
        except Exception:
            pass
    # pick by CV
    scores = [(nm, _rolling_origin_cv(y, fit, pred)) for (nm, _, fit, pred) in cands]
    best = min(scores, key=lambda x: x[1])[0] if scores else "EMA"
    # return best in-sample prediction
    if best == "EMA": return "EMA", yhat_ema
    if best == "MA":  return "MA",  yhat_ma
    if best == "ARIMA":
        try: return "ARIMA", _pred_arima(_fit_arima(y))
        except Exception: return "EMA", yhat_ema
    if best == "Prophet":
        try: return "Prophet", _pred_prophet(_fit_prophet(y))
        except Exception: return "EMA", yhat_ema
    return "EMA", yhat_ema

# --------------------------- Core predictors --------------------------
def _prepare_monthly(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=[date_col]).copy()
    # 월말 기준으로 고정
    _dates = pd.to_datetime(df[date_col], errors="coerce")
    p = _to_month_period_index(_dates)
    df2 = df.copy()
    df2["_p"] = p
    df2 = df2.dropna(subset=["_p"]).sort_values("_p")
    run = _longest_contiguous_month_run(df2["_p"])
    return df2[df2["_p"].isin(run)].reset_index(drop=True)

def _one_track_lastrow(
    monthly: pd.DataFrame,
    value_col: str,
    measure: str,
    pm_value: float
) -> Optional[Dict[str, Any]]:
    df = _prepare_monthly(monthly, "date")
    if df.empty or value_col not in df.columns: return None
    y = pd.Series(df[value_col].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2: return None
    model, yhat = _choose_model(y, measure=measure)
    resid = y.values - yhat
    error_last = float(resid[-1])
    sigma = _std_last(resid, w=6)
    z = float(error_last / sigma) if sigma > 0 else 0.0
    risk = _risk_score(abs(z), amount=float(y.iloc[-1]), pm=float(pm_value))
    return {
        "date": y.index[-1].to_timestamp(how='end'),
        "measure": measure,
        "actual": float(y.iloc[-1]),
        "predicted": float(yhat[-1]),
        "error": float(error_last),
        "z": float(z),
        "z_label": "resid_z",
        "risk": float(risk),
        "model": model,
    }

# ------------------------------- API ----------------------------------
def run_timeseries_for_account(
    monthly: pd.DataFrame,
    account: str,
    is_bs: bool,
    flow_col: str = "flow",
    balance_col: Optional[str] = None,
    allow_prophet: bool = True,   # kept for backward compatibility (no-op switch)
    pm_value: float = _PM_DEFAULT,
    **kwargs: Any,                # absorb legacy args safely
) -> pd.DataFrame:
    """
    단일 계정의 월별 데이터에서 마지막 포인트를 평가.
    - BS 계정: flow/balance 2행(해당 시 존재) 반환
    - PL 계정: flow 1행 반환
    반환 컬럼: ["date","account","measure","actual","predicted","error","z","risk","model"]
    """
    rows: List[Dict[str, Any]] = []
    # flow
    if flow_col in monthly.columns:
        r = _one_track_lastrow(monthly.rename(columns={flow_col: "val"}), "val", "flow", pm_value)
        if r: rows.append(r)
    # balance
    if is_bs:
        if balance_col and (balance_col in monthly.columns):
            r = _one_track_lastrow(monthly.rename(columns={balance_col: "val"}), "val", "balance", pm_value)
            if r: rows.append(r)
        else:
            if flow_col in monthly.columns:
                tmp = monthly[["date", flow_col]].copy()
                tmp["val"] = tmp[flow_col].astype(float).cumsum()
                r = _one_track_lastrow(tmp[["date","val"]], "val", "balance", pm_value)
                if r: rows.append(r)
    out = pd.DataFrame(rows)
    if not out.empty:
        out["account"] = account
        out = out[["date","account","measure","actual","predicted","error","z","risk","model"]]
        out = out.sort_values(["account","measure","date"]).reset_index(drop=True)
    else:
        out = pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"])
    return _ensure_ts_schema(out)

def run_timeseries_module(
    df: pd.DataFrame,
    *,
    account_col: str = "account",
    date_col: str = "date",
    amount_col: str = "amount",
    targets: Optional[List[str]] = None,
    pm_value: float = _PM_DEFAULT,
    make_balance: bool = False,  # 기본값 False로 변경: 필요 시 balance 구성
    output: str = "all",        # "all" | "flow" | "balance"
    evidence_adapter: Optional[Callable[[Dict[str, Any]], Any]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    집계형: 계정별 월합계(amount)만 주어진 경우.
    기본: flow만 계산. make_balance=True일 때 balance(누적합)도 함께 계산.
    output으로 최종 반환 필터링 가능("flow"/"balance").
    """
    if df is None or df.empty:
        return _ensure_ts_schema(pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"]))
    work = df[[account_col, date_col, amount_col]].copy()
    work.columns = ["account","date","amount"]
    work = work.sort_values(["account","date"])
    if targets:
        try:
            tgt = set(map(str, targets))
            work["account"] = work["account"].astype(str)
            work = work[work["account"].isin(tgt)]
        except Exception:
            pass
    # 최소 포인트 가드: 계정별 date 유니크가 부족하면 빈 결과 반환
    MIN_POINTS = 6
    if work["date"].nunique() < MIN_POINTS:
        return _ensure_ts_schema(pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"]))
    all_rows: List[pd.DataFrame] = []
    for acc, g in work.groupby("account", dropna=False):
        mon = g[["date","amount"]].rename(columns={"amount":"flow"}).copy()
        if make_balance:
            mon["balance"] = mon["flow"].astype(float).cumsum()
        out = run_timeseries_for_account(mon, str(acc), is_bs=make_balance, flow_col="flow",
                                         balance_col=("balance" if make_balance else None),
                                         pm_value=float(pm_value))
        if not out.empty:
            # CEAVOP 제안(간단 규칙): error>0 → E(존재), error<=0 → C(완전성)
            try:
                out["assertion"] = out["error"].map(lambda e: "E" if float(e) > 0 else "C")
            except Exception:
                out["assertion"] = "E"
        if output in ("flow", "balance") and not out.empty:
            out = out[out["measure"] == output]
        all_rows.append(out)
    result = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    if evidence_adapter is not None and not result.empty:
        rows = []
        for r in result.to_dict(orient="records"):
            d = dict(r)
            if "amount" not in d:
                d["amount"] = float(d.get("actual", 0.0))
            if "z_abs" not in d:
                try:
                    d["z_abs"] = abs(float(d.get("z", 0.0)))
                except Exception:
                    d["z_abs"] = 0.0
            if "assertion" not in d:
                try:
                    d["assertion"] = "E" if float(d.get("error", 0.0)) > 0 else "C"
                except Exception:
                    d["assertion"] = "E"
            rows.append(evidence_adapter(d))
        return rows  # type: ignore[return-value]
    return _ensure_ts_schema(result)

def run_timeseries_module_with_flag(
    df: pd.DataFrame,
    account_col: str,
    date_col: str,
    amount_col: str,
    account_name: str,
    is_bs: bool,
    backend: str = "ema",
    *,
    opening_map: Mapping[str, float] | None = None,
    return_mode: str = "insample",
) -> pd.DataFrame:
    """
    기존 단일 출력에서 확장: BS 계정은 balance/flow dual 로직 적용.
    balance = opening(전기말잔액 등) + flow.cumsum()
    """
    if df is None or df.empty:
        return _ensure_ts_schema(pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"]))

    # 월별 flow/balance 생성 (월말 00:00:00 보장)
    acct = str(df[account_col].iloc[0]) if (account_col in df.columns and not df.empty) else account_name
    opening = 0.0
    if opening_map:
        opening = float(opening_map.get(acct, opening_map.get(account_name, 0.0)))
    flow_s, bal_s = _monthly_flow_and_balance(df, date_col, amount_col, opening=opening)

    def _run(track: str, s: pd.Series) -> pd.DataFrame:
        base = pd.DataFrame({"date": s.index, track: s.values}).sort_values("date")
        if return_mode == "lastrow":
            ins = base.tail(1).rename(columns={track: "actual"})
            ins["predicted"] = np.nan; ins["error"] = np.nan; ins["z"] = np.nan; ins["risk"] = np.nan
            ins["model"] = backend.upper()
        else:
            ins = insample_predict_df(base, value_col=track, measure=track, pm_value=_PM_DEFAULT)
        ins["account"] = account_name
        ins["measure"] = track
        ins["date"] = to_month_end_index(ins["date"])  # 안전 보정
        return ins[["date","account","measure","actual","predicted","error","z","risk","model"]]

    res = [_run("flow", flow_s)]
    if is_bs:
        res.append(_run("balance", bal_s))
    final_df = pd.concat(res, ignore_index=True).sort_values(["measure","date"]) if res else pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"])
    return _ensure_ts_schema(final_df)

# ----------------------- Helper: in-sample prediction -------------------
def insample_predict_df(
    monthly: pd.DataFrame,
    value_col: str,
    measure: str,
    pm_value: float = _PM_DEFAULT,
) -> pd.DataFrame:
    """
    월별 데이터(monthly: ['date', value_col])에 대해 MoR이 고른 모델의 in-sample 예측선을 반환.
    반환 컬럼: date, actual, predicted, model, train_months, data_span, sigma_win, measure, value_col
    """
    df = _prepare_monthly(monthly, "date")
    if df.empty or value_col not in df.columns:
        return pd.DataFrame(columns=["date","actual","predicted","model","train_months","data_span","sigma_win","measure","value_col"])
    y = pd.Series(df[value_col].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2:
        return pd.DataFrame(columns=["date","actual","predicted","model","train_months","data_span","sigma_win","measure","value_col"])
    model, yhat = _choose_model(y, measure=measure)
    span = f"{y.index[0].strftime('%Y-%m')} ~ {y.index[-1].strftime('%Y-%m')}"
    # 월말 고정 + 초 단위로 내림
    idx = to_month_end_index(y.index)
    out = pd.DataFrame({
        "date": idx,
        "actual": y.values,
        "predicted": yhat,
        "model": model,
    })
    out["train_months"] = len(y)
    out["data_span"] = span
    out["sigma_win"] = 6  # z 계산에 쓰는 최근 분산 윈도우
    out["measure"] = str(measure)
    out["value_col"] = str(value_col)
    return out


# ------------------- Validation Builder (tidy helper) -------------------
def build_trend_validation_data(
    monthly: pd.DataFrame,
    *,
    flow_col: str = "flow",
    balance_col: Optional[str] = None,
    is_bs: bool = False,
    pm_value: float = _PM_DEFAULT,
) -> pd.DataFrame:
    """
    트렌드 검증용 tidy 데이터 생성:
      columns → ['date','actual','predicted','measure','model']
    - PL: flow만
    - BS: flow + balance(있으면 사용, 없으면 flow 누적합으로 생성)
    """
    rows: List[pd.DataFrame] = []

    # flow
    if flow_col in monthly.columns:
        df_flow = insample_predict_df(
            monthly[["date", flow_col]].rename(columns={flow_col: "val"}),
            value_col="val",
            measure="flow",
            pm_value=pm_value,
        )
        if not df_flow.empty:
            rows.append(df_flow[["date","actual","predicted","measure","model"]])

    # balance (BS만) — balance는 '월말' 시점 기준
    if is_bs:
        if balance_col and (balance_col in monthly.columns):
            base = monthly[["date", balance_col]].rename(columns={balance_col: "val"})
        else:
            # 누적합으로 balance 가상 생성(월말 시점으로 표시됨)
            if flow_col not in monthly.columns:
                base = None
            else:
                tmp = monthly[["date", flow_col]].copy()
                tmp["val"] = tmp[flow_col].astype(float).cumsum()
                base = tmp[["date","val"]]
        if base is not None:
            df_bal = insample_predict_df(base, value_col="val", measure="balance", pm_value=pm_value)
            if not df_bal.empty:
                rows.append(df_bal[["date","actual","predicted","measure","model"]])

    if not rows:
        return pd.DataFrame(columns=["date","actual","predicted","measure","model"])
    return pd.concat(rows, ignore_index=True)

# ------------------- Validation: reconcile with trend -------------------
def reconcile_with_trend(
    ts_flow: pd.Series,
    ts_bal: pd.Series,
    trend_flow: pd.Series,
    trend_bal: pd.Series,
    tol: int = 1,
) -> pd.DataFrame:
    """
    timeseries 입력(flow/balance)과 trend 산출치(flow/balance)를 월별 대조.
    tol 절대차(원) 초과인 행만 반환.
    """
    import pandas as _pd
    idx = sorted(set(_pd.to_datetime(getattr(ts_flow, 'index', [])).tolist()) |
                 set(_pd.to_datetime(getattr(trend_flow, 'index', [])).tolist()))
    rows = []
    for d in idx:
        af = int(_pd.Series(ts_flow).get(d, 0))
        tf = int(_pd.Series(trend_flow).get(d, 0))
        ab = int(_pd.Series(ts_bal).get(d, 0))
        tb = int(_pd.Series(trend_bal).get(d, 0))
        rows.append({
            "month": d,
            "flow(ts)": af, "flow(trend)": tf, "Δflow": af - tf,
            "bal(ts)": ab,  "bal(trend)": tb,  "Δbal": ab - tb,
        })
    df_chk = pd.DataFrame(rows).set_index("month")
    return df_chk[(df_chk["Δflow"].abs() > tol) | (df_chk["Δbal"].abs() > tol)]


# ------------------- Optional: lightweight validation summary -----------
def validation_summary(
    monthly: pd.DataFrame,
    *,
    date_col: str = "date",
    value_col: str = "amount",
    pm_value: float = _PM_DEFAULT,
    last_k: int = 6,
) -> Dict[str, Any]:
    """막대 대조 대신 쓰는 경량 숫자 진단 카드."""
    df = monthly[[date_col, value_col]].rename(columns={date_col: "date", value_col: "val"}).copy()
    df = _prepare_monthly(df, "date")
    if df.empty or "val" not in df.columns:
        return {"n_points": 0}
    y = pd.Series(df["val"].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2:
        return {"n_points": int(len(y))}
    scores = [("EMA", _rolling_origin_cv(y, _fit_ema, _pred_ema)),
              ("MA",  _rolling_origin_cv(y, _fit_ma,  _pred_ma))]
    reg = _model_registry()
    if reg["arima"]:
        try: scores.append(("ARIMA", _rolling_origin_cv(y, _fit_arima, _pred_arima)))
        except Exception: pass
    if reg["prophet"] and len(y) >= 12 and _has_seasonality(y):
        try: scores.append(("Prophet", _rolling_origin_cv(y, _fit_prophet, _pred_prophet)))
        except Exception: pass
    best_cv_name, best_cv_smape = min(scores, key=lambda x: x[1])
    model_name, yhat = _choose_model(y, measure="flow")
    resid = y.values - yhat
    k = min(last_k, len(y))
    smape_k = _smape(y.values[-k:], yhat[-k:])
    last_err = float(resid[-1])
    sigma = _std_last(resid, w=6)
    z = float(last_err / sigma) if sigma > 0 else 0.0
    pm_ratio = (abs(float(y[-1])) / float(pm_value)) if pm_value else 0.0
    return {
        "n_points": int(len(y)),
        "mor": model_name,
        "cv_smape": float(best_cv_smape),
        "smape_last_k": float(smape_k),
        "last_month": str(y.index[-1].to_timestamp(how='start').date()),
        "last_actual": float(y[-1]),
        "last_pred": float(yhat[-1]),
        "last_error": last_err,
        "last_z": z,
        "pm_ratio": pm_ratio,
    }


# ========================================================================
# ============= NEW: Functions moved from app.py for Refactoring =========
# ========================================================================

# ----------------------- TS Diagnostics (moved from app.py) -------------

def _adf_stationary(y_vals: np.ndarray) -> Tuple[bool, float]:
    """ADF test for stationarity."""
    try:
        from statsmodels.tsa.stattools import adfuller
        y_clean = np.asarray(y_vals, dtype=float)
        # ADF requires finite values and sufficient length
        if not np.all(np.isfinite(y_clean)) or len(y_clean) < 3:  # ADF needs at least 3 points for default settings
            return (False, np.nan)

        p = float(adfuller(y_clean)[1])
        return (p < 0.05, p)  # True=정상성 확보
    except Exception:
        # 간단 폴백 (원본 로직 유지)
        y = np.asarray(y_vals, dtype=float)
        if len(y) < 6:
            return (False, np.nan)

        std_orig = np.nanstd(y)
        std_diff = np.nanstd(np.diff(y))

        if not math.isfinite(std_orig) or not math.isfinite(std_diff):
            return (False, np.nan)

        # Avoid division by zero if original std is 0
        if std_orig == 0:
            return (std_diff == 0, np.nan)

        return (std_diff < 0.9 * std_orig, np.nan)

def _has_seasonality_safe(y_vals: np.ndarray) -> bool:
    """Safe wrapper for seasonality check."""
    try:
        return bool(_has_seasonality(pd.Series(y_vals)))
    except Exception:
        return False

# ----------------------- TS Visualization (moved from app.py) -----------

def create_timeseries_figure(
    df_hist: pd.DataFrame,
    measure: str,
    title: str,
    pm_value: float,
    show_dividers: bool = False
) -> Tuple[Optional[go.Figure], Optional[Dict[str, Any]]]:
    """
    Creates a timeseries figure (actual vs predicted) and returns the figure and stats.
    (Refactored from app.py's _make_ts_fig_with_stats, removing Streamlit dependencies)
    """
    vcol = 'flow' if measure == 'flow' else 'balance'
    work = df_hist.copy()
    if "date" not in work.columns:
        return None, {"error": "Column 'date' not found in data."}
    work["date"] = pd.to_datetime(work["date"], errors="coerce")
    work = work.dropna(subset=["date"]).sort_values("date")

    # 1) 모델링된 입력(actual/predicted)이면 그대로 사용
    if {"actual","predicted"}.issubset(work.columns):
        cols = ["date","actual","predicted"] + (["model"] if "model" in work.columns else [])
        ins = work[cols].copy()
    else:
        # 2) 값 컬럼을 잡아 in-sample 예측선 생성
        if vcol in work.columns:
            base = work[["date", vcol]].rename(columns={vcol: "val"})
        elif "actual" in work.columns:
            base = work[["date","actual"]].rename(columns={"actual":"val"})
        elif "value" in work.columns:
            base = work[["date","value"]].rename(columns={"value":"val"})
        else:
            return None, {"error": f"Neither '{vcol}' nor 'actual'/'value' column found."}
        base = base.dropna(subset=["val"])  # 안전 가드
        ins = insample_predict_df(
            base.rename(columns={"val": vcol}),
            value_col=vcol,
            measure=measure,
            pm_value=float(pm_value)
        )

    if ins.empty or len(ins) < 2:
        reason = "points<2" if (not ins.empty) else "empty"
        return None, {"error": f"Insufficient data for plot ({reason}).",
                      "diagnostics": {"n_months": int(len(ins))}}

    # --- 1. Figure Creation ---
    fig = go.Figure()
    # Using styles consistent with the original app.py
    fig.add_trace(go.Scatter(x=ins['date'], y=ins['actual'], mode='lines', name='actual'))
    fig.add_trace(go.Scatter(x=ins['date'], y=ins['predicted'], mode='lines', name='predicted', line=dict(dash='dot')))

    fig.update_layout(
        title=title,
        xaxis_title='month',
        yaxis_title='원'
    )
    fig.update_yaxes(tickformat=",.0f", separatethousands=True, ticksuffix="")

    # Add PM threshold
    try:
        fig = add_materiality_threshold(fig, float(pm_value))
    except Exception:
        pass  # Proceed without PM line if utility fails

    # Add time dividers (Year/Quarter) – 기본 OFF
    if show_dividers:
        try:
            fig = add_period_guides(fig, ins['date'])
        except Exception:
            pass  # Proceed without dividers if utility fails

    # --- 2. Statistics Calculation (Logic preserved from app.py) ---
    stats_output: Dict[str, Any] = {}
    y_vals = np.asarray(ins['actual'].values, dtype=float)
    n_months = int(np.isfinite(y_vals).sum())

    # Diagnostics (Seasonality, Stationarity)
    seas = _has_seasonality_safe(y_vals)
    stat_ok, pval = _adf_stationary(y_vals)

    stats_output["diagnostics"] = {
        "seasonality": seas,
        "stationary": stat_ok,
        "p_value": pval,
        "n_months": n_months,
        "is_short": n_months < 12,
    }

    # Model Metrics (MAE, MAPE, AIC/BIC)
    # Sticking to the original definitions used in app.py
    mae = float(np.mean(np.abs(ins['actual'] - ins['predicted'])))
    # Handle division by zero safely within the mean calculation
    actuals = ins['actual']
    predicted = ins['predicted']
    mape = float(np.mean(np.where(actuals != 0, np.abs((actuals - predicted) / actuals) * 100, 0)))

    aic = bic = np.nan
    best_model_name = str(ins['model'].iloc[-1]) if ('model' in ins.columns and not ins.empty) else "EMA"

    if best_model_name.upper() == "ARIMA":
        try:
            # Refit ARIMA to get AIC/BIC (as done in the original app.py)
            _y = ins['actual'].reset_index(drop=True)
            _ar = _fit_arima(_y)
            aic = float(getattr(_ar, "aic", np.nan))
            bic = float(getattr(_ar, "bic", np.nan))
        except Exception:
            pass

    # Trend analysis and Seasonality strength (logic moved exactly from app.py)
    recent_trend = False
    # Using y_vals (already defined as float array)
    if n_months >= 6:
        x = np.arange(len(y_vals))
        # Original logic was potentially unsafe with NaNs, but preserving it as requested
        try:
            slope = np.polyfit(x, y_vals, 1)[0]
            recent_trend = abs(slope) > 0.3 * (y_vals.std() + 1e-9)
        except Exception:
            pass  # Handle potential issues during polyfit

    try:
        # Calculate seasonality strength (logic moved exactly from app.py)
        ac = np.abs(np.fft.rfft((y_vals - y_vals.mean())))
        core = ac[2:] if ac.size > 2 else ac
        seas_strength_raw = float(core.max() / (core.mean() + 1e-9)) if core.size else 0.0
        seas_strength = max(0.0, min((seas_strength_raw - 1.0) / 4.0, 1.0))
    except Exception:
        seas_strength = 0.0

    # Prepare diagnostics dictionary for model_reason_text
    diagnostics_for_reasoning = {
        "n_points": n_months,
        "seasonality_strength": seas_strength,
        "stationary": bool(stat_ok),
        "recent_trend": bool(recent_trend),
        "cv_mape_rank": 1,  # Assuming this is the best model (Rank 1)
        "mae": mae, "mape": mape, "aic": aic, "bic": bic,
    }

    # Get model reasoning text
    reasoning_text = model_reason_text(best_model_name, diagnostics_for_reasoning)

    # Metadata
    train_months = int(ins['train_months'].iloc[-1]) if 'train_months' in ins.columns else int(len(ins))
    if 'data_span' in ins.columns and not ins['data_span'].empty:
        span_txt = str(ins['data_span'].iloc[-1])
    else:
        try:
            dmin = ins['date'].min(); dmax = ins['date'].max()
            span_txt = f"{dmin:%Y-%m} ~ {dmax:%Y-%m}"
        except Exception:
            span_txt = "-"
    sigma_window = int(ins['sigma_win'].iloc[-1]) if 'sigma_win' in ins.columns else 6

    stats_output["metrics"] = {"mae": mae, "mape": mape, "aic": aic, "bic": bic}
    stats_output["metadata"] = {
        "model": best_model_name, "train_months": train_months,
        "data_span": span_txt, "sigma_window": sigma_window,
        "reasoning": reasoning_text,
    }

    # Detailed stats for the "expander" view in UI
    stats_output["details"] = {
        "모델": best_model_name, "학습기간(월)": train_months, "데이터 구간": span_txt,
        "σ 윈도우(최근)": sigma_window, "CV(K)": 3,
    }

    return fig, stats_output


# --------------------------- Lightweight series API ---------------------------
def build_series(ledger, accounts: List[str]) -> Tuple[pd.DataFrame, str]:
    """
    원장(ledger.df)에서 날짜/금액 컬럼을 자동 탐색하여 월별 시계열을 생성.
    - 정상(ledger 모드): 원장에서 월별 합계를 산출하여 tidy 반환
    - 폴백(master 모드): meta.master_df의 잔액 3포인트(전전기말/전기말/당기말)를 반환

    반환: (df, mode)
      df columns → ['계정코드','계정명','month','value']
      mode → 'ledger' | 'master'
    """
    df = getattr(ledger, 'df', None)
    if df is None or df.empty:
        return pd.DataFrame(columns=['계정코드','계정명','month','value']), 'master'

    # 계정 필터 준비(문자열 통일)
    accounts = list(map(str, accounts or []))
    work = df.copy()
    if '계정코드' in work.columns:
        try:
            work['계정코드'] = work['계정코드'].astype(str)
        except Exception:
            pass

    date_col = _pick_col(work, DATE_CANDIDATES)
    amt_col  = _pick_col(work, AMT_CANDIDATES)

    if date_col and amt_col and ('계정코드' in work.columns) and ('계정명' in work.columns):
        # 날짜/금액 안전 정규화
        work[date_col] = pd.to_datetime(work[date_col], errors='coerce')
        work[amt_col] = pd.to_numeric(work[amt_col], errors='coerce')
        work = work.dropna(subset=[date_col, amt_col, '계정코드', '계정명'])

        # 대상 계정 필터 → 월말 고정
        tmp = work[work['계정코드'].astype(str).isin(accounts)].copy()
        if tmp.empty:
            return pd.DataFrame(columns=['계정코드','계정명','month','value']), 'ledger'
        tmp['_month'] = tmp[date_col].dt.to_period('M').dt.to_timestamp(how='end').floor('D')
        mon = (tmp.groupby(['계정코드','계정명','_month'], as_index=False)[amt_col].sum())
        mon = mon.rename(columns={'_month': 'month', amt_col: 'value'}).sort_values(['계정코드','month'])
        return mon, 'ledger'

    # --- 폴백: master 잔액 3포인트 ---
    m = getattr(ledger, 'meta', {}).get('master_df') if hasattr(ledger, 'meta') else None
    need = {'전전기말잔액','전기말잔액','당기말잔액'}
    if m is None or not need.issubset(set(m.columns)):
        # 완전 폴백 실패 시 빈 프레임 반환(호출측에서 메시지 처리)
        return pd.DataFrame(columns=['계정코드','계정명','month','value']), 'master'
    try:
        cy = int(getattr(ledger, 'meta', {}).get('CY', pd.Timestamp.today().year))
    except Exception:
        cy = pd.Timestamp.today().year
    dates = [pd.Timestamp(cy-2, 12, 31), pd.Timestamp(cy-1, 12, 31), pd.Timestamp(cy, 12, 31)]
    m2 = m.copy()
    try:
        m2['계정코드'] = m2['계정코드'].astype(str)
    except Exception:
        pass
    mm = m2[m2['계정코드'].isin(accounts)][['계정코드','계정명','전전기말잔액','전기말잔액','당기말잔액']]
    rows: List[pd.DataFrame] = []
    for _, r in mm.iterrows():
        vals = [r.get('전전기말잔액', 0), r.get('전기말잔액', 0), r.get('당기말잔액', 0)]
        rows.append(pd.DataFrame({
            '계정코드': r['계정코드'],
            '계정명' : r['계정명'],
            'month' : dates,
            'value' : vals
        }))
    g = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['계정코드','계정명','month','value'])
    return g, 'master'



==============================
📄 FILE: analysis/trend.py
==============================

import pandas as pd
import plotly.express as px
from typing import List, Dict, Any, Optional
from analysis.contracts import LedgerFrame, ModuleResult
from utils.viz import add_materiality_threshold
from utils.helpers import is_credit_account


def create_monthly_trend_figure(ledger_df: pd.DataFrame, master_df: pd.DataFrame, account_code: str, account_name: str):
    """BS/PL, 차/대변 성격을 반영하여 월별 추이 그래프를 생성합니다."""
    mrow = master_df[master_df['계정코드'] == account_code]
    if mrow.empty:
        return None  # 안전 가드
    master_row = mrow.iloc[0]
    bspl = str(master_row.get('BS/PL', 'PL') or 'PL').upper()
    dc = master_row.get('차변/대변', None)
    # 대변 성격이면 그래프 부호를 뒤집어 시각화
    sign = -1.0 if is_credit_account(bspl, dc) else 1.0

    if '연도' not in ledger_df.columns or ledger_df['연도'].isna().all():
        return None
    current_year = int(ledger_df['연도'].max())
    df_filtered = ledger_df[(ledger_df['계정코드'] == account_code) & (ledger_df['연도'].isin([current_year, current_year - 1]))]
    months = list(range(1, 13))
    plot_df_list = []

    if bspl == 'BS':
        def _f(x):
            try:
                v = float(x)
                return 0.0 if pd.isna(v) else v
            except Exception:
                return 0.0
        bop_cy = _f(master_row.get('전기말잔액', 0))
        bop_py = _f(master_row.get('전전기말잔액', 0))
        for year, bop, year_label in [(current_year, bop_cy, 'CY'), (current_year - 1, bop_py, 'PY')]:
            monthly_flow = df_filtered[df_filtered['연도'] == year].groupby('월')['거래금액'].sum() if '거래금액' in df_filtered.columns else pd.Series(dtype=float)
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(monthly_flow)
            monthly_balance = bop + monthly_series.cumsum()
            plot_df_list.append(pd.DataFrame({'월': months, '금액': monthly_balance.values * sign, '구분': year_label}))
        title_suffix = "월별 잔액 추이 (BS · 월말)"
    else:
        # PL: 금액 컬럼 유연 인식
        cand = ['거래금액', '발생액', '거래금액_절대값', 'amount', '금액']
        amt_col = next((c for c in cand if c in df_filtered.columns), None)
        if amt_col is None:
            return None
        monthly_sum = df_filtered.groupby(['연도', '월'])[amt_col].sum().reset_index()
        for year, year_label in [(current_year, 'CY'), (current_year - 1, 'PY')]:
            year_data = monthly_sum[monthly_sum['연도'] == year]
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(year_data.set_index('월')[amt_col])
            plot_df_list.append(pd.DataFrame({'월': months, '금액': monthly_series.values * sign, '구분': year_label}))
        title_suffix = "월별 발생액 추이 (PL · 월합계)"

    if not plot_df_list:
        return None

    plot_df = pd.concat(plot_df_list)
    fig = px.bar(
        plot_df,
        x='월', y='금액', color='구분', barmode='group',
        title=f"'{account_name}' ({account_code}) {title_suffix}",
        labels={'월': '월', '금액': '금액', '구분': '연도'},
        color_discrete_map={'PY': '#a9a9a9', 'CY': '#1f77b4'}
    )
    fig.update_xaxes(dtick=1)
    # 🔢 축/툴팁 포맷: 천단위 쉼표, SI 단위 제거
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none')
    _note = "시점=월말" if bspl == 'BS' else "집계=월합계"
    fig.update_traces(hovertemplate=f'월=%{{x}}<br>금액=%{{y:,.0f}} 원<br>{_note}<br>구분=%{{fullData.name}}<extra></extra>')
    fig.update_layout(xaxis_title="월", yaxis_title="금액(원)")
    return fig


# (제거됨) 자동 추천 로직: 사용자가 명시적으로 선택한 계정만 사용


def run_trend_module(lf: LedgerFrame, accounts: Optional[List[str]] = None) -> ModuleResult:
    """월별 추이 모듈: 선택 계정 필터 + 월별 발생액 tables 제공."""
    df = lf.df.copy()
    master_df = lf.meta.get("master_df")
    if master_df is None:
        return ModuleResult(
            name="trend",
            summary={},
            tables={},
            figures={},
            evidences=[],
            warnings=["Master DF가 없습니다."]
        )

    # 계정 필터 강제
    acc_codes = [str(a) for a in (accounts or [])]
    if acc_codes:
        df = df[df['계정코드'].astype(str).isin(acc_codes)]

    # 월별 발생액 집계 테이블
    try:
        if '발생액' not in df.columns:
            # 유연 인식: 거래금액/금액 등에서 대체 가능하면 사용
            cand = ['거래금액', '거래금액_절대값', 'amount', '금액']
            alt = next((c for c in cand if c in df.columns), None)
            if alt is not None:
                _df = df.rename(columns={alt: '발생액'})
            else:
                _df = df.copy()
        else:
            _df = df.copy()
        _df['월'] = _df['회계일자'].dt.to_period('M').astype(str)
        flow_tbl = (
            _df.groupby(['계정코드', '월'])['발생액'].sum().reset_index().rename(columns={'발생액': '월별발생액'})
        )
    except Exception:
        flow_tbl = df.head(0).copy()

    figures: Dict[str, Any] = {}
    warns: List[str] = []
    pm_value = (lf.meta or {}).get("pm_value")
    for code in sorted(flow_tbl['계정코드'].astype(str).unique().tolist() if not flow_tbl.empty else acc_codes):
        m = master_df[master_df['계정코드'].astype(str) == code]
        if m.empty:
            warns.append(f"계정코드 {code}가 Master에 없습니다.")
            continue
        name = m.iloc[0].get('계정명', str(code))
        fig = create_monthly_trend_figure(df, master_df, code, name)
        if fig:
            if pm_value:
                add_materiality_threshold(fig, pm_value=pm_value)
            figures[f"{code}:{name}"] = fig
        else:
            warns.append(f"{name}({code}) 그림 생성 불가(데이터 부족).")

    summary = {
        "picked_accounts": acc_codes or sorted(flow_tbl['계정코드'].astype(str).unique().tolist() if not flow_tbl.empty else []),
        "n_figures": len(figures),
        "period_tag_coverage": dict(df['period_tag'].value_counts()) if 'period_tag' in df.columns else {},
    }

    return ModuleResult(
        name="trend",
        summary=summary,
        tables={"monthly_flow": flow_tbl},
        figures=figures,
        evidences=[],
        warnings=warns
    )




==============================
📄 FILE: analysis/ts_v2.py
==============================

# analysis/ts_v2.py
from __future__ import annotations
import numpy as np
import pandas as pd
from .aggregation import aggregate_monthly, month_end_00


def run_timeseries_minimal(df, *, account_name, date_col, amount_col, is_bs, opening=0.0, pm_value=0.0):
    x = df[[date_col, amount_col]].copy()
    x.rename(columns={date_col: "date", amount_col: "amount"}, inplace=True)
    x["date"] = month_end_00(pd.to_datetime(x["date"], errors="coerce"))
    x["amount"] = pd.to_numeric(x["amount"], errors="coerce").fillna(0.0)
    x = x.dropna(subset=["date"])

    g = (x.groupby("date", as_index=False)["amount"].sum()
           .sort_values("date").reset_index(drop=True))

    y = g["amount"].astype(float)
    
    # === 미래구간 게이트(학습 N<6 → horizon=0) ===
    n = int(len(y))
    min_pts = 6
    
    # === MoR(자동 선택) 로그 노출(EMA vs MA 비교·선정 근거) ===
    candidates = []
    
    # EMA 후보들
    for alpha in [0.3, 0.5]:
        span_ema = int(2/alpha - 1)
        pred_ema = y.ewm(span=span_ema, adjust=False).mean().shift(1)
        err_ema = y - pred_ema
        mae_ema = err_ema.abs().mean()
        mape_ema = (err_ema.abs() / y.abs().replace(0, np.nan)).mean() * 100
        candidates.append({
            'name': f'EMA(α={alpha})',
            'pred': pred_ema,
            'mae': mae_ema,
            'mape': mape_ema if not np.isnan(mape_ema) else 999.0
        })
    
    # MA 후보들
    for w in [3, 6]:
        if len(y) >= w:
            pred_ma = y.rolling(window=w).mean().shift(1)
            err_ma = y - pred_ma
            mae_ma = err_ma.abs().mean()
            mape_ma = (err_ma.abs() / y.abs().replace(0, np.nan)).mean() * 100
            candidates.append({
                'name': f'MA({w})',
                'pred': pred_ma,
                'mae': mae_ma,
                'mape': mape_ma if not np.isnan(mape_ma) else 999.0
            })
    
    # 최적 선택: MAPE 최소 (tie -> 작은 MAE)
    best = min(candidates, key=lambda x: (x['mape'], x['mae']))
    winner_name = best['name']
    best_mape = best['mape']
    best_mae = best['mae']
    
    # MoR 로그 생성
    mor_log = {
        "winner": winner_name,
        "metric": "MAPE",
        "mape_best": float(best_mape),
        "mae_best": float(best_mae),
        "n_months": n,
        "used": winner_name
    }
    
    # 선택된 예측값 사용
    pred_flow = best['pred'] if n >= min_pts else y  # 포인트 부족시 완전 in-sample
    
    err  = y - pred_flow
    sig  = err.rolling(window=min(6, max(2, len(err))), min_periods=2).std(ddof=0)
    z    = err / sig.replace(0, np.nan)

    kit      = (y.abs() > float(pm_value)).astype(float)
    pm_ratio = np.minimum(1.0, err.abs() / float(pm_value) if pm_value else 0.0)
    risk     = np.minimum(1.0, 0.5 * (z.abs() / 3.0) + 0.3 * pm_ratio + 0.2 * kit)

    flow = g.assign(
        account=str(account_name),
        measure="flow",
        actual=y.values,
        predicted=pred_flow.values,
        error=err.values,
        z=z.values,
        risk=risk.values,
        model=winner_name,  # 표/툴팁에 노출
    )

    if is_bs:
        # === Balance 안전가드: opening + 누적만 사용 ===
        # opening 없으면 0.0 (안전가드)
        opening_safe = 0.0 if opening is None or pd.isna(opening) else float(opening)
        
        # flow_actual: 월별 합계(부호보정 포함) 시리즈
        flow_actual = flow["actual"].astype(float)
        flow_pred = flow["predicted"].astype(float)
        
        # Balance는 opening + cumsum(flow)만 사용
        balance_actual = opening_safe + flow_actual.cumsum()
        balance_pred = opening_safe + flow_pred.cumsum()
        
        bal = flow.copy()
        bal["measure"] = "balance"
        bal["actual"] = balance_actual.values
        bal["predicted"] = balance_pred.values
        bal["error"] = bal["actual"] - bal["predicted"]
        
        # balance error/z/risk는 flow와 동일 로직으로 파생(표준화 창 동일 k=6)
        bal_sig = bal["error"].rolling(window=min(6, max(2, len(bal))), min_periods=2).std(ddof=0)
        bal["z"] = bal["error"] / bal_sig.replace(0, np.nan)
        bal["risk"] = np.minimum(1.0, 0.5 * (bal["z"].abs() / 3.0) + 0.2 * (bal["actual"].abs() > float(pm_value)).astype(float))
        
        out = pd.concat([flow, bal], ignore_index=True)
    else:
        out = flow

    # === 출력 스키마 계약(Contract) 고정 + 날짜 정규화 ===
    # 필수 9컬럼 보장
    need_cols = ["date","account","measure","actual","predicted","error","z","risk","model"]
    for c in need_cols:
        if c not in out.columns:
            out[c] = np.nan

    # 날짜 정규화 (월말 00:00:00)
    out["date"] = month_end_00(pd.to_datetime(out["date"], errors="coerce"))

    # 숫자형 강제(표 포맷/다운로드 안정)
    num_cols = ["actual","predicted","error","z","risk"]
    out[num_cols] = out[num_cols].apply(pd.to_numeric, errors="coerce")
    
    # 문자형 강제
    out["model"]   = out["model"].astype(str)
    out["measure"] = out["measure"].astype(str)
    out["account"] = out["account"].astype(str)

    # 컬럼 순서 강제
    out = out[need_cols]
    
    # MoR 로그를 attrs에 저장
    out.attrs["mor_log"] = mor_log
    
    return out.reset_index(drop=True)


# --- BEGIN: TS Utilities (stats, anomaly table, future shading) ---

import numpy as np
import pandas as pd

def compute_series_stats(dfm: pd.DataFrame) -> dict:
    """
    dfm: 단일 계정×단일 measure(=flow/balance) 구간의 in-sample 결과 프레임
         (필수 컬럼: ['actual','predicted'])
    반환: {'MAE':..., 'MAPE':..., 'RMSE':..., 'N':...}
    """
    if dfm is None or dfm.empty:
        return {'MAE': np.nan, 'MAPE': np.nan, 'RMSE': np.nan, 'N': 0}
    s = dfm[['actual', 'predicted']].apply(pd.to_numeric, errors='coerce').dropna()
    n = int(len(s))
    if n == 0:
        return {'MAE': np.nan, 'MAPE': np.nan, 'RMSE': np.nan, 'N': 0}
    err = (s['actual'] - s['predicted']).to_numpy()
    mae = float(np.mean(np.abs(err)))
    rmse = float(np.sqrt(np.mean(err ** 2)))
    # 0 나눗셈 방지: |actual|<eps는 무시하여 MAPE를 NaN으로 처리 → 전체는 nanmean
    denom = s['actual'].to_numpy()
    denom = np.where(np.abs(denom) < 1e-9, np.nan, np.abs(denom))
    mape = float(np.nanmean(np.abs(err) / denom) * 100.0)
    return {'MAE': mae, 'MAPE': mape, 'RMSE': rmse, 'N': n}

def _ensure_z_with_rolling(dfm: pd.DataFrame, k: int = 6) -> pd.DataFrame:
    """
    z가 없거나 전부 결측이면 최근 k개월 롤링-표준편차(+expanding 보정)로 z를 계산.
    z = error / std_k  (초기 구간은 expanding std로 보강)
    """
    out = dfm.copy()
    if 'z' in out.columns and out['z'].notna().any():
        return out
    out['actual'] = pd.to_numeric(out.get('actual'), errors='coerce')
    out['predicted'] = pd.to_numeric(out.get('predicted'), errors='coerce')
    if 'error' not in out.columns:
        out['error'] = out['actual'] - out['predicted']
    out = out.sort_values('date')
    k = int(max(3, k))
    std_k = out['error'].rolling(window=k, min_periods=3).std()
    std_exp = out['error'].expanding(min_periods=3).std()
    std = std_k.fillna(std_exp).replace(0, np.nan)
    out['z'] = out['error'] / std
    return out

def build_anomaly_table(df_all: pd.DataFrame, *, topn: int = 10, k: int = 6, pm_value: float | None = None) -> pd.DataFrame:
    """
    단일 계정의 전체 measure(flow/balance)를 입력받아 Top-|z| 월 상위 N행을 반환.
    반환 컬럼: ['일자','실측','예측','잔차','z','PM대비','위험도','모델','기준','|z|']
    """
    frames = []
    for ms in ('flow', 'balance'):
        d = df_all[df_all['measure'].eq(ms)]
        if d.empty:
            continue
        d = _ensure_z_with_rolling(d, k=k)
        d['PM대비'] = (
            np.minimum(1.0, np.abs(d['error']) / float(pm_value))
            if pm_value and pm_value > 0 else np.nan
        )
        d['기준'] = np.where(d['measure'].eq('flow'), 'Flow', 'Balance')
        keep = ['date','actual','predicted','error','z','PM대비','risk','model','기준']
        for c in keep:
            if c not in d.columns:
                d[c] = np.nan
        frames.append(d[keep].copy())

    if not frames:
        cols = ['일자','실측','예측','잔차','z','PM대비','위험도','모델','기준','|z|']
        return pd.DataFrame(columns=cols)

    out = pd.concat(frames, ignore_index=True)
    out['|z|'] = np.abs(out['z'])
    out = out.sort_values(['|z|','date'], ascending=[False, False]).head(int(topn)).copy()
    out = out.rename(columns={
        'date':'일자','actual':'실측','predicted':'예측','error':'잔차','risk':'위험도','model':'모델'
    })
    return out

def add_future_shading(fig, last_date, horizon_months: int = 0):
    """
    미래 예측 시각 강조용 음영. horizon_months<=0이면 아무 것도 하지 않음.
    """
    if fig is None or not horizon_months or horizon_months <= 0:
        return fig
    last = pd.to_datetime(last_date)
    x0 = last + pd.offsets.MonthBegin(1)      # 다음 달 시작
    x1 = last + pd.offsets.MonthEnd(horizon_months)
    try:
        fig.add_vrect(
            x0=x0, x1=x1,
            fillcolor="LightGrey", opacity=0.15, line_width=0,
            layer="below"  # ← 데이터/경계선 아래로 깔기
        )
    except Exception:
        pass
    return fig

# --- END: TS Utilities ---


==============================
📄 FILE: analysis/vendor.py
==============================

from typing import List, Dict, Any
import pandas as pd
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from itertools import product
from analysis.contracts import LedgerFrame, ModuleResult
from utils.viz import add_materiality_threshold, add_pm_badge


# 공통: 금액 컬럼 후보에서 하나를 고르고, 필요 시 절대값으로 변환
_AMOUNT_CANDIDATES = ["거래금액_절대값", "거래금액", "발생액", "amount", "금액"]

def _pick_amount_column(df: pd.DataFrame) -> str | None:
    for c in _AMOUNT_CANDIDATES:
        if c in df.columns:
            return c
    return None


def create_pareto_figure(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True, pm_value: float | None = None):
    """거래처별 거래금액 파레토 차트.
    - min_amount 이상인 거래처만 개별 표기
    - 나머지는 '기타'로 합산(옵션)
    - 1차 Y축에 PM 점선/라벨 표시
    """
    if ledger_df is None or ledger_df.empty or "연도" not in ledger_df.columns:
        return None

    cy_df = ledger_df[ledger_df["연도"] == ledger_df["연도"].max()]
    if "거래처" not in cy_df.columns or cy_df["거래처"].nunique() < 1:
        return None

    amt_col = _pick_amount_column(cy_df)
    if amt_col is None:
        return None

    # 절대값 기준 합계 (표시의 일관성을 위해)
    series_raw = cy_df.groupby("거래처")[amt_col].sum()
    vendor_sum = series_raw.abs()

    if vendor_sum.empty:
        return None

    # 임계치 필터
    above = vendor_sum[vendor_sum >= float(min_amount)].sort_values(ascending=False)
    etc_sum = float(vendor_sum.sum() - above.sum())

    series = above
    if include_others and etc_sum > 0:
        series = pd.concat([above, pd.Series({"기타": etc_sum})])

    cum_ratio = series.cumsum() / series.sum() * 100.0

    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(go.Bar(x=series.index, y=series.values, name="거래 금액"), secondary_y=False)
    fig.add_trace(go.Scatter(x=series.index, y=cum_ratio.values, name="누적 비율(%)", mode="lines+markers"), secondary_y=True)
    fig.update_layout(title="거래처 집중도 분석 (Pareto)", yaxis_title="금액", yaxis2_title="누적 비율(%)")

    # 🔢 축/툴팁 포맷
    fig.update_yaxes(separatethousands=True, tickformat=",.0f", showexponent="none", exponentformat="none", secondary_y=False)
    fig.update_yaxes(tickformat=".1f", ticksuffix="%", secondary_y=True)
    fig.update_traces(hovertemplate="%{x}<br>%{y:,.0f} 원<extra></extra>", selector=dict(type="bar"))
    fig.update_traces(hovertemplate="%{x}<br>누적 비율=%{y:.1f}%<extra></extra>", selector=dict(type="scatter"))

    # PM 점선/라벨 (1차 y축)
    if pm_value and float(pm_value) > 0:
        add_materiality_threshold(fig, pm_value=float(pm_value))

    return fig


def create_vendor_heatmap(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True, pm_value: float | None = None):
    """거래처별 월별 활동 히트맵.
    - min_amount 이상인 거래처만 개별 표기
    - 나머지는 '기타'로 월별 합산(옵션)
    - 우측 상단에 PM 배지 표시
    """
    if ledger_df is None or ledger_df.empty or "거래처" not in ledger_df.columns:
        return None

    df = ledger_df.copy()
    if "회계일자" not in df.columns:
        return None

    amt_col = _pick_amount_column(df)
    if amt_col is None:
        return None

    df["연월"] = pd.to_datetime(df["회계일자"], errors="coerce").dt.to_period("M").astype(str)
    pivot = df.pivot_table(index="거래처", columns="연월", values=amt_col, aggfunc="sum").fillna(0).abs()
    if pivot.empty:
        return None

    totals = pivot.sum(axis=1)
    above_idx = totals >= float(min_amount)
    pivot_above = pivot.loc[above_idx].copy()
    pivot_above["_tot_"] = pivot_above.sum(axis=1)
    pivot_above = pivot_above.sort_values("_tot_", ascending=False).drop(columns=["_tot_"])

    pivot_final = pivot_above
    if include_others:
        below = pivot.loc[~above_idx]
        if not below.empty:
            etc_row = pd.DataFrame([below.sum(axis=0)], index=["기타"])
            pivot_final = pd.concat([pivot_above, etc_row], axis=0)

    fig = px.imshow(pivot_final, title="거래처 월별 활동 히트맵", labels=dict(x="연월", y="거래처", color="거래금액"))

    # 🔢 컬러바/툴팁 포맷
    fig.update_coloraxes(colorbar=dict(tickformat=",.0f"))
    fig.update_traces(hovertemplate="연월=%{x}<br>거래처=%{y}<br>거래금액=%{z:,.0f} 원<extra></extra>")

    # PM 배지
    if pm_value and float(pm_value) > 0:
        add_pm_badge(fig, pm_value=float(pm_value))

    return fig


def create_vendor_detail_figure(ledger_df: pd.DataFrame, vendor_name: str, all_months: List[str]):
    """특정 거래처의 월별 거래액을 계정별 누적 막대그래프로 생성합니다. (전체 기간 X축 보장)"""
    vendor_df = ledger_df[ledger_df["거래처"] == vendor_name].copy()
    amt_col = _pick_amount_column(vendor_df)
    if amt_col is None:
        return None

    vendor_df["연월"] = pd.to_datetime(vendor_df["회계일자"], errors="coerce").dt.to_period("M").astype(str)
    summary = vendor_df.groupby(["연월", "계정명"], as_index=False)[amt_col].sum()
    summary[amt_col] = summary[amt_col].abs()

    unique_accounts = vendor_df["계정명"].unique()
    axis_labels = {"연월": "거래월", amt_col: "거래금액"}

    if len(unique_accounts) == 0:
        empty_df = pd.DataFrame({"연월": all_months, "계정명": [None] * len(all_months), amt_col: [0] * len(all_months)})
        fig = px.bar(empty_df, x="연월", y=amt_col, labels=axis_labels)
    else:
        template_df = pd.DataFrame(list(product(all_months, unique_accounts)), columns=["연월", "계정명"])
        merged_summary = pd.merge(template_df, summary, on=["연월", "계정명"], how="left").fillna(0)
        fig = px.bar(
            merged_summary,
            x="연월", y=amt_col, color="계정명",
            category_orders={"연월": all_months},
            labels=axis_labels,
        )

    fig.update_layout(
        barmode="stack",
        title=f"'{vendor_name}' 거래처 월별/계정별 상세 내역"
    )
    # 🔢 축/툴팁 포맷: 천단위 쉼표, SI 제거
    fig.update_yaxes(separatethousands=True, tickformat=",.0f", showexponent="none", exponentformat="none")
    fig.update_traces(hovertemplate="연월=%{x}<br>금액=%{y:,.0f} 원<br>계정명=%{fullData.name}<extra></extra>")
    return fig


def run_vendor_module(lf: LedgerFrame, account_codes: List[str] | None = None,
                      min_amount: float = 0, include_others: bool = True) -> ModuleResult:
    """거래처 모듈: 선택 계정 필터 + 최소 거래금액 필터('기타' 합산) + PM 라벨/배지."""
    df = lf.df
    use_df = df.copy()
    if account_codes:
        acs = [str(a) for a in account_codes]
        use_df = use_df[use_df["계정코드"].astype(str).isin(acs)]

    figures: Dict[str, Any] = {}
    warnings: List[str] = []
    pm_value = (lf.meta or {}).get("pm_value")

    pareto = create_pareto_figure(use_df, min_amount=min_amount, include_others=include_others, pm_value=pm_value)
    heatmap = create_vendor_heatmap(use_df, min_amount=min_amount, include_others=include_others, pm_value=pm_value)

    if pareto: figures["pareto"] = pareto
    else: warnings.append("Pareto 그래프 생성 불가(데이터 부족).")
    if heatmap: figures["heatmap"] = heatmap
    else: warnings.append("히트맵 생성 불가(데이터 부족).")

    # 요약 정보
    if "연도" in use_df.columns:
        cy = use_df[use_df["연도"] == use_df["연도"].max()]
    else:
        cy = use_df.iloc[:0]

    amt_col = _pick_amount_column(cy) if not cy.empty else None
    vendor_sum = cy.groupby("거래처")[amt_col].sum().abs() if (amt_col and "거래처" in cy.columns and not cy.empty) else pd.Series(dtype=float)
    n_above = int((vendor_sum >= float(min_amount)).sum()) if not vendor_sum.empty else 0
    n_below = int((vendor_sum < float(min_amount)).sum()) if not vendor_sum.empty else 0

    summary = {
        "filtered_accounts": [str(a) for a in account_codes] if account_codes else [],
        "min_amount": float(min_amount),
        "include_others": bool(include_others),
        "n_above_threshold": n_above,
        "n_below_threshold": n_below,
        "n_figures": len(figures),
        "period_tag_coverage": dict(use_df["period_tag"].value_counts()) if "period_tag" in use_df.columns else {},
    }
    return ModuleResult(
        name="vendor",
        summary=summary,
        tables={},
        figures=figures,
        evidences=[],
        warnings=warnings
    )



==============================
📄 FILE: analysis/__init__.py
==============================






==============================
📄 FILE: infra/env_loader.py
==============================

from __future__ import annotations
import os, sys
from typing import Optional


def _read_kv_file(path: str) -> dict:
    # 단순 .env 파서 (python-dotenv 없이도 작동)
    data = {}
    if not os.path.exists(path):
        return data
    with open(path, "r", encoding="utf-8") as f:
        for raw in f:
            line = raw.strip()
            if not line or line.startswith("#"):
                continue
            if "=" not in line:
                continue
            k, v = line.split("=", 1)
            k = k.strip()
            v = v.strip().strip("\"'")  # 양쪽 따옴표 제거
            data[k] = v
    return data


def _maybe_import_dotenv():
    try:
        from dotenv import load_dotenv  # type: ignore
        return load_dotenv
    except Exception:
        return None


# 동의어 키 -> 표준 키 정규화
_OPENAI_ALIASES = [
    "OPENAI_API_KEY", "OPENAI_KEY", "OPENAI_TOKEN", "OPENAIAPIKEY", "OPENAI_APIKEY",
    # Azure/OpenAI 변형들 (있으면 그대로도 허용)
    "AZURE_OPENAI_API_KEY",
]


def _normalize_env(d: dict) -> None:
    # 표준 키가 없고, 동의어가 있으면 끌어와서 OPENAI_API_KEY 세팅
    if not d.get("OPENAI_API_KEY"):
        for k in _OPENAI_ALIASES:
            if k in d and d[k]:
                d["OPENAI_API_KEY"] = d[k]
                break


def ensure_api_keys_loaded() -> bool:
    # 1) python-dotenv가 있으면 먼저 시도
    load_dotenv = _maybe_import_dotenv()
    if load_dotenv:
        # 두 파일을 모두 시도(존재하는 것만 적용)
        for p in (".env", "API_KEY.env"):
            try:
                load_dotenv(dotenv_path=p, override=False)
            except Exception:
                pass

    # 2) 수동 파싱 (dotenv가 없거나, 못 읽은 경우 대비)
    merged = {}
    for p in (".env", "API_KEY.env"):
        try:
            merged.update(_read_kv_file(p))
        except Exception:
            pass

    # 3) 동의어 정규화 → OPENAI_API_KEY
    _normalize_env(merged)

    # 4) 환경변수에 반영(존재하지 않는 경우에만 세팅)
    for k, v in merged.items():
        if k not in os.environ and v:
            os.environ[k] = v

    ok = bool(os.environ.get("OPENAI_API_KEY") or os.environ.get("AZURE_OPENAI_API_KEY"))
    # 가시적 플래그
    os.environ["LLM_AVAILABLE"] = "1" if ok else "0"
    return ok


def is_llm_ready() -> bool:
    return os.environ.get("LLM_AVAILABLE", "0") == "1"


def log_llm_status(logger=None):
    # 한국어 상태 로그
    if is_llm_ready():
        msg = "🔌 OpenAI Key 감지: 온라인 LLM 모드로 생성합니다. (클러스터/요약 LLM 사용)"
    else:
        msg = "🔌 OpenAI Key 없음: 오프라인 리포트 모드로 생성합니다. (클러스터/요약 LLM 미사용)"
    if logger:
        try:
            logger.info(msg)
            return
        except Exception:
            pass
    print(msg, file=sys.stderr)


# 앱 부팅 시 사용할 진입점 (import만으로 부팅초기화하고 싶을 때)
def boot():
    ensure_api_keys_loaded()
    log_llm_status()





==============================
📄 FILE: services/cache.py
==============================

# services/cache.py  (전체 교체본)
# - 임베딩 캐시(SQLite)
# - LLM 매핑 캐시(승인/버전)
# - 캐시 정보 헬퍼

from __future__ import annotations
import os, sqlite3, json, hashlib, threading, time
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from config import EMBED_CACHE_DIR

# 모델별 DB 파일 접근 시 레이스를 막기 위한 락 맵
_LOCKS: Dict[str, threading.Lock] = {}

def _model_dir(model: str) -> Path:
    # 모델명을 안전한 폴더명으로 변환
    safe = model.replace("/", "_").replace(":", "_")
    d = Path(EMBED_CACHE_DIR) / safe
    d.mkdir(parents=True, exist_ok=True)
    return d

def _db_path(model: str) -> str:
    return str(_model_dir(model) / "embeddings.sqlite3")

def _sha1(s: str) -> str:
    """
    FIPS 모드에서도 동작하도록 안전 가드:
    1) hashlib.sha1(..., usedforsecurity=False) 시도
    2) 파라미터 미지원/차단 시 hashlib.sha1(b) 시도
    3) 그래도 실패하면 hashlib.new("sha1") 시도
    4) 최종 폴백: blake2s(20바이트) — 캐시 키 용도라 160-bit 길이만 유지되면 충분
    """
    b = s.encode("utf-8")
    try:
        return hashlib.sha1(b, usedforsecurity=False).hexdigest()  # type: ignore[call-arg]
    except TypeError:
        try:
            return hashlib.sha1(b).hexdigest()
        except Exception:
            pass
    except Exception:
        pass
    try:
        h = hashlib.new("sha1", b)
        return h.hexdigest()
    except Exception:
        # final fallback: 160-bit digest for key-length parity
        return hashlib.blake2s(b, digest_size=20).hexdigest()

def _get_lock(model: str) -> threading.Lock:
    if model not in _LOCKS:
        _LOCKS[model] = threading.Lock()
    return _LOCKS[model]

def _ensure_db(conn: sqlite3.Connection):
    # 기본 테이블 스키마 보장
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS emb (
            k TEXT PRIMARY KEY,
            text TEXT,
            vec TEXT
        )
    """)
    conn.commit()

def _open(model: str) -> sqlite3.Connection:
    p = _db_path(model)
    conn = sqlite3.connect(p, timeout=30)
    _ensure_db(conn)
    return conn

def cache_get_many(model: str, texts: List[str]) -> Dict[str, List[float]]:
    # 여러 텍스트에 대한 캐시 조회
    if not texts:
        return {}
    keys = [(t, _sha1(f"{model}|{t}")) for t in texts]
    with _get_lock(model):
        conn = _open(model)
        try:
            cur = conn.cursor()
            qmarks = ",".join("?" for _ in keys)
            cur.execute(f"SELECT k, vec FROM emb WHERE k IN ({qmarks})", [k for _, k in keys])
            rows = {k: json.loads(vec) for (k, vec) in cur.fetchall()}
        finally:
            conn.close()
    out: Dict[str, List[float]] = {}
    for t, k in keys:
        if k in rows:
            out[t] = rows[k]
    return out

def cache_put_many(model: str, pairs: List[Tuple[str, List[float]]]) -> None:
    # 여러 텍스트-벡터 쌍을 캐시에 저장
    if not pairs:
        return
    records = [(_sha1(f"{model}|{t}"), t, json.dumps(vec)) for (t, vec) in pairs]
    with _get_lock(model):
        conn = _open(model)
        try:
            conn.executemany("INSERT OR REPLACE INTO emb(k,text,vec) VALUES (?,?,?)", records)
            conn.commit()
        finally:
            conn.close()

def get_or_embed_texts(
    texts: List[str],
    client,
    *,
    model: str,
    batch_size: int,
    timeout: int,
    max_retry: int,
) -> Dict[str, List[float]]:
    # 텍스트 목록에 대해 캐시를 우선 조회하고, 누락분만 임베딩 API 호출
    texts = list(dict.fromkeys([str(t) for t in texts]))
    cached = cache_get_many(model, texts)
    missing = [t for t in texts if t not in cached]
    if not missing:
        return cached
    out = dict(cached)
    for s in range(0, len(missing), batch_size):
        sub = missing[s:s+batch_size]
        last_err = None
        for attempt in range(max_retry):
            try:
                try:
                    resp = client.embeddings.create(model=model, input=sub, timeout=timeout)
                except TypeError:
                    # 일부 클라이언트는 timeout 파라미터를 지원하지 않음
                    resp = client.embeddings.create(model=model, input=sub)
                vecs = [d.embedding for d in resp.data]
                pairs = list(zip(sub, vecs))
                cache_put_many(model, pairs)
                out.update({sub[i]: vecs[i] for i in range(len(sub))})
                last_err = None
                break
            except Exception as e:
                last_err = e
        if last_err:
            # 재시도 실패 시, 마지막 예외를 전파
            raise last_err
    return out

# ===== LLM 매핑 캐시 (승인/버전 고정) =====
DEFAULT_CACHE_PATH = os.path.join(".cache", "llm_mappings.json")

class LLMMappingCache:
    # 간단한 파일 기반 승인/버전 관리
    def __init__(self, path: str = DEFAULT_CACHE_PATH):
        self.path = path
        self._lock = threading.Lock()
        self._state: Dict[str, Any] = {"approved": {}, "proposed": {}, "versions": {}}
        self._load()

    def _load(self) -> None:
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        if os.path.exists(self.path):
            try:
                with open(self.path, "r", encoding="utf-8") as f:
                    self._state = json.load(f)
            except Exception:
                # 파손 파일은 조용히 무시
                pass

    def _save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self._state, f, ensure_ascii=False, indent=2)

    def get_approved(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["approved"].get(key)
            return dict(item) if item else None

    def get_proposed(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["proposed"].get(key)
            return dict(item) if item else None

    def propose(self, key: str, value: Any, *, model: str, meta: Optional[Dict[str, Any]] = None) -> None:
        with self._lock:
            self._state["proposed"][key] = {
                "value": value,
                "model": model,
                "meta": meta or {},
                "ts": time.time(),
            }
            self._save()

    def approve(self, key: str, *, value_override: Any | None = None, note: str = "") -> Dict[str, Any]:
        # 초안을 승인하여 버전을 올리고 확정
        with self._lock:
            src = self._state["proposed"].get(key) or self._state["approved"].get(key)
            if not src:
                raise KeyError(f"No proposed/approved entry for key={key!r}")
            prev_ver = int(self._state["versions"].get(key, 0))
            new_ver = prev_ver + 1
            final_value = src["value"] if value_override is None else value_override
            rec = {
                "value": final_value,
                "version": f"v{new_ver}",
                "note": note,
                "approved_ts": time.time(),
                "model": src.get("model"),
                "meta": src.get("meta", {}),
            }
            self._state["approved"][key] = rec
            self._state["versions"][key] = new_ver
            if key in self._state["proposed"]:
                del self._state["proposed"][key]
            self._save()
            return dict(rec)

# ===== 임베딩 캐시 정보 헬퍼 (app.py 사이드바 등에서 사용) =====
def get_cache_info(model: str) -> Dict[str, Any]:
    p = _db_path(model)
    info = {"model": model, "path": p, "exists": os.path.exists(p)}
    if not os.path.exists(p):
        return info
    try:
        conn = sqlite3.connect(p, timeout=10)
        _ensure_db(conn)
        try:
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) FROM emb")
            nrows = int(cur.fetchone()[0])
            size = os.path.getsize(p)
            info.update({"rows": nrows, "size_bytes": size})
        finally:
            conn.close()
    except Exception as e:
        info["error"] = str(e)
    return info



==============================
📄 FILE: services/cluster_naming.py
==============================

from __future__ import annotations
from typing import Optional, Callable, List, Dict

# --- Factories: analysis 계층에 넘겨줄 콜백 생성 ---

def make_llm_name_fn(client, model: str = "gpt-4o-mini") -> Callable[[List[str], List[str]], Optional[str]]:
    """(services 전용) 클러스터 이름을 생성하는 콜백을 만들어 반환."""
    def _name_fn(samples_desc: List[str], samples_vendor: List[str]) -> Optional[str]:
        import textwrap
        prompt = textwrap.dedent(f"""
        너는 회계 감사 보조 AI다. 아래 거래 샘플을 보고 이 그룹의 성격을 가장 잘 드러내는
        한국어 **클러스터 이름 1개만**, 10자 내외로 제시해라.
        숫자/기호/따옴표/접두사 금지. 예: 직원 급여, 통신비, 임원 복지.

        [거래 적요 샘플]
        - {'\n- '.join(samples_desc) if samples_desc else '(샘플 부족)'}

        [주요 거래처 샘플]
        - {'\n- '.join(samples_vendor) if samples_vendor else '(샘플 부족)'}

        정답:
        """).strip()
        try:
            resp = client.chat.completions.create(
                model=model, messages=[{"role":"user","content":prompt}], temperature=0
            )
            cand = (resp.choices[0].message.content or "").strip().splitlines()[0].strip(" \"'[]()")
            if not cand:
                return None
            bad = {"클러스터","unknown","이름없음","미정","기타"}
            if cand.lower() in bad or cand.startswith("클러스터"):
                return None
            return cand[:20]
        except Exception:
            return None
    return _name_fn


def make_synonym_confirm_fn(client, model: str = "gpt-4o-mini") -> Callable[[str, str], bool]:
    """(services 전용) 두 이름이 사실상 같은 의미인지 YES/NO로 확인하는 콜백."""
    def _confirm(a: str, b: str) -> bool:
        try:
            q = (
                "너는 회계 감사 보조 AI다. 다음 두 표현이 '회계 거래 카테고리' 이름으로서 "
                "사실상 같은 의미인지 YES/NO로만 답하라.\n"
                f"A: {a}\nB: {b}\n정답:"
            )
            resp = client.chat.completions.create(
                model=model, messages=[{"role": "user", "content": q}], temperature=0
            )
            ans = (resp.choices[0].message.content or "").strip().split()[0].upper()
            return ans.startswith("Y")
        except Exception:
            return False
    return _confirm


def unify_cluster_labels_llm(names: List[str], client, model: str = "gpt-4o-mini") -> Dict[str, str]:
    """
    유사 의미의 한글 클러스터명을 LLM으로 묶어 canonical name 매핑을 리턴.
    입력: ["경비 관리","경비 처리","관리 경비", ...]
    출력: {"경비 처리":"경비 관리", ...}
    """
    uniq = sorted([n for n in set([str(x) for x in names]) if n and n.lower() != 'nan'])
    if not uniq:
        return {}
    prompt = (
        "다음 한국어 클러스터 이름들을 의미가 같은 것끼리 묶어 하나의 대표명으로 통합하세요.\n"
        "규칙: 1) 가장 일반적/짧은 표현을 대표명으로, 2) JSON 객체로만 응답, 3) 형식: {원래명:대표명, ...}.\n"
        f"목록: {uniq}"
    )
    try:
        resp = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt}], temperature=0
        )
        import json
        txt = (resp.choices[0].message.content or "").strip()
        mapping = json.loads(txt)
        if isinstance(mapping, dict):
            return mapping
    except Exception:
        pass
    return {n: n for n in uniq}





==============================
📄 FILE: services/cycles_store.py
==============================

# services/cycles_store.py
from __future__ import annotations
from typing import Dict, Iterable, List
import os, json

# -------------------------------------------------
# 0) 사이클 코드/라벨
# -------------------------------------------------
ALLOWED_CYCLES: List[str] = [
    "CLOSE", "REV", "PUR", "HR",
    "TREASURY_INVEST", "TREASURY_FINANCE",
    "TAX", "PPE", "INTANG", "LEASE", "OTHER",
]

CYCLE_KO: Dict[str, str] = {
    "CLOSE":            "결산",
    "REV":              "매출",
    "PUR":              "매입·비용",
    "HR":               "인사",
    "TREASURY_INVEST":  "자금운용",
    "TREASURY_FINANCE": "자금조달",
    "TAX":              "세무",
    "PPE":              "유형자산",
    "INTANG":           "무형자산",
    "LEASE":            "리스",
    "OTHER":            "기타",
}
KO_TO_CODE = {v: k for k, v in CYCLE_KO.items()}

def code_to_ko(code: str) -> str:
    return CYCLE_KO.get(str(code).upper(), "기타")

def ko_to_code(label: str) -> str:
    return KO_TO_CODE.get(str(label), "OTHER")

# -------------------------------------------------
# 1) 업로드별 계정→사이클 ‘매핑’ 저장/조회 (세션 메모리)
# -------------------------------------------------
_MEM: Dict[str, Dict[str, str]] = {}   # {upload_id: {account_code: cycle_code}}

def set_cycles_map(upload_id: str, mapping: Dict[str, str]) -> None:
    """업로드 식별자 별 계정→사이클 매핑 저장."""
    if not upload_id:
        upload_id = "_default"
    cleaned: Dict[str, str] = {}
    for k, v in (mapping or {}).items():
        lab = (str(v).strip().upper() if v is not None else "OTHER")
        cleaned[str(k)] = lab if lab in ALLOWED_CYCLES else "OTHER"
    _MEM[upload_id] = cleaned

def get_cycles_map(upload_id: str) -> Dict[str, str]:
    """해당 업로드의 매핑 조회(없으면 빈 dict)."""
    if not upload_id:
        upload_id = "_default"
    return dict(_MEM.get(upload_id, {}))

def get_effective_cycles(upload_id: str | None = None) -> Dict[str, str]:
    """
    UI 편의용: 현재 업로드의 '계정→사이클 매핑'만 반환.
    (⚠️ 프리셋과 다름. 인자 필요)
    """
    if upload_id and upload_id in _MEM:
        return dict(_MEM[upload_id])
    return {}

def accounts_for_cycles(mapping: Dict[str, str], cycles: Iterable[str]) -> List[str]:
    want = set(map(str, (cycles or [])))
    return [code for code, cyc in (mapping or {}).items() if str(cyc) in want]

def accounts_for_cycles_ko(mapping: Dict[str, str], cycles_ko: Iterable[str]) -> List[str]:
    codes = [ko_to_code(x) for x in (cycles_ko or [])]
    return accounts_for_cycles(mapping, codes)

# -------------------------------------------------
# 2) ‘프리셋’(사이클 카테고리 정의) — 표준 + 사용자 오버라이드
# -------------------------------------------------
try:
    from config import STANDARD_ACCOUNTING_CYCLES, CYCLES_USER_OVERRIDES_PATH
except Exception:
    STANDARD_ACCOUNTING_CYCLES, CYCLES_USER_OVERRIDES_PATH = {}, ""

def _load_overrides() -> Dict[str, List[str]]:
    p = CYCLES_USER_OVERRIDES_PATH
    if not p or not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        return {str(k): [str(x) for x in v] for k, v in data.items() if isinstance(v, list)}
    except Exception:
        return {}

def get_cycles_preset() -> Dict[str, List[str]]:
    """
    '사이클 코드 → [키워드/샘플계정 등]' 프리셋 정의 반환.
    (⚠️ 매핑 아님. 인자 없음)
    """
    base = {str(k): list(v) for k, v in (STANDARD_ACCOUNTING_CYCLES or {}).items()}
    ov = _load_overrides()
    for k, v in ov.items():
        base[str(k)] = list(v)
    return base

# -------------------------------------------------
# 3) 룰 기반 초안 + (선택) LLM 보정 → 매핑 빌드
# -------------------------------------------------
def rule_based_guess(code_to_name: Dict[str, str]) -> Dict[str, str]:
    KW = {
        "CLOSE":  ["결산","조정","대손","충당금","평가손실","평가이익","외화환산"],
        "REV":    ["매출","용역수익","수익","외상매출","매출채권","Sales","Revenue"],
        "PUR":    ["매입","구매","원재료","용역비","지급수수료","운반비","광고","접대","임차","수수료","수도","통신"],
        "HR":     ["급여","임금","상여","퇴직","복리후생","연금","식대","경조","의료"],
        "TREASURY_INVEST": ["예금","CMA","금융상품","유가증권","투자","파생","이자수익","배당"],
        "TREASURY_FINANCE":["차입금","대출","사채","어음","이자비용","금융비용"],
        "TAX":    ["부가세","법인세","원천","지방세","가산세","세금과공과"],
        "PPE":    ["유형자산","건설중","기계","비품","차량","건물","토지","감가상각","처분손익"],
        "INTANG": ["무형자산","개발비","소프트웨어","상표권","영업권","상각"],
        "LEASE":  ["리스","사용권자산","리스부채","리스료"],
    }
    out: Dict[str, str] = {}
    for code, nm in (code_to_name or {}).items():
        name = str(nm or "")
        label = "OTHER"
        for cyc, kws in KW.items():
            if any(kw in name for kw in kws):
                label = cyc
                break
        out[str(code)] = label
    return out

def merge_cycles(base: Dict[str, str], override: Dict[str, str]) -> Dict[str, str]:
    out = dict(base)
    for k, v in (override or {}).items():
        if v:
            out[str(k)] = str(v)
    return out

def build_cycles_preset(upload_id: str, code_to_name: Dict[str, str], use_llm: bool = False) -> Dict[str, str]:
    """
    계정→사이클 매핑 초안 생성 후 저장. (룰베이스 + 선택적 LLM 보정)
    """
    base = rule_based_guess(code_to_name)
    if use_llm:
        try:
            from services.llm import suggest_cycles_for_accounts
            llm_map = suggest_cycles_for_accounts(code_to_name)
            base = merge_cycles(base, llm_map)
        except Exception:
            pass
    set_cycles_map(upload_id, base)
    return base

__all__ = [
    "ALLOWED_CYCLES","CYCLE_KO","code_to_ko","ko_to_code",
    "set_cycles_map","get_cycles_map","get_effective_cycles",
    "get_cycles_preset","accounts_for_cycles","accounts_for_cycles_ko",
    "rule_based_guess","merge_cycles","build_cycles_preset",
]



==============================
📄 FILE: services/external.py
==============================




==============================
📄 FILE: services/llm.py
==============================

"""
# services/llm.py
# - 하이브리드 클라이언트: 키가 있으면 OpenAI 온라인 모드, 없으면 오프라인 스텁
"""

import os
from functools import lru_cache


def openai_available() -> bool:
    try:
        # 부팅 시 env_loader가 LLM_AVAILABLE 플래그를 셋업
        from infra.env_loader import ensure_api_keys_loaded, is_llm_ready
        ensure_api_keys_loaded()
        return bool(is_llm_ready())
    except Exception:
        # 직접 환경변수 확인 (폴백)
        return bool(os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY"))


class _DummyEmbeddings:
    def create(self, *args, **kwargs):
        raise RuntimeError("Embeddings API not available in offline stub.")


class _DummyChat:
    class _Resp:
        class Choice:
            class Msg:
                content = ""
            message = Msg()
        choices = [Choice()]

    def completions(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")

    def completions_create(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")


class _DummyClient:
    embeddings = _DummyEmbeddings()
    chat = _DummyChat()


@lru_cache(maxsize=1)
def _openai_client():
    try:
        from openai import OpenAI  # type: ignore
    except Exception as e:
        return None
    # 키는 env_loader가 이미 주입함
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY")
    try:
        return OpenAI(api_key=api_key) if api_key else OpenAI()
    except Exception:
        return None


class LLMClient:
    def __init__(self, model: str | None = None, temperature: float | None = None, json_mode: bool | None = None):
        self._online = openai_available() and (_openai_client() is not None)
        self.client = _openai_client() if self._online else _DummyClient()
        # 호출 편의 파라미터 저장(online generate에서 사용)
        self.model = model or os.getenv("LLM_MODEL", "gpt-4o")
        try:
            self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.2" if temperature is None else str(temperature)))
        except Exception:
            self.temperature = 0.2
        self.json_mode = True if (os.getenv("LLM_JSON_MODE", "true").lower() in ("1","true","yes")) else False

    def generate(self, system: str, user: str, tools=None, *, model: str | None = None, max_tokens: int | None = None, force_json: bool | None = None) -> str:
        if not self._online or self.client is None:
            raise RuntimeError("LLM not available: no API key or client. Run in offline mode.")
        try:
            use_model = model or self.model
            kwargs = dict(
                model=use_model,
                temperature=float(self.temperature),
                messages=[{"role":"system","content":system},{"role":"user","content":user}],
            )
            if max_tokens is not None:
                kwargs["max_tokens"] = int(max_tokens)
            if tools:
                kwargs["tools"] = tools
                try:
                    first_tool = tools[0]["function"]["name"]
                    if first_tool:
                        kwargs["tool_choice"] = {"type": "function", "function": {"name": first_tool}}
                except Exception:
                    pass
            else:
                use_force_json = self.json_mode if force_json is None else bool(force_json)
                if use_force_json:
                    kwargs["response_format"] = {"type": "json_object"}

            try:
                resp = self.client.chat.completions.create(**kwargs, timeout=60)
            except TypeError:
                resp = self.client.chat.completions.create(**kwargs)
            msg = resp.choices[0].message
            try:
                tool_calls = getattr(msg, "tool_calls", None)
                if tool_calls:
                    call = tool_calls[0]
                    args = getattr(call.function, "arguments", None)
                    return (args or "").strip()
            except Exception:
                pass
            return (msg.content or "").strip()
        except Exception as e:
            raise

    def name_cluster(self, samples_desc: list[str], samples_vendor: list[str]) -> str | None:
        """거래 샘플을 기반으로 클러스터의 이름을 생성합니다."""
        import textwrap
        if not self._online or self.client is None:
            return None
        prompt = textwrap.dedent(f"""
        너는 회계 감사 보조 AI다. 아래 거래 샘플을 보고 이 그룹의 성격을 가장 잘 드러내는
        한국어 **클러스터 이름 1개만**, 10자 내외로 제시해라.
        숫자/기호/따옴표/접두사 금지. 예: 직원 급여, 통신비, 임원 복지.

        [거래 적요 샘플]
        - {'\n- '.join(samples_desc) if samples_desc else '(샘플 부족)'}

        [주요 거래처 샘플]
        - {'\n- '.join(samples_vendor) if samples_vendor else '(샘플 부족)'}

        정답:
        """).strip()
        try:
            resp = self.client.chat.completions.create(
                model=self.model, messages=[{"role":"user","content":prompt}], temperature=0
            )
            cand = (resp.choices[0].message.content or "").strip().splitlines()[0].strip(" \"'[]()")
            if not cand:
                return None
            bad = {"클러스터","unknown","이름없음","미정","기타"}
            if cand.lower() in bad or cand.startswith("클러스터"):
                return None
            return cand[:20]
        except Exception:
            return None


import json, re
from typing import Dict

LLM_ENABLED = True

CYCLE_DEFS = {
    "CLOSE":"결산·조정, 충당금/평가손익/환산 등 기말조정",
    "REV":"매출·수익 관련",
    "PUR":"매입·구매·비용(광고, 수수료, 임차료 등 포함)",
    "HR":"급여·상여·퇴직·복리후생",
    "TREASURY_INVEST":"예금·금융상품·투자·파생·이자/배당수익",
    "TREASURY_FINANCE":"차입·사채·어음·이자비용",
    "TAX":"부가세·법인세·원천 등 세무",
    "PPE":"유형자산·감가상각·건설중",
    "INTANG":"무형자산·상각",
    "LEASE":"리스·사용권자산·리스부채",
    "OTHER":"기타(정말 분류가 불가할 때만)",
}

def _normalize_cycle_label(label: str) -> str:
    t = str(label).strip().upper()
    aliases = {
        "결산": "CLOSE", "매출": "REV", "매입": "PUR", "매입·비용":"PUR", "비용":"PUR",
        "인사":"HR", "급여":"HR", "복리후생":"HR",
        "자금운용":"TREASURY_INVEST", "투자":"TREASURY_INVEST", "파생":"TREASURY_INVEST",
        "자금조달":"TREASURY_FINANCE", "차입":"TREASURY_FINANCE", "사채":"TREASURY_FINANCE",
        "세무":"TAX", "유형자산":"PPE", "무형자산":"INTANG", "리스":"LEASE",
        "기타":"OTHER"
    }
    return aliases.get(t, t if t in CYCLE_DEFS else "OTHER")

def suggest_cycles_for_accounts(account_names: Dict[str, str]) -> Dict[str, str]:
    if not LLM_ENABLED or not account_names:
        return {}
    try:
        client = LLMClient()
        if not client._online or client.client is None:
            return {}
        def short_defs():
            items = [f"- {k}: {v}" for k, v in CYCLE_DEFS.items() if k != "OTHER"]
            items.append("- OTHER: 위 어떤 범주에도 명확히 속하지 않을 때만")
            return "\n".join(items)
        examples = (
            '{"1000":"현금및현금성자산","1130":"매출채권","5110":"광고선전비","2210":"단기차입금","8100":"법인세비용"}'
        )
        expected = (
            '{"1000":"TREASURY_INVEST","1130":"REV","5110":"PUR","2210":"TREASURY_FINANCE","8100":"TAX"}'
        )
        prompt = (
            "아래 '허용 라벨' 중 하나로 각 계정코드를 분류하세요. 한 항목은 오직 하나의 라벨.\n"
            "'OTHER'는 최후수단입니다. 애매하면 가장 가까운 라벨을 고르세요.\n"
            "반드시 JSON(object)만 출력하세요. 추가 텍스트/설명 금지.\n\n"
            f"허용 라벨(정의):\n{short_defs()}\n\n"
            f"예시 입력:\n{examples}\n\n예시 출력(JSON만):\n{expected}\n\n"
            f"입력(JSON):\n{json.dumps(account_names, ensure_ascii=False)}"
        )
        raw = client.generate("You are an expert accountant.", prompt, force_json=True, max_tokens=2000)
        data = json.loads(raw)
        out: Dict[str, str] = {}
        if isinstance(data, dict):
            for code, lab in data.items():
                out[str(code)] = _normalize_cycle_label(lab)
        return out
    except Exception:
        return {}




==============================
📄 FILE: services/__init__.py
==============================






==============================
📄 FILE: tests/conftest.py
==============================

# tests/conftest.py
import os, sys
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)



==============================
📄 FILE: tests/test_anomaly.py
==============================

# Z-Score 및 위험 점수 단조성 테스트
import pandas as pd
from analysis.anomaly import calculate_grouped_stats_and_zscore, _risk_from

def test_zscore_and_risk_monotonic():
    df = pd.DataFrame({
        "계정코드": ["100"]*5 + ["200"]*5,
        "차변":     [0,0,0,0,0] + [0,0,0,0,0],
        "대변":     [10,20,30,40,50] + [5,5,5,5,5],
    })
    out = calculate_grouped_stats_and_zscore(df, target_accounts=["100","200"])
    assert "Z-Score" in out.columns
    a1 = _risk_from(1.0, amount=1_000, pm=100_000)[-1]
    a2 = _risk_from(3.0, amount=1_000, pm=100_000)[-1]
    b1 = _risk_from(1.0, amount= 50_000, pm=100_000)[-1]
    b2 = _risk_from(1.0, amount=150_000, pm=100_000)[-1]
    assert a2 > a1
    assert b2 > b1





==============================
📄 FILE: tests/test_architecture_imports.py
==============================

from pathlib import Path

FORBIDDEN = ("from services", "import services")


def test_analysis_not_import_services():
    bad = []
    for p in Path("analysis").rglob("*.py"):
        try:
            t = p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        if any(x in t for x in FORBIDDEN):
            bad.append(str(p))
    assert not bad, f"analysis must not import services: {bad}"





==============================
📄 FILE: tests/test_evidence_schema.py
==============================

from dataclasses import asdict
from analysis.anomaly import run_anomaly_module
from analysis.contracts import LedgerFrame
import pandas as pd


def test_anomaly_emits_evidence_minimal():
    # 간단 가짜 DF
    df = pd.DataFrame({
        "row_id":["a","b","c"],
        "회계일자": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03"]),
        "계정코드": ["400","400","400"],
        "계정명":   ["매출","매출","매출"],
        "차변": [0, 0, 0],
        "대변": [10_000_000, 100, 50],
    })
    lf = LedgerFrame(df=df, meta={})
    mod = run_anomaly_module(lf, target_accounts=["400"], topn=2, pm_value=500_000_000)
    assert isinstance(mod.evidences, list)
    assert len(mod.evidences) >= 1
    d = asdict(mod.evidences[0])
    for key in ["row_id","reason","anomaly_score","financial_impact","risk_score","is_key_item","impacted_assertions","links"]:
        assert key in d




==============================
📄 FILE: tests/test_kdmeans_basic.py
==============================

# 간단 단위테스트: KDMeans가 잘 학습되고 속성들이 채워지는지 확인
import numpy as np
from analysis.kdmeans_shim import HDBSCAN

def test_kdmeans_fixed_k():
    rng = np.random.default_rng(0)
    X = np.vstack([
        rng.normal(loc=[0,0], scale=0.1, size=(25,2)),
        rng.normal(loc=[3,3], scale=0.1, size=(25,2)),
    ])
    model = HDBSCAN(n_clusters=2, random_state=0).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ == 2
    assert set(model.labels_) == {0,1}
    assert np.allclose(model.probabilities_, 1.0)

def test_kdmeans_auto_k_runs():
    rng = np.random.default_rng(1)
    X = rng.normal(size=(200, 4))
    model = HDBSCAN(n_clusters=None, random_state=1).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ is not None




==============================
📄 FILE: tests/test_report_units.py
==============================

# 리포트 내 원(₩) 단위 강제 변환 테스트
from analysis.report import _enforce_won_units

def test_unit_enforcement():
    s = "총액은 3억 5,072만 원이며 이전에는 2억 원이었다."
    out = _enforce_won_units(s)
    assert "350,720,000원" in out
    assert "200,000,000원" in out





==============================
📄 FILE: tests/test_risk_boundaries.py
==============================

import math
import pytest
from analysis.anomaly import _risk_from


@pytest.mark.parametrize("z_abs", [0.0, 1.0, 3.0, 10.0])
@pytest.mark.parametrize("pm_value", [0.0, 1.0, 1e6, 1e12])
def test_risk_from_boundary_is_finite(z_abs, pm_value):
    a, f, k, score = _risk_from(z_abs=z_abs, amount=1_000_000, pm=pm_value)
    assert 0.0 <= score <= 1.0
    assert math.isfinite(score)


def test_risk_from_monotonic_in_z_when_pm_fixed():
    pm = 1e6
    s1 = _risk_from(0.5, amount=1_000_000, pm=pm)[-1]
    s2 = _risk_from(3.0, amount=1_000_000, pm=pm)[-1]
    s3 = _risk_from(10.0, amount=1_000_000, pm=pm)[-1]
    assert s1 <= s2 <= s3


@pytest.mark.parametrize("pm_lo,pm_hi", [(0.0, 1e9)])
def test_risk_from_non_decreasing_in_pm(pm_lo, pm_hi):
    z = 3.0
    slo = _risk_from(z, amount=1_000_000, pm=pm_lo)[-1]
    shi = _risk_from(z, amount=1_000_000, pm=pm_hi)[-1]
    assert slo <= shi




==============================
📄 FILE: tests/test_risk_score.py
==============================

import math
from analysis.anomaly import _risk_from
from config import Z_SIGMOID_SCALE


def test_risk_from_basic():
    a, f, k, score = _risk_from(z_abs=3.0, amount=1_000_000_000, pm=500_000_000)
    exp_a = 1.0 / (1.0 + math.exp(-(3.0/float(Z_SIGMOID_SCALE or 1.0))))
    assert abs(a - exp_a) < 1e-9
    assert f == 1.0                 # PM 대비 캡 1
    assert k == 1.0                 # KIT
    # 가중합: 0.5*a + 0.4*1 + 0.1*1
    expected = 0.5*a + 0.4 + 0.1
    assert abs(score - expected) < 1e-9


def test_risk_from_zero_pm_guard():
    a, f, k, score = _risk_from(z_abs=0.0, amount=0, pm=0)
    assert f == 0.0 and k == 0.0    # 분모 가드 동작
    assert 0.0 <= a <= 0.5          # z=0이면 a는 0.5 근처




==============================
📄 FILE: tests/test_risk_score_more.py
==============================

from analysis.anomaly import _risk_from
from analysis.anomaly import _assertions_for_row


def test_risk_from_none_and_negative_pm():
    for pm in (None, -1, -1000):
        a, f, k, score = _risk_from(z_abs=2.0, amount=1_000_000, pm=pm)
        assert f == 0.0 and k == 0.0
        assert 0.0 <= a <= 1.0
        assert 0.0 <= score <= 1.0

def test_assertions_mapping_rules():
    # 항상 A 포함
    assert "A" in _assertions_for_row(0.0)
    # 큰 양의 이탈 → E 포함
    assert set(_assertions_for_row(+2.5)) >= {"A","E"}
    # 큰 음의 이탈 → C 포함
    assert set(_assertions_for_row(-2.5)) >= {"A","C"}




==============================
📄 FILE: tests/test_snapshot_core.py
==============================

import pandas as pd
from dataclasses import asdict
from analysis.contracts import LedgerFrame
from analysis.anomaly import run_anomaly_module


def _mini_df():
    df = pd.DataFrame({
        "row_id": ["file|L:2","file|L:3","file|L:4","file|L:5"],
        "회계일자": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03","2024-01-04"]),
        "계정코드": ["101","101","201","201"],
        "계정명":   ["현금","현금","매출","매출"],
        "차변": [0, 0, 0, 0],
        "대변": [5_000_000, 100, 50_000_000, 200],
    })
    return df


def test_snapshot_evidence_and_matrix_stable():
    lf = LedgerFrame(df=_mini_df(), meta={})
    mod = run_anomaly_module(lf, target_accounts=["101","201"], topn=10, pm_value=500_000_000)
    # Evidence 스냅샷(핵심 필드만 비교)
    snap = [{
        "row_id": e.row_id,
        "risk_score": round(e.risk_score, 6),
        "is_key_item": e.is_key_item,
        "assertions": tuple(e.impacted_assertions),
        "acct": e.links.get("account_name") or e.links.get("account_code")
    } for e in mod.evidences]
    # 고정 기대값(리스크 가중치/PM이 바뀌면 실패하도록)
    assert any(s["acct"] == "매출" for s in snap)
    # (위험평가/매트릭스 임시 제거: 관련 단언 삭제)




==============================
📄 FILE: tests/test_timeseries_naive.py
==============================

# 시계열 모듈의 간단 백엔드(EMA 등) 동작성 테스트
import pandas as pd
import numpy as np
from analysis.timeseries import run_timeseries_module, model_registry, z_and_risk, run_timeseries_for_account


def test_timeseries_naive_backend():
    df = pd.DataFrame({
        "account": ["A"]*7 + ["B"]*7,
        "date":    list(range(1,8)) + list(range(1,8)),
        "amount":  [10,11,10,12,11,10, 20] + [8,8,8,8,8,8, 5],
    })
    res = run_timeseries_module(df, account_col="account", date_col="date",
                                amount_col="amount", backend="ema", window=3)
    assert set(res["account"]) == {"A","B"}
    assert set(res["assertion"]).issubset({"E","C"})


def test_z_and_risk_basic():
    # 한글: 0 중심 대칭 잔차에 대해 |z|가 크면 risk가 커진다
    resid = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])
    z, r = z_and_risk(resid)
    assert len(z) == len(resid)
    assert len(r) == len(resid)
    assert float(r[0]) > float(r[1])
    assert float(r[-1]) == float(r[0])


def test_model_registry_keys_present():
    reg = model_registry()
    for k in ["ma","ema","arima","prophet"]:
        assert k in reg
    assert reg["ma"] is True and reg["ema"] is True


def test_run_timeseries_for_account_dual():
    # 한글: 12개월 샘플로 flow 누적 balance 생성하여 두 트랙 모두 반환
    dates = pd.period_range("2024-01", periods=12, freq="M").to_timestamp()
    flow = np.linspace(100, 210, num=12)
    df = pd.DataFrame({"date": dates, "flow": flow})
    df["balance"] = df["flow"].cumsum()
    out = run_timeseries_for_account(df, account="매출채권", is_bs=True, flow_col="flow", balance_col="balance")
    assert set(out["measure"]).issuperset({"flow","balance"})
    assert set(out.columns) == {"date","account","measure","actual","predicted","error","z","risk","model"}





==============================
📄 FILE: tests/test_zbins.py
==============================

import numpy as np, pandas as pd
from analysis.anomaly import _z_bins_025_sigma


def test_zbins_label_and_count():
    s = pd.Series(np.linspace(-4, 4, 101))
    df, order = _z_bins_025_sigma(s)
    assert len(df) == len(order) == 26      # 테일 포함 26개
    assert int(df["건수"].sum()) == 101     # 총합 보존




==============================
📄 FILE: ui/inputs.py
==============================

import re
import streamlit as st

# KRW 입력 위젯(천단위 쉼표) - 안정형
# - 사용자는 자유롭게 타이핑(쉼표/공백/문자 섞여도 무시)하고,
#   포커스 아웃/엔터 시에만 정규화(숫자만 유지 → 쉼표 포맷)합니다.
# - 실제 숫자값은 session_state[key] (int)로 보관합니다.
# - 표시용 문자열은 session_state[f"{key}__txt"] 로 관리합니다.

def _parse_krw_text(s: str) -> int:
    """문자열에서 숫자만 추출하여 안전하게 int로 변환(음수 방어, 공란=0)."""
    if s is None:
        return 0
    s = str(s).replace(",", "").strip()
    s = re.sub(r"[^\d]", "", s)  # 숫자 이외 제거
    if s == "":
        return 0
    try:
        return max(0, int(s))
    except Exception:
        return 0

def _fmt_krw(n: int) -> str:
    """정수를 천단위 쉼표 문자열로 포맷."""
    try:
        return f"{int(n):,}"
    except Exception:
        return "0"

def krw_input(label: str, key: str, default_value: int = 0, help_text: str = "") -> int:
    """
    KRW 입력(천단위 쉼표) 통합 위젯.
    - 숫자 상태: st.session_state[key] (int)
    - 표시 상태: st.session_state[f"{key}__txt"] (str, '1,234,567')
    - on_change 시에만 정규화하여 잔고장(500,00 등) 방지
    """
    # 초기 상태 보정
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if f"{key}__txt" not in st.session_state:
        st.session_state[f"{key}__txt"] = _fmt_krw(st.session_state[key])

    def _commit():
        """사용자 입력 완료(포커스 아웃/엔터) 시 숫자/문자 상태 동기화."""
        raw = st.session_state.get(f"{key}__txt", "")
        val = _parse_krw_text(raw)
        st.session_state[key] = val
        st.session_state[f"{key}__txt"] = _fmt_krw(val)
        # Streamlit은 on_change 후 자동 rerun → 그래프/표 갱신에 충분

    # 표시 입력창(타이핑 중에는 포맷 강제하지 않음)
    st.text_input(
        label,
        key=f"{key}__txt",
        help=help_text,
        placeholder="예: 500,000,000",
        on_change=_commit,
    )
    return int(st.session_state.get(key, int(default_value)))





==============================
📄 FILE: ui/validation_helpers.py
==============================

from __future__ import annotations

import streamlit as st
import pandas as pd
import plotly.express as px

from typing import Optional
from analysis.timeseries import build_trend_validation_data
from utils.viz import add_period_guides, add_materiality_threshold


def render_trend_validation_section(
    monthly: pd.DataFrame,
    *,
    section_id: str,
    is_bs: bool,
    pm_value: Optional[float] = None,
    title: str = "데이터 검증: 월별 추세 분석(막대그래프로 직접 대조)",
):
    """
    - monthly: ['date','flow'] (+ 'balance' 선택)
    - section_id: 토글 고유키에 섞을 식별자(계정코드 등)
    - is_bs: BS 여부
    - pm_value: PM 표시(점선 보조선)
    """
    df_val = build_trend_validation_data(monthly, is_bs=is_bs)
    if df_val.empty:
        st.info("검증용 데이터가 없습니다.")
        return

    st.subheader(title)

    # 고유 key로 토글 충돌 방지
    show_cfg = st.toggle("이 차트의 통계 설정 보기", key=f"trend_cfg_{section_id}")

    # 막대: 실측, 선: 예측
    fig = px.bar(
        df_val,
        x="date",
        y="actual",
        color="measure",
        barmode="group",
        title="actual vs predicted (검증)",
    )
    # 동일 x축 위에 예측선 오버레이
    for m in df_val["measure"].unique():
        sub = df_val[df_val["measure"] == m]
        fig.add_scatter(
            x=sub["date"],
            y=sub["predicted"],
            mode="lines+markers",
            name=f"predicted ({m})",
        )

    add_period_guides(fig, df_val["date"])
    if pm_value:
        add_materiality_threshold(fig, pm_value)

    # 숫자 포맷
    fig.update_yaxes(separatethousands=True, tickformat=",.0f", showexponent="none", exponentformat="none")
    fig.update_traces(hovertemplate="month=%{x}<br>value=%{y:,.0f}<extra></extra>")

    st.plotly_chart(fig, use_container_width=True)

    if show_cfg:
        by_m = df_val.groupby("measure").agg(
            n_points=("date", "count"),
            last_actual=("actual", "last"),
            last_pred=("predicted", "last"),
        )
        st.dataframe(by_m, use_container_width=True)





==============================
📄 FILE: ui/__init__.py
==============================

# 패키지 초기화 (비어있어도 무방)




==============================
📄 FILE: utils/drilldown.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional


def ensure_rowid(df: pd.DataFrame, id_col: str = "row_id") -> pd.DataFrame:
    """row_id 컬럼이 없으면 생성하지 않고 그대로 반환(계약 준수는 상위 단계에서)."""
    return df if id_col in df.columns else df


def attach_customdata(df: pd.DataFrame, cols: Iterable[str], id_col: str = "row_id"):
    """
    Plotly에 올릴 customdata 배열 생성.
    반환: (df, customdata(ndarray), header_labels(list))
    """
    import numpy as np
    use_cols = [c for c in cols if c in df.columns]
    if id_col not in use_cols and id_col in df.columns:
        use_cols = [id_col] + use_cols
    arr = df[use_cols].to_numpy()
    return df, np.asarray(arr), use_cols


def fmt_money(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return str(x)





==============================
📄 FILE: utils/helpers.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional

def find_column_by_keyword(columns: Iterable[str], keyword: str) -> Optional[str]:
    """열 이름에서 keyword(부분일치, 대소문자 무시)를 우선 탐색."""
    keyword = str(keyword or "").lower()
    cols = [str(c) for c in columns]
    # 1) 완전 일치 우선
    for c in cols:
        if c.lower() == keyword:
            return c
    # 2) 부분 일치
    for c in cols:
        if keyword in c.lower():
            return c
    return None

def add_provenance_columns(df: pd.DataFrame) -> pd.DataFrame:
    """업로드 출처 정보가 없어도 row_id를 강제로 부여."""
    out = df.copy()
    if "row_id" not in out.columns:
        out["row_id"] = out.reset_index().index.astype(str)
    return out

def add_period_tag(df: pd.DataFrame) -> pd.DataFrame:
    """연도 최대값 기준으로 CY/PY/Other 태그를 부여."""
    out = df.copy()
    if "연도" not in out.columns:
        out["period_tag"] = "Other"
        return out
    y_max = out["연도"].max()
    out["period_tag"] = out["연도"].apply(lambda y: "CY" if y == y_max else ("PY" if y == y_max - 1 else "Other"))
    return out


# --- NEW: 차트 전용 대변계정 판정(그래프에서만 부호 반전) ---
def is_credit_account(account_type: str | None, dc: str | None = None) -> bool:
    """
    계정 성격이 대변(Credit)인지 판정합니다.
    - dc가 주어지면 우선 사용(예: '차변'/'대변' 또는 'D'/'C')
    - 아니면 account_type으로 간접 판정: 부채/자본/수익 → Credit
    """
    try:
        if dc is not None:
            s = str(dc).strip().upper()
            return s.startswith("C") or ("대변" in s)
    except Exception:
        pass
    try:
        t = str(account_type or "").strip()
        return t in {"부채", "자본", "수익"}
    except Exception:
        return False


# --- NEW: 모델 선택 이유 설명 텍스트 ---
def model_reason_text(name: str, d: dict) -> str:
    """
    간단한 규칙 기반으로 MoR 선택 사유를 자연어로 요약합니다.
    기대 키: cv_mape_rank, seasonality_strength(0~1), stationary(bool), recent_trend(bool), n_points(int)
    """
    try:
        why = []
        nm = str(name or "").lower()
        if d.get("cv_mape_rank") == 1:
            why.append("교차검증에서 가장 낮은 MAPE를 기록했습니다.")
        if float(d.get("seasonality_strength", 0.0)) > 0.4 and nm.startswith("prophet"):
            why.append("연/분기 수준의 계절성이 강하게 관측되었습니다.")
        if bool(d.get("stationary")) and nm.startswith("arima"):
            why.append("차분 후 정상성이 확보되어 ARIMA 적합이 유리했습니다.")
        if bool(d.get("recent_trend")) and (nm.startswith("ema") or nm.startswith("holt") or nm.startswith("exp")):
            why.append("최근 추세 변화가 커서 최근값 가중 모델이 더 잘 맞았습니다.")
        if int(d.get("n_points", 0)) < 18 and (nm.startswith("ma") or nm.startswith("ema")):
            why.append("관측치가 짧아 단순 이동평균 계열이 과적합 위험이 낮았습니다.")
        if not why:
            why.append("오차지표(MAE/MAPE)와 정보량(AIC/BIC)을 종합해 최적 모델로 선택되었습니다.")
        return " / ".join(why)
    except Exception:
        return "오차지표(MAE/MAPE)와 정보량(AIC/BIC)을 종합해 최적 모델로 선택되었습니다."


# --- NEW: 시계열용 날짜/금액 컬럼 자동 탐색 ---
def guess_time_and_amount_cols(df: pd.DataFrame):
    """시계열용 날짜/금액 컬럼을 유연하게 탐색한다."""
    date_candidates = ["회계일자", "전표일자", "거래일자", "일자", "date", "Date"]
    amt_candidates  = [
        "거래금액", "발생액", "금액", "금액(원)", "거래금액_절대값",
        "발생액_절대값", "순액", "순액(원)"
    ]
    cols = list(df.columns) if df is not None else []
    date_col = next((c for c in date_candidates if c in cols), None)
    amt_col  = next((c for c in amt_candidates  if c in cols), None)
    return date_col, amt_col


# --- NEW: 공용 add_or_replace (df.insert 대체) ---
def add_or_replace(df: pd.DataFrame, loc: int, col: str, values):
    """df.insert 대체: 이미 있으면 교체, 없으면 지정 위치에 추가."""
    import pandas as pd
    if col in df.columns:
        df[col] = values
        return df
    df.insert(loc, col, values)
    return df


==============================
📄 FILE: utils/monthly.py
==============================

import pandas as pd


def monthly_flow(df: pd.DataFrame, date_col: str, amt_col: str) -> pd.DataFrame:
    d = df[[date_col, amt_col]].copy()
    p = pd.to_datetime(d[date_col]).dt.to_period("M")
    g = d.assign(_p=p).groupby("_p", as_index=False)[amt_col].sum()
    g = g.rename(columns={"_p": "month"}).rename(columns={amt_col: "flow"})
    g["label"] = g["month"].astype(str)
    try:
        g["order"] = g["month"].astype(int)
    except Exception:
        # Fallback: use numeric YYYYMM from label
        g["order"] = g["label"].str.replace("-", "", regex=False).astype(int)
    return g[["month", "label", "order", "flow"]]


def monthly_balance_from_col(df: pd.DataFrame, date_col: str, bal_col: str) -> pd.DataFrame:
    d = df[[date_col, bal_col]].copy()
    p = pd.to_datetime(d[date_col]).dt.to_period("M")
    g = d.assign(_p=p).sort_values(date_col).groupby("_p", as_index=False).last()
    g = g.rename(columns={"_p": "month", bal_col: "balance"})
    g["label"] = g["month"].astype(str)
    try:
        g["order"] = g["month"].astype(int)
    except Exception:
        g["order"] = g["label"].str.replace("-", "", regex=False).astype(int)
    return g[["month", "label", "order", "balance"]]


def monthly_balance_from_flow(flow_df: pd.DataFrame, opening: float = 0.0) -> pd.DataFrame:
    g = flow_df[["month", "label", "order", "flow"]].copy()
    g["balance"] = float(opening) + g["flow"].astype(float).cumsum()
    return g[["month", "label", "order", "balance"]]





==============================
📄 FILE: utils/viz.py
==============================

# utils/viz.py
# 목적: Materiality(Performance Materiality, PM) 보조선/배지 추가 유틸
# - Plotly Figure에 빨간 점선(가로선) + "PM=xxx원" 라벨을 안전하게 추가
# - 현재 y축 범위에 PM이 없으면 축을 자동 확장해서 선이 보이게 함
# - Pareto(보조축 있음)에서도 1차 y축에 정확히 그려줌

from __future__ import annotations
from typing import Optional
import pandas as pd
import math


def _is_plotly_fig(fig) -> bool:
    try:
        # 지연 임포트 (환경에 plotly 미설치일 때도 함수 자체는 import 가능하도록)
        import plotly.graph_objects as go  # noqa: F401
        from plotly.graph_objs import Figure
        return isinstance(fig, Figure)
    except Exception:
        return False


def _get_primary_y_data_bounds(fig):
    """
    1차 y축 데이터의 (min, max) 추정.
    - secondary_y=True로 올라간 trace는 제외
    - trace.y가 수치 배열일 때만 집계
    """
    ymin, ymax = math.inf, -math.inf
    for tr in getattr(fig, "data", []):
        # 보조축 여부: trace.yaxis 가 'y2'/'y3'... 이면 보조축
        yaxis = getattr(tr, "yaxis", "y")
        if yaxis and str(yaxis).lower() != "y":  # 'y2' 등은 제외
            continue
        y = getattr(tr, "y", None)
        if y is None:
            continue
        try:
            for v in y:
                if v is None:
                    continue
                fv = float(v)
                if math.isfinite(fv):
                    ymin = min(ymin, fv)
                    ymax = max(ymax, fv)
        except Exception:
            # 숫자 배열이 아니면 스킵
            continue
    if ymin is math.inf:  # 데이터가 비어있는 경우
        return (0.0, 0.0)
    return (ymin, ymax)


def _ensure_y_contains(fig, y_value: float, pad_ratio: float = 0.05):
    """
    y_value가 y축 범위에 포함되도록 레이아웃을 조정.
    - 기존 auto-range라도 PM이 축 밖이면 강제로 range 부여
    - pad_ratio만큼 여유를 둬서 라벨이 잘리지 않게 함
    """
    if not math.isfinite(y_value):
        return
    # 현재 1차 y축 데이터 범위 추정
    ymin_data, ymax_data = _get_primary_y_data_bounds(fig)
    # 데이터가 전부 음수이거나 전부 양수일 수 있음 → PM이 더 큰 쪽에 있으면 확장
    base_min = min(0.0, ymin_data) if math.isfinite(ymin_data) else 0.0
    base_max = max(0.0, ymax_data) if math.isfinite(ymax_data) else 0.0
    tgt_min = min(base_min, y_value)
    tgt_max = max(base_max, y_value)
    if tgt_min == tgt_max:
        # 완전 평평하면 살짝 폭 추가
        span = abs(y_value) if y_value != 0 else 1.0
        tgt_min -= span * 0.5
        tgt_max += span * 0.5
    # 여유 패딩
    span = (tgt_max - tgt_min) or 1.0
    pad = span * float(pad_ratio)
    final_min = tgt_min - pad
    final_max = tgt_max + pad
    # yaxis는 레이아웃 키 'yaxis' (서브플롯 아닌 기본 도면 기준)
    if "yaxis" not in fig.layout:
        fig.update_layout(yaxis=dict(range=[final_min, final_max]))
    else:
        fig.layout.yaxis.update(range=[final_min, final_max])


def add_materiality_threshold(fig, pm_value: Optional[float], *, label: bool = True):
    """
    Plotly Figure에 PM 가로 점선 + 라벨 추가.
    - pm_value가 None/0/음수면 아무 것도 하지 않음
    - Pareto(보조축)도 1차 y축에 라인을 그림 (yref='y')
    - 축 범위를 자동 확장해서 항상 보이게 함
    반환: 동일 Figure (in-place 수정 후)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    # y축 범위에 PM이 포함되도록 먼저 보장
    _ensure_y_contains(fig, pm, pad_ratio=0.08)

    # 점선 라인 추가
    # xref='paper'로 0~1 전폭에 걸쳐 수평선, yref='y'로 1차 y축 기준 고정
    line_shape = dict(
        type="line",
        xref="paper", x0=0, x1=1,
        yref="y",     y0=pm, y1=pm,
        line=dict(color="red", width=2, dash="dot"),
        layer="above"
    )
    shapes = list(fig.layout.shapes) if getattr(fig.layout, "shapes", None) else []
    shapes.append(line_shape)
    fig.update_layout(shapes=shapes)

    # 라벨(오른쪽 끝)
    if label:
        annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
        annotations.append(dict(
            x=1.0, xref="paper",
            y=pm, yref="y",
            xanchor="left", yanchor="bottom",
            text=f"PM {pm:,.0f}원",
            showarrow=False,
            font=dict(color="red", size=11),
            bgcolor="rgba(255,255,255,0.6)",
            bordercolor="red",
            borderwidth=0.5,
            align="left"
        ))
        fig.update_layout(annotations=annotations)

    return fig


def add_pm_badge(fig, pm_value: Optional[float], *, text: str | None = None):
    """
    Heatmap처럼 선을 긋기 애매한 그래프에 우측 상단 배지 추가.
    반환: 동일 Figure (in-place)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    label = text or f"PM {pm:,.0f}원"
    annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
    annotations.append(dict(
        x=0.995, xref="paper",
        y=0.995, yref="paper",
        xanchor="right", yanchor="top",
        text=label,
        showarrow=False,
        font=dict(color="red", size=11),
        bgcolor="rgba(255,255,255,0.6)",
        bordercolor="red",
        borderwidth=0.5,
        align="right"
    ))
    fig.update_layout(annotations=annotations)
    return fig



def add_time_dividers(fig, xdates, show_quarter: bool = True, show_year_label: bool = True):
    """
    xdates: datetime Series/List (차트의 x 값)
    - 연도 경계(1/1): 굵은 실선
    - 분기 경계(4/1, 7/1, 10/1): 얇은 점선
    """
    if xdates is None:
        return fig
    try:
        ts = pd.to_datetime(pd.Series(xdates)).dropna().sort_values()
    except Exception:
        return fig
    if ts.empty:
        return fig

    # 연도 경계 (첫 해는 제외, 다음 해 1/1 지점)
    years = ts.dt.year.unique()
    for y in years[1:]:
        x = pd.Timestamp(year=y, month=1, day=1)
        if x < ts.iloc[0] or x > ts.iloc[-1]:
            continue
        try:
            fig.add_vline(
                x=x, line_width=2, line_dash="solid",
                line_color="rgba(0,0,0,0.35)",
                annotation_text=(str(y) if show_year_label else None),
                annotation_position="top", annotation_font_color="rgba(0,0,0,0.55)"
            )
        except Exception:
            continue

    # 분기 경계: 4/1, 7/1, 10/1
    if show_quarter:
        start = pd.Timestamp(ts.iloc[0].year, ts.iloc[0].month, 1)
        end   = pd.Timestamp(ts.iloc[-1].year, ts.iloc[-1].month, 1)
        for x in pd.date_range(start, end, freq="MS"):
            if x.month in (4, 7, 10):
                try:
                    fig.add_vline(
                        x=x, line_width=1, line_dash="dot",
                        line_color="rgba(0,0,0,0.18)"
                    )
                except Exception:
                    continue
    return fig


def add_period_guides(fig, x_series):
    """
    월 단위 시계열에 연/분기 경계선 추가.
    - 연말(12월): 굵은 점선(검정)
    - 분기말(3/6/9/12월): 얇은 점선(회색)
    """
    import pandas as _pd
    try:
        xs = _pd.to_datetime(x_series)
    except Exception:
        try:
            xs = _pd.to_datetime(_pd.Index(x_series))
        except Exception:
            return fig
    if xs is None or len(xs) == 0:
        return fig
    x_min, x_max = xs.min(), xs.max()
    if _pd.isna(x_min) or _pd.isna(x_max):
        return fig
    months = _pd.date_range(x_min, x_max, freq="M")

    # 연말: 12월(굵은 선)
    year_ends = [m for m in months if m.month == 12]
    for x in year_ends:
        try:
            fig.add_vline(x=x, line=dict(width=2, dash="dash"), line_color="black")
        except Exception:
            continue

    # 분기말: 3/6/9/12월(얇은 점선)
    quarter_ends = [m for m in months if m.month in (3, 6, 9, 12)]
    for x in quarter_ends:
        try:
            fig.add_vline(x=x, line=dict(width=1, dash="dot"), line_color="gray")
        except Exception:
            continue
    return fig




==============================
📄 FILE: utils/__init__.py
==============================




==============================
📄 FILE: viz/guides.py
==============================

# Materiality 가이드라인(붉은 점선) 유틸
# - matplotlib/plotly 모두 지원. 사용하는 라이브러리에 맞춰 호출만 붙이면 됨.

def add_materiality_lines_matplotlib(ax, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # y축 기준 수평선
    if y_threshold is not None and ax is not None:
        ax.axhline(y_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)  # 붉은 점선
        ax.text(ax.get_xlim()[0], y_threshold, f"{label_prefix}: {y_threshold:,.0f}",
                va="bottom", ha="left", fontsize=9, color="red", alpha=0.9)
    # x축 기준 수직선
    if x_threshold is not None and ax is not None:
        ax.axvline(x_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)
        ax.text(x_threshold, ax.get_ylim()[1], f"{label_prefix}: {x_threshold:,.0f}",
                va="top", ha="right", fontsize=9, color="red", alpha=0.9)


def add_materiality_lines_plotly(fig, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # Plotly Figure에 가이드라인 추가
    if fig is None:
        return fig
    if y_threshold is not None:
        try:
            fig.add_hline(y=y_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(xref="paper", x=0.0, y=y_threshold, yref="y",
                               text=f"{label_prefix}: {y_threshold:,.0f}",
                               showarrow=False, align="left", yanchor="bottom", font=dict(color="red", size=10))
        except Exception:
            pass
    if x_threshold is not None:
        try:
            fig.add_vline(x=x_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(yref="paper", y=1.0, x=x_threshold, xref="x",
                               text=f"{label_prefix}: {x_threshold:,.0f}",
                               showarrow=False, align="right", xanchor="right", font=dict(color="red", size=10))
        except Exception:
            pass
    return fig




