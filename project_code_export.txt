
==============================
ğŸ“„ FILE: app.py
==============================

# app_v0.17.py (ê±°ë˜ì²˜ ìƒì„¸ ë¶„ì„ ì˜¤ë¥˜ ìˆ˜ì •)
# --- BEGIN: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---
try:
    from infra.env_loader import boot as _llm_boot
    _llm_boot()  # í‚¤ ë¡œë“œ + ìƒíƒœ ë¡œê·¸
except Exception as _e:
    # ìµœì•…ì˜ ê²½ìš°ì—ë„ ì•±ì€ ëœ¨ê²Œ í•˜ê³ , ìƒíƒœë¥¼ stderrë¡œë§Œ ì•Œë¦¼
    import sys
    print(f"[env_loader] ì´ˆê¸°í™” ì‹¤íŒ¨: {_e}", file=sys.stderr)
# --- END: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
from utils.helpers import find_column_by_keyword, add_provenance_columns, add_period_tag
from analysis.integrity import analyze_reconciliation
from analysis.contracts import LedgerFrame, ModuleResult
from analysis.trend import create_monthly_trend_figure, run_trend_module
from analysis.anomaly import run_anomaly_module, compute_amount_columns
from analysis.correlation import run_correlation_module
from analysis.vendor import (
    create_pareto_figure,
    create_vendor_heatmap,
    create_vendor_detail_figure,
    run_vendor_module,
)
from analysis.report import generate_rag_context, run_final_analysis, build_methodology_note
from analysis.embedding import (
    ensure_rich_embedding_text,
    perform_embedding_and_clustering,
    perform_embedding_only,
)
from analysis.anomaly import calculate_grouped_stats_and_zscore
from services.llm import LLMClient
from config import EMB_USE_LARGE_DEFAULT, HDBSCAN_RESCUE_TAU
try:
    from config import PM_DEFAULT, PROVISIONAL_RULE_NAME, provisional_risk_formula_str
except Exception:
    PM_DEFAULT = 500_000_000
from utils.viz import add_materiality_threshold, add_pm_badge
from analysis.assertion_risk import build_matrix

# --- KRW ì…ë ¥(ì²œë‹¨ìœ„ ì½¤ë§ˆ) ìœ í‹¸: ì½œë°± ê¸°ë°˜ìœ¼ë¡œ ì•ˆì •í™” ---
def _krw_input(label: str, key: str, *, default_value: int, help_text: str = "") -> int:
    """
    í•œêµ­ ì›í™” ì…ë ¥ ìœ„ì ¯(ì²œë‹¨ìœ„ ì½¤ë§ˆ). í•µì‹¬ ê·œì¹™:
    1) ìœ„ì ¯ í‚¤(pm_value__txt ë“±)ë¥¼ ëŸ° ë£¨í”„ì—ì„œ ì§ì ‘ ëŒ€ì…í•˜ì§€ ì•ŠëŠ”ë‹¤.
    2) ì½¤ë§ˆ ì¬í¬ë§·ì€ on_change ì½œë°± ì•ˆì—ì„œë§Œ ìˆ˜í–‰í•œë‹¤.
    3) ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ê°’ì€ st.session_state[key]ì— ë³´ê´€í•œë‹¤.
    """
    txt_key = f"{key}__txt"  # ì‹¤ì œ text_input ìœ„ì ¯ì´ ë°”ì¸ë”©ë˜ëŠ” í‚¤

    # ì´ˆê¸° ì…‹ì—…: ìˆ«ì/ë¬¸ì ìƒíƒœë¥¼ ìœ„ì ¯ ìƒì„± ì „ì— ì¤€ë¹„
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if txt_key not in st.session_state:
        st.session_state[txt_key] = f"{int(st.session_state[key]):,}"

    # ì½œë°±: í¬ì»¤ìŠ¤ ì•„ì›ƒ/Enter ì‹œ ì½¤ë§ˆ í¬ë§·ì„ ì ìš©í•˜ê³  ìˆ«ì ìƒíƒœë¥¼ ë™ê¸°í™”
    def _on_blur_format():
        raw_now = st.session_state.get(txt_key, "")
        digits = re.sub(r"[^\d]", "", str(raw_now or ""))
        val = int(digits) if digits else 0
        if val < 0:
            val = 0
        st.session_state[key] = int(val)            # ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ìƒíƒœ
        st.session_state[txt_key] = f"{int(val):,}"  # ìœ„ì ¯ í‘œì‹œ í…ìŠ¤íŠ¸(ì½¤ë§ˆ)

    # ìœ„ì ¯ ìƒì„±
    raw = st.text_input(
        label,
        value=st.session_state[txt_key],
        key=txt_key,
        help=help_text,
        placeholder="ì˜ˆ: 500,000,000",
        on_change=_on_blur_format,
    )

    # ë¼ì´ë¸Œ íƒ€ì´í•‘ ë™ì•ˆì—ë„ ê·¸ë˜í”„ê°€ ì¦‰ì‹œ ë°˜ì˜ë˜ë„ë¡ ì •ìˆ˜ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸(ìœ„ì ¯ í‚¤ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    digits_live = re.sub(r"[^\d]", "", str(raw or ""))
    live_val = int(digits_live) if digits_live else 0
    if live_val < 0:
        live_val = 0
    st.session_state[key] = int(live_val)

    return int(st.session_state[key])


# --- 3. UI ë¶€ë¶„ ---
st.set_page_config(page_title="AI ë¶„ì„ ì‹œìŠ¤í…œ v0.18", layout="wide")
st.title("AI ë¶„ì„ ì‹œìŠ¤í…œ v0.18: ìµœì¢… ê°œì„  ğŸ—ï¸")
st.markdown("---")

for key in ['mapping_confirmed', 'analysis_done']:
    if key not in st.session_state:
        st.session_state[key] = False


# (removed) number_input ê¸°ë°˜ ëŒ€ì²´ êµ¬í˜„: ì‰¼í‘œ ë¯¸í‘œì‹œÂ·í‚¤ ì¶©ëŒ ìœ ë°œ ê°€ëŠ¥ì„± â†’ ë‹¨ì¼ êµ¬í˜„ìœ¼ë¡œ í†µì¼

with st.sidebar:
    st.header("1. ë°ì´í„° ì¤€ë¹„")
    uploaded_file = st.file_uploader("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.", type=["xlsx", "xlsm"])
    if 'last_file' not in st.session_state or st.session_state.last_file != uploaded_file:
        st.session_state.mapping_confirmed = False
        st.session_state.analysis_done = False
        st.session_state.last_file = uploaded_file

    st.markdown("---")
    st.header("2. ë¶„ì„ ê¸°ê°„")
    default_scope = st.session_state.get("period_scope", "ë‹¹ê¸°")
    st.session_state.period_scope = st.radio(
        "ë¶„ì„ ìŠ¤ì½”í”„(íŠ¸ë Œë“œ ì œì™¸):",
        options=["ë‹¹ê¸°", "ë‹¹ê¸°+ì „ê¸°"],
        index=["ë‹¹ê¸°","ë‹¹ê¸°+ì „ê¸°"].index(default_scope),
        horizontal=True,
        help="ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ ëª¨ë“ˆì— ì ìš©ë©ë‹ˆë‹¤. íŠ¸ë Œë“œëŠ” ì„¤ê³„ìƒ CY vs PY ë¹„êµ ìœ ì§€."
    )
    st.markdown("---")
    st.header("3. Embedding / Clustering")
    st.session_state.use_large_embedding = st.toggle(
        "Use Large Embedding (cost â†‘)",
        value=st.session_state.get("use_large_embedding", EMB_USE_LARGE_DEFAULT),
        help="Large model improves semantics but is slower and more expensive."
    )
    st.session_state.rescue_tau = st.slider(
        "Noise rescue Ï„ (cosine)",
        min_value=0.60, max_value=0.90, step=0.01,
        value=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
        help="Reassign -1 (noise) to nearest cluster if similarity â‰¥ Ï„."
    )
    st.markdown("---")
    st.header("4. Materiality")
    pm_val = _krw_input(
        "Performance Materiality (KRW)",
        key="pm_value",
        default_value=PM_DEFAULT,
        help_text="Used for KIT (PM exceed) and integrated risk scoring."
    )
    st.caption("â“˜ The PM threshold is drawn as a red dotted line on applicable charts. "
               "Y-axis scaling may change to accommodate this line.")

    # ğŸ§¹ ìºì‹œ ê´€ë¦¬
    with st.expander("ğŸ§¹ ìºì‹œ ê´€ë¦¬", expanded=False):
        if st.button("ì„ë² ë”© ìºì‹œ ë¹„ìš°ê¸°"):
            import shutil
            from services.cache import _model_dir
            for m in ["text-embedding-3-small", "text-embedding-3-large"]:
                try:
                    shutil.rmtree(_model_dir(m), ignore_errors=True)
                except Exception as e:
                    st.warning(f"{m} ì‚­ì œ ì‹¤íŒ¨: {e}")
            st.success("ì„ë² ë”© ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ë°ì´í„° ìºì‹œ ë¹„ìš°ê¸°"):
            st.cache_data.clear()
            st.success("Streamlit ë°ì´í„° ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ìºì‹œ ì •ë³´ ë³´ê¸°"):
            from services.cache import get_cache_info
            try:
                st.write(get_cache_info("text-embedding-3-small"))
                st.write(get_cache_info("text-embedding-3-large"))
            except Exception as e:
                st.info(f"ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")


@st.cache_data(show_spinner=False)
def _read_excel(_file, sheet_name=None):
    return pd.read_excel(_file, sheet_name=sheet_name)


@st.cache_data(show_spinner=False)
def _read_xls(_file):
    # pickle ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ë§Œ ìºì‹œ â†’ ì‹œíŠ¸ëª… ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    return pd.ExcelFile(_file).sheet_names

# (removed duplicated definition) _krw_input â€” ìœ„ì˜ ë‹¨ì¼ ë²„ì „ë§Œ ìœ ì§€

def _apply_scope(df: pd.DataFrame, scope: str) -> pd.DataFrame:
    """ìŠ¤ì½”í”„ ì ìš© ì‹œ ê²°ì¸¡ ì»¬ëŸ¼ ë°©ì–´: 'period_tag' ë¯¸ì¡´ì¬ë©´ ì›ë³¸ ë°˜í™˜.
    df.get('period_tag','')ê°€ ë¬¸ìì—´ì„ ë°˜í™˜í•  ê²½ìš° .eq í˜¸ì¶œ AttributeErrorë¥¼ ë°©ì§€í•œë‹¤.
    """
    if df is None or df.empty or 'period_tag' not in df.columns:
        return df
    if scope == "ë‹¹ê¸°":
        return df[df['period_tag'].eq('CY')]
    if scope == "ë‹¹ê¸°+ì „ê¸°":
        return df[df['period_tag'].isin(['CY', 'PY'])]
    return df

def _lf_by_scope() -> LedgerFrame:
    """ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì½”í”„ ì ìš© LedgerFrame."""
    hist = st.session_state.get('lf_hist')
    scope = st.session_state.get('period_scope', 'ë‹¹ê¸°')
    if hist is None:
        return None
    return LedgerFrame(df=_apply_scope(hist.df, scope), meta=hist.meta)

# (removed) êµ¬ë²„ì „ í…ìŠ¤íŠ¸ì…ë ¥ + Â±step / âœ–reset ë³€í˜•ë“¤ â€” ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ë²„íŠ¼ë¥˜ ì‚­ì œ ë° ë‹¨ì¼í™”


if uploaded_file is not None:
    if not st.session_state.mapping_confirmed:
        # ... ì»¬ëŸ¼ ë§¤í•‘ UI ...
        try:
            st.info("2ë‹¨ê³„: ì—‘ì…€ì˜ ì»¬ëŸ¼ì„ ë¶„ì„ í‘œì¤€ í•„ë“œì— ë§ê²Œ ì§€ì •í•´ì£¼ì„¸ìš”.")
            sheet_names = _read_xls(uploaded_file)
            first_ledger_sheet = next((s for s in sheet_names if 'ledger' in s.lower()), None)
            if first_ledger_sheet is None:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            ledger_cols = _read_excel(uploaded_file, sheet_name=first_ledger_sheet).columns.tolist()
            ledger_map = {}
            st.markdown("#### **Ledger ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            cols = st.columns(6)
            ledger_fields = {'íšŒê³„ì¼ì': 'ì¼ì', 'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê±°ë˜ì²˜': 'ê±°ë˜ì²˜', 'ì ìš”': 'ì ìš”', 'ì°¨ë³€': 'ì°¨ë³€', 'ëŒ€ë³€': 'ëŒ€ë³€'}
            for i, (key, keyword) in enumerate(ledger_fields.items()):
                with cols[i]:
                    is_optional = key == 'ê±°ë˜ì²˜'
                    default_col = find_column_by_keyword(ledger_cols, keyword)
                    options = ['ì„ íƒ ì•ˆ í•¨'] + ledger_cols if is_optional else ledger_cols
                    default_index = options.index(default_col) if default_col in options else 0
                    ledger_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", options, index=default_index, key=f"map_ledger_{key}")
            st.markdown("---")
            st.markdown("#### **Master ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            master_cols = _read_excel(uploaded_file, sheet_name='Master').columns.tolist()
            master_map = {}
            cols = st.columns(7)
            master_fields = {'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…': 'ê³„ì •ëª…', 'BS/PL': 'BS/PL', 'ì°¨ë³€/ëŒ€ë³€': 'ì°¨ë³€/ëŒ€ë³€', 'ë‹¹ê¸°ë§ì”ì•¡': 'ë‹¹ê¸°ë§', 'ì „ê¸°ë§ì”ì•¡': 'ì „ê¸°ë§', 'ì „ì „ê¸°ë§ì”ì•¡': 'ì „ì „ê¸°ë§'}
            for i, (key, keyword) in enumerate(master_fields.items()):
                with cols[i]:
                    default_col = find_column_by_keyword(master_cols, keyword)
                    default_index = master_cols.index(default_col) if default_col in master_cols else 0
                    master_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", master_cols, index=default_index, key=f"map_master_{key}")
            if st.button("âœ… ë§¤í•‘ í™•ì¸ ë° ë°ì´í„° ì²˜ë¦¬", type="primary"):
                st.session_state.ledger_map = ledger_map
                st.session_state.master_map = master_map
                st.session_state.mapping_confirmed = True
                st.rerun()
        except Exception as e:
            st.error(f"ì—‘ì…€ íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    else:  # ë§¤í•‘ í™•ì¸ í›„
        try:
            ledger_map, master_map = st.session_state.ledger_map, st.session_state.master_map
            master_df = _read_excel(uploaded_file, sheet_name='Master')
            sheet_names = _read_xls(uploaded_file)
            ledger_sheets = [s for s in sheet_names if 'ledger' in s.lower()]
            all_parts = []
            for s in ledger_sheets:
                part = _read_excel(uploaded_file, sheet_name=s)
                part['source_sheet'] = s
                part = add_provenance_columns(part)
                all_parts.append(part)
            ledger_df = pd.concat(all_parts, ignore_index=True)
            # row_id: íŒŒì¼ëª…|ì‹œíŠ¸:í–‰  (ì„¸ì…˜/ì¬ì‹¤í–‰ì—ë„ ì•ˆì •)
            try:
                base = Path(getattr(uploaded_file, "name", "uploaded.xlsx")).stem
                if 'row_id' in ledger_df.columns:
                    ledger_df['row_id'] = base + "|" + ledger_df['row_id'].astype(str)
            except Exception:
                pass
            ledger_df.rename(columns={v: k for k, v in ledger_map.items() if v != 'ì„ íƒ ì•ˆ í•¨'}, inplace=True)
            master_df.rename(columns={v: k for k, v in master_map.items()}, inplace=True)

            # ğŸ”§ ë³‘í•© ì „ì— íƒ€ì…/í¬ë§·ì„ ë¨¼ì € í†µì¼
            for df_ in [ledger_df, master_df]:
                if 'ê³„ì •ì½”ë“œ' in df_.columns:
                    df_['ê³„ì •ì½”ë“œ'] = (
                        df_['ê³„ì •ì½”ë“œ']
                        .astype(str)
                        .str.replace(r'\.0$', '', regex=True)
                        .str.strip()
                    )

            master_essentials = master_df[['ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…']].drop_duplicates()
            ledger_df = pd.merge(ledger_df, master_essentials, on='ê³„ì •ì½”ë“œ', how='left')
            ledger_df['ê³„ì •ëª…'] = ledger_df['ê³„ì •ëª…'].fillna('ë¯¸ì§€ì • ê³„ì •')

            ledger_df['íšŒê³„ì¼ì'] = pd.to_datetime(ledger_df['íšŒê³„ì¼ì'], errors='coerce')
            ledger_df.dropna(subset=['íšŒê³„ì¼ì'], inplace=True)
            for col in ['ì°¨ë³€', 'ëŒ€ë³€']:
                ledger_df[col] = pd.to_numeric(ledger_df[col], errors='coerce').fillna(0)
            for col in ['ë‹¹ê¸°ë§ì”ì•¡', 'ì „ê¸°ë§ì”ì•¡', 'ì „ì „ê¸°ë§ì”ì•¡']:
                if col in master_df.columns:
                    master_df[col] = pd.to_numeric(master_df[col], errors='coerce').fillna(0)
                else:
                    master_df[col] = 0
            ledger_df['ê±°ë˜ê¸ˆì•¡'] = ledger_df['ì°¨ë³€'] - ledger_df['ëŒ€ë³€']
            ledger_df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] = abs(ledger_df['ê±°ë˜ê¸ˆì•¡'])
            ledger_df['ì—°ë„'] = ledger_df['íšŒê³„ì¼ì'].dt.year
            ledger_df['ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.month
            # âœ… ë¶„ì„ ê·œì¹™: ê³„ì • ì„œë¸Œì…‹ ë¶„ì„ ì‹œì—ë„ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•œ í¸ì˜ íŒŒìƒ
            ledger_df['ì—°ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
            # âœ… period_tag ì¶”ê°€(CY/PY/Other)
            ledger_df = add_period_tag(ledger_df)
            if 'ê±°ë˜ì²˜' not in ledger_df.columns:
                ledger_df['ê±°ë˜ì²˜'] = 'ì •ë³´ ì—†ìŒ'
            ledger_df['ê±°ë˜ì²˜'] = ledger_df['ê±°ë˜ì²˜'].fillna('ì •ë³´ ì—†ìŒ').astype(str)

            if st.button("ğŸš€ ì „ì²´ ë¶„ì„ ì‹¤í–‰", type="primary"):
                with st.spinner('ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                    # âœ… ì •í•©ì„±ì€ ì‚¬ìš©ì ê¸°ê°„ ì„ íƒê³¼ ë¬´ê´€í•˜ê²Œ ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
                    st.session_state.recon_status, st.session_state.recon_df = analyze_reconciliation(ledger_df, master_df)
                    # âœ… í‘œì¤€ LedgerFrame êµ¬ì„±(ì •í•©ì„±ì€ í•­ìƒ ì „ì²´ ê¸°ì¤€: DF_hist)
                    lf_hist = LedgerFrame(df=ledger_df, meta={
                        "file_name": getattr(uploaded_file, "name", "uploaded.xlsx"),
                        "master_df": master_df,
                    })
                    # ì´ˆê¸°ì—” focus=hist (í›„ì† ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì í•„í„° ì—°ê²°)
                    lf_focus = lf_hist

                    st.session_state.master_df = master_df
                    st.session_state.ledger_df = ledger_df
                    st.session_state.lf_hist = lf_hist
                    st.session_state.lf_focus = lf_focus
                    st.session_state.analysis_done = True
                st.rerun()

            if st.session_state.analysis_done:
                st.success("âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                with st.expander("ğŸ” ë¹ ë¥¸ ì§„ë‹¨(ë°ì´í„° í’ˆì§ˆ ì²´í¬)", expanded=False):
                    df = st.session_state.ledger_df.copy()
                    issues = []

                    invalid_date = int(df['íšŒê³„ì¼ì'].isna().sum())
                    if invalid_date > 0:
                        issues.append(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ(NaT): {invalid_date:,}ê±´")

                    if 'ê±°ë˜ì²˜' in df.columns:
                        missing_vendor = int((df['ê±°ë˜ì²˜'].isna() | (df['ê±°ë˜ì²˜'] == 'ì •ë³´ ì—†ìŒ')).sum())
                        if missing_vendor > 0:
                            issues.append(f"â„¹ï¸ ê±°ë˜ì²˜ ì •ë³´ ì—†ìŒ/ê²°ì¸¡: {missing_vendor:,}ê±´")

                    zero_abs = int((df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] == 0).sum())
                    issues.append(f"â„¹ï¸ ê¸ˆì•¡ ì ˆëŒ€ê°’ì´ 0ì¸ ì „í‘œ: {zero_abs:,}ê±´")

                    unlinked = int(df['ê³„ì •ëª…'].eq('ë¯¸ì§€ì • ê³„ì •').sum())
                    if unlinked > 0:
                        issues.append(f"â— Masterì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì „í‘œ(ê³„ì •ëª… ë¯¸ì§€ì •): {unlinked:,}ê±´")

                    st.write("**ì²´í¬ ê²°ê³¼**")
                    if issues:
                        for line in issues:
                            st.write("- " + line)
                    else:
                        st.success("ë¬¸ì œ ì—†ì´ ê¹”ë”í•©ë‹ˆë‹¤!")
                tab1, tab2, tab3, tab4, tab_ts, tab5, tab6 = st.tabs(["ğŸ“ˆ ëŒ€ì‹œë³´ë“œ", "ğŸŒŠ ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„", "ğŸ¢ ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„", "ğŸ”¬ ì´ìƒ íŒ¨í„´ íƒì§€", "ğŸ“‰ ì‹œê³„ì—´/ì˜ˆì¸¡", "âš ï¸ ìœ„í—˜ í‰ê°€", "ğŸ¤– AI ë¦¬í¬íŠ¸"])

                with tab1:  # ...
                    st.header("í•µì‹¬ ìš”ì•½ ëŒ€ì‹œë³´ë“œ")
                    recon_status, ledger_df_res = st.session_state.recon_status, st.session_state.ledger_df
                    st.subheader("ë°ì´í„° í˜„í™©")
                    kpi_cols = st.columns(3)
                    kpi_cols[0].metric("ì´ ê±°ë˜ ê±´ìˆ˜", f"{len(ledger_df_res):,} ê±´")
                    kpi_cols[1].metric("ì´ ê±°ë˜ ê¸ˆì•¡ (ì ˆëŒ€ê°’)", f"{st.session_state.ledger_df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'].sum():,.0f} ì›")
                    kpi_cols[2].metric("ë¶„ì„ ëŒ€ìƒ ê³„ì • ìˆ˜", f"{ledger_df_res['ê³„ì •ì½”ë“œ'].nunique()} ê°œ")
                    st.subheader("ë°ì´í„° ë¬´ê²°ì„±")
                    if recon_status == 'Pass':
                        st.success("âœ… ë°ì´í„° ì •í•©ì„±: ëª¨ë“  ê³„ì • ì¼ì¹˜")
                    elif recon_status == 'Warning':
                        st.warning("âš ï¸ ë°ì´í„° ì •í•©ì„±: ì¼ë¶€ ê³„ì •ì—ì„œ ì‚¬ì†Œí•œ ì°¨ì´ ë°œê²¬")
                    else:
                        st.error("ğŸš¨ ë°ì´í„° ì •í•©ì„±: ì¼ë¶€ ê³„ì •ì—ì„œ ì¤‘ëŒ€í•œ ì°¨ì´ ë°œê²¬")
                with tab2:  # ...
                    st.header("ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    st.subheader("1. ë°ì´í„° ì •í•©ì„± ê²€ì¦ ê²°ê³¼")
                    status, result_df = st.session_state.recon_status, st.session_state.recon_df
                    if status == "Pass":
                        st.success("âœ… ëª¨ë“  ê³„ì •ì˜ ë°ì´í„°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤.")
                    elif status == "Warning":
                        st.warning("âš ï¸ ì¼ë¶€ ê³„ì •ì—ì„œ ì‚¬ì†Œí•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.error("ğŸš¨ ì¼ë¶€ ê³„ì •ì—ì„œ ì¤‘ëŒ€í•œ ì°¨ì´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    def highlight_status(row):
                        if row.ìƒíƒœ == 'Fail':
                            return ['background-color: #ffcccc'] * len(row)
                        elif row.ìƒíƒœ == 'Warning':
                            return ['background-color: #fff0cc'] * len(row)
                        return [''] * len(row)

                    format_dict = {col: '{:,.0f}' for col in result_df.select_dtypes(include=np.number).columns}
                    st.dataframe(result_df.style.apply(highlight_status, axis=1).format(format_dict), use_container_width=True)
                    st.markdown("---")
                    st.subheader("2. ê³„ì •ë³„ ì›”ë³„ ì¶”ì´ (PY vs CY)")
                    # âœ… ìë™ ì¶”ì²œ ì œê±°: ì‚¬ìš©ìê°€ ê³„ì •ì„ ì„ íƒí•œ ê²½ìš°ì—ë§Œ ê·¸ë˜í”„ ë Œë”
                    account_list = st.session_state.master_df['ê³„ì •ëª…'].unique()
                    selected_accounts = st.multiselect(
                        "ë¶„ì„í•  ê³„ì •ì„ ì„ íƒí•˜ì„¸ìš” (1ê°œ ì´ìƒ í•„ìˆ˜)",
                        account_list, default=[]
                    )
                    if not selected_accounts:
                        st.info("ê³„ì •ì„ 1ê°œ ì´ìƒ ì„ íƒí•˜ë©´ ì›”ë³„ ì¶”ì´ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        lf_use = st.session_state.get('lf_focus') or st.session_state.get('lf_hist')
                        # ì„ íƒëœ ê³„ì •ëª…ì„ ê³„ì •ì½”ë“œë¡œ ë³€í™˜
                        mdf = st.session_state.master_df
                        accounts_codes = (
                            mdf[mdf['ê³„ì •ëª…'].isin(selected_accounts)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        mod = run_trend_module(lf_use, accounts=accounts_codes)
                        for w in mod.warnings:
                            st.warning(w)
                        if mod.figures:
                            for title, fig in mod.figures.items():
                                # PM ì„ê³„ì„ (í•­ìƒ í‘œì‹œ; ë²”ìœ„ ë°–ì´ë©´ ìë™ í™•ì¥)
                                st.plotly_chart(
                                    add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT))),
                                    use_container_width=True
                                )
                        else:
                            st.info("í‘œì‹œí•  ì¶”ì´ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("3. ê³„ì • ê°„ ìƒê´€ íˆíŠ¸ë§µ")
                    # âœ… ë²„íŠ¼ ì—†ì´ ì¦‰ì‹œ ë Œë”: ê³„ì • 2ê°œ ì´ìƒ ì„ íƒ + ì„ê³„ì¹˜ ìŠ¬ë¼ì´ë” ì œê³µ
                    corr_accounts = st.multiselect(
                        "ìƒê´€ ë¶„ì„ ëŒ€ìƒ ê³„ì •(2ê°œ ì´ìƒ ì„ íƒ)",
                        account_list,
                        default=selected_accounts,
                        help="ì„ íƒí•œ ê³„ì •ë“¤ ê°„ ì›”ë³„ íë¦„ì˜ í”¼ì–´ìŠ¨ ìƒê´€ì„ ê³„ì‚°í•©ë‹ˆë‹¤."
                    )
                    corr_thr = st.slider(
                        "ìƒê´€ ì„ê³„ì¹˜(ê°•í•œ ìƒê´€ìŒ í‘œ ì „ìš©)",
                        min_value=0.50, max_value=0.95, step=0.05, value=0.70,
                        help="ì ˆëŒ€ê°’ ê¸°ì¤€ ì„ê³„ì¹˜ ì´ìƒì¸ ê³„ì •ìŒë§Œ í‘œì— í‘œì‹œí•©ë‹ˆë‹¤."
                    )
                    if len(corr_accounts) < 2:
                        st.info("ê³„ì •ì„ **2ê°œ ì´ìƒ** ì„ íƒí•˜ë©´ íˆíŠ¸ë§µì´ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        lf_use = _lf_by_scope()
                        mdf = st.session_state.master_df
                        codes = mdf[mdf['ê³„ì •ëª…'].isin(corr_accounts)]['ê³„ì •ì½”ë“œ'].astype(str).tolist()
                        cmod = run_correlation_module(lf_use, accounts=codes, corr_threshold=float(corr_thr))
                        for w in cmod.warnings:
                            st.warning(w)
                        if cmod.figures:
                            st.plotly_chart(cmod.figures['heatmap'], use_container_width=True)
                        if 'strong_pairs' in cmod.tables and not cmod.tables['strong_pairs'].empty:
                            st.markdown("**ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ**")
                            st.dataframe(cmod.tables['strong_pairs'], use_container_width=True)
                        if 'excluded_accounts' in cmod.tables and not cmod.tables['excluded_accounts'].empty:
                            with st.expander("ì œì™¸ëœ ê³„ì • ë³´ê¸°(ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)", expanded=False):
                                st.dataframe(cmod.tables['excluded_accounts'], use_container_width=True)

                with tab3:
                    st.header("ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")

                    st.subheader("ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± (ê³„ì •ë³„)")
                    master_df_res = st.session_state.master_df
                    account_list_vendor = master_df_res['ê³„ì •ëª…'].unique()
                    selected_accounts_vendor = st.multiselect("ë¶„ì„í•  ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”.", account_list_vendor, default=[])

                    # ğŸ”§ ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„° â€” KRW ì…ë ¥(ì»¤ë°‹ ì‹œ ì‰¼í‘œ ì •ê·œí™”)
                    min_amount_vendor = _krw_input(
                        "ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„°",
                        key="vendor_min_amount",
                        default_value=0,
                        help_text="CY ê¸°ì¤€ ê±°ë˜ê¸ˆì•¡ í•©ê³„ê°€ ì´ ê°’ ë¯¸ë§Œì¸ ê±°ë˜ì²˜ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°ë©ë‹ˆë‹¤."
                    )
                    include_others_vendor = st.checkbox("ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°", value=True)

                    if selected_accounts_vendor:
                        selected_codes = (
                            master_df_res[master_df_res['ê³„ì •ëª…'].isin(selected_accounts_vendor)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        lf_use = _lf_by_scope()
                        vmod = run_vendor_module(
                            lf_use,
                            account_codes=selected_codes,
                            min_amount=float(min_amount_vendor),
                            include_others=bool(include_others_vendor),
                        )
                        if vmod.figures:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'pareto' in vmod.figures:
                                    figp = vmod.figures['pareto']
                                    figp = add_materiality_threshold(figp, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figp, use_container_width=True)
                            with col2:
                                if 'heatmap' in vmod.figures:
                                    figh = add_pm_badge(vmod.figures['heatmap'], float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figh, use_container_width=True)
                        else:
                            st.warning("ì„ íƒí•˜ì‹  ê³„ì •ì—ëŠ” ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        for w in vmod.warnings:
                            st.warning(w)
                    else:
                        st.info("ê³„ì •ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ê³„ì •ì˜ ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± ë¶„ì„ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("ê±°ë˜ì²˜ë³„ ì„¸ë¶€ ë¶„ì„ (ì „ì²´ ê³„ì •)")
                    full_ledger_df = st.session_state.ledger_df
                    vendor_list = sorted(full_ledger_df[full_ledger_df['ê±°ë˜ì²˜'] != 'ì •ë³´ ì—†ìŒ']['ê±°ë˜ì²˜'].unique())

                    if len(vendor_list) > 0:
                        options = ['ì„ íƒí•˜ì„¸ìš”...'] + vendor_list
                        selected_vendor = st.selectbox("ìƒì„¸ ë¶„ì„í•  ê±°ë˜ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.", options, index=0)

                        if selected_vendor != 'ì„ íƒí•˜ì„¸ìš”...':
                            all_months_in_data = pd.period_range(
                                start=full_ledger_df['íšŒê³„ì¼ì'].min(),
                                end=full_ledger_df['íšŒê³„ì¼ì'].max(),
                                freq='M'
                            ).strftime('%Y-%m').tolist()
                            detail_fig = create_vendor_detail_figure(full_ledger_df, selected_vendor, all_months_in_data)
                            # PM line on vendor detail (stacked bars)
                            try:
                                detail_fig = add_materiality_threshold(detail_fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                            except Exception:
                                pass
                            if detail_fig:
                                st.plotly_chart(detail_fig, use_container_width=True)
                    else:
                        st.info("ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                with tab4:
                    st.header("ì´ìƒ íŒ¨í„´ íƒì§€ (v1: Z-Score)")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    mdf = st.session_state.master_df
                    acct_names = mdf['ê³„ì •ëª…'].unique()
                    pick = st.multiselect("ëŒ€ìƒ ê³„ì • ì„ íƒ(ë¯¸ì„ íƒ ì‹œ ìë™ ì¶”ì²œ)", acct_names, default=[])
                    topn = st.slider("í‘œì‹œ ê°œìˆ˜(ìƒìœ„ |Z|)", min_value=10, max_value=500, value=20, step=10)
                    if st.button("ì´ìƒì¹˜ ë¶„ì„ ì‹¤í–‰"):
                        lf_use = _lf_by_scope()
                        codes = None
                        if pick:
                            codes = mdf[mdf['ê³„ì •ëª…'].isin(pick)]['ê³„ì •ì½”ë“œ'].astype(str).tolist()
                        amod = run_anomaly_module(lf_use, target_accounts=codes, topn=topn, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                        for w in amod.warnings: st.warning(w)
                        if 'anomaly_top' in amod.tables:
                            _tbl = amod.tables['anomaly_top'].copy()
                            fmt = {}
                            if 'ë°œìƒì•¡' in _tbl.columns: fmt['ë°œìƒì•¡'] = '{:,.0f}'
                            if 'Z-Score' in _tbl.columns: fmt['Z-Score'] = '{:.2f}'
                            st.dataframe(_tbl.style.format(fmt), use_container_width=True)
                        if 'zscore_hist' in amod.figures:
                            st.plotly_chart(amod.figures['zscore_hist'], use_container_width=True)

                with tab_ts:
                    st.header("ì‹œê³„ì—´ ì˜ˆì¸¡(MoR) â€” ë§ˆì§€ë§‰ í¬ì¸íŠ¸ ìš”ì•½ + ë¼ì¸ì°¨íŠ¸")
                    st.caption("â€» ê¸°ë³¸ê°’ì€ 'ì›”ë³„ ë°œìƒì•¡(Î”ì”ì•¡)'ì´ë©°, BS ê³„ì •ì€ ì”ì•¡(balance) ê¸°ì¤€ë„ ë³‘í–‰í•©ë‹ˆë‹¤.")
                    with st.expander("ğŸ“˜ í•´ì„ ê°€ì´ë“œ", expanded=False):
                        st.markdown(
                            "- **error = ì‹¤ì œ âˆ’ ì˜ˆì¸¡** (ì–‘ìˆ˜ë©´ ì˜ˆìƒë³´ë‹¤ í¼)\n"
                            "- **z**: errorê°€ ê³¼ê±° ë³€ë™ì„±(Ïƒ) ëŒ€ë¹„ ëª‡ Ïƒì¸ì§€ (Â±2 ì£¼ì˜, Â±3 ì´ë¡€)\n"
                            "- **risk**: |z|, PM ëŒ€ë¹„ ë¹„ì¤‘, KIT ì—¬ë¶€ë¥¼ ê²°í•©í•œ 0~1 ì ìˆ˜"
                        )
                        st.caption("ê¸ˆì•¡ ë‹¨ìœ„ê°€ í° ê³„ì •ì€ ê¸ˆì•¡ ìì²´ë³´ë‹¤ z í¬ê¸°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ë³´ì„¸ìš”.")
                    lf_use = _lf_by_scope()
                    mdf = st.session_state.master_df
                    dfm = lf_use.df.copy()
                    dfm['ì—°ì›”'] = dfm['íšŒê³„ì¼ì'].dt.to_period('M').dt.to_timestamp('M')
                    agg = (dfm.groupby(['ê³„ì •ëª…','ì—°ì›”'])['ê±°ë˜ê¸ˆì•¡'].sum()
                               .reset_index().rename(columns={'ê³„ì •ëª…':'account','ì—°ì›”':'date','ê±°ë˜ê¸ˆì•¡':'amount'}))
                    pick_accounts_ts = st.multiselect("ëŒ€ìƒ ê³„ì •", sorted(agg['account'].unique()), default=[], key="ts_accounts")
                    use_ts = agg if not pick_accounts_ts else agg[agg['account'].isin(pick_accounts_ts)]
                    from analysis.timeseries import run_timeseries_module as _ts_run
                    res = _ts_run(use_ts.rename(columns={'account':'account','date':'date','amount':'amount'}),
                                  account_col='account', date_col='date', amount_col='amount',
                                  pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                    if not res.empty:
                        out = res.copy()
                        out = out.rename(columns={'account':'ê³„ì •'})
                        for c in ['actual','predicted','error','z','risk']:
                            out[c] = pd.to_numeric(out[c], errors='coerce')
                        _disp = out[['date','ê³„ì •','actual','predicted','error','z','risk']].rename(columns={
                            'predicted': 'ì˜ˆìƒ ë°œìƒì•¡(ì›” í•©ê³„)',
                            'error': 'ì°¨ì´(ì‹¤ì œ-ì˜ˆìƒ)'
                        })
                        st.caption("MoR(ìµœì  ëª¨ë¸) ê¸°ì¤€. BSëŠ” balance ê¸°ì¤€ë„ ë³‘í–‰ ê³„ì‚°í•©ë‹ˆë‹¤(ìš”ì•½ í…Œì´ë¸”ì—ëŠ” flowê°€ ê¸°ë³¸).")
                        st.dataframe(_disp.style.format({
                            'actual':'{:,.0f}', 'ì˜ˆìƒ ë°œìƒì•¡(ì›” í•©ê³„)':'{:,.0f}', 'ì°¨ì´(ì‹¤ì œ-ì˜ˆìƒ)':'{:,.0f}', 'z':'{:+.2f}', 'risk':'{:.2f}'
                        }), use_container_width=True)
                    else:
                        st.info("ì˜ˆì¸¡ì„ í‘œì‹œí•  ì¶©ë¶„í•œ ì›”ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                with tab5:
                    st.header("ê³„ì • Ã— ì£¼ì¥(CEAVOP) ìœ„í—˜ í‰ê°€")
                    # ì ì • ê¸°ì¤€ ì•ˆë‚´ ë°°ì§€
                    try:
                        st.info(f"â“˜ í†µí•© ìœ„í—˜ì ìˆ˜ëŠ” {PROVISIONAL_RULE_NAME}ì— ë”°ë¼ {provisional_risk_formula_str()}ë¡œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.caption("â“˜ CEAVOP ì£¼ì¥ì€ ê¸°ë³¸ ê·œì¹™(ì˜ˆ: ì˜ˆì¸¡ ìƒíšŒâ†’E, í•˜íšŒâ†’C)ì— ë”°ë¼ ìë™ ì œì•ˆë˜ì—ˆìœ¼ë©°, ì „ë¬¸ê°€ì˜ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    except Exception:
                        pass
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')} Â· PM={float(st.session_state.get('pm_value', PM_DEFAULT)):,.0f}ì›")
                    # âœ… í•œê¸€ ê°€ì´ë“œ: ì¢Œ/ìš° 2ì—´ ë ˆì´ì•„ì›ƒ
                    g_left, g_right = st.columns([0.48, 0.52])
                    with g_left:
                        st.markdown("#### ì–´ë–»ê²Œ ì½ë‚˜ìš”?")
                        st.markdown(
                            "- íˆíŠ¸ë§µì˜ ê° ì…€ì€ **ê³„ì • Ã— ì£¼ì¥** ì¡°í•©ì— ëŒ€í•œ í†µí•© ìœ„í—˜ì ìˆ˜(0~1)ì˜ *ìµœëŒ€ê°’*ì…ë‹ˆë‹¤.\n"
                            "- ìœ„í—˜ì ìˆ˜ëŠ” **|Z-Score|**, **PM ëŒ€ë¹„ ê¸ˆì•¡ë¹„ìœ¨**, **Key Item(PM ì´ˆê³¼)** ì—¬ë¶€ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.\n"
                            "- ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì˜ **Performance Materiality(PM)** ë¥¼ ì¡°ì •í•˜ë©´ KIT í”Œë˜ê·¸ì™€ íˆíŠ¸ë§µ ê°•ë„ê°€ í•¨ê»˜ ë³€í•©ë‹ˆë‹¤."
                        )
                        st.markdown("#### CEAVOP(ì£¼ì¥) ê°„ë‹¨ í•´ì„¤")
                        st.info(
                            "C(ì™„ì „ì„±): ëˆ„ë½ ì—†ì´ ëª¨ë‘ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?\n\n"
                            "E(ì¡´ì¬): ê¸°ë¡ëœ ê±°ë˜ê°€ ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ê°€?\n\n"
                            "A(ì •í™•ì„±): ê¸ˆì•¡/ê³„ì‚°ì´ ì •í™•í•œê°€?\n\n"
                            "V(í‰ê°€Â·ë°°ë¶„): ì ì ˆí•œ í‰ê°€Â·ë°°ë¶„ì´ ë˜ì—ˆëŠ”ê°€?\n\n"
                            "O(ë°œìƒ): ë°œìƒì‚¬ì‹¤/ê¶Œë¦¬Â·ì˜ë¬´ê°€ íƒ€ë‹¹í•œê°€?\n\n"
                            "P(í‘œì‹œÂ·ê³µì‹œ): ì ì ˆíˆ ë¶„ë¥˜Â·í‘œì‹œÂ·ê³µì‹œë˜ì—ˆëŠ”ê°€?"
                        )
                    with g_right:
                        st.markdown("#### íˆíŠ¸ë§µ")
                    lf_use = _lf_by_scope()
                    # ì „ì²´ ìŠ¤ì½”í”„ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ ëª¨ë“ˆì„ ì‹¤í–‰(ë¦¬ìŠ¤í¬ ì—ë¹„ë˜ìŠ¤ í™•ë³´)
                    amod_full = run_anomaly_module(lf_use, target_accounts=None, topn=200, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))

                    # --- [ADD] íƒ€ì„ì‹œë¦¬ì¦ˆ Evidence ìƒì„± & ê²°í•© ---
                    from analysis.timeseries import run_timeseries_module
                    from analysis.contracts import EvidenceDetail, ModuleResult
                    from analysis.anomaly import _risk_from  # anomaly_score ì •í•©ì„± ìœ ì§€ìš©

                    pm_cur = float(st.session_state.get("pm_value", PM_DEFAULT))

                    # 1) ì›” ì‹œê³„ì—´ ì§‘ê³„(ê³„ì • Ã— ì›”, ê¸ˆì•¡=ê±°ë˜ê¸ˆì•¡ í•©ê³„)
                    ts_base = lf_use.df.copy()
                    ts_base['ì—°ì›”'] = ts_base['íšŒê³„ì¼ì'].dt.to_period('M')
                    ts_monthly = (
                        ts_base.groupby(['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ì—°ì›”'], as_index=False)['ê±°ë˜ê¸ˆì•¡']
                               .sum()
                    )
                    # run_timeseries_moduleëŠ” account/date/amount 3ì»¬ëŸ¼ë§Œ ì‚¬ìš© â†’ code|nameë¡œ ë©”íƒ€ ë³´ì¡´
                    ts_monthly['account'] = ts_monthly.apply(lambda r: f"{str(r['ê³„ì •ì½”ë“œ'])}|{str(r['ê³„ì •ëª…'])}", axis=1)
                    ts_monthly['date'] = ts_monthly['ì—°ì›”'].dt.to_timestamp('M')
                    ts_monthly['amount'] = ts_monthly['ê±°ë˜ê¸ˆì•¡']

                    def _ts_adapter(r: dict) -> EvidenceDetail:
                        # r: {'account','date','amount','predicted','error','z','z_abs','assertion','risk',...}
                        acc_key = str(r.get('account', ''))
                        if '|' in acc_key:
                            acc_code, acc_name = acc_key.split('|', 1)
                        else:
                            acc_code, acc_name = acc_key, acc_key
                        dt = r.get('date')
                        yyyymm = dt.strftime('%Y-%m') if hasattr(dt, 'strftime') else str(dt)
                        a, f, k, score = _risk_from(float(r.get('z_abs', 0.0)), float(r.get('amount', 0.0)), pm_cur)
                        return EvidenceDetail(
                            row_id=f"TS::{acc_code}::{yyyymm}",
                            reason=f"ì˜ˆì¸¡ ëŒ€ë¹„ {'ìƒíšŒ' if float(r.get('error',0))>0 else 'í•˜íšŒ'}: z={float(r.get('z',0)):+.2f}",
                            anomaly_score=float(a),
                            financial_impact=abs(float(r.get('amount', 0.0))),
                            risk_score=float(r.get('risk', score)),
                            is_key_item=bool(abs(float(r.get('amount',0.0))) >= pm_cur),
                            measure="flow",
                            sign_rule="assets/expensesâ†‘=+, liabilities/equityâ†‘=-",
                            impacted_assertions=sorted({ "A", str(r.get('assertion','A')) }),
                            links={"account_code": str(acc_code), "account_name": str(acc_name), "period_tag": "CY"}
                        )

                    # 2) EvidenceDetail ë¦¬ìŠ¤íŠ¸ ìƒì„±
                    ts_evidences = run_timeseries_module(
                        ts_monthly[['account','date','amount']],
                        evidence_adapter=_ts_adapter,
                    )

                    ts_mod = ModuleResult(
                        name="timeseries",
                        summary={"n_rows": len(ts_monthly), "n_evidences": len(ts_evidences)},
                        tables={},
                        figures={},
                        evidences=ts_evidences,
                        warnings=[]
                    )

                    # 3) ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤: ì´ìƒì¹˜ + ì˜ˆì¸¡ Evidence ë™ì‹œ ë°˜ì˜
                    mat, emap = build_matrix([amod_full, ts_mod])
                    if mat.empty:
                        st.info("ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•  Evidenceê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        import plotly.express as px
                        fig = px.imshow(mat, aspect='auto', origin='upper',
                                        title="ê³„ì • Ã— ì£¼ì¥ ìœ„í—˜ íˆíŠ¸ë§µ (max risk_score, 0~1)",
                                        labels=dict(x="Assertion", y="Account", color="Risk"))
                        fig.update_coloraxes(cmin=0, cmax=1)
                        st.plotly_chart(fig, use_container_width=True)
                        with st.expander("ìˆ˜ì¹˜ í…Œì´ë¸” ë³´ê¸°", expanded=False):
                            st.dataframe(mat, use_container_width=True)
                        # ğŸ”½ ì „ì²´ Evidence CSV ë‚´ë³´ë‚´ê¸° (í–‰ë ¬ ê·¼ê±° ì „ë¶€)
                        from dataclasses import asdict
                        all_evs = (amod_full.evidences or []) + (ts_mod.evidences or [])
                        if all_evs:
                            ev_all_df = pd.DataFrame([asdict(e) for e in all_evs])
                            # impacted_assertions ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´í™”
                            if 'impacted_assertions' in ev_all_df.columns:
                                ev_all_df['impacted_assertions'] = ev_all_df['impacted_assertions'].apply(
                                    lambda xs: ",".join(xs) if isinstance(xs, list) else str(xs)
                                )
                            # links í‰íƒ„í™”
                            if 'links' in ev_all_df.columns:
                                _lnk = pd.json_normalize(ev_all_df['links']).add_prefix('links.')
                                ev_all_df = pd.concat([ev_all_df.drop(columns=['links']), _lnk], axis=1)
                            # ì»¬ëŸ¼ ìˆœì„œ ê³ ì •
                            _ORDER = ['row_id','risk_score','anomaly_score','financial_impact','is_key_item',
                                      'impacted_assertions','links.account_code','links.account_name','links.period_tag','reason']
                            for col in _ORDER:
                                if col not in ev_all_df.columns:
                                    ev_all_df[col] = ""
                            ev_all_df = ev_all_df[_ORDER]
                            st.download_button(
                                "ğŸ“¥ Evidence ì „ì²´ CSV ë‹¤ìš´ë¡œë“œ",
                                ev_all_df.to_csv(index=False).encode('utf-8-sig'),
                                file_name="evidence_all.csv",
                                mime="text/csv"
                            )

                        # ë“œë¦´ë‹¤ìš´: ê³„ì •/ì£¼ì¥ ì„ íƒ â†’ ê·¼ê±° í‘œì‹œ
                        st.subheader("ğŸ” ë“œë¦´ë‹¤ìš´: íŠ¹ì • ì…€ì˜ ê·¼ê±°(Evidence)")
                        acct = st.selectbox("ê³„ì •(í–‰)", ["ì„ íƒí•˜ì„¸ìš”..."] + mat.index.tolist(), index=0, help="ì¡°ì‚¬í•  ê³„ì •(í–‰)ì„ ì„ íƒí•˜ì„¸ìš”.", key="risk_dd_account")
                        asrt = st.selectbox("ì£¼ì¥(ì—´)", ["ì„ íƒí•˜ì„¸ìš”..."] + list(mat.columns), index=0, help="ê´€ë¦¬ìì˜ ì£¼ì¥(CEAVOP) ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.", key="risk_dd_assertion")
                        # ì„ íƒí•œ ì£¼ì¥ì— ëŒ€í•œ ì§§ì€ ì„¤ëª…
                        _asrt_help = {
                            "C":"ì™„ì „ì„±", "E":"ì¡´ì¬", "A":"ì •í™•ì„±", "V":"í‰ê°€Â·ë°°ë¶„",
                            "O":"ë°œìƒ", "P":"í‘œì‹œÂ·ê³µì‹œ"
                        }
                        if asrt in _asrt_help:
                            st.caption(f"ì„ íƒí•œ ì£¼ì¥ ì„¤ëª…: **{asrt} â€“ {_asrt_help[asrt]}**")
                        if acct != "ì„ íƒí•˜ì„¸ìš”..." and asrt != "ì„ íƒí•˜ì„¸ìš”...":
                            from dataclasses import asdict
                            ev_all = st.session_state.get('amod_full_evidences') or amod_full.evidences
                            st.session_state['amod_full_evidences'] = ev_all
                            def _match_ev(e, acct_name, asrt_code):
                                name_ok = (e.links.get("account_name") == acct_name) or (e.links.get("account_code") == acct_name)
                                asrt_ok = asrt_code in (e.impacted_assertions or [])
                                return bool(name_ok and asrt_ok)
                            direct_hits = [asdict(e) for e in ev_all if _match_ev(e, acct, asrt)]
                            row_ids = emap.get((acct, asrt), [])
                            by_id_hits = [asdict(e) for e in ev_all if str(e.row_id) in row_ids]
                            rows = direct_hits or by_id_hits
                            ev_df = pd.DataFrame(rows)
                            if not ev_df.empty:
                                ev_df['impacted_assertions'] = ev_df['impacted_assertions'].apply(lambda xs: ",".join(xs) if isinstance(xs, list) else str(xs))
                                show_cols = ['row_id','risk_score','is_key_item','anomaly_score','financial_impact','impacted_assertions','reason']
                                st.dataframe(ev_df[show_cols].sort_values('risk_score', ascending=False), use_container_width=True)
                            else:
                                st.info("í•´ë‹¹ ì…€ì—ì„œ í‘œì‹œí•  Evidence ë ˆì½”ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í‚¤ ë¯¸ìŠ¤ë§¤ì¹˜ ë°©ì§€ ë¡œì§ ì ìš© ì™„ë£Œ)")

                        # ì˜ˆì¸¡ Evidence ìš”ì•½(ë“€ì–¼ ê¸°ì¤€ + MoR)
                        with st.expander("ğŸ”® ì˜ˆì¸¡ ê¸°ë°˜ Evidence(ìš”ì•½) â€” ê³„ì •ë³„ ë§ˆì§€ë§‰ í¬ì¸íŠ¸", expanded=False):
                            def _acc_label(k: str) -> str:
                                return (k.split('|',1)[1] if '|' in str(k) else str(k))
                            # ê³„ì •ë³„ ì›” ì§‘ê³„
                            base = ts_monthly[['account','date','amount']].copy()
                            base = base.rename(columns={'amount':'flow'})
                            base['balance'] = base.groupby('account')['flow'].cumsum()
                            # BS/PL ë§¤í•‘
                            bs_map = st.session_state.master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','BS/PL']].drop_duplicates()
                            bs_map['key'] = bs_map['ê³„ì •ì½”ë“œ'].astype(str) + '|' + bs_map['ê³„ì •ëª…'].astype(str)
                            bs_flag = bs_map.set_index('key')['BS/PL'].map(lambda x: str(x).upper()=='BS').to_dict()
                            from analysis.timeseries import run_timeseries_for_account
                            rows_ts = []
                            for acc, g in base.groupby('account'):
                                is_bs = bool(bs_flag.get(str(acc), True))  # ì •ë³´ì—†ìœ¼ë©´ Trueë¡œ ë³´ìˆ˜ì  ì²˜ë¦¬
                                out = run_timeseries_for_account(
                                    g[['date','flow','balance']], _acc_label(str(acc)),
                                    is_bs=is_bs, flow_col='flow', balance_col='balance',
                                    pm_value=float(st.session_state.get("pm_value", PM_DEFAULT))
                                )
                                if not out.empty:
                                    rows_ts.append(out)
                            # ë³´ê¸° ë²”ìœ„ í† ê¸€ì€ í•­ìƒ ë…¸ì¶œ
                            scope = st.selectbox("ë³´ê¸° ë²”ìœ„", options=["flow","balance","both"], index=2, key="ts_scope_main")
                            if rows_ts:
                                df_ts = pd.concat(rows_ts, ignore_index=True)
                                st.caption("BS ê³„ì •ì€ ì”ì•¡Â·ë°œìƒì•¡ ê¸°ì¤€ì„ ë³‘í–‰ ê³„ì‚°í•©ë‹ˆë‹¤. í‘œì‹œ ê¸°ì¤€ì€ ìœ„ í† ê¸€ì„ ë”°ë¦…ë‹ˆë‹¤.")
                                st.caption("ë³¸ ì˜ˆì¸¡ì€ **PY+CY ì—°ì† ì›”**(ë³´ê°„ ì—†ìŒ)ë¡œ í•™ìŠµí•˜ê³  MoR(ìµœì  ëª¨ë¸)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. errorëŠ” zì™€ í•¨ê»˜ í•´ì„í•˜ì„¸ìš”.")
                                df_view = df_ts if scope == "both" else df_ts[df_ts['measure'] == scope]
                                st.dataframe(df_view[['date','account','measure','actual','predicted','error','z','risk','model']], use_container_width=True)

                                # === (êµì²´) ê³„ì •/ê¸°ì¤€ ì„ íƒ ë¼ì¸/ìŒì°¨íŠ¸ ===
                                import plotly.graph_objects as go

                                def _make_ts_fig(df_hist: pd.DataFrame, measure: str, title: str):
                                    """EMA ê¸°ë°˜ ì˜ˆì¸¡ì„ ì„ ê°™ì´ ê·¸ë ¤ì£¼ëŠ” ê°„ë‹¨ ë¼ì¸ì°¨íŠ¸(ì‹¤ì„ =actual, ì ì„ =pred)."""
                                    s = df_hist[['date', measure]].rename(columns={measure: 'actual'}).sort_values('date').copy()
                                    if s.empty:
                                        return None
                                    # EMA ì˜ˆì¸¡ì„ (shift 1)
                                    s['predicted'] = s['actual'].ewm(span=6, adjust=False).mean().shift(1)
                                    fig = go.Figure()
                                    fig.add_trace(go.Scatter(x=s['date'], y=s['actual'], mode='lines', name='actual'))
                                    fig.add_trace(go.Scatter(x=s['date'], y=s['predicted'], mode='lines', name='predicted', line=dict(dash='dot')))
                                    fig.update_layout(title=title, xaxis_title='month', yaxis_title=measure)
                                    try:
                                        return add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    except Exception:
                                        return fig

                                # â”€â”€ ì„ íƒ ê³„ì •
                                sel_acc = st.selectbox("ê³„ì • ì„ íƒ", sorted(df_ts["account"].unique()), key="ts_plot_acc")

                                # íˆìŠ¤í† ë¦¬(ì›”ë³„ flow/balance ì¬êµ¬ì„±)
                                hist_base = use_ts.copy()  # use_tsëŠ” ìœ„ì—ì„œ ë§Œë“  monthly agg (account,date,amount)
                                # ê³„ì •ë³„ë¡œ flow/balance ë™ì‹œ êµ¬ì„±
                                hist_base = hist_base.rename(columns={'amount':'flow'})
                                hist_base['balance'] = hist_base.sort_values('date').groupby('account')['flow'].cumsum()

                                cur_hist = hist_base[hist_base['account'] == sel_acc].copy()
                                if cur_hist.empty:
                                    st.info("ì„ íƒ ê³„ì •ì˜ ì›”ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                else:
                                    # BS ì—¬ë¶€ íŒë‹¨ (Masterì˜ BS/PL í™œìš©)
                                    _mdf = st.session_state.master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','BS/PL']].drop_duplicates()
                                    # ê³„ì •ëª…ì´ ê°™ì€ í•­ëª©ì„ ì°¾ì•„ BS/PL í™•ì¸ (ì—†ìœ¼ë©´ PL ì·¨ê¸‰)
                                    try:
                                        is_bs = bool(_mdf[_mdf['ê³„ì •ëª…'] == sel_acc]['BS/PL'].astype(str).str.upper().eq('BS').any())
                                    except Exception:
                                        is_bs = False

                                    pair = st.toggle("ìŒì°¨íŠ¸ ë³´ê¸°(Flow+Balance)", value=is_bs, disabled=not is_bs)
                                    if pair and is_bs:
                                        c1, c2 = st.columns(2)
                                        with c1:
                                            f1 = _make_ts_fig(cur_hist, 'flow', f"{sel_acc} â€” Flow (actual vs MoR)")
                                            if f1: st.plotly_chart(f1, use_container_width=True)
                                        with c2:
                                            f2 = _make_ts_fig(cur_hist, 'balance', f"{sel_acc} â€” Balance (actual vs MoR)")
                                            if f2: st.plotly_chart(f2, use_container_width=True)
                                    else:
                                        measure = st.radio("ê¸°ì¤€(Measure)", ["flow","balance"], horizontal=True, index=0 if not is_bs else 0,
                                                           help="BSê°€ ì•„ë‹Œ ê³„ì •ì€ balanceê°€ ì˜ë¯¸ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                        fig = _make_ts_fig(cur_hist, measure, f"{sel_acc} â€” {measure.title()} (actual vs MoR)")
                                        if fig: st.plotly_chart(fig, use_container_width=True)

                                # CSV ë‚´ë³´ë‚´ê¸°(í•­ìƒ ë‘ ê¸°ì¤€ í¬í•¨)
                                st.download_button(
                                    "ğŸ“¥ ì˜ˆì¸¡ ìš”ì•½ CSV ë‹¤ìš´ë¡œë“œ(ë“€ì–¼ê¸°ì¤€)",
                                    data=df_ts.to_csv(index=False).encode('utf-8-sig'),
                                    file_name="evidence_timeseries_dual.csv",
                                    mime="text/csv",
                                    key="ts_csv_dl"
                                )
                            else:
                                st.info("ìµœê·¼ í¬ì¸íŠ¸ ê¸°ì¤€ ì˜ˆì¸¡ ì´íƒˆ Evidenceê°€ ì—†ìŠµë‹ˆë‹¤.")

                        # (ì¤‘ë³µ) ë“œë¦´ë‹¤ìš´ ë¸”ëŸ­ ì œê±° â€” ìœ„ì— ì´ë¯¸ 1íšŒ ì¡´ì¬

                with tab6:
                    st.header("AI ë¦¬í¬íŠ¸ ë° ì±„íŒ…")
                    # LLM í‚¤ ë¯¸ê°€ìš©ì´ì–´ë„ ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„± ê°€ëŠ¥
                    LLM_OK = False
                    try:
                        from services.llm import openai_available
                        LLM_OK = bool(openai_available())
                    except Exception:
                        LLM_OK = False
                    if not LLM_OK:
                        st.info("ğŸ”Œ OpenAI Key ì—†ìŒ: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ë¯¸ì‚¬ìš©)")
                    rendered_report = False

                    # === ëª¨ë¸/í† í° ì˜µì…˜ UI ===
                    colm1, colm2, colm3 = st.columns([1,1,1])
                    with colm1:
                        llm_model_choice = st.selectbox(
                            "LLM ëª¨ë¸", options=["gpt-5", "gpt-4o"], index=1,
                            help="gpt-5 ë¯¸ê°€ìš© ì‹œ ìë™ìœ¼ë¡œ gpt-4oë¡œ ëŒ€ì²´í•˜ì„¸ìš”(ì½”ë“œì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬)."
                        )
                    with colm2:
                        desired_tokens = st.number_input(
                            "ë³´ê³ ì„œ ìµœëŒ€ ì¶œë ¥ í† í°", min_value=512, max_value=32000, value=16000, step=512,
                            help="ì‹¤ì œ ì „ì†¡ê°’ì€ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ì™€ ì…ë ¥ í† í°ì„ ê³ ë ¤í•´ ì•ˆì „ í´ë¨í”„ë©ë‹ˆë‹¤."
                        )
                    with colm3:
                        st.caption("ê¸ˆì•¡Â·í¬ë§·ì€ ì½”ë“œì—ì„œ ê°•ì œë©ë‹ˆë‹¤.")

                    # --- ì…ë ¥ ì˜ì—­ ---
                    mdf = st.session_state.master_df
                    ldf = st.session_state.ledger_df

                    # â‘  ê³„ì • ì„ íƒ(í•„ìˆ˜) â€” ìë™ ì¶”ì²œ ì œê±°
                    acct_names_all = sorted(mdf['ê³„ì •ëª…'].dropna().unique().tolist())
                    pick_accounts = st.multiselect(
                        "ë³´ê³ ì„œ ëŒ€ìƒ ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”. (ìµœì†Œ 1ê°œ)",
                        options=acct_names_all,
                        default=[]
                    )
                    # â‘¡ ì˜µì…˜ ì œê±°: í•­ìƒ ìˆ˜í–‰ í”Œë˜ê·¸
                    opt_knn_evidence = True
                    opt_patterns = True
                    opt_patterns_py = True

                    # â‘¢ ì‚¬ìš©ì ë©”ëª¨(ì„ íƒ)
                    manual_ctx = st.text_area(
                        "ë³´ê³ ì„œì— ì¶”ê°€í•  ë©”ëª¨/ì£¼ì˜ì‚¬í•­(ì„ íƒ)",
                        placeholder="ì˜ˆ: 5~7ì›” ëŒ€í˜• ìº í˜ì¸ ì§‘í–‰ ì˜í–¥, 3ë¶„ê¸°ë¶€í„° ë‹¨ê°€ ì¸ìƒ ì˜ˆì • ë“±"
                    )

                    # â‘£ ì„ íƒ ê³„ì •ì½”ë“œ ë§¤í•‘
                    pick_codes = (
                        mdf[mdf['ê³„ì •ëª…'].isin(pick_accounts)]['ê³„ì •ì½”ë“œ']
                        .astype(str).tolist()
                    )

                    colA, colB, colC = st.columns([1,1,1])
                    with colA: st.write("ì„ íƒ ê³„ì •ì½”ë“œ:", ", ".join(pick_codes) if pick_codes else "-")
                    with colB: st.write("ê¸°ì¤€ ì—°ë„(CY):", int(ldf['ì—°ë„'].max()))
                    with colC: st.write("ë³´ê³ ì„œ ê¸°ì¤€:", "Current Year GL")

                    # ë²„íŠ¼ì€ ê³„ì • ë¯¸ì„ íƒ ì‹œ ë¹„í™œì„±í™”
                    btn = st.button("ğŸ“ ë³´ê³ ì„œ ìƒì„±", type="primary", disabled=(len(pick_codes) == 0))
                    if len(pick_codes) == 0:
                        st.info("ê³„ì • 1ê°œ ì´ìƒ ì„ íƒ ì‹œ ë²„íŠ¼ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

                    if btn:
                        import time
                        from analysis.anomaly import compute_amount_columns
                        from analysis.embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
                        from analysis.report import generate_rag_context, run_final_analysis, build_methodology_note, run_offline_fallback_report
                        from services.llm import LLMClient
                        from analysis.anomaly import ensure_zscore

                        t0 = time.perf_counter()
                        with st.status("ë³´ê³ ì„œ ì¤€ë¹„ ì¤‘...", expanded=True) as s:
                            # Step 1) ë°ì´í„° ìŠ¬ë¼ì´ì‹±
                            s.write("â‘  ìŠ¤ì½”í”„ ì ìš© ë° ë°ì´í„° ìŠ¬ë¼ì´ì‹±(CY/PY)â€¦")
                            cur_year = ldf['ì—°ë„'].max()
                            df_cy = ldf[(ldf['period_tag'] == 'CY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            df_py = ldf[(ldf['period_tag'] == 'PY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            s.write(f"    â”” CY {len(df_cy):,}ê±´ / PY {len(df_py):,}ê±´")

                            # Step 2) í•„ìˆ˜ íŒŒìƒ(ë°œìƒì•¡/ìˆœì•¡)
                            s.write("â‘¡ ê¸ˆì•¡ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±(ë°œìƒì•¡/ìˆœì•¡)â€¦")
                            df_cy = compute_amount_columns(df_cy)
                            df_py = compute_amount_columns(df_py)

                            # Step 3) (ì„ íƒ) íŒ¨í„´ìš”ì•½: ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ (LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œì—ë§Œ)
                            cl_ok = False
                            if LLM_OK and opt_patterns and not df_cy.empty:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰(ì„ íƒ)â€¦")
                                # ì…ë ¥ í…ìŠ¤íŠ¸ í’ë¶€í™” + ì„ë² ë”© + HDBSCAN (ìµœëŒ€ N ì œí•œìœ¼ë¡œ ì•ˆì „ê°€ë“œ)
                                df_cy_small = df_cy.copy()
                                max_rows = 8000
                                if len(df_cy_small) > max_rows:
                                    df_cy_small = df_cy_small.sample(max_rows, random_state=42)
                                    s.write(f"    â”” ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                df_cy_small = ensure_rich_embedding_text(df_cy_small)
                                try:
                                    emb_client = LLMClient().client  # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´
                                    # LLM naming is mandatory for the report
                                    df_clu, ok = perform_embedding_and_clustering(
                                        df_cy_small, emb_client,
                                        name_with_llm=True, must_name_with_llm=True,
                                        use_large=bool(st.session_state.get("use_large_embedding", False)),
                                        rescue_tau=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
                                    )
                                    if ok:
                                        # unify near-duplicate names using LLM
                                        from analysis.embedding import unify_cluster_names_with_llm, unify_cluster_labels_llm
                                        df_clu, name_map = unify_cluster_names_with_llm(df_clu, emb_client)
                                        # ì¶”ê°€ LLM ë¼ë²¨ í†µí•©(JSON ë§¤í•‘ ë°©ì‹) â€” CYì˜ cluster_groupì€ ìœ ì§€
                                        try:
                                            raw_map = unify_cluster_labels_llm(df_clu['cluster_name'].dropna().unique().tolist(), emb_client)
                                            if raw_map:
                                                df_clu['cluster_name'] = df_clu['cluster_name'].map(lambda x: raw_map.get(str(x), x))
                                                # â— cluster_groupëŠ” unify_cluster_names_with_llm()ì´ ì •í•œ canonicalì„ ìœ ì§€
                                        except Exception:
                                            pass
                                        # ê°„ë‹¨ ìš”ì•½(ìƒìœ„ 5ê°œ)
                                        topc = (df_clu.groupby('cluster_group')['ë°œìƒì•¡']
                                                .agg(['count','sum']).sort_values('sum', ascending=False).head(5))
                                        s.write("    â”” í´ëŸ¬ìŠ¤í„° ìƒìœ„ 5ê°œ ìš”ì•½:")
                                        st.dataframe(
                                            topc.rename(columns={'count':'ê±´ìˆ˜','sum':'ë°œìƒì•¡í•©ê³„'})
                                                .style.format({'ë°œìƒì•¡í•©ê³„':'{:,.0f}'}),
                                            use_container_width=True
                                        )
                                        # Quality telemetry
                                        try:
                                            n = int(len(df_clu))
                                            noise_rate = float((df_clu['cluster_id'] == -1).mean()) if n else 0.0
                                            n_clusters = int(df_clu.loc[df_clu['cluster_id'] != -1, 'cluster_id'].nunique())
                                            if n_clusters > 0:
                                                avg_size = float(df_clu[df_clu['cluster_id'] != -1].groupby('cluster_id').size().mean())
                                            else:
                                                avg_size = 0.0
                                            rescue_rate = float(df_clu.get('rescued', False).mean()) if 'rescued' in df_clu.columns else 0.0
                                            model_used = df_clu.attrs.get('embedding_model', 'unknown')
                                            umap_on = bool(df_clu.attrs.get('umap_used', False))
                                            s.write(
                                                f"    â”” Quality: N={n:,}, noise={noise_rate*100:.1f}%, "
                                                f"clusters={n_clusters}, avg_size={avg_size:.1f}, rescued={rescue_rate*100:.1f}%"
                                            )
                                            s.write(
                                                f"    â”” Model/UMAP: {model_used} | UMAP={'on' if umap_on else 'off'} | Ï„={float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)):.2f}"
                                            )
                                            # Persist metrics for dashboard card
                                            st.session_state['cluster_quality'] = {
                                                "N": n,
                                                "noise_rate": noise_rate,
                                                "n_clusters": n_clusters,
                                                "avg_size": avg_size,
                                                "rescued_rate": rescue_rate,
                                                "model": model_used,
                                                "umap": umap_on,
                                                "tau": float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)),
                                            }
                                        except Exception:
                                            pass
                                        # ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸ì— ë°˜ì˜: group/label ë™ì‹œ ë¶€ì°©
                                        df_cy = df_cy.merge(
                                            df_clu[['row_id','cluster_id','cluster_name','cluster_group']],
                                            on='row_id', how='left'
                                        )
                                        # í•„ìš” ì‹œ vectorë„ í•¨ê»˜ ë³‘í•© ê°€ëŠ¥:
                                        # df_cy = df_cy.merge(df_clu[['row_id','vector']], on='row_id', how='left')
                                        # (í˜„ì¬ëŠ” perform_embedding_only ë‹¨ê³„ì—ì„œ CY/PY dfì— vectorê°€ ì§ì ‘ ë¶€ì—¬ë¨)
                                        # --- PY clustering and alignment (optional) ---
                                        if opt_patterns_py and not df_py.empty:
                                            try:
                                                from analysis.embedding import cluster_year, align_yearly_clusters, unify_cluster_labels_llm
                                                # sampling guard similar to CY
                                                df_py_small = df_py.copy()
                                                max_rows = 8000
                                                if len(df_py_small) > max_rows:
                                                    df_py_small = df_py_small.sample(max_rows, random_state=42)
                                                    s.write(f"    â”” PY ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                                df_py_clu = cluster_year(df_py_small, emb_client)
                                                # push back columns to df_py via row_id if available
                                                if not df_py_clu.empty and 'row_id' in df_py.columns:
                                                    df_py = df_py.merge(df_py_clu, on='row_id', how='left', suffixes=("", "_pyclu"))
                                                # alignment: map PY cluster IDs to CY
                                                if 'cluster_id' in df_py_clu.columns:
                                                    mapping = align_yearly_clusters(df_clu, df_py_clu, sim_threshold=0.70)
                                                    # cluster_id â†’ (aligned_cy_cluster, aligned_sim)
                                                    cy_id_to_name = df_clu.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()
                                                    def _get_pair(cid):
                                                        try:
                                                            if pd.isna(cid):
                                                                return (np.nan, np.nan)
                                                            cid_int = int(cid)
                                                            return mapping.get(cid_int, (np.nan, np.nan))
                                                        except Exception:
                                                            return (np.nan, np.nan)
                                                    if 'cluster_id' in df_py.columns:
                                                        pairs = df_py['cluster_id'].map(_get_pair)
                                                        df_py[['aligned_cy_cluster', 'aligned_sim']] = pd.DataFrame(pairs.tolist(), index=df_py.index)
                                                        # ì´ë¦„ì€ CYì˜ ì´ë¦„ìœ¼ë¡œ ì •ë ¬(ê°€ëŠ¥í•œ ê²½ìš°)
                                                        df_py['cluster_name'] = df_py['aligned_cy_cluster'].map(cy_id_to_name).fillna(df_py.get('cluster_name'))
                                                # final unification over union of names â€” CYì˜ cluster_group ë¶ˆë³€, PYëŠ” í‘œì‹œëª…/ê·¸ë£¹ì„ canonicalë¡œ ì •ë ¬
                                                try:
                                                    all_names = pd.Series([], dtype=object)
                                                    if 'cluster_name' in df_cy.columns:
                                                        all_names = pd.concat([all_names, df_cy['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    if 'cluster_name' in df_py.columns:
                                                        all_names = pd.concat([all_names, df_py['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    all_names = all_names.dropna().unique().tolist()
                                                    canon = unify_cluster_labels_llm(all_names, emb_client)
                                                    if canon:
                                                        if 'cluster_name' in df_cy.columns:
                                                            df_cy['cluster_name'] = df_cy['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_name' in df_py.columns:
                                                            df_py['cluster_name'] = df_py['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_group' in df_py.columns:
                                                            df_py['cluster_group'] = df_py['cluster_name']
                                                except Exception:
                                                    pass
                                            except Exception as e:
                                                s.write(f"    â”” PY í´ëŸ¬ìŠ¤í„°ë§/ì •ë ¬ ìŠ¤í‚µ: {e}")
                                        # ì»¨í…ìŠ¤íŠ¸ì— ë³„ë„ ë…¸íŠ¸ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                                        cl_ok = True
                                    else:
                                        s.write("    â”” LLM í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ â†’ ë³´ê³ ì„œ ìƒì„± ìš”ê±´ ë¯¸ì¶©ì¡±")
                                except Exception as e:
                                    s.write(f"    â”” ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
                            else:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§: LLM ë¯¸ê°€ìš© ë˜ëŠ” ì˜µì…˜ ë¹„í™œì„± â†’ ìŠ¤í‚µ")

                            # Step 3-1) (ì˜µì…˜ A) ê·¼ê±° ì¸ìš©(KNN)ìš© ì„ë² ë”©ë§Œ ìˆ˜í–‰ (LLM ê°€ëŠ¥ ì‹œ)
                            if LLM_OK and opt_knn_evidence:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš©ìš© ì„ë² ë”©(CY/PY)â€¦")
                                from analysis.embedding import perform_embedding_only, ensure_rich_embedding_text
                                emb_client2 = LLMClient().client
                                df_cy = ensure_rich_embedding_text(df_cy)
                                df_py = ensure_rich_embedding_text(df_py)
                                df_cy = perform_embedding_only(
                                    df_cy, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False))
                                )
                                df_py = perform_embedding_only(
                                    df_py, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False))
                                )
                            elif not LLM_OK:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš© ì„ë² ë”©: LLM ë¯¸ê°€ìš© â†’ ìŠ¤í‚µ")

                            # Step 3-2) Z-Score: ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ í•¨
                            s.write("â‘¢-2 Z-Score ê³„ì‚°/ê²€ì¦â€¦")
                            df_cy, z_ok = ensure_zscore(df_cy, pick_codes)
                            df_py, _    = ensure_zscore(df_py, pick_codes)  # ì „ê¸°ì—ë„ Z-Score ê³„ì‚°(ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©)
                            if not z_ok:
                                s.write("    â”” Z-Score ë¯¸ê³„ì‚° ë˜ëŠ” ì „ë¶€ ê²°ì¸¡")

                            # âœ… ê²Œì´íŠ¸ ì™„í™”: Z-Scoreë§Œ í™•ë³´ë˜ë©´ ë³´ê³ ì„œ ì§„í–‰.
                            #    (í´ëŸ¬ìŠ¤í„° ì‹¤íŒ¨ ì‹œ ê´€ë ¨ ì„¹ì…˜ì€ ìë™ ì¶•ì•½/ìƒëµ)
                            if not z_ok:
                                st.error("ë³´ê³ ì„œ ìƒì„± ì¤‘ë‹¨: Z-Score ì—†ìŒ.")
                                s.update(label="ë³´ê³ ì„œ ìš”ê±´ ë¯¸ì¶©ì¡±", state="error")
                                st.stop()
                            if not cl_ok:
                                s.write("    â”” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì—†ìŒ â†’ ë¦¬í¬íŠ¸ì—ì„œ í´ëŸ¬ìŠ¤í„° ì„¹ì…˜ì€ ìƒëµ/ì¶•ì•½ë©ë‹ˆë‹¤.")

                            # Step 4) ì»¨í…ìŠ¤íŠ¸ ìƒì„± + ë°©ë²•ë¡  ë…¸íŠ¸
                            s.write("â‘£ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±â€¦")
                            ctx = generate_rag_context(
                                mdf, df_cy, df_py,
                                account_codes=pick_codes,
                                manual_context=manual_ctx,
                                include_risk_summary=True,
                                pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                            )
                            note = build_methodology_note(report_accounts=pick_codes)

                            # Step 5) LLM í˜¸ì¶œ ì „ ì ê²€(ê¸¸ì´/í† í°)
                            s.write("â‘¤ LLM í”„ë¡¬í”„íŠ¸ ì ê²€â€¦")
                            prompt_len = len(ctx) + len(note)
                            s.write(f"    â”” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {prompt_len:,} chars")
                            try:
                                import tiktoken
                                enc = tiktoken.get_encoding("cl100k_base")
                                est_tokens = len(enc.encode(ctx)) + len(enc.encode(note))
                                s.write(f"    â”” ì˜ˆìƒ í† í° ìˆ˜: ~{est_tokens:,} tokens")
                            except Exception:
                                s.write("    â”” tiktoken ë¯¸ì„¤ì¹˜: í† í° ì¶”ì • ìƒëµ")

                            # Step 6) ë³´ê³ ì„œ ìƒì„±: LLM ê°€ëŠ¥í•˜ë©´ ì‹œë„, ì‹¤íŒ¨/ë¶ˆê°€ ì‹œ ì˜¤í”„ë¼ì¸ í´ë°±
                            final_report = None
                            if LLM_OK:
                                s.write("â‘¥ LLM ìš”ì•½ ìƒì„± í˜¸ì¶œâ€¦")
                                try:
                                    t_llm0 = time.perf_counter()
                                    final_report = run_final_analysis(
                                        context=ctx + "\n" + note,
                                        account_codes=pick_codes,
                                        model=llm_model_choice,
                                        max_tokens=int(desired_tokens),
                                    )
                                    s.write(f"    â”” LLM ì™„ë£Œ (ê²½ê³¼ {time.perf_counter()-t_llm0:.1f}s)")
                                except Exception as e:
                                    s.write(f"    â”” LLM ì‹¤íŒ¨: {e} â†’ ì˜¤í”„ë¼ì¸ í´ë°±ìœ¼ë¡œ ì „í™˜")

                            if final_report is None:
                                s.write("â‘¥-í´ë°±: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±â€¦")
                                final_report = run_offline_fallback_report(
                                    current_df=df_cy,
                                    previous_df=df_py,
                                    account_codes=pick_codes,
                                    pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                                )

                            s.update(label="ë³´ê³ ì„œ ì¤€ë¹„ ì™„ë£Œ", state="complete")

                            # ê²°ê³¼ ì¶œë ¥ ë° ì„¸ì…˜ ë³´ì¡´
                            st.session_state['last_report'] = final_report
                            st.session_state['last_context'] = ctx + "\n" + note
                            st.session_state['last_dfcy'] = df_cy
                            st.session_state['last_dfpy'] = df_py

                            st.success("ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                            st.markdown(final_report)

                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])

                        # ZIP ë‹¨ì¼ ë‹¤ìš´ë¡œë“œ + RAW ë¯¸ë¦¬ë³´ê¸°
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()

                        raw_df = _build_raw_evidence(st.session_state['last_dfcy'])
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")

                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_current"  # ê³ ìœ  í‚¤(í˜„ì¬ ê²°ê³¼)
                        )

                        st.caption(f"â± ì´ ì†Œìš”: {time.perf_counter()-t0:.1f}s")
                        rendered_report = True

                    # === ìºì‹œëœ ì´ì „ ê²°ê³¼ ë Œë”(ë²„íŠ¼ ë¯¸í´ë¦­ ì‹œì—ë§Œ) ===
                    if st.session_state.get('last_report') and not btn:
                        st.success("ë³´ê³ ì„œê°€ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                        st.markdown(st.session_state['last_report'])
                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])
                        # RAW ë¯¸ë¦¬ë³´ê¸° + ZIP ë²„íŠ¼ ì¬ì¶œë ¥
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()
                        raw_df = _build_raw_evidence(st.session_state.get('last_dfcy', pd.DataFrame()))
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")
                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_cached"  # ê³ ìœ  í‚¤(ìºì‹œ ê²°ê³¼)
                        )
                        # Cluster quality card (if available)
                        cq = st.session_state.get("cluster_quality")
                        if cq:
                            st.markdown("---")
                            st.subheader("í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ìš”ì•½")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("Noise rate", f"{cq['noise_rate']*100:.1f}%")
                            c2.metric("#Clusters", f"{cq['n_clusters']}")
                            c3.metric("Avg size", f"{cq['avg_size']:.1f}")
                            c4.metric("Rescued", f"{cq['rescued_rate']*100:.1f}%")
                            st.caption(f"Model: {cq['model']} | UMAP: {'on' if cq['umap'] else 'off'} | Ï„={cq['tau']:.2f} | N={cq['N']:,}")
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            if st.button("ë§¤í•‘ ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸°"):
                st.session_state.mapping_confirmed = False
                st.rerun()
else:
    st.info("â¬…ï¸ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")





==============================
ğŸ“„ FILE: config.py
==============================

LLM_MODEL = "gpt-4o"
LLM_TEMPERATURE = 0.2
LLM_JSON_MODE = True
PM_DEFAULT = 500_000_000  # Project-wide Performance Materiality (KRW)

EMBED_BATCH_SIZE = 256
EMBED_CACHE_DIR = ".cache/embeddings"

# í›ˆë‹˜ ê²°ì • ë°˜ì˜ âœ…
SHAP_TOP_N_PER_ACCOUNT_DEFAULT = 25   # ì‚¬ìš©ì UIì—ì„œ 20~30 ë²”ìœ„ ì„ íƒ ê°€ëŠ¥
CYCLE_RECOMMENDER = "llm_only"        # LLM 100% ìë™ ì¶”ì²œ
PM_DEFAULT = PM_DEFAULT              # (kept above; single source of truth)

# ---- NEW: Embedding & Clustering defaults ----
# Embedding model switch (Small by default; Large improves semantics at higher cost)
EMB_MODEL_SMALL = "text-embedding-3-small"
EMB_MODEL_LARGE = "text-embedding-3-large"
EMB_USE_LARGE_DEFAULT = False           # UI/auto-upscale can override per run

# UMAP threshold (apply UMAP â†’ HDBSCAN only when N is large)
UMAP_APPLY_THRESHOLD = 8000             # set 0/None to disable
UMAP_N_COMPONENTS = 20
UMAP_N_NEIGHBORS = 30
UMAP_MIN_DIST = 0.0

# HDBSCAN noise-rescue cosine threshold
HDBSCAN_RESCUE_TAU = 0.75               # 0.72~0.78 usually works well

# Adaptive clustering knobs (computed per run, not static)
# min_cluster_size = max(8, int(sqrt(N))); min_samples = max(2, int(0.5 * min_cluster_size))

# ===== NEW: Materiality & Risk Weights =====
# === í†µí•© ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜(ê³ ì •ê°’; v2.0-RC ë™ê²°) ===
# ì ìˆ˜ = 0.5*A(|Z|ì •ê·œí™”) + 0.4*F(PM ëŒ€ë¹„ ë¹„ìœ¨ capped 1) + 0.1*K(PM ì´ˆê³¼=1)
RISK_WEIGHT_A = 0.5
RISK_WEIGHT_F = 0.4
RISK_WEIGHT_K = 0.1

# --- NEW: Z-Score â†’ sigmoid ìŠ¤ì¼€ì¼ ì¡°ì • (ë¡œë“œë§µ í˜¸í™˜)
# anomaly_score = sigmoid(|Z| / Z_SIGMOID_DIVISOR)
# ë¡œë“œë§µ ê¶Œê³ : 3.0 (ê³¼ë„ í¬í™” ì™„í™”)
Z_SIGMOID_DIVISOR = 3.0
Z_SIGMOID_SCALE = Z_SIGMOID_DIVISOR  # í•˜ìœ„í˜¸í™˜

# --- í‘œì¤€ íšŒê³„ ì‚¬ì´í´ (STANDARD_ACCOUNTING_CYCLES) ---
# í‚¤: ì‚¬ì´í´ ì‹ë³„ì, ê°’: í•´ë‹¹ ì‚¬ì´í´ì— ë§¤í•‘ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³„ì •ëª… í‚¤ì›Œë“œ(ë¶€ë¶„ì¼ì¹˜)
# *í•œêµ­ì–´/ì˜ë¬¸ í˜¼ìš©. í•„ìš” ì‹œ í”„ë¡œì íŠ¸ ë„ë©”ì¸ì— ë§ì¶° ë³´ê°•í•˜ì„¸ìš”.
STANDARD_ACCOUNTING_CYCLES = {
    "Cash": ["í˜„ê¸ˆ", "ì˜ˆê¸ˆ", "ë‹¨ê¸°ê¸ˆìœµ", "Cash", "Bank"],
    "Revenue": ["ë§¤ì¶œ", "íŒë§¤ìˆ˜ìµ", "Sales", "Revenue"],
    "Receivables": ["ë§¤ì¶œì±„ê¶Œ", "ì™¸ìƒë§¤ì¶œê¸ˆ", "ë¯¸ìˆ˜ê¸ˆ", "Receivable", "A/R"],
    "Inventory": ["ì¬ê³ ", "ìƒí’ˆ", "ì œí’ˆ", "ì›ì¬ë£Œ", "ì¬ê³µí’ˆ", "Inventory"],
    "Payables": ["ë§¤ì…ì±„ë¬´", "ì™¸ìƒë§¤ì…ê¸ˆ", "ë¯¸ì§€ê¸‰ê¸ˆ", "Payable", "A/P"],
    "Expenses": ["ë³µë¦¬í›„ìƒë¹„", "ê¸‰ì—¬", "ì„ì°¨ë£Œ", "ì ‘ëŒ€ë¹„", "ê°ê°€ìƒê°ë¹„", "ë¹„ìš©", "Expense"],
    "FixedAssets": ["ìœ í˜•ìì‚°", "ê°ê°€ìƒê°ëˆ„ê³„", "ê¸°ê³„ì¥ì¹˜", "ê±´ë¬¼", "ë¹„í’ˆ", "PPE", "Fixed Asset"],
    "Equity": ["ìë³¸ê¸ˆ", "ì´ìµì‰ì—¬ê¸ˆ", "ìë³¸ì‰ì—¬ê¸ˆ", "Equity", "Capital"],
}

# --- Provisional rule naming (ë„ë©”ì¸ í•©ì˜ ì „) ---
PROVISIONAL_RULE_VERSION = "v1.0"
PROVISIONAL_RULE_NAME = f"ì ì • ê¸°ì¤€({PROVISIONAL_RULE_VERSION})"

def provisional_risk_formula_str() -> str:
    """UI/ë¦¬í¬íŠ¸ ì•ˆë‚´ë¬¸ì— ì“°ì¼ ê°€ì¤‘ì¹˜ ìš”ì•½ ë¬¸ìì—´ì„ ë™ì ìœ¼ë¡œ ìƒì„±"""
    a = int(RISK_WEIGHT_A * 100)
    f = int(RISK_WEIGHT_F * 100)
    k = int(RISK_WEIGHT_K * 100)
    return f"í†µê³„ì  ì´ìƒ({a}%) + ì¬ë¬´ì  ì˜í–¥({f}%) + KIT ì—¬ë¶€({k}%)"

# ë¦¬í¬íŠ¸(ìµœì¢…ë³¸) í¬í•¨ ì¡°ê±´ ë…¸ë¸Œ (ê¸°ë³¸: í¬í•¨ ì•ˆ í•¨)
INCLUDE_RISK_MATRIX_SUMMARY_IN_FINAL = False
# â€˜ìƒìœ„ Nâ€™ ê²°ê³¼ê°€ ì´ ê°’ ë¯¸ë§Œì´ë©´ ìµœì¢…ë³¸ì— ìƒëµ (ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ì—” ìœ ì§€)
RISK_MATRIX_SECTION_MIN_ITEMS = 3

# --- TimeSeries forecast knobs ---
FORECAST_MIN_POINTS = 8         # Prophet/ARIMA ì‚¬ìš© ê¶Œì¥ ìµœì†Œ ê¸¸ì´(ê¶Œê³ ì¹˜)
ARIMA_DEFAULT_ORDER = (1,1,1)

# --- User overrides for STANDARD_ACCOUNTING_CYCLES ---
CYCLES_USER_OVERRIDES_PATH = ".cache/cycles_overrides.json"



==============================
ğŸ“„ FILE: analysis/anomaly.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import List, Optional
from utils.helpers import find_column_by_keyword


def compute_amount_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ë°œìƒì•¡(ì ˆëŒ€ ê·œëª¨) / ìˆœì•¡(ì°¨-ëŒ€) ê³„ì‚°."""
    dcol = find_column_by_keyword(df.columns, 'ì°¨ë³€')
    ccol = find_column_by_keyword(df.columns, 'ëŒ€ë³€')
    df = df.copy()
    if not dcol or not ccol:
        df['ë°œìƒì•¡'] = 0.0; df['ìˆœì•¡'] = 0.0; df['ê±°ë˜ê¸ˆì•¡'] = 0.0
        return df
    d = pd.to_numeric(df[dcol], errors='coerce').fillna(0.0)
    c = pd.to_numeric(df[ccol], errors='coerce').fillna(0.0)
    row_amt = np.where((d > 0) & (c == 0), d,
              np.where((c > 0) & (d == 0), c,
              np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
    df['ë°œìƒì•¡'] = row_amt
    df['ìˆœì•¡']  = d - c
    df['ê±°ë˜ê¸ˆì•¡'] = df['ìˆœì•¡']
    return df


def calculate_grouped_stats_and_zscore(df: pd.DataFrame, target_accounts: List[str], data_type: str = "ë‹¹ê¸°") -> pd.DataFrame:
    """ì„ íƒ ê³„ì • ê·¸ë£¹ì˜ ë°œìƒì•¡ ë¶„í¬ ê¸°ì¤€ Z-Score ì‚°ì¶œ."""
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    df = compute_amount_columns(df.copy())
    if not acct_col:
        df['Z-Score'] = 0.0
        return df
    is_target = df[acct_col].astype(str).isin([str(x) for x in target_accounts])
    tgt = df.loc[is_target, 'ë°œìƒì•¡'].astype(float)
    df['Z-Score'] = 0.0
    if tgt.empty:
        return df
    mu = float(tgt.mean()); std = float(tgt.std(ddof=1))
    if std and std > 0:
        df.loc[is_target, 'Z-Score'] = (df.loc[is_target, 'ë°œìƒì•¡'] - mu) / std
    else:
        med = float(tgt.median()); mad = float((np.abs(tgt - med)).median())
        df.loc[is_target, 'Z-Score'] = 0.0 if mad == 0 else 0.6745 * (df.loc[is_target, 'ë°œìƒì•¡'] - med) / mad
    return df

# --- NEW: ensure_zscore ---
def ensure_zscore(df: pd.DataFrame, account_codes: List[str]):
    """
    Recompute Z-Score for the given account subset and return (df, ok).
    ok=True only if Z-Score column exists and has at least one non-null value.
    """
    df2 = calculate_grouped_stats_and_zscore(df.copy(), target_accounts=[str(x) for x in account_codes] if account_codes else [])
    z = df2.get('Z-Score')
    ok = (z is not None) and (z.notna().any())
    return df2, bool(ok)




# === (ADD) v0.18: ModuleResult ëŸ¬ë„ˆ ===
from analysis.contracts import ModuleResult, EvidenceDetail
from config import PM_DEFAULT, RISK_WEIGHT_A, RISK_WEIGHT_F, RISK_WEIGHT_K, Z_SIGMOID_SCALE, Z_SIGMOID_DIVISOR
import plotly.express as px
import numpy as np
import pandas as pd


def _z_bins_025_sigma(series: pd.Series):
    """0.25Ïƒ ê°„ê²© bin (Â±3Ïƒ í…Œì¼ í¬í•¨)."""
    # ê²½ê³„ì— +3.0 í¬í•¨(+inf í…Œì¼) â†’ ì´ bin ìˆ˜ = 24(ì½”ì–´) + 2(í…Œì¼) = 26
    edges = [-np.inf] + [round(x, 2) for x in np.arange(-3.0, 3.0 + 0.25, 0.25)] + [np.inf]
    core_lefts = [x for x in np.arange(-3.0, 3.0, 0.25)]  # 24ê°œ
    labels_mid = [f"{a:.2f}~{a+0.25:.2f}Ïƒ" for a in core_lefts]
    labels = ["â‰¤-3Ïƒ"] + labels_mid + ["â‰¥3Ïƒ"]               # 26ê°œ
    cats = pd.cut(
        series.astype(float),
        bins=edges,
        labels=labels,
        right=False,
        include_lowest=True,
    )
    # ë¹ˆ êµ¬ê°„ë„ 0ìœ¼ë¡œ ì±„ì›Œ ìˆœì„œ ìœ ì§€
    counts = cats.value_counts(sort=False).reindex(labels, fill_value=0)
    out = pd.DataFrame({"êµ¬ê°„": labels, "ê±´ìˆ˜": counts.values})
    order = labels
    return out, order


def _sigmoid(x: float) -> float:
    import math
    return 1.0 / (1.0 + math.exp(-x))


def _risk_from(z_abs: float, amount: float, pm: float):
    """ë¦¬ìŠ¤í¬ ì ìˆ˜ êµ¬ì„±ìš”ì†Œ ê³„ì‚°.
    - a: ì‹œê·¸ëª¨ì´ë“œ ì •ê·œí™”ëœ ì´íƒˆ ê°•ë„(|Z|/scale). scaleì€ ì„¤ì •ê°’.
    - f: PM ëŒ€ë¹„ ê¸ˆì•¡ë¹„ìœ¨(0~1ë¡œ ìº¡). PMì´ 0/ìŒìˆ˜ë©´ 0ìœ¼ë¡œ ê°•ì œ.
    - k: Key Item í”Œë˜ê·¸(PM ì´ˆê³¼ì‹œ 1). PMì´ 0/ìŒìˆ˜ë©´ 0ìœ¼ë¡œ ê°•ì œ.
    """
    # ìš°ì„ ìˆœìœ„: Z_SIGMOID_DIVISOR(ì‹ ê·œ ë…¸ë¸Œ) > Z_SIGMOID_SCALE(êµ¬ëª…). ê¸°ë³¸ 1.0
    div = None
    try:
        div = float(Z_SIGMOID_DIVISOR)
    except Exception:
        div = None
    if not div or div <= 0:
        try:
            div = float(Z_SIGMOID_SCALE)
        except Exception:
            div = 1.0
    if not div or div <= 0:
        div = 1.0
    a = _sigmoid(float(abs(z_abs)) / float(div))      # anomaly_score
    # PM ê°€ë“œ: pm<=0ì´ë©´ f=0, k=0
    if pm is None or float(pm) <= 0:
        f = 0.0
        k = 0.0
    else:
        f = min(1.0, abs(float(amount)) / float(pm))  # PM ratio (capped at 1)
        k = 1.0 if abs(float(amount)) >= float(pm) else 0.0
    score = RISK_WEIGHT_A * a + RISK_WEIGHT_F * f + RISK_WEIGHT_K * k
    return a, f, k, score


def _assertions_for_row(z_val: float) -> List[str]:
    # ê¸°ë³¸ ê·œì¹™: AëŠ” í•­ìƒ í¬í•¨. ìŒì˜ í° ì´íƒˆ(C), ì–‘ì˜ í° ì´íƒˆ(E)ì„ ë³´ê°•.
    out = {"A"}
    try:
        if float(z_val) <= -2.0:
            out.add("C")
        if float(z_val) >=  2.0:
            out.add("E")
    except Exception:
        pass
    return sorted(out)


def run_anomaly_module(lf, target_accounts=None, topn=20, pm_value: Optional[float] = None):
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    if not acct_col:
        return ModuleResult("anomaly", {}, {}, {}, [], ["ê³„ì •ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."])

    # ëŒ€ìƒ ê³„ì • ì„œë¸Œì…‹
    if target_accounts:
        codes = [str(x) for x in target_accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    # Z-Score ê³„ì‚°
    df = calculate_grouped_stats_and_zscore(df, target_accounts=df[acct_col].astype(str).unique().tolist())
    if 'íšŒê³„ì¼ì' in df.columns:
        df['ì—°ì›”'] = df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)

    # ì´ìƒì¹˜ í”Œë˜ê·¸ (Â±3Ïƒ)
    df['is_outlier'] = df['Z-Score'].abs() >= 3

    # ì´ìƒì¹˜ í›„ë³´ í…Œì´ë¸” (ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒìœ„)
    out_cols = [c for c in ['row_id','íšŒê³„ì¼ì','ì—°ì›”','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','Z-Score'] if c in df.columns]
    cand = (df.assign(absz=df['Z-Score'].abs())
              .sort_values('absz', ascending=False)
              .drop(columns=['absz'])
              .head(int(topn)))
    table = cand[out_cols + (['is_outlier'] if 'is_outlier' in cand.columns and 'is_outlier' not in out_cols else [])] if out_cols else cand

    # === EvidenceDetail ìƒì„± (KIT + |Z| ê¸°ì¤€) ===
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    ev_rows: List[EvidenceDetail] = []
    # ì¦ê±° ì±„ì§‘ ëŒ€ìƒ: (1) PM ì´ˆê³¼ or (2) |Z|>=2.5 or (3) ìƒìœ„ topn
    mask_key = df['ë°œìƒì•¡'].abs() >= pm if 'ë°œìƒì•¡' in df.columns else pd.Series(False, index=df.index)
    mask_z   = df['Z-Score'].abs() >= 2.5 if 'Z-Score' in df.columns else pd.Series(False, index=df.index)
    idx_sel  = set(df.index[mask_key | mask_z].tolist()) | set(table.index.tolist())
    sub = df.loc[sorted(idx_sel)].copy() if len(idx_sel)>0 else df.head(0).copy()
    for _, r in sub.iterrows():
        z  = float(r.get('Z-Score', 0.0)) if pd.notna(r.get('Z-Score', np.nan)) else 0.0
        za = abs(z)
        amt = float(r.get('ë°œìƒì•¡', 0.0))
        a, f, k, score = _risk_from(za, amt, pm)
        ev_rows.append(EvidenceDetail(
            row_id=str(r.get('row_id','')),
            reason=f"|Z|={za:.2f}",
            anomaly_score=float(a),
            financial_impact=abs(amt),
            risk_score=float(score),
            is_key_item=bool(abs(amt) >= pm),
            impacted_assertions=_assertions_for_row(z),
            links={
                "account_code": str(r.get('ê³„ì •ì½”ë“œ','')),
                "account_name": str(r.get('ê³„ì •ëª…','')),
                "period_tag": str(r.get('period_tag','')),
            }
        ))

    # step-Ïƒ bin ë¶„í¬ ë§‰ëŒ€
    figures = {}
    try:
        dist_df, order = _z_bins_025_sigma(df['Z-Score'])
        total_n = int(len(df))
        outlier_rate = float((df['Z-Score'].abs() >= 3).mean() * 100) if total_n else 0.0
        title = f"Z-Score ë¶„í¬ (0.25Ïƒ bin, Â±3Ïƒ ì§‘ê³„) â€” N={total_n:,}, outlierâ‰ˆ{outlier_rate:.1f}%"
        fig = px.bar(dist_df, x='êµ¬ê°„', y='ê±´ìˆ˜', title=title)
        fig.update_yaxes(separatethousands=True)
        fig.update_layout(bargap=0.10)
        figures = {"zscore_hist": fig}
    except Exception:
        pass

    summary = {
        "n_rows": int(len(df)),
        "n_candidates": int(len(table)),
        "accounts": sorted(df[acct_col].astype(str).unique().tolist()),
        "period_tag_coverage": dict(df.get('period_tag', pd.Series(dtype=str)).value_counts()) if 'period_tag' in df.columns else {}
    }
    # Evidence ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸”(ì„ íƒ)
    try:
        import pandas as _pd
        ev_tbl = _pd.DataFrame([{
            "row_id": e.row_id,
            "ê³„ì •ì½”ë“œ": e.links.get("account_code",""),
            "ê³„ì •ëª…":   e.links.get("account_name",""),
            "risk_score": e.risk_score,
            "is_key_item": e.is_key_item,
            "impacted": ",".join(e.impacted_assertions),
            "reason": e.reason,
        } for e in ev_rows]).sort_values("risk_score", ascending=False).head(100)
    except Exception:
        ev_tbl = None

    return ModuleResult(
        name="anomaly",
        summary=summary,
        tables={"anomaly_top": table, **({"evidence_preview": ev_tbl} if ev_tbl is not None else {})},
        figures=figures,
        evidences=ev_rows,
        warnings=[]
    )


==============================
ğŸ“„ FILE: analysis/assertion_risk.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from analysis.contracts import ModuleResult, EvidenceDetail, ASSERTIONS


HEATMAP_BS_RISK = "max"  # or "balance_only" / "weighted"


def _agg_bs_risk(rows: pd.DataFrame) -> float:
    """BS ì…€ ìœ„í—˜ë„ ì§‘ê³„ ê·œì¹™(ê¸°ë³¸ max).
    - EvidenceDetail.measureê°€ ì œê³µë˜ëŠ” ê²½ìš°ì—ë§Œ ì ìš© ê°€ëŠ¥.
    - 'weighted'ëŠ” balance 0.6, flow 0.4 ê°€ì¤‘.
    """
    if rows is None or rows.empty:
        return 0.0
    if HEATMAP_BS_RISK == "balance_only":
        r = rows.loc[rows.get("measure", pd.Series()).eq("balance"), "risk_score"]
        return float(r.max() if not r.empty else rows["risk_score"].max())
    if HEATMAP_BS_RISK == "weighted":
        w = rows.get("measure", pd.Series(index=rows.index)).map({"balance": 0.6, "flow": 0.4}).fillna(0.5)
        try:
            return float(np.average(rows["risk_score"].astype(float), weights=w))
        except Exception:
            return float(rows["risk_score"].max())
    return float(rows["risk_score"].max())


def build_matrix(modules: List[ModuleResult]):
    """
    ëª¨ë“ˆ EvidenceDetail â†’ (ê³„ì • Ã— ì£¼ì¥) ìµœëŒ€ risk_score ë§¤íŠ¸ë¦­ìŠ¤ + ë“œë¦´ë‹¤ìš´ ë§µ
    ë°˜í™˜: (matrix_df[account_name x ASSERTIONS], evidence_map[(acct, asrt)] -> [row_id...])
    """
    bucket_rows: Dict[Tuple[str,str], List[Dict]] = {}
    emap: Dict[Tuple[str,str], List[str]] = {}
    accts: set[str] = set()

    for mod in modules:
        for ev in (mod.evidences or []):
            acct = ev.links.get("account_name") or ev.links.get("account_code") or "UNMAPPED"
            accts.add(acct)
            for a in (ev.impacted_assertions or []):
                key = (acct, a)
                bucket_rows.setdefault(key, []).append({
                    "risk_score": float(ev.risk_score),
                    "measure": getattr(ev, "measure", None)
                })
                emap.setdefault(key, []).append(str(ev.row_id))

    idx = sorted(accts)
    mat = pd.DataFrame(index=idx, columns=ASSERTIONS, data=0.0)
    for (acct, asrt), rows in bucket_rows.items():
        df = pd.DataFrame(rows)
        mat.loc[acct, asrt] = _agg_bs_risk(df) if not df.empty else 0.0
    return mat.fillna(0.0), emap





==============================
ğŸ“„ FILE: analysis/contracts.py
==============================

from dataclasses import dataclass, field
import pandas as pd
from typing import Dict, List, Any, Optional, Literal
# --- New: Measure íƒ€ì… íŒíŠ¸("flow" ë˜ëŠ” "balance") ---
Measure = Literal["flow", "balance"]


@dataclass(frozen=True)
class LedgerFrame:
    df: pd.DataFrame
    meta: Dict[str, Any]  # ì˜ˆ: {"company": "...", "file_name": "...", "uploaded_at": ...}

# CEAVOP assertions
ASSERTIONS = ["C","E","A","V","O","P"]

@dataclass(frozen=True)
class EvidenceDetail:
    row_id: str
    reason: str                  # e.g., "|Z|=3.1 (CY group mean-based)"
    anomaly_score: float         # 0~1 normalized
    financial_impact: float      # KRW absolute amount
    risk_score: float            # integrated score
    is_key_item: bool            # PM exceed flag
    # --- NEW: measurement basis and sign rule ---
    measure: Measure = "flow"     # "flow"(ì›”ë³„ ë°œìƒì•¡, Î”ì”ì•¡/ìˆœì•¡) ë˜ëŠ” "balance"
    sign_rule: str = "assets/expensesâ†‘=+, liabilities/equityâ†‘=-"
    # --- NEW: ì‹œê³„ì—´ ì˜ˆì¸¡ ë©”íƒ€ (ì˜µì…”ë„) ---
    model: Optional[str] = None           # ì‚¬ìš©ëœ ëª¨ë¸ëª… (ì˜ˆ: EMA/MA/ARIMA/Prophet)
    window_policy: Optional[str] = None   # ì˜ˆ: "PY+CY"
    data_span: Optional[str] = None       # ì˜ˆ: "YYYY-MM ~ YYYY-MM"
    train_months: Optional[int] = None    # í•™ìŠµ ì›” ìˆ˜
    horizon: Optional[int] = None         # ì˜ˆì¸¡ ìˆ˜í‰(ì›”)
    basis_note: Optional[str] = None      # ì˜ˆ: "BSëŠ” ì”ì•¡Â·ë°œìƒì•¡ ë³‘ë ¬ ê³„ì‚°"
    extra: Optional[Dict[str, Any]] = field(default_factory=dict)
    impacted_assertions: List[str] = field(default_factory=list)  # e.g., ["A","C"]
    links: Dict[str, Any] = field(default_factory=dict)           # e.g., {"account_code": "...", "account_name": "..."}

@dataclass(frozen=True)
class ModuleResult:
    name: str
    summary: Dict[str, Any]             # LLM ì…ë ¥ìš© í•µì‹¬ ìˆ˜ì¹˜/ì§€í‘œ
    tables: Dict[str, pd.DataFrame]
    figures: Dict[str, Any]             # plotly Figure
    evidences: List[EvidenceDetail]     # structured evidences
    warnings: List[str]


# ê³µê°œ API ëª…ì‹œ(ìŠ¤í‚¤ë§ˆ ê³ ì •ì— ë„ì›€)
__all__ = [
    "LedgerFrame", "EvidenceDetail", "ModuleResult", "ASSERTIONS"
]




==============================
ğŸ“„ FILE: analysis/correlation.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
import plotly.express as px
from typing import List, Dict, Any, Tuple, Optional
import re
try:
    from services.cycles_store import get_effective_cycles
except Exception:
    # í´ë°±: configì˜ STANDARD_ACCOUNTING_CYCLES ì‚¬ìš©
    try:
        from config import STANDARD_ACCOUNTING_CYCLES as _STD_CYCLES
    except Exception:
        _STD_CYCLES = {}
    def get_effective_cycles():  # type: ignore
        return dict(_STD_CYCLES)
from analysis.contracts import LedgerFrame, ModuleResult
from utils.helpers import find_column_by_keyword


def _monthly_pivot(df: pd.DataFrame, acct_col: str) -> pd.DataFrame:
    """ê³„ì •ì½”ë“œÃ—ì—°ì›” í”¼ë²—(ê±°ë˜ê¸ˆì•¡ í•©ê³„). PL/BS ëª¨ë‘ ì›” íë¦„ ê¸°ì¤€."""
    if 'íšŒê³„ì¼ì' not in df.columns:
        raise ValueError("íšŒê³„ì¼ì í•„ìš”")
    g = (df.assign(ì—°ì›”=df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str))
           .groupby([acct_col, 'ì—°ì›”'])['ê±°ë˜ê¸ˆì•¡'].sum()
           .unstack('ì—°ì›”', fill_value=0.0)
           .sort_index())
    return g
 
def _filter_accounts_for_corr(piv: pd.DataFrame, min_active_months: int = 6) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    - Drop accounts with zero variance across months (std == 0) OR
      with insufficient active months (abs(value)>0 in fewer than min_active_months months).
    - Return filtered pivot and an exclusions dataframe with reasons.
    """
    if piv.empty:
        return piv, pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ì‚¬ìœ ','í™œë™ì›”ìˆ˜','í‘œì¤€í¸ì°¨'])
    std = piv.std(axis=1)
    active = (piv.abs() > 0).sum(axis=1)
    reason = []
    idx = piv.index.astype(str)
    keep = (std > 0) & (active >= int(min_active_months))
    for code, s, a, k in zip(idx, std, active, keep):
        if k:
            continue
        r = []
        if s == 0:
            r.append("ë³€ë™ì—†ìŒ(í‘œì¤€í¸ì°¨ 0)")
        if a < int(min_active_months):
            r.append(f"í™œë™ ì›” ë¶€ì¡±(<{int(min_active_months)})")
        reason.append((code, " & ".join(r) if r else "ì œì™¸", int(a), float(s)))
    excluded = pd.DataFrame(reason, columns=['ê³„ì •ì½”ë“œ','ì‚¬ìœ ','í™œë™ì›”ìˆ˜','í‘œì¤€í¸ì°¨'])
    return piv.loc[keep], excluded


def _infer_cycle(account_name: str) -> Optional[str]:
    """
    STANDARD_ACCOUNTING_CYCLES ê¸°ë°˜ì˜ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤í•‘.
    ê°€ì¥ ë¨¼ì € ë§¤ì¹­ë˜ëŠ” ì‚¬ì´í´ì„ ë°˜í™˜(ìš°ì„ ìˆœìœ„: dict ì •ì˜ ìˆœì„œ).
    """
    name = str(account_name or "").lower()
    for cycle, keywords in get_effective_cycles().items():
        for kw in keywords:
            if kw and re.search(re.escape(str(kw).lower()), name):
                return cycle
    return None


def map_accounts_to_cycles(accounts: List[str]) -> Dict[str, Optional[str]]:
    """ë°°ì¹˜ ë§¤í•‘: ê³„ì •ëª… ë¦¬ìŠ¤íŠ¸ â†’ {ê³„ì •ëª…: ì‚¬ì´í´(or None)}"""
    return {acc: _infer_cycle(acc) for acc in accounts}


def run_correlation_module(lf: LedgerFrame, accounts: List[str] | None = None, corr_threshold: float = 0.7, min_active_months: int = 6) -> ModuleResult:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    if not acct_col:
        return ModuleResult("correlation", {}, {}, {}, [], ["ê³„ì •ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."])

    # ëŒ€ìƒ ê³„ì • í•„í„°
    if accounts:
        codes = [str(a) for a in accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    if df.empty:
        return ModuleResult("correlation", {}, {}, {}, [], ["ì„ íƒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."])

    piv = _monthly_pivot(df, acct_col)
    piv_f, excluded = _filter_accounts_for_corr(piv, min_active_months=min_active_months)
    if piv_f.shape[0] < 2:
        warn = "ìƒê´€ì„ ê³„ì‚°í•  ê³„ì •ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤."
        if not excluded.empty:
            warn += f" (ì œì™¸ëœ ê³„ì • {len(excluded)}ê°œ: ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)"
        return ModuleResult("correlation", {}, {"excluded_accounts": excluded}, {}, [], [warn])

    corr = piv_f.T.corr(method='pearson')  # ê³„ì •Ã—ê³„ì •
    fig = px.imshow(corr, text_auto=False, title="ê³„ì • ê°„ ì›”ë³„ ìƒê´€ íˆíŠ¸ë§µ", labels=dict(x="ê³„ì •ì½”ë“œ", y="ê³„ì •ì½”ë“œ", color="ìƒê´€ê³„ìˆ˜"), aspect='auto')
    fig.update_coloraxes(cmin=-1, cmax=1)
    fig.update_xaxes(type='category')
    fig.update_yaxes(type='category')

    # ì„ê³„ ìƒê´€ìŒ í…Œì´ë¸”
    pairs: List[Tuple[str,str,float]] = []
    idx = corr.index.astype(str).tolist()
    for i in range(len(idx)):
        for j in range(i+1, len(idx)):
            r = float(corr.iloc[i, j])
            if abs(r) >= corr_threshold:
                pairs.append((idx[i], idx[j], r))
    pairs_df = pd.DataFrame(pairs, columns=['ê³„ì •ì½”ë“œ_A','ê³„ì •ì½”ë“œ_B','ìƒê´€ê³„ìˆ˜']).sort_values('ìƒê´€ê³„ìˆ˜', ascending=False)

    # ì‚¬ì´í´ ë§¤í•‘ ìš”ì•½(ê³„ì •ëª… í•„ìš”í•˜ë¯€ë¡œ ë³„ë„ í‘œì—ì„œëŠ” ê³„ì •ëª… ë§¤í•‘ í•„ìš” ì‹œ upstreamì—ì„œ ì²˜ë¦¬)
    summary = {
        "n_accounts": int(corr.shape[0]),
        "n_pairs_over_threshold": int(len(pairs_df)),
        "corr_threshold": float(corr_threshold)
    }
    return ModuleResult(
        name="correlation",
        summary=summary,
        tables={"strong_pairs": pairs_df, "corr_matrix": corr, "excluded_accounts": excluded},
        figures={"heatmap": fig},
        evidences=[],
        warnings=([f"ì œì™¸ëœ ê³„ì • {len(excluded)}ê°œ(ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)."] if not excluded.empty else [])
    )





==============================
ğŸ“„ FILE: analysis/embedding.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
import time
from typing import Dict, List, Optional, Tuple
# --- KDMeans ê¸°ë°˜ HDBSCAN ëŒ€ì²´ ì‚¬ìš© ---
from analysis.kdmeans_shim import HDBSCAN   # (ì£¼ì˜) ì‹¤ì œë¡œëŠ” KMeans ê¸°ë°˜
_HAS_HDBSCAN = True
# ---------------------------------------

from utils.helpers import find_column_by_keyword
from services.cache import get_or_embed_texts
from config import (
    EMB_MODEL_SMALL, EMB_MODEL_LARGE, EMB_USE_LARGE_DEFAULT,
    UMAP_APPLY_THRESHOLD, UMAP_N_COMPONENTS, UMAP_N_NEIGHBORS, UMAP_MIN_DIST,
    HDBSCAN_RESCUE_TAU,
)

# Embedding call defaults (can be overridden via pick_emb_model / params)
EMB_BATCH_SIZE = 128
EMB_TIMEOUT = 60
EMB_MAX_RETRY = 4
EMB_TRUNC_CHARS = 2000


def embed_texts_batched(
    texts: List[str],
    client,
    model: str,
    batch_size: int = EMB_BATCH_SIZE,
    timeout: int = EMB_TIMEOUT,
    max_retry: int = EMB_MAX_RETRY,
    trunc_chars: int = EMB_TRUNC_CHARS,
) -> Dict[str, List[float]]:
    """ë°°ì¹˜ ì„ë² ë”© ìœ í‹¸. {ì›ë³¸ë¬¸ìì—´: ë²¡í„°} ë°˜í™˜."""
    if not texts:
        return {}
    san = []
    for t in texts:
        s = t if isinstance(t, str) else str(t)
        san.append(s[:trunc_chars] if trunc_chars and len(s) > trunc_chars else s)

    # Use persistent cache (SQLite per model)
    return get_or_embed_texts(
        san, client, model=model, batch_size=batch_size, timeout=timeout, max_retry=max_retry
    )


def _clean_text_series(s: pd.Series) -> pd.Series:
    """Lightweight denoising: collapse long numbers, squeeze spaces, trim."""
    s = s.astype(str)
    s = s.str.replace(r"\d{8,}", "#NUM", regex=True)
    s = s.str.replace(r"\s+", " ", regex=True).str.strip()
    return s

def ensure_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure df['embedding_text'] exists (desc+vendor) and is cleaned."""
    if 'embedding_text' not in df.columns:
        desc = df['ì ìš”'].fillna('').astype(str) if 'ì ìš”' in df.columns else ''
        cp   = df['ê±°ë˜ì²˜'].fillna('').astype(str) if 'ê±°ë˜ì²˜' in df.columns else ''
        df['embedding_text'] = desc + " (ê±°ë˜ì²˜: " + cp + ")"
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def _amount_bucket(a: float) -> str:
    a = float(abs(a))
    if a < 1_000_000:   return "1ë°±ë§Œ ë¯¸ë§Œ"
    if a < 10_000_000:  return "1ì²œë§Œ ë¯¸ë§Œ"
    if a < 100_000_000: return "1ì–µì› ë¯¸ë§Œ"
    if a < 500_000_000: return "5ì–µì› ë¯¸ë§Œ"
    if a < 1_000_000_000:return "10ì–µì› ë¯¸ë§Œ"
    if a < 5_000_000_000:return "50ì–µì› ë¯¸ë§Œ"
    return "50ì–µì› ì´ìƒ"


def ensure_rich_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """ì ìš”+ê±°ë˜ì²˜+ì›”+ê¸ˆì•¡êµ¬ê°„+ì°¨/ëŒ€ ì„±ê²©ì„ ì¡°í•©í•´ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±."""
    # ë°œìƒì•¡/ìˆœì•¡ì€ anomaly.compute_amount_columnsë¥¼ ì“°ë©´ ìˆœí™˜ importê°€ ìƒê¹€ â†’ ìµœì†Œ í•„ë“œë§Œ ê³„ì‚°
    def _compute_amount_cols(_df: pd.DataFrame) -> pd.DataFrame:
        dcol = find_column_by_keyword(_df.columns, 'ì°¨ë³€')
        ccol = find_column_by_keyword(_df.columns, 'ëŒ€ë³€')
        if not dcol or not ccol:
            _df['ë°œìƒì•¡'] = 0.0; _df['ìˆœì•¡'] = 0.0
            return _df
        d = pd.to_numeric(_df[dcol], errors='coerce').fillna(0.0)
        c = pd.to_numeric(_df[ccol], errors='coerce').fillna(0.0)
        row_amt = np.where((d > 0) & (c == 0), d,
                  np.where((c > 0) & (d == 0), c,
                  np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
        _df['ë°œìƒì•¡'] = row_amt
        _df['ìˆœì•¡']  = d - c
        return _df

    df = _compute_amount_cols(df.copy())
    month = df['íšŒê³„ì¼ì'].dt.month.fillna(0).astype(int).astype(str).str.zfill(2) if 'íšŒê³„ì¼ì' in df.columns else "00"
    amtbin = df['ë°œìƒì•¡'].apply(_amount_bucket)
    sign   = np.where(df['ìˆœì•¡'] >= 0, "ì°¨ë³€ì„±", "ëŒ€ë³€ì„±")
    desc = df['ì ìš”'].fillna('').astype(str) if 'ì ìš”' in df.columns else ''
    cp   = df['ê±°ë˜ì²˜'].fillna('').astype(str) if 'ê±°ë˜ì²˜' in df.columns else ''
    df['embedding_text'] = desc + " | ê±°ë˜ì²˜:" + cp + " | ì›”:" + month + " | ê¸ˆì•¡êµ¬ê°„:" + amtbin + " | ì„±ê²©:" + sign
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def perform_embedding_only(df: pd.DataFrame, client, text_col: str = 'embedding_text', *, use_large: bool|None=None) -> pd.DataFrame:
    """df[text_col]ì„ ë°°ì¹˜ ì„ë² ë”©í•´ì„œ df['vector'] ì¶”ê°€"""
    if df.empty: return df
    if text_col not in df.columns:
        raise ValueError(f"ì„ë² ë”© í…ìŠ¤íŠ¸ ì»¬ëŸ¼ '{text_col}'ì´ ì—†ìŠµë‹ˆë‹¤.")
    uniq = df[text_col].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(uniq, client, model=model)
    df = df.copy()
    df['vector'] = df[text_col].astype(str).map(mapping)
    # ëˆ„ë½ ë³´ê°• ì‹œë„
    if df['vector'].isna().any():
        miss = df.loc[df['vector'].isna(), text_col].astype(str).unique().tolist()
        if miss:
            fb = embed_texts_batched(miss, client, model=model)
            df.loc[df['vector'].isna(), 'vector'] = df.loc[df['vector'].isna(), text_col].astype(str).map(fb)
    return df


def _l2_normalize(X: np.ndarray) -> np.ndarray:
    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)

def _adaptive_hdbscan(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    n = int(X.shape[0])
    model = HDBSCAN(
        n_clusters=None,           # ìë™ k ì„ íƒ(ì‹¤ë£¨ì—£ ê¸°ë°˜)
        min_cluster_size=max(8, int(np.sqrt(max(2, n)))),  # ë„ˆë¬´ ë§ì€ êµ°ì§‘ ë°©ì§€
        max_k=None,                # í•„ìš”ì‹œ ìƒí•œ ì§€ì • ê°€ëŠ¥
        k_search="silhouette",     # íœ´ë¦¬ìŠ¤í‹± ëŒ€ì‹  ì‹¤ë£¨ì—£ ê¸°ë°˜
        sample_size=2000,
        random_state=42,
        n_init="auto",
    )
    model.fit(X)
    labels = model.labels_.astype(int)
    try:
        probs = model.probabilities_.astype(float)
    except Exception:
        probs = np.ones(shape=(n,), dtype=float)
    return labels, probs

def _optional_umap(X: np.ndarray, enabled: Optional[bool] = None) -> Tuple[np.ndarray, bool]:
    """Dimensionality reduction control.
    - enabled=True: always try UMAP; on failure return (X, False)
    - enabled=False: skip â†’ (X, False)
    - enabled=None: apply only if dataset size >= UMAP_APPLY_THRESHOLD
    Returns (X_or_reduced, used_flag).
    """
    if enabled is False:
        return X, False
    force = enabled is True
    try:
        thr = int(UMAP_APPLY_THRESHOLD) if UMAP_APPLY_THRESHOLD else None
    except Exception:
        thr = None
    if force or (thr and X.shape[0] >= thr):
        try:
            import umap
            reducer = umap.UMAP(
                n_components=int(UMAP_N_COMPONENTS),
                n_neighbors=int(UMAP_N_NEIGHBORS),
                min_dist=float(UMAP_MIN_DIST),
                random_state=42,
                metric="euclidean",
            )
            return reducer.fit_transform(X), True
        except Exception:
            return X, False
    return X, False

def _rescue_noise(df: pd.DataFrame, tau: float = HDBSCAN_RESCUE_TAU) -> pd.DataFrame:
    # KDMeansëŠ” ë…¸ì´ì¦ˆ(-1) ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ êµ¬ì¡°ì  ë¦¬ìŠ¤í ë¶ˆí•„ìš”
    return df

def pick_emb_model(use_large: bool|None=None) -> str:
    """Select embedding model (small/large)."""
    flag = EMB_USE_LARGE_DEFAULT if use_large is None else bool(use_large)
    return EMB_MODEL_LARGE if flag else EMB_MODEL_SMALL


def postprocess_cluster_names(df: pd.DataFrame) -> pd.DataFrame:
    """(ê°„ì†Œí™”) LLMì´ ì¤€ cluster_nameì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤. íƒœê·¸/ì ‘ë¯¸ì‚¬ ë¯¸ë¶€ì—¬."""
    return df


def _llm_cluster_name(client, model: str, samples_desc: list[str], samples_vendor: list[str]) -> str | None:
    import textwrap
    prompt = textwrap.dedent(f"""
    ë„ˆëŠ” íšŒê³„ ê°ì‚¬ ë³´ì¡° AIë‹¤. ì•„ë˜ ê±°ë˜ ìƒ˜í”Œì„ ë³´ê³  ì´ ê·¸ë£¹ì˜ ì„±ê²©ì„ ê°€ì¥ ì˜ ë“œëŸ¬ë‚´ëŠ”
    í•œêµ­ì–´ **í´ëŸ¬ìŠ¤í„° ì´ë¦„ 1ê°œë§Œ**, 10ì ë‚´ì™¸ë¡œ ì œì‹œí•´ë¼.
    ìˆ«ì/ê¸°í˜¸/ë”°ì˜´í‘œ/ì ‘ë‘ì‚¬ ê¸ˆì§€. ì˜ˆ: ì§ì› ê¸‰ì—¬, í†µì‹ ë¹„, ì„ì› ë³µì§€.

    [ê±°ë˜ ì ìš” ìƒ˜í”Œ]
    - {'\n- '.join(samples_desc) if samples_desc else '(ìƒ˜í”Œ ë¶€ì¡±)'}

    [ì£¼ìš” ê±°ë˜ì²˜ ìƒ˜í”Œ]
    - {'\n- '.join(samples_vendor) if samples_vendor else '(ìƒ˜í”Œ ë¶€ì¡±)'}

    ì •ë‹µ:
    """).strip()
    try:
        resp = client.chat.completions.create(
            model=model, messages=[{"role":"user","content":prompt}], temperature=0
        )
        cand = (resp.choices[0].message.content or "").strip().splitlines()[0].strip(" \"'[]()")
        if not cand:
            return None
        bad = {"í´ëŸ¬ìŠ¤í„°","unknown","ì´ë¦„ì—†ìŒ","ë¯¸ì •","ê¸°íƒ€"}
        if cand.lower() in bad or cand.startswith("í´ëŸ¬ìŠ¤í„°"):
            return None
        return cand[:20]
    except Exception:
        return None

def perform_embedding_and_clustering(
    df: pd.DataFrame,
    client,
    name_with_llm: bool = True,
    must_name_with_llm: bool = False,
    llm_model: str = "gpt-4o-mini",
    *,
    use_large: bool|None = None,
    rescue_tau: float = HDBSCAN_RESCUE_TAU,
    umap_enabled: bool|None = None,   # None => use config threshold
):
    """
    Embedding + (optional UMAP) + L2-normalized Euclidean HDBSCAN + noise rescue + (LLM naming).
    Returns: (df, ok)
    ok=False if: no vectors, or LLM naming required but missing/failed.
    """
    df = ensure_embedding_text(df.copy())
    uniq = df['embedding_text'].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(uniq, client, model=model)
    df['vector'] = df['embedding_text'].astype(str).map(mapping)
    # keep only valid vectors
    mask = df['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)
    df = df.loc[mask].copy()
    if df.empty:
        return None, False

    X = np.vstack(df['vector'].values).astype(float)
    # Optional UMAP if dataset large (threshold controlled by config)
    X, umap_used = _optional_umap(X, enabled=umap_enabled)
    # L2 normalize and cluster with Euclidean (â‰ˆ cosine)
    Xn = _l2_normalize(X)
    labels, probs = _adaptive_hdbscan(Xn)
    df['cluster_id'] = labels
    df['cluster_prob'] = probs
    # telemetry attrs
    try:
        df.attrs['embedding_model'] = model
        df.attrs['umap_used'] = bool(umap_used)
        df.attrs['rescue_tau'] = float(rescue_tau) if rescue_tau is not None else None
    except Exception:
        pass

    # --- LLM cluster naming (with graceful fallback names) ---
    labels_uniq = sorted(pd.Series(labels).unique())
    names = {}
    if name_with_llm and hasattr(client, "chat"):
        for cid in labels_uniq:
            if cid == -1:
                names[cid] = "í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)"
                continue
            sub = df[df['cluster_id'] == cid]
            descs = sub['ì ìš”'].dropna().astype(str).unique().tolist()[:5] if 'ì ìš”' in sub.columns else []
            vendors = sub['ê±°ë˜ì²˜'].dropna().astype(str).unique().tolist()[:5] if 'ê±°ë˜ì²˜' in sub.columns else []
            cand = _llm_cluster_name(client, llm_model, descs, vendors)
            # fallback rule-based name if LLM failed
            if not cand or cand == "ì´ë¦„ ìƒì„± ì‹¤íŒ¨":
                # heuristic: frequent vendor or keyword + amount tag
                amt_tag = "ê·œëª¨ ì¤‘ê°„"
                try:
                    abs_amt = sub.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs().median()
                    if float(abs_amt) >= 1e8: amt_tag = "1ì–µì› ì´ìƒ"
                    elif float(abs_amt) >= 1e7: amt_tag = "1ì²œë§Œ~1ì–µ"
                except Exception:
                    pass
                top_vendor = sub.get('ê±°ë˜ì²˜', pd.Series(dtype=str)).value_counts().index.tolist()
                vname = top_vendor[0] if top_vendor else "ì¼ë°˜"
                cand = f"{vname} ì¤‘ì‹¬({amt_tag})"
            names[cid] = cand
    else:
        for cid in labels_uniq:
            names[cid] = "í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)" if cid == -1 else "ì´ë¦„ ìƒì„± ì‹¤íŒ¨"

    df['cluster_name'] = df['cluster_id'].map(names)
    df = postprocess_cluster_names(df)

    # --- Noise rescue: reassign -1 to nearest centroid if cosine >= tau ---
    if rescue_tau and float(rescue_tau) > 0:
        df = _rescue_noise(df, tau=float(rescue_tau))

    # gate: if must_name_with_llm, all non-noise clusters must have valid names
    if must_name_with_llm:
        non_noise = df[df['cluster_id'] != -1]
        has_any = not non_noise.empty
        invalid = non_noise['cluster_name'].isna() | non_noise['cluster_name'].astype(str).str.contains("^ì´ë¦„ ìƒì„± ì‹¤íŒ¨|^í´ëŸ¬ìŠ¤í„°\\s", regex=True)
        if (not has_any) or bool(invalid.any()):
            return df, False

    # default reporting group equals the (validated) cluster_name; may be unified later
    df['cluster_group'] = df['cluster_name']
    return df, True



# --- NEW: LLM synonym grouping for cluster names ---
def _cosine_sim_matrix(vecs: list[list[float]]):
    import numpy as np
    V = np.asarray(vecs, dtype=float)
    if V.ndim != 2 or V.shape[0] == 0:
        return np.zeros((0, 0))
    Vn = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    return Vn @ Vn.T


def unify_cluster_names_with_llm(
    df: pd.DataFrame,
    client,
    sim_threshold: float = 0.90,
    emb_model: str = EMB_MODEL_SMALL,
    llm_model: str = "gpt-4o-mini",
):
    """
    Collapse clusters with effectively identical names.
    Strategy:
      1) Embed unique names (excluding noise), preselect candidate pairs via cosine >= sim_threshold.
      2) Ask LLM YES/NO if two names are synonyms for accounting transaction categories.
      3) Union-Find merge; choose canonical = most frequent name in df (fallback shortest).
    Returns: (df_with_cluster_group, mapping{name->canonical})
    """
    import numpy as np
    import itertools
    base = df.copy()
    if 'cluster_name' not in base.columns:
        base['cluster_group'] = base.get('cluster_name', None)
        return base, {}
    names = (
        base.loc[base['cluster_id'] != -1, 'cluster_name']
        .dropna().astype(str).unique().tolist()
    )
    if not names:
        base['cluster_group'] = base['cluster_name']
        return base, {}

    # Embedding prefilter
    name2vec = embed_texts_batched(names, client, model=emb_model)
    ordered = [n for n in names if n in name2vec]
    vecs = [name2vec[n] for n in ordered]
    S = _cosine_sim_matrix(vecs)

    # Union-Find
    parent = {n: n for n in ordered}
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    # LLM pair confirmation
    for i, j in itertools.combinations(range(len(ordered)), 2):
        if S[i, j] < float(sim_threshold):
            continue
        a, b = ordered[i], ordered[j]
        try:
            q = (
                "ë„ˆëŠ” íšŒê³„ ê°ì‚¬ ë³´ì¡° AIë‹¤. ë‹¤ìŒ ë‘ í‘œí˜„ì´ 'íšŒê³„ ê±°ë˜ ì¹´í…Œê³ ë¦¬' ì´ë¦„ìœ¼ë¡œì„œ "
                "ì‚¬ì‹¤ìƒ ê°™ì€ ì˜ë¯¸ì¸ì§€ YES/NOë¡œë§Œ ë‹µí•˜ë¼.\n"
                f"A: {a}\nB: {b}\nì •ë‹µ:"
            )
            resp = client.chat.completions.create(
                model=llm_model,
                messages=[{"role": "user", "content": q}],
                temperature=0
            )
            ans = (resp.choices[0].message.content or "").strip().split()[0].upper()
            if ans.startswith("Y"):  # YES
                union(a, b)
        except Exception:
            # On failure, skip merging
            pass

    # Build groups
    groups = {}
    for n in ordered:
        r = find(n)
        groups.setdefault(r, []).append(n)

    # Choose canonical per group
    freq = base['cluster_name'].value_counts().to_dict()
    mapping = {}
    for root, members in groups.items():
        cand = sorted(members, key=lambda x: (-freq.get(x, 0), len(x)))[0]
        for m in members:
            mapping[m] = cand

    base['cluster_group'] = base['cluster_name'].map(lambda x: mapping.get(x, x))
    return base, mapping


# --- NEW: Utilities for PYâ†’CY mapping and label unification ---
def _cosine(a, b):
    import numpy as np
    if a is None or b is None:
        return np.nan
    a = np.asarray(a)
    b = np.asarray(b)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    return float(np.dot(a, b) / denom) if denom else np.nan


def map_previous_to_current_clusters(df_cur: pd.DataFrame, df_prev: pd.DataFrame) -> pd.DataFrame:
    """
    ì „ê¸° ì „í‘œë¥¼ ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ì„¼íŠ¸ë¡œì´ë“œì— ìµœê·¼ì ‘ ë°°ì •í•˜ì—¬ (mapped_cluster_id/name, mapped_sim) ë¶€ì—¬.
    - ë…¸ì´ì¦ˆ(-1) ì„¼íŠ¸ë¡œì´ë“œëŠ” ì œì™¸
    - ë°˜í™˜: prev_df(with mapped_cluster_id, mapped_cluster_name, mapped_sim)
    """
    import numpy as np
    import pandas as pd
    need_cols = ['cluster_id', 'cluster_name', 'vector']
    if any(c not in df_cur.columns for c in need_cols) or 'vector' not in df_prev.columns:
        return df_prev.copy()
    cur = df_cur[df_cur['cluster_id'] != -1].copy()
    if cur.empty:
        return df_prev.copy()
    # ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°
    cents = (
        cur.groupby(['cluster_id', 'cluster_name'])['vector']
           .apply(lambda s: np.mean(np.vstack(list(s)), axis=0))
           .reset_index()
    )
    prev = df_prev.copy()

    def _pick(row: pd.Series) -> pd.Series:
        v = row.get('vector', None)
        if v is None:
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        sims = cents['vector'].apply(lambda c: _cosine(v, c))
        if len(sims) == 0 or sims.isna().all():
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        idx = int(sims.idxmax())
        return pd.Series({
            'mapped_cluster_id': int(cents.loc[idx, 'cluster_id']),
            'mapped_cluster_name': cents.loc[idx, 'cluster_name'],
            'mapped_sim': float(sims.max()) if not np.isnan(sims.max()) else np.nan,
        })

    prev[['mapped_cluster_id', 'mapped_cluster_name', 'mapped_sim']] = prev.apply(_pick, axis=1)
    return prev


def unify_cluster_labels_llm(names, client) -> dict:
    """
    ìœ ì‚¬ ì˜ë¯¸ì˜ í•œê¸€ í´ëŸ¬ìŠ¤í„°ëª…ì„ LLMìœ¼ë¡œ ë¬¶ì–´ canonical name ë§¤í•‘ì„ ë¦¬í„´.
    ì…ë ¥: ì˜ˆ) ["ê²½ë¹„ ê´€ë¦¬","ê²½ë¹„ ì²˜ë¦¬","ê´€ë¦¬ ê²½ë¹„", ...]
    ì¶œë ¥: {"ê²½ë¹„ ê´€ë¦¬":"ê²½ë¹„ ê´€ë¦¬","ê²½ë¹„ ì²˜ë¦¬":"ê²½ë¹„ ê´€ë¦¬", ...}
    """
    uniq = sorted([n for n in set([str(x) for x in names]) if n and n.lower() != 'nan'])
    if not uniq:
        return {}
    prompt = (
        "ë‹¤ìŒ í•œêµ­ì–´ í´ëŸ¬ìŠ¤í„° ì´ë¦„ë“¤ì„ ì˜ë¯¸ê°€ ê°™ì€ ê²ƒë¼ë¦¬ ë¬¶ì–´ í•˜ë‚˜ì˜ ëŒ€í‘œëª…ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.\n"
        "ê·œì¹™: 1) ê°€ì¥ ì¼ë°˜ì /ì§§ì€ í‘œí˜„ì„ ëŒ€í‘œëª…ìœ¼ë¡œ, 2) JSON ê°ì²´ë¡œë§Œ ì‘ë‹µ, 3) í˜•ì‹: {ì›ë˜ëª…:ëŒ€í‘œëª…, ...}.\n"
        f"ëª©ë¡: {uniq}"
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        import json
        txt = (resp.choices[0].message.content or "").strip()
        mapping = json.loads(txt)
        if isinstance(mapping, dict):
            return mapping
    except Exception:
        pass
    return {n: n for n in uniq}


# --- NEW: Yearly clustering helpers and alignment ---
def cluster_year(df: pd.DataFrame, client) -> pd.DataFrame:
    """
    ë‹¹ê¸°/ì „ê¸° ë“± ì…ë ¥ dfì— ëŒ€í•´ í’ë¶€ ì„ë² ë”© í…ìŠ¤íŠ¸ë¥¼ ë³´ì¥í•˜ê³  HDBSCAN+LLM ë„¤ì´ë°ì„ ì‹¤í–‰.
    ë°˜í™˜: ['row_id','cluster_id','cluster_name','cluster_prob','vector']ê°€ í¬í•¨ëœ DataFrame(ë¶€ë¶„ì§‘í•© ê°€ëŠ¥).
    ì…ë ¥ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜.
    """
    if df is None or df.empty:
        return pd.DataFrame()
    from .embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
    df_in = ensure_rich_embedding_text(df.copy())
    df_out, ok = perform_embedding_and_clustering(df_in, client, name_with_llm=True, must_name_with_llm=False)
    if not ok or df_out is None:
        return pd.DataFrame()
    keep = [c for c in ['row_id','cluster_id','cluster_name','cluster_prob','vector'] if c in df_out.columns]
    return df_out[keep].copy()


def compute_centroids(df: pd.DataFrame) -> pd.DataFrame:
    """
    ['cluster_id','vector']ë¥¼ ê°–ëŠ” dfì—ì„œ í´ëŸ¬ìŠ¤í„°ë³„ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°(-1 ì œì™¸).
    'cluster_name'ì´ ìˆìœ¼ë©´ í•¨ê»˜ ìœ ì§€.
    ë°˜í™˜: columns=['cluster_id','cluster_name','vector']
    """
    import numpy as np
    import pandas as pd
    need = ['cluster_id','vector']
    if df is None or df.empty or any(c not in df.columns for c in need):
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    base = df[df['cluster_id'] != -1].copy()
    if base.empty:
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    def _mean_stack(s):
        try:
            return np.mean(np.vstack(list(s)), axis=0)
        except Exception:
            return None
    cents = base.groupby('cluster_id')['vector'].apply(_mean_stack).reset_index()
    if 'cluster_name' in base.columns:
        name_map = base.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name']
        cents['cluster_name'] = cents['cluster_id'].map(name_map)
    else:
        cents['cluster_name'] = None
    # re-order columns
    cents = cents[['cluster_id','cluster_name','vector']]
    # drop rows with invalid vectors
    cents = cents[cents['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)]
    return cents.reset_index(drop=True)


def align_yearly_clusters(df_cy: pd.DataFrame, df_py: pd.DataFrame, sim_threshold: float = 0.70) -> dict:
    """
    CY/PY ì„¼íŠ¸ë¡œì´ë“œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê¸°ë°˜ Hungarian ë§¤ì¹­(cost=1-sim).
    ë°˜í™˜: {py_cluster_id: (cy_cluster_id, sim)} (ì„ê³„ì¹˜ ë¯¸ë§Œì€ ê°’ None)
    """
    import numpy as np
    py_c = compute_centroids(df_py)
    cy_c = compute_centroids(df_cy)
    if py_c.empty or cy_c.empty:
        return {}
    # build similarity matrix
    py_vecs = list(py_c['vector'].values)
    cy_vecs = list(cy_c['vector'].values)
    S_py = _cosine_sim_matrix(py_vecs)
    S_cy = _cosine_sim_matrix(cy_vecs)
    # We need PY x CY sims; compute directly
    # Efficient: normalize and dot
    import numpy as np
    def _norm(V):
        V = np.asarray([np.asarray(v, dtype=float) for v in V], dtype=float)
        return V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    Npy = _norm(py_vecs)
    Ncy = _norm(cy_vecs)
    sim = Npy @ Ncy.T  # shape: [n_py, n_cy]
    # Hungarian matching on cost = 1 - sim
    try:
        from scipy.optimize import linear_sum_assignment
        cost = 1.0 - sim
        row_ind, col_ind = linear_sum_assignment(cost)
    except Exception:
        # Fallback: greedy matching by highest sim without replacement
        pairs = []
        used_py = set(); used_cy = set()
        # flatten and sort
        flat = [
            (i, j, float(sim[i, j]))
            for i in range(sim.shape[0])
            for j in range(sim.shape[1])
        ]
        flat.sort(key=lambda x: x[2], reverse=True)
        for i, j, s in flat:
            if i in used_py or j in used_cy:
                continue
            pairs.append((i, j))
            used_py.add(i); used_cy.add(j)
        row_ind = np.array([p[0] for p in pairs], dtype=int)
        col_ind = np.array([p[1] for p in pairs], dtype=int)
    mapping: dict[int, tuple[int, float] | None] = {}
    for k in range(len(row_ind)):
        i = int(row_ind[k]); j = int(col_ind[k])
        s = float(sim[i, j])
        py_id = int(py_c.loc[i, 'cluster_id'])
        cy_id = int(cy_c.loc[j, 'cluster_id'])
        if s >= float(sim_threshold):
            mapping[py_id] = (cy_id, s)
        else:
            mapping[py_id] = None
    # Ensure all PY clusters are present in mapping
    for py_id in py_c['cluster_id'].tolist():
        if py_id not in mapping:
            mapping[py_id] = None
    return mapping




==============================
ğŸ“„ FILE: analysis/evidence.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import Optional


def build_knn_index(prev_df: pd.DataFrame):
    """prev_df['vector']ë¡œ KNN ìƒì„±."""
    if 'vector' not in prev_df.columns or prev_df['vector'].isna().any():
        raise ValueError("build_knn_index: prev_dfì— 'vector' í•„ìš”")
    from sklearn.neighbors import NearestNeighbors
    X = np.vstack(prev_df['vector'].values)
    knn = NearestNeighbors(metric='cosine', n_neighbors=min(10, len(X))).fit(X)
    return knn, X


def cluster_centroid_vector(cluster_df: pd.DataFrame):
    if 'vector' not in cluster_df.columns or cluster_df.empty:
        return None
    return np.mean(np.vstack(cluster_df['vector'].values), axis=0)


def retrieve_similar_from_previous(prev_df, prev_knn, prev_X, query_vec, topk=5, dedup_by_vendor=True, min_sim=0.7):
    if query_vec is None or prev_X is None or len(prev_X) == 0:
        return pd.DataFrame()
    dist, idx = prev_knn.kneighbors([query_vec], n_neighbors=min(max(10, topk*3), len(prev_X)))
    cands = prev_df.iloc[idx[0]].copy()
    cands['similarity'] = (1 - dist[0])
    cands = cands[cands['similarity'] >= min_sim]
    if dedup_by_vendor and 'ê±°ë˜ì²˜' in cands.columns:
        cands = cands.sort_values('similarity', ascending=False).drop_duplicates('ê±°ë˜ì²˜', keep='first')
    cands = cands.sort_values('similarity', ascending=False).head(topk)
    cols = ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','similarity']
    for c in cols:
        if c not in cands.columns: cands[c] = np.nan
    return cands[cols]


def build_cluster_evidence_block(current_df: pd.DataFrame, previous_df: pd.DataFrame,
                                 topk: int = 3, restrict_same_months: bool = True, min_sim: float = 0.7,
                                 dedup_by_vendor: bool = True) -> str:
    if any(col not in current_df.columns for col in ['cluster_id','vector']):
        return "\n\n## ê·¼ê±° ì¸ìš©(ì „ê¸° ìœ ì‚¬ ê±°ë˜)\n- í˜„ì¬ ë°ì´í„°ì— í´ëŸ¬ìŠ¤í„°/ë²¡í„°ê°€ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## ê·¼ê±° ì¸ìš©(ì „ê¸° ìœ ì‚¬ ê±°ë˜)\n- ì „ê¸° ë°ì´í„° ì„ë² ë”©ì´ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    def _ok_vec(v):
        return v is not None and isinstance(v, (list, tuple, np.ndarray)) and len(v) > 0
    lines = ["\n\n## ê·¼ê±° ì¸ìš©(ì „ê¸° ìœ ì‚¬ ê±°ë˜)"]
    for cid in sorted(current_df['cluster_id'].unique()):
        cur_c = current_df[current_df['cluster_id'] == cid]
        if cur_c.empty: continue
        cname = cur_c['cluster_name'].iloc[0] if 'cluster_name' in cur_c.columns else str(cid)
        lines.append(f"[í´ëŸ¬ìŠ¤í„° #{cid} | {cname}]")
        prev_subset = previous_df.copy()
        if restrict_same_months and 'íšŒê³„ì¼ì' in cur_c.columns and cur_c['íšŒê³„ì¼ì'].notna().any():
            months = set(cur_c['íšŒê³„ì¼ì'].dt.month.dropna().unique().tolist())
            filtered = previous_df[previous_df['íšŒê³„ì¼ì'].dt.month.isin(months)]
            prev_subset = filtered if not filtered.empty else previous_df
        if 'vector' in prev_subset.columns:
            prev_subset = prev_subset[prev_subset['vector'].apply(_ok_vec)].copy()
        if prev_subset.empty:
            lines.append("    â”” ì „ê¸° ìœ ì‚¬ ë²¡í„° ì—†ìŒ"); continue
        try:
            knn, X = build_knn_index(prev_subset)
        except Exception as e:
            lines.append(f"    â”” ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}"); continue
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_subset, knn, X, qv, topk=topk, dedup_by_vendor=dedup_by_vendor, min_sim=min_sim)
        if ev.empty:
            lines.append("    â”” ìœ ì‚¬ ì „í‘œ: ì—†ìŒ")
        else:
            def _fmt_date(x): 
                try: return x.strftime('%Y-%m-%d') if pd.notna(x) else ""
                except: return ""
            def _fmt_money(x):
                try: return f"{int(x):,}ì›"
                except: return str(x)
            def _fmt_sim(s):
                try: return f"{float(s):.2f}"
                except: return "N/A"
            for rank, (_, r) in enumerate(ev.sort_values('similarity', ascending=False).iterrows(), 1):
                lines.append(f"    {rank}) {_fmt_date(r['íšŒê³„ì¼ì'])} | {str(r['ê±°ë˜ì²˜'])} | {_fmt_money(r['ë°œìƒì•¡'])} | sim {_fmt_sim(r['similarity'])}")
    return "\n".join(lines)



def build_transaction_evidence_block(current_df, previous_df, topn=10, per_tx_topk=3, min_sim=0.8):
    import numpy as np, pandas as pd
    def _ok_vec(v): return isinstance(v, (list, tuple, np.ndarray)) and len(v)>0
    if current_df.empty or 'vector' not in current_df.columns: 
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- í˜„ì¬ ë°ì´í„°ì— ë²¡í„°ê°€ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- ì „ê¸° ë°ì´í„° ì„ë² ë”©ì´ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    cur = current_df.copy()
    if 'Z-Score' in cur.columns and cur['Z-Score'].notna().any():
        order_idx = cur['Z-Score'].abs().sort_values(ascending=False).index
    else:
        # Z-Score ë¯¸ì‹œí–‰ ì‹œ ë°œìƒì•¡ ìƒìœ„
        amt = cur.get('ë°œìƒì•¡', pd.Series(dtype=float))
        order_idx = amt.sort_values(ascending=False).index
    cur = cur.reindex(order_idx).head(int(topn))

    # ì „ê¸° ë²¡í„° ìœ íš¨ì„± í•„í„°
    prev = previous_df.copy()
    prev = prev[prev['vector'].apply(_ok_vec)]
    if prev.empty:
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- ì „ê¸° ë°ì´í„° ë²¡í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    from .evidence import build_knn_index, retrieve_similar_from_previous
    try:
        knn, X = build_knn_index(prev)
    except Exception:
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- ì „ê¸° KNN ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨."

    lines = [f"\n\n## ê±°ë˜ë³„ ê·¼ê±° (ìƒìœ„ {len(cur)}ê±´)"]
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        if qv is None: continue
        # ë™ì›” ìš°ì„ 
        psub = prev
        if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']):
            m = r['íšŒê³„ì¼ì'].month
            cand = prev[prev['íšŒê³„ì¼ì'].dt.month == m]
            if not cand.empty: psub = cand
            knn, X = build_knn_index(psub)
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=int(per_tx_topk), dedup_by_vendor=True, min_sim=float(min_sim))
        dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
        amt = r.get('ë°œìƒì•¡', 0.0); z = r.get('Z-Score', np.nan)
        ztxt = f" | Z={z:+.2f}" if not pd.isna(z) else ""
        lines.append(f"[{i}] {dt} | ê±°ë˜ì²˜:{r.get('ê±°ë˜ì²˜','')} | ê¸ˆì•¡:{int(amt):,}ì›{ztxt}")
        if ev.empty:
            lines.append("    â”” ìœ ì‚¬ ì „í‘œ: ì—†ìŒ")
        else:
            lines.append(f"    â”” ì „ê¸° ìœ ì‚¬ Top {len(ev)}")
            for _, rr in ev.iterrows():
                d2 = rr['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(rr['íšŒê³„ì¼ì']) else ''
                lines.append(f"       â€¢ {d2} | {rr['ê±°ë˜ì²˜']} | {int(rr['ë°œìƒì•¡']):,}ì› | sim {rr['similarity']:.2f}")
    return "\n".join(lines)

# --- NEW: Structured evidence blocks for the redesigned context ---
def build_current_cluster_block(current_df: pd.DataFrame) -> str:
    """
    ## ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡
    - One bullet per cluster_group: total absolute amount, count, and ONE example voucher.
    """
    import pandas as pd
    if current_df.empty or 'cluster_group' not in current_df.columns:
        return "\n\n## ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì—†ìŒ)"
    lines = ["\n\n## ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡"]
    grp = current_df.copy()
    grp['abs_amt'] = grp.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs()
    for name, cdf in grp.groupby('cluster_group', dropna=False):
        tot = cdf['abs_amt'].sum()
        cnt = len(cdf)
        ex = cdf.sort_values('abs_amt', ascending=False).head(1).iloc[0]
        dt = ex['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in ex and pd.notna(ex['íšŒê³„ì¼ì']) else ''
        vend = ex.get('ê±°ë˜ì²˜', '')
        amt = int(ex.get('ë°œìƒì•¡', 0.0))
        lines.append(f"- [{name}] ê±´ìˆ˜ {cnt}ê±´, ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›")
        lines.append(f"  Â· ì˜ˆì‹œ: {dt} | {vend} | {amt:,.0f}ì›")
    return "\n".join(lines)

def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float = 0.70) -> str:
    """
    ## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡
    Project PY vouchers onto CY cluster centroids; report total abs amount, avg similarity, and ONE example.
    """
    import pandas as pd
    import numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous, cluster_centroid_vector
    if current_df.empty or previous_df.empty or 'vector' not in previous_df.columns or 'cluster_group' not in current_df.columns:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° ë°ì´í„°/ë²¡í„°/í´ëŸ¬ìŠ¤í„° ì •ë³´ ì—†ìŒ)"
    lines = ["\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡"]
    prev_ok = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)]
    if prev_ok.empty:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° ìœ íš¨ ë²¡í„° ì—†ìŒ)"
    try:
        knn, X = build_knn_index(prev_ok)
    except Exception:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° KNN ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨)"
    for name, cur_c in current_df.groupby('cluster_group', dropna=False):
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_ok, knn, X, qv, topk=10, dedup_by_vendor=True, min_sim=float(min_sim))
        if ev.empty:
            lines.append(f"- [{name}] ìœ ì‚¬ ì „í‘œ ì—†ìŒ")
            continue
        ev['abs_amt'] = ev.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs()
        tot = ev['abs_amt'].sum()
        avg_sim = ev['similarity'].mean()
        ex = ev.sort_values('similarity', ascending=False).head(1).iloc[0]
        dt = ex['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(ex['íšŒê³„ì¼ì']) else ''
        lines.append(f"- [{name}] ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›, í‰ê·  ìœ ì‚¬ë„ {avg_sim:.2f}")
        lines.append(f"  Â· ì˜ˆì‹œ: {dt} | {ex['ê±°ë˜ì²˜']} | {int(ex['ë°œìƒì•¡']):,}ì› | sim {ex['similarity']:.2f}")
    return "\n".join(lines)

def build_zscore_top5_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## Z-score ê¸°ì¤€ TOP5 ì „í‘œ
    List top |Z| vouchers with one counterpart from PY (same-month preferred), no row-id.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous
    if current_df.empty or 'Z-Score' not in current_df.columns:
        return "\n\n## Z-score ê¸°ì¤€ TOP5 ì „í‘œ\n- (Z-Score ë¯¸ê³„ì‚°)"
    cur = current_df.copy()
    order = cur['Z-Score'].abs().sort_values(ascending=False).index
    cur = cur.reindex(order).head(int(topn))
    lines = [f"\n\n## Z-score ê¸°ì¤€ TOP5 ì „í‘œ"]
    if previous_df.empty or 'vector' not in previous_df.columns:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    # KNN on PY (same-month preferred)
    prev = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if prev.empty:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    knn_all, X_all = build_knn_index(prev)
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
        head = f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}"
        if qv is None:
            lines.append(head)
            continue
        psub = prev
        if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']):
            m = r['íšŒê³„ì¼ì'].month
            cand = prev[prev['íšŒê³„ì¼ì'].dt.month == m]
            if not cand.empty:
                psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  Â· ì „ê¸° ëŒ€ì‘: ì—†ìŒ")
        else:
            rr = ev.iloc[0]
            d2 = rr['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(rr['íšŒê³„ì¼ì']) else ''
            lines.append(f"  Â· ì „ê¸° ëŒ€ì‘: {d2} | {rr['ê±°ë˜ì²˜']} | {int(rr['ë°œìƒì•¡']):,}ì› | sim {rr['similarity']:.2f}")
    return "\n".join(lines)


# --- NEW: ì „ê¸° ê¸°ì¤€ TOP5 ë¸”ë¡ ---
def build_zscore_top5_block_for_py(previous_df: pd.DataFrame, current_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## ì „ê¸° Z-score ê¸°ì¤€ TOP5 ì „í‘œ
    ì „ê¸° ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ |Z| ìƒìœ„ 5ê±´ì„ ë‚˜ì—´í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° ë‹¹ê¸° ëŒ€ì‘ 1ê±´ì„ í•¨ê»˜ í‘œì‹œ.
    previous_dfì— Z-Scoreê°€ ìˆì–´ì•¼ í•œë‹¤.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous

    if previous_df.empty or 'Z-Score' not in previous_df.columns:
        return "\n\n## ì „ê¸° Z-score ê¸°ì¤€ TOP5 ì „í‘œ\n- (ì „ê¸° Z-Score ë¯¸ê³„ì‚°)"

    prev = previous_df.copy()
    order = prev['Z-Score'].abs().sort_values(ascending=False).index
    prev = prev.reindex(order).head(int(topn))

    lines = [f"\n\n## ì „ê¸° Z-score ê¸°ì¤€ TOP5 ì „í‘œ"]

    if current_df.empty or 'vector' not in current_df.columns:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    cur_ok = current_df[current_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if cur_ok.empty:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    knn_all, X_all = build_knn_index(cur_ok)

    for i, (_, r) in enumerate(prev.iterrows(), 1):
        dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
        head = f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}"

        qv = r.get('vector', None)
        if qv is None:
            lines.append(head); continue

        psub = cur_ok
        if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']):
            m = r['íšŒê³„ì¼ì'].month
            cand = cur_ok[cur_ok['íšŒê³„ì¼ì'].dt.month == m]
            if not cand.empty: psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all

        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  Â· ë‹¹ê¸° ëŒ€ì‘: ì—†ìŒ")
        else:
            rr = ev.iloc[0]
            d2 = rr['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(rr['íšŒê³„ì¼ì']) else ''
            lines.append(f"  Â· ë‹¹ê¸° ëŒ€ì‘: {d2} | {rr['ê±°ë˜ì²˜']} | {int(rr['ë°œìƒì•¡']):,}ì› | sim {rr['similarity']:.2f}")

    return "\n".join(lines)


==============================
ğŸ“„ FILE: analysis/integrity.py
==============================

import pandas as pd


def analyze_reconciliation(ledger_df: pd.DataFrame, master_df: pd.DataFrame):
    """Masterì™€ Ledger ë°ì´í„° ê°„ì˜ ì •í•©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.

    ë°˜í™˜ê°’:
    - overall_status: "Pass" | "Warning" | "Fail"
    - ê²°ê³¼ DataFrame
    """
    results, overall_status = [], "Pass"
    cy_ledger_df = ledger_df[ledger_df['ì—°ë„'] == ledger_df['ì—°ë„'].max()]
    for _, master_row in master_df.iterrows():
        account_code = master_row['ê³„ì •ì½”ë“œ']
        bspl = master_row.get('BS/PL', 'PL').upper()
        bop = master_row.get('ì „ê¸°ë§ì”ì•¡', 0)
        eop_master = master_row.get('ë‹¹ê¸°ë§ì”ì•¡', 0)

        net_change_gl = cy_ledger_df[cy_ledger_df['ê³„ì •ì½”ë“œ'] == account_code]['ê±°ë˜ê¸ˆì•¡'].sum()
        eop_gl = (bop + net_change_gl) if bspl == 'BS' else net_change_gl

        difference = eop_master - eop_gl
        diff_pct = abs(difference) / max(abs(eop_master), 1)
        status = "Fail" if diff_pct > 0.001 else "Warning" if abs(difference) > 0 else "Pass"
        if status == "Fail":
            overall_status = "Fail"
        elif status == "Warning" and overall_status == "Pass":
            overall_status = "Warning"

        results.append({
            'ê³„ì •ì½”ë“œ': account_code,
            'ê³„ì •ëª…': master_row.get('ê³„ì •ëª…', ''),
            'êµ¬ë¶„': bspl,
            'ê¸°ì´ˆì”ì•¡(Master)': bop,
            'ë‹¹ê¸°ì¦ê°ì•¡(Ledger)': net_change_gl,
            'ê³„ì‚°ëœ ê¸°ë§ì”ì•¡(GL)': eop_gl,
            'ê¸°ë§ì”ì•¡(Master)': eop_master,
            'ì°¨ì´': difference,
            'ìƒíƒœ': status
        })
    return overall_status, pd.DataFrame(results)




==============================
ğŸ“„ FILE: analysis/kdmeans_shim.py
==============================

from __future__ import annotations
from typing import Optional, Sequence
import numpy as np

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
except Exception as e:
    raise ImportError("scikit-learnì´ í•„ìš”í•©ë‹ˆë‹¤. `pip install scikit-learn`") from e


class HDBSCAN:
    """
    KDMeans: KMeansë¥¼ ì‚¬ìš©í•˜ë˜ HDBSCANì˜ ìµœì†Œ ì†ì„± ì¸í„°í˜ì´ìŠ¤ë¥¼ í‰ë‚´ëƒ„.
    - fit(X): labels_, probabilities_ ì„¤ì •
    - labels_: np.ndarray[int], [0..k-1]
    - probabilities_: np.ndarray[float], 0~1 (KDMeansì—ì„œëŠ” ì „ë¶€ 1.0ë¡œ ì„¤ì •)
    ë§¤ê°œë³€ìˆ˜:
      - n_clusters: ê³ ì • k (Noneì´ë©´ ìë™ ì„ íƒ)
      - min_cluster_size: k ìƒí•œì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ íŒíŠ¸(ë„ˆë¬´ ë§ì€ êµ°ì§‘ ë°©ì§€)
      - max_k: ìë™ ì„ íƒ ì‹œ k ìƒí•œ(ê¸°ë³¸: ë°ì´í„° í¬ê¸°ì™€ min_cluster_sizeë¡œ ìœ ë„)
      - k_search: "silhouette" | "heuristic"
      - sample_size: ìë™ ì„ íƒ ì‹œ ì‹¤ë£¨ì—£ ê³„ì‚°ì— ì‚¬ìš©í•  ìƒ˜í”Œ í¬ê¸°(ê¸°ë³¸ 2000)
      - random_state: ì¬í˜„ì„±
      - n_init: KMeans ì´ˆê¸°í™” íšŸìˆ˜(ë˜ëŠ” "auto")
    """
    def __init__(
        self,
        n_clusters: Optional[int] = None,
        min_cluster_size: int = 8,
        max_k: Optional[int] = None,
        k_search: str = "silhouette",
        sample_size: int = 2000,
        random_state: int = 42,
        n_init: str | int = "auto",
    ):
        self.n_clusters = n_clusters
        self.min_cluster_size = max(2, int(min_cluster_size))
        self.max_k = max_k
        self.k_search = k_search
        self.sample_size = int(sample_size)
        self.random_state = int(random_state)
        self.n_init = n_init

        # í•™ìŠµ í›„ ì†ì„±(HDBSCAN í˜¸í™˜)
        self.labels_: Optional[np.ndarray] = None
        self.probabilities_: Optional[np.ndarray] = None
        # ì¶”ê°€ í…”ë ˆë©”íŠ¸ë¦¬
        self.chosen_k_: Optional[int] = None
        self.silhouette_: Optional[float] = None

    # --- ë‚´ë¶€: k í›„ë³´ ì‚°ì • ---
    def _candidate_ks(self, n: int) -> Sequence[int]:
        if n < 2:
            return [1]
        base = max(2, int(np.sqrt(n)))
        # ìµœì†Œ í¬ê¸° ì œì•½ ê¸°ë°˜ ìƒí•œ
        max_by_min = max(2, n // self.min_cluster_size)
        # ì™¸ë¶€ ìƒí•œ ì ìš©
        if self.max_k is not None:
            max_by_min = min(max_by_min, int(self.max_k))
        # ì§€ë‚˜ì¹˜ê²Œ í° këŠ” ê³„ì‚° ë¹„ìš© ì´ìŠˆ â†’ ì‹¤ë¬´ì ìœ¼ë¡œ ìº¡
        hard_cap = 24 if n >= 1200 else 12
        k_hi = max(2, min(max_by_min, hard_cap))

        ks = {2, 3, 5, base - 1, base, base + 1, int(np.log2(n)) + 1, k_hi}
        ks = {int(k) for k in ks if 2 <= int(k) <= k_hi}
        return sorted(ks)

    # --- ë‚´ë¶€: ìƒ˜í”Œë§ ---
    def _sample(self, X: np.ndarray) -> np.ndarray:
        n = X.shape[0]
        if n <= self.sample_size:
            return X
        rng = np.random.default_rng(self.random_state)
        idx = rng.choice(n, size=self.sample_size, replace=False)
        return X[idx]

    # --- ë‚´ë¶€: k ìë™ ì„ íƒ (ì‹¤ë£¨ì—£) ---
    def _choose_k(self, X: np.ndarray) -> int:
        n = X.shape[0]
        if n < 2:
            return 1
        if self.n_clusters is not None:
            return max(1, int(self.n_clusters))

        # í›„ë³´ ëª©ë¡
        ks = self._candidate_ks(n)
        if len(ks) == 0:
            return max(2, int(np.sqrt(n)))

        if self.k_search != "silhouette":
            # íœ´ë¦¬ìŠ¤í‹±: âˆšnì— ê°€ì¥ ê°€ê¹Œìš´ ê°’
            base = max(2, int(np.sqrt(n)))
            return min(ks, key=lambda k: abs(k - base))

        Xs = self._sample(X)
        best_k, best_s = None, -1.0

        for k in ks:
            if k >= len(Xs):   # ìƒ˜í”Œë³´ë‹¤ í° k ë¶ˆê°€
                continue
            try:
                km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
                labels = km.fit_predict(Xs)
                # ëª¨ë“  ë¼ë²¨ì´ í•˜ë‚˜ë©´ ì‹¤ë£¨ì—£ ê³„ì‚° ë¶ˆê°€
                if len(set(labels)) < 2:
                    continue
                s = silhouette_score(Xs, labels, metric="euclidean")
                if s > best_s:
                    best_k, best_s = int(k), float(s)
            except Exception:
                continue

        if best_k is None:
            # í´ë°±: âˆšn ì¸ê·¼
            base = max(2, int(np.sqrt(n)))
            best_k = min(ks, key=lambda k: abs(k - base))
            best_s = float("nan")

        self.silhouette_ = best_s
        return int(best_k)

    # --- ê³µê°œ API ---
    def fit(self, X: np.ndarray):
        X = np.asarray(X, dtype=float)
        if X.ndim != 2 or X.shape[0] < 1:
            raise ValueError("X must be 2D array with at least 1 row")

        k = self._choose_k(X)
        self.chosen_k_ = k

        km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
        labels = km.fit_predict(X)

        # HDBSCAN í˜¸í™˜ ì†ì„± ë¶€ì—¬
        self.labels_ = labels.astype(int)
        self.probabilities_ = np.ones(shape=(X.shape[0],), dtype=float)
        return self

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        return self.fit(X).labels_





==============================
ğŸ“„ FILE: analysis/report.py
==============================

from __future__ import annotations
import pandas as pd
from typing import List
from .evidence import (
    build_current_cluster_block,
    # build_previous_projection_block,  # íŒŒì¼ í•˜ë‹¨ ë¡œì»¬ ì •ì˜ ì‚¬ìš©
    build_zscore_top5_block,
    build_zscore_top5_block_for_py,
)
from .timeseries import run_timeseries_module
from .embedding import map_previous_to_current_clusters
import numpy as np
from config import PM_DEFAULT
import re
import json
import time


def _fmt_money(x):
    try:
        return f"{float(x):,.0f}ì›"
    except Exception:
        return str(x)


# --- ë‹¨ìœ„ ê°•ì œ í›„ì²˜ë¦¬: ì–µ/ë§Œ â†’ ì› ë‹¨ìœ„ ---
_NUM = r'(?:\d{1,3}(?:,\d{3})*|\d+)'


def _to_int(s):
    return int(str(s).replace(',', ''))


def _replace_korean_units(m):
    # ì¼€ì´ìŠ¤: "3ì–µ 5,072ë§Œ ì›" / "54ì–µ 1,444ë§Œ ì›" / "2ì–µ ì›" / "370ë§Œ ì›"
    eok = m.group('eok')
    man = m.group('man')
    won = m.group('won')
    total = 0
    if eok:
        total += _to_int(eok) * 100_000_000
    if man:
        total += _to_int(man) * 10_000
    if won:
        total += _to_int(won)
    return f"{total:,.0f}ì›"


def _enforce_won_units(text: str) -> str:
    # 1) ì–µ/ë§Œ/ì› í˜¼í•©ì„ ì› ë‹¨ìœ„ë¡œ ì¹˜í™˜
    pat = re.compile(
        rf'(?:(?P<eok>{_NUM})\s*ì–µ)?\s*(?:(?P<man>{_NUM})\s*ë§Œ)?\s*(?:(?P<won>{_NUM})\s*ì›)?'
        r'(?!\s*ë‹¨ìœ„)', flags=re.IGNORECASE)

    def _smart_sub(s):
        out = []
        last = 0
        for m in pat.finditer(s):
            # ì˜ë¯¸ ì—†ëŠ” ë¹ˆ ë§¤ì¹­ ë°©ì§€: ì–µ/ë§Œì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ(ì´ë¯¸ ì› ë‹¨ìœ„ì¼ ê°€ëŠ¥ì„±)
            if not any(m.group(g) for g in ('eok', 'man')):
                continue
            out.append(s[last:m.start()])
            out.append(_replace_korean_units(m))
            last = m.end()
        out.append(s[last:])
        return ''.join(out)

    return _smart_sub(text)


def _boldify_bracket_headers(text: str) -> str:
    # [ìš”ì•½], [ì£¼ìš” ê±°ë˜], [ê²°ë¡ ], [ìš©ì–´ ì„¤ëª…] â†’ **[...]**\n
    text = re.sub(r'^\[(ìš”ì•½|ì£¼ìš” ê±°ë˜|ê²°ë¡ |ìš©ì–´ ì„¤ëª…)\]\s*', r'**[\1]**\n', text, flags=re.MULTILINE)
    return text


def _safe_load(s: str):
    """ì—„ê²©í•œ JSON ë¡œë”: ì½”ë“œ íœìŠ¤ ì œê±° í›„ strict json.loads.
    ì£¼ë³€ í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ í—ˆìš©í•˜ì§€ ì•ŠìŒ.
    """
    text = (s or "").strip()
    # ì‹œì‘ íœìŠ¤ ì œê±°
    text = re.sub(r"^\s*```(?:json|JSON)?\s*\n", "", text)
    # ë íœìŠ¤ ì œê±°
    text = re.sub(r"\n\s*```\s*$", "", text)
    text = text.strip()
    return json.loads(text)


def generate_rag_context(master_df: pd.DataFrame, current_df: pd.DataFrame, previous_df: pd.DataFrame,
                         account_codes: List[str], manual_context: str = "",
                         include_risk_summary: bool = False, pm_value: float | None = None) -> str:
    acc_info = master_df[master_df['ê³„ì •ì½”ë“œ'].astype(str).isin(account_codes)]
    acc_names = ", ".join(acc_info['ê³„ì •ëª…'].unique().tolist())
    master_summary = f"- ë¶„ì„ ëŒ€ìƒ ê³„ì • ê·¸ë£¹: {acc_names} ({', '.join(account_codes)})"
    if not acc_info.empty:
        acct_type = acc_info.iloc[0].get('BS/PL', 'PL')
        has_dates_cur = ('íšŒê³„ì¼ì' in current_df.columns) and current_df['íšŒê³„ì¼ì'].notna().any()
        has_dates_prev = ('íšŒê³„ì¼ì' in previous_df.columns) and previous_df['íšŒê³„ì¼ì'].notna().any()
        if str(acct_type).upper() == 'PL' and has_dates_cur:
            min_date = current_df['íšŒê³„ì¼ì'].min(); max_date = current_df['íšŒê³„ì¼ì'].max()
            if has_dates_prev:
                # ë˜í•‘ êµ¬ê°„(ì˜ˆ: 11~2ì›”) ì˜¤íŒ ë°©ì§€: ì—°-ì›” Periodë¡œ ë¹„êµ
                cur_months = current_df['íšŒê³„ì¼ì'].dt.to_period('M')
                prev_months = previous_df['íšŒê³„ì¼ì'].dt.to_period('M')
                mask = prev_months.isin(cur_months.unique())
                prev_f = previous_df.loc[mask]
            else:
                prev_f = previous_df.copy()
            # Net first (ìˆœì•¡: ì°¨-ëŒ€), absolute as reference (ê·œëª¨(ì ˆëŒ€ê°’))
            cur_net = current_df.get('ìˆœì•¡', pd.Series(dtype=float)).sum()
            prev_net = prev_f.get('ìˆœì•¡', pd.Series(dtype=float)).sum()
            cur_abs = current_df.get('ë°œìƒì•¡', pd.Series(dtype=float)).sum()
            prev_abs = prev_f.get('ë°œìƒì•¡', pd.Series(dtype=float)).sum()
            var = cur_net - prev_net
            var_pct = (var / prev_net * 100) if prev_net not in (0, 0.0) else float('inf')
            period = f"{min_date.strftime('%mì›”')}~{max_date.strftime('%mì›”')}"
            master_summary += (
                f"\n- ë‹¹ê¸° **ìˆœì•¡(ì°¨-ëŒ€)** í•©ê³„ ({period}): {cur_net:,.0f}ì›"
                f" | ì „ê¸° ë™ê¸°ê°„ ìˆœì•¡: {prev_net:,.0f}ì› | ìˆœì•¡ ì¦ê°: {var:,.0f}ì› ({var_pct:+.2f}%)"
                f"\n- (ì°¸ê³ ) **ê·œëª¨(ì ˆëŒ€ê°’)** ë°œìƒì•¡: ë‹¹ê¸° {cur_abs:,.0f}ì› | ì „ê¸° {prev_abs:,.0f}ì›"
                f" | ì°¨ì´: {cur_abs - prev_abs:,.0f}ì›"
            )
        else:
            cur_bal = acc_info.get('ë‹¹ê¸°ë§ì”ì•¡', pd.Series(dtype=float)).sum()
            prior_bal = acc_info.get('ì „ê¸°ë§ì”ì•¡', pd.Series(dtype=float)).sum()
            var = cur_bal - prior_bal
            var_pct = (var / prior_bal * 100) if prior_bal not in (0, 0.0) else float('inf')
            master_summary += f"\n- ë‹¹ê¸°ë§ ì”ì•¡(í•©ì‚°): {cur_bal:,.0f}ì› | ì „ê¸°ë§ ì”ì•¡(í•©ì‚°): {prior_bal:,.0f}ì› | ì¦ê°: {var:,.0f}ì› ({var_pct:+.2f}%)"

    manual_summary = f"\n\n## ì‚¬ìš©ì ì œê³µ ì¶”ê°€ ì •ë³´\n{manual_context}" if manual_context and not manual_context.isspace() else ""

    # --- New context layout ---
    sec_info = f"## ë¶„ì„ëŒ€ìƒ ê³„ì •ì •ë³´\n{master_summary}{manual_summary}"
    sec_cur = build_current_cluster_block(current_df)
    # Prior-year: ì „í‘œ ì „ì²´ë¥¼ CY ì„¼íŠ¸ë¡œì´ë“œì— ìµœê·¼ì ‘ ë§¤í•‘í•˜ì—¬ í•©ì‚°í•˜ëŠ” ì¦ê±° ë¸”ë¡ ì‚¬ìš©
    sec_prev = build_previous_projection_block(current_df, previous_df)
    sec_top5_cy = build_zscore_top5_block(current_df, previous_df, topn=5)
    sec_top5_py = build_zscore_top5_block_for_py(previous_df, current_df, topn=5)
    sec_ts = build_timeseries_summary_block(current_df)

    # --- NEW: ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½(ì„ íƒ)
    sec_risk = ""
    if include_risk_summary:
        try:
            sec_risk = _build_risk_matrix_section(current_df, pm_value=pm_value)
        except Exception:
            sec_risk = ""

    parts = [sec_info, sec_cur, sec_prev, sec_ts, sec_top5_cy, sec_top5_py]
    if sec_risk:
        parts.insert(2, sec_risk)  # ì •ë³´â†’(ìœ„í—˜)â†’í´ëŸ¬ìŠ¤í„° ìˆœ
    return "\n".join(parts)


def build_timeseries_summary_block(current_df: pd.DataFrame, topn: int = 5) -> str:
    """
    ## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½
    ê³„ì •ë³„ ì›”ë³„ í•©ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì§€ë§‰ í¬ì¸íŠ¸ì˜ ì˜ˆì¸¡ ëŒ€ë¹„ ì´íƒˆì„ ìš”ì•½.
    """
    if current_df is None or current_df.empty or 'íšŒê³„ì¼ì' not in current_df.columns:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ë°ì´í„° ì—†ìŒ)"
    df = current_df.copy()
    if 'ê³„ì •ëª…' not in df.columns:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ê³„ì •ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤)"
    df['ì—°ì›”'] = df['íšŒê³„ì¼ì'].dt.to_period('M')
    m = (df.groupby(['ê³„ì •ëª…','ì—°ì›”'], as_index=False)['ê±°ë˜ê¸ˆì•¡'].sum())
    m['account'] = m['ê³„ì •ëª…']
    m['date'] = m['ì—°ì›”'].dt.to_timestamp('M')
    m['amount'] = m['ê±°ë˜ê¸ˆì•¡']

    rows = run_timeseries_module(m[['account','date','amount']])
    if rows is None or rows.empty:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ìœ ì˜ë¯¸í•œ ì´íƒˆ ì—†ìŒ)"

    rows = rows.sort_values('risk', ascending=False).head(int(topn))
    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime('%Y-%m-%d') if _pd.notna(x) else ""
        except:
            return ""
    lines = [
        "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½",
        "â€» ê¸°ë³¸ì€ 'ì›”ë³„ ë°œìƒì•¡(Î”ì”ì•¡/flow)'. BS ê³„ì •ì€ **balance** ê¸°ì¤€ë„ ë‚´ë¶€ í‰ê°€í•˜ë©°, ì•„ë˜ í‘œê¸°ëŠ” MoRê³¼ zÂ·riskë¥¼ í•¨ê»˜ ë³´ì—¬ì¤ë‹ˆë‹¤."
    ]
    for _, r in rows.iterrows():
        lines.append(
            f"- [{_fmt_dt(r['date'])}] {r['account']} ({r.get('measure','flow')}, MoR={r.get('model','-')})"
            f" | ì‹¤ì œ {r['actual']:,.0f}ì› vs ì˜ˆì¸¡ {r['predicted']:,.0f}ì›"
            f" â†’ {'ìƒíšŒ' if float(r['error'])>0 else 'í•˜íšŒ'} | z={float(r['z']):+.2f} | risk={float(r['risk']):.2f}"
        )
    return "\n".join(lines)


def _build_risk_matrix_section(current_df: pd.DataFrame, pm_value: float | None = None) -> str:
    """
    ë‹¹ê¸° ë°ì´í„°ë¡œ ì´ìƒì¹˜ ëª¨ë“ˆì„ í•œ ë²ˆ ëŒë ¤ (ê³„ì •Ã—ì£¼ì¥) ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìƒìœ„ ì…€ì„ ìš”ì•½.
    """
    from .anomaly import run_anomaly_module
    from .assertion_risk import build_matrix
    from config import PM_DEFAULT

    if current_df is None or current_df.empty:
        return "\n\n## ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½\n- (ë°ì´í„° ì—†ìŒ)"

    # ê°„ì´ LedgerFrame êµ¬ì„±
    from analysis.contracts import LedgerFrame
    lf = LedgerFrame(df=current_df.copy(), meta={})

    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    mod = run_anomaly_module(lf, target_accounts=None, topn=200, pm_value=pm)
    mat, _ = build_matrix([mod])
    if mat.empty:
        return "\n\n## ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½\n- (ìƒì„±ëœ Evidence ì—†ìŒ)"

    # ìƒìœ„ 10 ì…€ ì¶”ì¶œ
    flat = (
        mat.stack()
          .rename("risk")
          .reset_index()
          .sort_values("risk", ascending=False)
    )
    top = flat[flat["risk"] > 0].head(10)
    if top.empty:
        return "\n\n## ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½\n- (ìœ ì˜ë¯¸í•œ ìœ„í—˜ê°’ ì—†ìŒ)"

    lines = ["\n\n## ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½ (ìƒìœ„ 10)"]
    for _, r in top.iterrows():
        acct = str(r["level_0"]); asrt = str(r["level_1"]); val = float(r["risk"])
        lines.append(f"- {acct} Ã— {asrt} â‡’ {val:.2f}")
    return "\n".join(lines)


def build_methodology_note(report_accounts=None) -> str:
    lines = [
        "\n\n## ë¶„ì„ ê¸°ì¤€(ì•Œë¦¼)",
        "- ì´ë²ˆ ë¶„ì„ì€ UIì—ì„œ ì„ íƒëœ ê³„ì • ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "- ìš”ì•½ ìˆ˜ì¹˜: **ìˆœì•¡(ì°¨-ëŒ€)** ê¸°ì¤€. (ë°œìƒì•¡=ê·œëª¨(ì ˆëŒ€ê°’)ì€ ì°¸ê³ ìš©)",
        "- Z-Score: ì„ íƒ ê³„ì •ë“¤ì˜ **ë°œìƒì•¡(ì ˆëŒ€ê°’)** ë¶„í¬ ê¸°ì¤€.",
        "- ìœ ì‚¬ë„/ê·¼ê±°: **ì ìš”+ê±°ë˜ì²˜** ì„ë² ë”© í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ì „ê¸° ë™ì›” ìš°ì„ ).",
        "- 'í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)'ëŠ” ì˜ë¯¸ê°€ ì¶©ë¶„íˆ ëª¨ì´ì§€ ì•Šì•„ ìë™ìœ¼ë¡œ ë¬¶ì´ì§€ ì•Šì€ ì‚°ë°œì  ê±°ë˜ ë¬¶ìŒì…ë‹ˆë‹¤.",
    ]
    return "\n".join(lines)


# LLM ë³´ê³ ì„œëŠ” ì„œë¹„ìŠ¤ ê³„ì¸µ(stub) ì‚¬ìš© ê¶Œì¥
from services.llm import LLMClient


def _format_from_json(obj: dict) -> str:
    """
    ë‹¨ìˆœ ìŠ¤í‚¤ë§ˆ(JSON) â†’ ìµœì¢… ë§ˆí¬ë‹¤ìš´.
    - key_transactions: LLMì´ ì‘ì„±í•œ ì „ì²´ ì„¹ì…˜ ë§ˆí¬ë‹¤ìš´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - glossary: í•„ìˆ˜ í•­ëª© ë³´ê°•(ì—†ì„ ê²½ìš° ê¸°ë³¸ ì •ì˜ ì¶”ê°€)
    """
    summary = (obj.get("summary") or "").strip()
    kt_val = obj.get("key_transactions")
    # ê³¼ê±° í˜¸í™˜(ì˜¤ë¸Œì íŠ¸ê°€ ì˜¤ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì‹œë„)
    if isinstance(kt_val, dict):
        parts = []
        for k, v in kt_val.items():
            if isinstance(v, str):
                parts.append(v.strip())
        key_tx_md = "\n\n".join(p for p in parts if p)
    else:
        key_tx_md = (kt_val or "").strip()

    conclusion = (obj.get("conclusion") or "").strip()
    glossary_list = obj.get("glossary") or []

    # í•„ìˆ˜ ìš©ì–´ ë³´ê°•: Z-Score, í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)
    g_text = " ".join(map(str, glossary_list))
    need_z = ("z-score" not in g_text.lower()) and ("Z-Score" not in g_text)
    need_noise = ("í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ" not in g_text)
    if need_z:
        glossary_list.append("Z-Score: í‘œë³¸ì˜ ê°’ì´ í‰ê· ì—ì„œ ëª‡ ê°œì˜ í‘œì¤€í¸ì°¨ë§Œí¼ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ(â€˜í‘œì¤€í¸ì°¨ì˜ ë°°ìˆ˜â€™). |Z|ê°€ í´ìˆ˜ë¡ ì´ë¡€ì ì„.")
    if need_noise:
        glossary_list.append("í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1): ì˜ë¯¸ê°€ ì¶©ë¶„íˆ ëª¨ì´ì§€ ì•Šì•„ ìë™ìœ¼ë¡œ ë¬¶ì´ì§€ ì•Šì€ ì‚°ë°œì  ê±°ë˜ ë¬¶ìŒ.")
    glossary = "\n".join(f"- {str(x)}" for x in glossary_list)

    md = (
        f"**[ìš”ì•½]**\n{summary}\n\n"
        f"**[ì£¼ìš” ê±°ë˜]**\n{key_tx_md}\n\n"
        f"**[ê²°ë¡ ]**\n{conclusion}\n\n"
        f"**[ìš©ì–´ ì„¤ëª…]**\n{glossary}"
    )
    return md


# --- ì „ê¸° ì „ì²´ ë§¤í•‘ í•©ì‚° ë°©ì‹ìœ¼ë¡œ êµì²´: ì´ì „ ì „í‘œë¥¼ CY í´ëŸ¬ìŠ¤í„°ì— ìµœê·¼ì ‘ ë§¤í•‘ í›„ í•©ì‚° ---
def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float | None = None) -> str:
    """
    Project all PY vouchers onto CY cluster centroids and aggregate absolute amounts by the CY cluster_group.
    - No similarity computation is shown or used for filtering.
    - Output contains only total absolute amount and ONE example voucher (no sim).
    """
    import pandas as pd
    if current_df is None or previous_df is None or current_df.empty or previous_df.empty:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° ë°ì´í„° ì—†ìŒ)"
    need_cur = {'cluster_id','cluster_name','vector'}
    if not need_cur.issubset(current_df.columns) or 'vector' not in previous_df.columns:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (í´ëŸ¬ìŠ¤í„°/ë²¡í„° ì •ë³´ ë¶€ì¡±)"

    prev_m = map_previous_to_current_clusters(current_df, previous_df)
    if prev_m is None or prev_m.empty or 'mapped_cluster_id' not in prev_m.columns:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ë§¤í•‘ ì‹¤íŒ¨)"

    if 'cluster_group' in current_df.columns:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_group'].to_dict()
    else:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()

    prev_m = prev_m.copy()
    prev_m['mapped_group'] = prev_m['mapped_cluster_id'].map(id2group)
    prev_m['abs_amt'] = prev_m.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs()

    agg = (
        prev_m.groupby('mapped_group', dropna=False)
              .agg(ê·œëª¨=('abs_amt','sum'))
              .reset_index()
              .sort_values('ê·œëª¨', ascending=False)
    )

    lines = ["\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡"]
    for _, row in agg.iterrows():
        g = row['mapped_group'] if pd.notna(row['mapped_group']) else '(ë¯¸ë§¤í•‘)'
        tot = row['ê·œëª¨']
        sub = prev_m[prev_m['mapped_group'] == row['mapped_group']]
        if not sub.empty:
            ex = sub.sort_values('abs_amt', ascending=False).head(1).iloc[0]
            raw_dt = ex.get('íšŒê³„ì¼ì', None)
            if pd.notna(raw_dt):
                try:
                    _dt = pd.to_datetime(raw_dt, errors='coerce')
                    dt = _dt.strftime('%Y-%m-%d') if pd.notna(_dt) else ''
                except Exception:
                    dt = ''
            else:
                dt = ''
            lines.append(f"- [{g}] ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›")
            lines.append(f"  Â· ì˜ˆì‹œ: {dt} | {ex.get('ê±°ë˜ì²˜','')} | {int(ex.get('ë°œìƒì•¡',0)):,.0f}ì›")
        else:
            lines.append(f"- [{g}] ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›")
    return "\n".join(lines)


def run_final_analysis(context: str, account_codes: list[str], *, model: str | None = None, max_tokens: int | None = 16000) -> str:
    system = (
        "You are a CPA. Do all hidden reasoning internally and output ONLY the JSON object in the EXACT schema below. "
        "Language: Korean (ko-KR) for every natural-language value.\n"
        "Schema: {"
        '"summary": str,'
        '"key_transactions": str,'
        '"conclusion": str,'
        '"glossary": [str]'
        "}\n"
        "Authoring rules:\n"
        "â€¢ Monetary values MUST be formatted in KRW like '1,234ì›' (never ì–µ/ë§Œì›).\n"
        "â€¢ [ìš”ì•½]ì€ ê³„ì •êµ° ìˆ˜ì¤€ì˜ ë³€ë™ê³¼ ê·œëª¨ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…ë£Œíˆ.\n"
        "â€¢ [ì£¼ìš” ê±°ë˜]ëŠ” ì „ì²´ ì„œìˆ ì„ ë„¤ê°€ ì„¤ê³„í•˜ë˜, ì»¨í…ìŠ¤íŠ¸(CY/PY í´ëŸ¬ìŠ¤í„°, ë§¤í•‘, Z-score ìƒìœ„ í•­ëª©)ë¥¼ ê·¼ê±°ë¡œ êµ¬ì„± ë¹„ì¤‘/ì´ìƒì¹˜/ì „ê¸° ëŒ€ì‘ê´€ê³„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë¼. í•„ìš”í•˜ë©´ ë¶ˆë¦¿Â·ì†Œì œëª©ì„ ì„ì˜ë¡œ ì‚¬ìš©í•´ ê°€ë…ì„±ì„ ë†’ì—¬ë¼.\n"
        "â€¢ [ê²°ë¡ ]ì€ ì›ì¸Â·ë¦¬ìŠ¤í¬Â·í†µì œÂ·ì•¡ì…˜ì•„ì´í…œ ì¤‘ì‹¬ìœ¼ë¡œ ì‹¤ë¬´ì  ì œì•ˆ ìœ„ì£¼ë¡œ ì‘ì„±í•œë‹¤.\n"
        "â€¢ [ìš©ì–´ ì„¤ëª…]ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ë‘ í•­ëª©ì´ í¬í•¨ë˜ë„ë¡ í•œë‹¤:   1) 'í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)' ì •ì˜, 2) 'Z-Score'ê°€ â€˜í‰ê· ì—ì„œ ëª‡ í‘œì¤€í¸ì°¨â€™ì¸ì§€ì˜ ì§ê´€ì  ì˜ë¯¸.\n"
        "Compliance: Output MUST be the JSON object itself, with no markdown/code-fences or extra text."
    )
    user = (
        f"Target accounts: {', '.join(account_codes)}\n"
        f"{context}\n"
        "Return ONLY the JSON per schema via function call."
    )

    tool_schema = {
        "type": "function",
        "function": {
            "name": "emit_report",
            "description": "Return the report strictly in the fixed JSON schema.",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "key_transactions": {"type": "string"},
                    "conclusion": {"type": "string"},
                    "glossary": {"type": "array","items":{"type":"string"}}
                },
                "required": ["summary","key_transactions","conclusion","glossary"]
            }
        }
    }

    llm = LLMClient()
    max_retries = 2
    last_err = None

    for attempt in range(max_retries + 1):
        try:
            raw = llm.generate(
                system=system, user=user, model=model,
                max_tokens=max_tokens, tools=[tool_schema], force_json=False
            )
            obj = _safe_load(raw)
            text = _format_from_json(obj)
            return _enforce_won_units(text)
        except Exception as e:
            last_err = e
            if attempt < max_retries:
                time.sleep(1.0)
            continue

    raise ValueError(f"LLM failed to produce valid JSON report after retries. Details: {last_err}")


# --- NEW: LLM ë¯¸ì‚¬ìš©/ì‹¤íŒ¨ ì‹œ í´ë°± ë¦¬í¬íŠ¸ (ìˆœìˆ˜ ë¡œì»¬ ê³„ì‚°) ---
def run_offline_fallback_report(current_df: pd.DataFrame,
                                previous_df: pd.DataFrame,
                                account_codes: list[str],
                                pm_value: float | None = None) -> str:
    """
    ì™¸ë¶€ LLMì„ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê°„ë‹¨ ë³´ê³ ì„œë¥¼ ìƒì„±í•œë‹¤.
    - ìš”ì•½: CY/PY ìˆœì•¡/ê·œëª¨ ë¹„êµ
    - ì£¼ìš” ê±°ë˜: |Z| Top 5 (ê°€ëŠ¥í•˜ë©´ Z ê¸°ì¤€, ì—†ìœ¼ë©´ ë°œìƒì•¡ ìƒìœ„)
    - ê²°ë¡ : KIT(â‰¥PM) ê±´ìˆ˜/ë¹„ì¤‘ ë° ë¦¬ìŠ¤í¬ ì£¼ì˜
    - ìš©ì–´: Z-Score, Key Item ê¸°ë³¸ ì •ì˜
    """
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    cur = current_df.copy()
    prev = previous_df.copy()

    def _safe_sum(df, col): 
        return float(df.get(col, pd.Series(dtype=float)).sum()) if not df.empty else 0.0

    cur_net  = _safe_sum(cur, "ìˆœì•¡")
    prev_net = _safe_sum(prev, "ìˆœì•¡")
    cur_abs  = _safe_sum(cur, "ë°œìƒì•¡")
    prev_abs = _safe_sum(prev, "ë°œìƒì•¡")

    var_net = cur_net - prev_net
    var_abs = cur_abs - prev_abs
    var_pct = (var_net / prev_net * 100.0) if prev_net not in (0, 0.0) else float("inf")

    # Top 5: Z-Score ìš°ì„ , ì—†ìœ¼ë©´ ë°œìƒì•¡ ìƒìœ„
    top_df = cur.copy()
    if "Z-Score" in top_df.columns and top_df["Z-Score"].notna().any():
        top_df = top_df.reindex(top_df["Z-Score"].abs().sort_values(ascending=False).index)
    else:
        top_df = top_df.reindex(top_df.get("ë°œìƒì•¡", pd.Series(dtype=float)).abs().sort_values(ascending=False).index)
    top_df = top_df.head(5)

    # KIT ì§‘ê³„(ì ˆëŒ€ë°œìƒì•¡ ê¸°ì¤€)
    kit_mask = top_df.get("ë°œìƒì•¡", pd.Series(dtype=float)).abs() >= pm if not top_df.empty else pd.Series([], dtype=bool)
    kit_cnt  = int(kit_mask.sum()) if not top_df.empty else 0

    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime("%Y-%m-%d") if _pd.notna(x) else ""
        except Exception:
            return ""

    # Compose sections (ê°„ë‹¨ Markdown)
    summary = (
        f"ì„ íƒ ê³„ì •({', '.join(account_codes)}) ê¸°ì¤€ìœ¼ë¡œ ë‹¹ê¸° **ìˆœì•¡** {cur_net:,.0f}ì›,"
        f" ì „ê¸° {prev_net:,.0f}ì› â†’ ì¦ê° {var_net:,.0f}ì› ({var_pct:+.2f}%).\n"
        f"(ì°¸ê³ ) **ê·œëª¨(ë°œìƒì•¡ ì ˆëŒ€ê°’)** ë‹¹ê¸° {cur_abs:,.0f}ì›, ì „ê¸° {prev_abs:,.0f}ì› â†’ ì°¨ì´ {var_abs:,.0f}ì›."
    )

    kt_lines = []
    if not top_df.empty:
        for i, (_, r) in enumerate(top_df.iterrows(), 1):
            dt = _fmt_dt(r.get("íšŒê³„ì¼ì"))
            vend = str(r.get("ê±°ë˜ì²˜", "") or "")
            amt = float(r.get("ë°œìƒì•¡", 0.0))
            z   = r.get("Z-Score", np.nan)
            ztxt = f" | Z={float(z):+.2f}" if not pd.isna(z) else ""
            kt_lines.append(f"- [{i}] {dt} | {vend} | {amt:,.0f}ì›{ztxt}")
    key_tx = "\n".join(kt_lines) if kt_lines else "- ìƒìœ„ í•­ëª©ì„ ì‚°ì¶œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    conclusion = (
        f"PM {pm:,.0f}ì› ê¸°ì¤€ **Key Item(KIT)** í›„ë³´ëŠ” ìƒìœ„ ë¦¬ìŠ¤íŠ¸ ì¤‘ {kit_cnt}ê±´ì…ë‹ˆë‹¤. "
        "Z-Scoreê°€ í° í•­ëª©ì€ ì ìš”Â·ê±°ë˜ì²˜ ë“± ê·¼ê±° í™•ì¸ê³¼ ì›ì¸ íŒŒì•…ì´ í•„ìš”í•©ë‹ˆë‹¤. "
        "ì£¼ìš” ë³€ë™ì€ ì›”ë³„ ì¶”ì´/ìƒê´€ ë¶„ì„ê³¼ í•¨ê»˜ êµì°¨ê²€í† í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    )
    glossary = (
        "- Z-Score: í‘œë³¸ ê°’ì´ í‰ê· ì—ì„œ ëª‡ í‘œì¤€í¸ì°¨ë§Œí¼ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ(â€˜í‘œì¤€í¸ì°¨ì˜ ë°°ìˆ˜â€™). |Z|ê°€ í´ìˆ˜ë¡ ì´ë¡€ì .\n"
        "- Key Item(KIT): ë‹¨ì¼ í•­ëª© ì ˆëŒ€ê¸ˆì•¡ì´ PM ì´ìƒì¸ ì „í‘œ."
    )
    return (
        f"**[ìš”ì•½]**\n{summary}\n\n"
        f"**[ì£¼ìš” ê±°ë˜]**\n{key_tx}\n\n"
        f"**[ê²°ë¡ ]**\n{conclusion}\n\n"
        f"**[ìš©ì–´ ì„¤ëª…]**\n{glossary}"
    )




==============================
ğŸ“„ FILE: analysis/summarization.py
==============================




==============================
ğŸ“„ FILE: analysis/timeseries.py
==============================

# timeseries.py
# v3 â€” Compact TS module with PY+CY window, MoR(EMA/MA/ARIMA/Prophet), dual-basis(flow/balance)
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple, Callable
import math
import numpy as np
import pandas as pd

# -------- Optional config / anomaly imports with safe fallbacks --------
try:
    from config import PM_DEFAULT as _PM_DEFAULT
except Exception:
    _PM_DEFAULT = 0.7
try:
    from config import FORECAST_MIN_POINTS as _FORECAST_MIN_POINTS
except Exception:
    _FORECAST_MIN_POINTS = 8
try:
    from config import ARIMA_DEFAULT_ORDER as _ARIMA_DEFAULT_ORDER
except Exception:
    _ARIMA_DEFAULT_ORDER = (1, 1, 1)

def _risk_from_fallback(z_abs: float, amount: float, pm: float) -> float:
    # simple logistic mapping as a fallback
    return float(1.0 / (1.0 + math.exp(-abs(z_abs))))

try:
    from analysis.anomaly import _risk_from as _RISK_EXTERNAL  # type: ignore
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        try:
            r = _RISK_EXTERNAL(z_abs, amount=amount, pm=float(pm))
            return float(r[-1] if isinstance(r, (list, tuple)) else r)
        except Exception:
            return _risk_from_fallback(z_abs, amount, pm)
except Exception:
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        return _risk_from_fallback(z_abs, amount, pm)

# ----------------------------- Utilities ------------------------------
def _to_month_period_index(dates: pd.Series) -> pd.PeriodIndex:
    return pd.to_datetime(dates).dt.to_period("M")

def _longest_contiguous_month_run(periods: pd.PeriodIndex) -> pd.PeriodIndex:
    if len(periods) <= 1: return periods
    p = pd.PeriodIndex(np.unique(np.asarray(periods)), freq="M")
    best_s = best_e = cur_s = 0
    for i in range(1, len(p)):
        if (p[i] - p[i-1]).n != 1:
            if i-1 - cur_s > best_e - best_s:
                best_s, best_e = cur_s, i-1
            cur_s = i
    if len(p)-1 - cur_s > best_e - best_s:
        best_s, best_e = cur_s, len(p)-1
    return p[best_s:best_e+1]

def _smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    denom = np.maximum(1e-12, np.abs(yt) + np.abs(yp))
    return float(np.mean(200.0 * np.abs(yt - yp) / denom))

def _std_last(x: np.ndarray, w: int = 6) -> float:
    if len(x) < 2: return 0.0
    s = np.std(x[-min(w, len(x)):], ddof=1) if len(x) > 1 else 0.0
    return float(s if math.isfinite(s) else 0.0)

def z_and_risk(residuals: np.ndarray, pm: float = _PM_DEFAULT) -> Tuple[np.ndarray, np.ndarray]:
    """ì”ì°¨ ì‹œí€€ìŠ¤ì— ëŒ€í•´ í‘œì¤€í™” zì™€ ìœ„í—˜ë„ ë°°ì—´ì„ ë°˜í™˜.
    í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„ ìœ„í•´ ê°„ë‹¨í•œ ì •ê·œí™”ì™€ |z|â†’risk ë§¤í•‘ì„ ì‚¬ìš©.
    """
    r = np.asarray(residuals, dtype=float)
    if r.size <= 1:
        z = np.zeros_like(r)
    else:
        sd = float(np.std(r, ddof=1))
        z = (r / sd) if sd > 0 else np.zeros_like(r)
    risk_vals = np.array([_risk_score(abs(float(zi)), amount=1.0, pm=float(pm)) for zi in z], dtype=float)
    return z, risk_vals

def _has_seasonality(y: pd.Series) -> bool:
    y = pd.Series(y, dtype=float)
    if len(y) < 12: return False
    ac = np.abs(np.fft.rfft((y - y.mean()).values))
    core = ac[2:] if len(ac) > 2 else ac
    return bool(core.size and (core.max() / (core.mean() + 1e-9) > 5.0))

# --------------------------- Model backends ---------------------------
def _model_registry() -> Dict[str, bool]:
    ok_arima = ok_prophet = False
    try:
        import statsmodels.api as _  # noqa
        ok_arima = True
    except Exception:
        pass
    try:
        from prophet import Prophet as _  # noqa
        ok_prophet = True
    except Exception:
        pass
    return {"ema": True, "ma": True, "arima": ok_arima, "prophet": ok_prophet}

def model_registry() -> Dict[str, bool]:
    """ê³µê°œ API: ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë°˜í™˜."""
    return _model_registry()

# EMA
def _fit_ema(y: pd.Series, alpha: float = 0.3) -> Dict[str, Any]:
    return {"alpha": float(alpha), "y": y}

def _pred_ema(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]
    alpha = float(m["alpha"])
    pred = y.ewm(alpha=alpha, adjust=False).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# MA
def _fit_ma(y: pd.Series, window: int = 6) -> Dict[str, Any]:
    return {"window": int(window), "y": y}

def _pred_ma(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]; w = int(m["window"])
    pred = y.rolling(w, min_periods=1).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# ARIMA
def _fit_arima(y: pd.Series, order: Tuple[int,int,int] = _ARIMA_DEFAULT_ORDER):
    import statsmodels.api as sm
    return sm.tsa.ARIMA(y, order=tuple(order)).fit()

def _pred_arima(m, steps: Optional[int] = None) -> np.ndarray:
    if steps is None:
        fv = pd.Series(m.fittedvalues).shift(1).fillna(method="bfill")
        return fv.values
    return np.asarray(m.forecast(steps=int(steps)))

# Prophet
def _fit_prophet(y: pd.Series):
    from prophet import Prophet
    df = pd.DataFrame({"ds": y.index.to_timestamp(), "y": y.values})
    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    m.fit(df)
    return {"m": m, "idx": y.index}

def _pred_prophet(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    model = m["m"]; idx: pd.PeriodIndex = m["idx"]
    if steps is None:
        fit = model.predict(pd.DataFrame({"ds": idx.to_timestamp()}))["yhat"].values
        return np.roll(fit, 1)  # 1-step ahead approx.
    last = idx[-1].to_timestamp()
    future = pd.date_range(last + pd.offsets.MonthBegin(1), periods=int(steps), freq="MS")
    return model.predict(pd.DataFrame({"ds": future}))["yhat"].values

# -------------------- Model selection (MoR) via rolling CV -------------
def _rolling_origin_cv(
    y: pd.Series,
    fit_fn, pred_fn,
    k: int = 3, min_train: int = 6
) -> float:
    y = y.dropna(); n = len(y)
    if n < max(min_train + k, 8):
        m = fit_fn(y); yhat = pred_fn(m)
        yhat = np.asarray(yhat)[:n] if yhat is not None else np.repeat(y.iloc[:1].values, n)
        return _smape(y.values, yhat)
    step = max((n - min_train) // (k + 1), 1)
    scores = []
    for i in range(min_train, n, step):
        tr = y.iloc[:i]; te = y.iloc[i:i+step]
        if te.empty: break
        m = fit_fn(tr); yh = pred_fn(m, steps=len(te))
        scores.append(_smape(te.values, np.asarray(yh)[:len(te)]))
    return float(np.mean(scores)) if scores else 999.0

def _choose_model(y: pd.Series, measure: str) -> Tuple[str, np.ndarray]:
    reg = _model_registry()
    cands: List[Tuple[str, np.ndarray, Any, Any]] = []
    # always EMA/MA
    m_ema = _fit_ema(y); yhat_ema = _pred_ema(m_ema); cands.append(("EMA", yhat_ema, _fit_ema, _pred_ema))
    m_ma  = _fit_ma(y);  yhat_ma  = _pred_ma(m_ma);   cands.append(("MA",  yhat_ma,  _fit_ma,  _pred_ma))
    # ARIMA
    if reg["arima"]:
        try:
            m = _fit_arima(y); yhat = _pred_arima(m); cands.append(("ARIMA", yhat, _fit_arima, _pred_arima))
        except Exception:
            pass
    # Prophet: only for flow, enough data & seasonal
    if measure == "flow" and reg["prophet"] and len(y) >= 12 and _has_seasonality(y):
        try:
            m = _fit_prophet(y); yhat = _pred_prophet(m); cands.append(("Prophet", yhat, _fit_prophet, _pred_prophet))
        except Exception:
            pass
    # pick by CV
    scores = [(nm, _rolling_origin_cv(y, fit, pred)) for (nm, _, fit, pred) in cands]
    best = min(scores, key=lambda x: x[1])[0] if scores else "EMA"
    # return best in-sample prediction
    if best == "EMA": return "EMA", yhat_ema
    if best == "MA":  return "MA",  yhat_ma
    if best == "ARIMA":
        try: return "ARIMA", _pred_arima(_fit_arima(y))
        except Exception: return "EMA", yhat_ema
    if best == "Prophet":
        try: return "Prophet", _pred_prophet(_fit_prophet(y))
        except Exception: return "EMA", yhat_ema
    return "EMA", yhat_ema

# --------------------------- Core predictors --------------------------
def _prepare_monthly(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=[date_col]).copy()
    p = _to_month_period_index(df[date_col])
    df2 = df.copy()
    df2["_p"] = p
    df2 = df2.dropna(subset=["_p"]).sort_values("_p")
    run = _longest_contiguous_month_run(df2["_p"])
    return df2[df2["_p"].isin(run)].reset_index(drop=True)

def _one_track_lastrow(
    monthly: pd.DataFrame,
    value_col: str,
    measure: str,
    pm_value: float
) -> Optional[Dict[str, Any]]:
    df = _prepare_monthly(monthly, "date")
    if df.empty or value_col not in df.columns: return None
    y = pd.Series(df[value_col].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2: return None
    model, yhat = _choose_model(y, measure=measure)
    resid = y.values - yhat
    error_last = float(resid[-1])
    sigma = _std_last(resid, w=6)
    z = float(error_last / sigma) if sigma > 0 else 0.0
    risk = _risk_score(abs(z), amount=float(y.iloc[-1]), pm=float(pm_value))
    return {
        "date": df["_p"].iloc[-1].to_timestamp(),
        "measure": measure,
        "actual": float(y.iloc[-1]),
        "predicted": float(yhat[-1]),
        "error": float(error_last),
        "z": float(z),
        "risk": float(risk),
        "model": model,
    }

# ------------------------------- API ----------------------------------
def run_timeseries_for_account(
    monthly: pd.DataFrame,
    account: str,
    is_bs: bool,
    flow_col: str = "flow",
    balance_col: Optional[str] = None,
    allow_prophet: bool = True,   # kept for backward compatibility (no-op switch)
    pm_value: float = _PM_DEFAULT,
    **kwargs: Any,                # absorb legacy args safely
) -> pd.DataFrame:
    """
    ë‹¨ì¼ ê³„ì •ì˜ ì›”ë³„ ë°ì´í„°ì—ì„œ ë§ˆì§€ë§‰ í¬ì¸íŠ¸ë¥¼ í‰ê°€.
    - BS ê³„ì •: flow/balance 2í–‰(í•´ë‹¹ ì‹œ ì¡´ì¬) ë°˜í™˜
    - PL ê³„ì •: flow 1í–‰ ë°˜í™˜
    ë°˜í™˜ ì»¬ëŸ¼: ["date","account","measure","actual","predicted","error","z","risk","model"]
    """
    rows: List[Dict[str, Any]] = []
    # flow
    if flow_col in monthly.columns:
        r = _one_track_lastrow(monthly.rename(columns={flow_col: "val"}), "val", "flow", pm_value)
        if r: rows.append(r)
    # balance
    if is_bs:
        if balance_col and (balance_col in monthly.columns):
            r = _one_track_lastrow(monthly.rename(columns={balance_col: "val"}), "val", "balance", pm_value)
            if r: rows.append(r)
        else:
            if flow_col in monthly.columns:
                tmp = monthly[["date", flow_col]].copy()
                tmp["val"] = tmp[flow_col].astype(float).cumsum()
                r = _one_track_lastrow(tmp[["date","val"]], "val", "balance", pm_value)
                if r: rows.append(r)
    out = pd.DataFrame(rows)
    if not out.empty:
        out["account"] = account
        out = out[["date","account","measure","actual","predicted","error","z","risk","model"]]
        out = out.sort_values(["account","measure","date"]).reset_index(drop=True)
    else:
        out = pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"])
    return out

def run_timeseries_module(
    df: pd.DataFrame,
    *,
    account_col: str = "account",
    date_col: str = "date",
    amount_col: str = "amount",
    pm_value: float = _PM_DEFAULT,
    make_balance: bool = False,  # ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½: í•„ìš” ì‹œ balance êµ¬ì„±
    output: str = "all",        # "all" | "flow" | "balance"
    evidence_adapter: Optional[Callable[[Dict[str, Any]], Any]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    ì§‘ê³„í˜•: ê³„ì •ë³„ ì›”í•©ê³„(amount)ë§Œ ì£¼ì–´ì§„ ê²½ìš°.
    ê¸°ë³¸: flowë§Œ ê³„ì‚°. make_balance=Trueì¼ ë•Œ balance(ëˆ„ì í•©)ë„ í•¨ê»˜ ê³„ì‚°.
    outputìœ¼ë¡œ ìµœì¢… ë°˜í™˜ í•„í„°ë§ ê°€ëŠ¥("flow"/"balance").
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    work = df[[account_col, date_col, amount_col]].copy()
    work.columns = ["account","date","amount"]
    work = work.sort_values(["account","date"])
    all_rows: List[pd.DataFrame] = []
    for acc, g in work.groupby("account", dropna=False):
        mon = g[["date","amount"]].rename(columns={"amount":"flow"}).copy()
        if make_balance:
            mon["balance"] = mon["flow"].astype(float).cumsum()
        out = run_timeseries_for_account(mon, str(acc), is_bs=make_balance, flow_col="flow",
                                         balance_col=("balance" if make_balance else None),
                                         pm_value=float(pm_value))
        if not out.empty:
            # CEAVOP ì œì•ˆ(ê°„ë‹¨ ê·œì¹™): error>0 â†’ E(ì¡´ì¬), error<=0 â†’ C(ì™„ì „ì„±)
            try:
                out["assertion"] = out["error"].map(lambda e: "E" if float(e) > 0 else "C")
            except Exception:
                out["assertion"] = "E"
        if output in ("flow", "balance") and not out.empty:
            out = out[out["measure"] == output]
        all_rows.append(out)
    result = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    if evidence_adapter is not None and not result.empty:
        rows = []
        for r in result.to_dict(orient="records"):
            d = dict(r)
            if "amount" not in d:
                d["amount"] = float(d.get("actual", 0.0))
            if "z_abs" not in d:
                try:
                    d["z_abs"] = abs(float(d.get("z", 0.0)))
                except Exception:
                    d["z_abs"] = 0.0
            if "assertion" not in d:
                try:
                    d["assertion"] = "E" if float(d.get("error", 0.0)) > 0 else "C"
                except Exception:
                    d["assertion"] = "E"
            rows.append(evidence_adapter(d))
        return rows  # type: ignore[return-value]
    return result

def run_timeseries_module_with_flag(
    df: pd.DataFrame,
    *,
    account_col: str = "account",
    date_col: str = "date",
    amount_col: str = "amount",
    is_bs_col: str = "is_bs",
    pm_value: float = _PM_DEFAULT,
) -> pd.DataFrame:
    """
    í˜¼í•© ë°ì´í„°ì…‹ì—ì„œ ê³„ì •ë³„ BS ì—¬ë¶€ì— ë”°ë¼ ë“€ì–¼(Flow+Balance) ë˜ëŠ” ë‹¨ì¼(Flow)ë¡œ ì²˜ë¦¬.
    - is_bs=True: flow + balance(ëˆ„ì í•©) ê³„ì‚°
    - is_bs=False: flowë§Œ ê³„ì‚°
    ë°˜í™˜ ì»¬ëŸ¼: ["account","date","measure","actual","predicted","error","z","risk","model"]
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    work = df[[account_col, date_col, amount_col, is_bs_col]].copy()
    work.columns = ["account","date","amount","is_bs"]
    work = work.sort_values(["account","date"])
    outs: List[pd.DataFrame] = []
    for acc, g in work.groupby("account", dropna=False):
        is_bs = bool(g["is_bs"].iloc[-1])
        mon = g[["date","amount"]].rename(columns={"amount":"flow"}).copy()
        balance_col = None
        if is_bs:
            mon["balance"] = mon["flow"].astype(float).cumsum()
            balance_col = "balance"
        out = run_timeseries_for_account(
            mon, str(acc), is_bs=is_bs, flow_col="flow",
            balance_col=balance_col, pm_value=float(pm_value)
        )
        if not out.empty:
            try:
                out["assertion"] = out["error"].map(lambda e: "E" if float(e) > 0 else "C")
            except Exception:
                out["assertion"] = "E"
        outs.append(out)
    return pd.concat(outs, ignore_index=True) if outs else pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])



==============================
ğŸ“„ FILE: analysis/trend.py
==============================

import pandas as pd
import plotly.express as px
from typing import List, Dict, Any
from analysis.contracts import LedgerFrame, ModuleResult


def create_monthly_trend_figure(ledger_df: pd.DataFrame, master_df: pd.DataFrame, account_code: str, account_name: str):
    """BS/PL, ì°¨/ëŒ€ë³€ ì„±ê²©ì„ ë°˜ì˜í•˜ì—¬ ì›”ë³„ ì¶”ì´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    mrow = master_df[master_df['ê³„ì •ì½”ë“œ'] == account_code]
    if mrow.empty:
        return None  # ì•ˆì „ ê°€ë“œ
    master_row = mrow.iloc[0]
    bspl = master_row.get('BS/PL', 'PL').upper()
    nature = master_row.get('ì°¨ë³€/ëŒ€ë³€', 'ì°¨ë³€').strip()
    sign = -1.0 if 'ëŒ€ë³€' in nature else 1.0

    current_year = ledger_df['ì—°ë„'].max()
    df_filtered = ledger_df[(ledger_df['ê³„ì •ì½”ë“œ'] == account_code) & (ledger_df['ì—°ë„'].isin([current_year, current_year - 1]))]
    months = list(range(1, 13))
    plot_df_list = []

    if bspl == 'BS':
        bop_cy = master_row.get('ì „ê¸°ë§ì”ì•¡', 0)
        bop_py = master_row.get('ì „ì „ê¸°ë§ì”ì•¡', 0)
        for year, bop, year_label in [(current_year, bop_cy, 'CY'), (current_year - 1, bop_py, 'PY')]:
            monthly_flow = df_filtered[df_filtered['ì—°ë„'] == year].groupby('ì›”')['ê±°ë˜ê¸ˆì•¡'].sum()
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(monthly_flow)
            monthly_balance = bop + monthly_series.cumsum()
            plot_df_list.append(pd.DataFrame({'ì›”': months, 'ê¸ˆì•¡': monthly_balance.values * sign, 'êµ¬ë¶„': year_label}))
        title_suffix = "ì›”ë³„ ì”ì•¡ ì¶”ì´ (BS)"
    else:
        monthly_sum = df_filtered.groupby(['ì—°ë„', 'ì›”'])['ê±°ë˜ê¸ˆì•¡'].sum().reset_index()
        for year, year_label in [(current_year, 'CY'), (current_year - 1, 'PY')]:
            year_data = monthly_sum[monthly_sum['ì—°ë„'] == year]
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(year_data.set_index('ì›”')['ê±°ë˜ê¸ˆì•¡'])
            plot_df_list.append(pd.DataFrame({'ì›”': months, 'ê¸ˆì•¡': monthly_series.values * sign, 'êµ¬ë¶„': year_label}))
        title_suffix = "ì›”ë³„ ë°œìƒì•¡ ì¶”ì´ (PL)"

    if not plot_df_list:
        return None

    plot_df = pd.concat(plot_df_list)
    fig = px.bar(
        plot_df,
        x='ì›”', y='ê¸ˆì•¡', color='êµ¬ë¶„', barmode='group',
        title=f"'{account_name}' ({account_code}) {title_suffix}",
        labels={'ì›”': 'ì›”', 'ê¸ˆì•¡': 'ê¸ˆì•¡', 'êµ¬ë¶„': 'ì—°ë„'},
        color_discrete_map={'PY': '#a9a9a9', 'CY': '#1f77b4'}
    )
    fig.update_xaxes(dtick=1)
    # ğŸ”¢ ì¶•/íˆ´íŒ í¬ë§·: ì²œë‹¨ìœ„ ì‰¼í‘œ, SI ë‹¨ìœ„ ì œê±°
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none')
    fig.update_traces(hovertemplate='ì›”=%{x}<br>ê¸ˆì•¡=%{y:,.0f} ì›<br>êµ¬ë¶„=%{fullData.name}<extra></extra>')
    return fig


# (ì œê±°ë¨) ìë™ ì¶”ì²œ ë¡œì§: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒí•œ ê³„ì •ë§Œ ì‚¬ìš©


def run_trend_module(lf: LedgerFrame, accounts: List[str] | None = None) -> ModuleResult:
    """ì›”ë³„ ì¶”ì´ ëª¨ë“ˆ: ì‚¬ìš©ìê°€ ì„ íƒí•œ ê³„ì •ë§Œ ê·¸ë¦°ë‹¤(ìë™ ì¶”ì²œ ì—†ìŒ)."""
    df = lf.df
    master_df = lf.meta.get("master_df")
    if master_df is None:
        return ModuleResult(
            name="trend",
            summary={},
            tables={},
            figures={},
            evidences=[],
            warnings=["Master DFê°€ ì—†ìŠµë‹ˆë‹¤."]
        )

    # âœ… ìë™ ì¶”ì²œ ì œê±°: ê³„ì •ì´ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
    if not accounts:
        return ModuleResult(
            name="trend",
            summary={"picked_accounts": [], "n_figures": 0, "period_tag_coverage": {}},
            tables={},
            figures={},
            evidences=[],
            warnings=["ê³„ì •ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ìë™ ì¶”ì²œ ë¹„í™œì„±í™”)"]
        )

    acc_codes = [str(a) for a in accounts]
    figures: Dict[str, Any] = {}
    warns: List[str] = []
    for code in acc_codes:
        m = master_df[master_df['ê³„ì •ì½”ë“œ'].astype(str) == code]
        if m.empty:
            warns.append(f"ê³„ì •ì½”ë“œ {code}ê°€ Masterì— ì—†ìŠµë‹ˆë‹¤.")
            continue
        name = m.iloc[0].get('ê³„ì •ëª…', str(code))
        fig = create_monthly_trend_figure(df, master_df, code, name)
        if fig:
            figures[f"{code}:{name}"] = fig
        else:
            warns.append(f"{name}({code}) ê·¸ë¦¼ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±).")

    summary = {
        "picked_accounts": acc_codes,
        "n_figures": len(figures),
        "period_tag_coverage": dict(df['period_tag'].value_counts()) if 'period_tag' in df.columns else {},
    }

    return ModuleResult(
        name="trend",
        summary=summary,
        tables={},
        figures=figures,
        evidences=[],
        warnings=warns
    )




==============================
ğŸ“„ FILE: analysis/vendor.py
==============================

from typing import List, Dict, Any
import pandas as pd
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from itertools import product
from analysis.contracts import LedgerFrame, ModuleResult


def create_pareto_figure(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True):
    """ê±°ë˜ì²˜ë³„ ê±°ë˜ê¸ˆì•¡ íŒŒë ˆí†  ì°¨íŠ¸.
    - min_amount ì´ìƒì¸ ê±°ë˜ì²˜ë§Œ ê°œë³„ í‘œê¸°
    - ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°(ì˜µì…˜)
    """
    cy_df = ledger_df[ledger_df['ì—°ë„'] == ledger_df['ì—°ë„'].max()]
    if 'ê±°ë˜ì²˜' not in cy_df.columns or cy_df['ê±°ë˜ì²˜'].nunique() < 1:
        return None

    vendor_sum = cy_df.groupby('ê±°ë˜ì²˜')['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'].sum()
    if vendor_sum.empty:
        return None

    if min_amount and min_amount > 0:
        above = vendor_sum[vendor_sum >= min_amount].sort_values(ascending=False)
    else:
        above = vendor_sum.sort_values(ascending=False)

    # 'ê¸°íƒ€' í•©ì‚°
    etc_sum = float(vendor_sum.sum() - above.sum())
    series = above
    if include_others and etc_sum > 0:
        import pandas as pd
        series = pd.concat([above, pd.Series({'ê¸°íƒ€': etc_sum})])

    # ëˆ„ì  ë¹„ìœ¨(í‘œì‹œëœ ë§‰ëŒ€ ê¸°ì¤€; 'ê¸°íƒ€' í¬í•¨ ì‹œ 100%ë¡œ ìˆ˜ë ´)
    cum_ratio = series.cumsum() / series.sum() * 100.0

    from plotly.subplots import make_subplots
    import plotly.graph_objects as go
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(go.Bar(x=series.index, y=series.values, name='ê±°ë˜ ê¸ˆì•¡'), secondary_y=False)
    fig.add_trace(go.Scatter(x=series.index, y=cum_ratio.values, name='ëˆ„ì  ë¹„ìœ¨(%)', mode='lines+markers'), secondary_y=True)
    fig.update_layout(title='ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë¶„ì„ (Pareto)', yaxis_title='ê¸ˆì•¡', yaxis2_title='ëˆ„ì  ë¹„ìœ¨(%)')

    # ğŸ”¢ ì¶•/íˆ´íŒ í¬ë§·
    # ì¢Œì¸¡ ê¸ˆì•¡ì¶•: ì²œë‹¨ìœ„, SI ì œê±°
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none', secondary_y=False)
    # ìš°ì¸¡ %ì¶•
    fig.update_yaxes(tickformat='.1f', ticksuffix='%', secondary_y=True)

    # íˆ´íŒ í¬ë§·
    fig.update_traces(hovertemplate='%{x}<br>%{y:,.0f} ì›<extra></extra>', selector=dict(type='bar'))
    fig.update_traces(hovertemplate='%{x}<br>ëˆ„ì  ë¹„ìœ¨=%{y:.1f}%<extra></extra>', selector=dict(type='scatter'))

    return fig


def create_vendor_heatmap(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True):
    """ê±°ë˜ì²˜ë³„ ì›”ë³„ í™œë™ íˆíŠ¸ë§µ.
    - min_amount ì´ìƒì¸ ê±°ë˜ì²˜ë§Œ ê°œë³„ í‘œê¸°
    - ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ ì›”ë³„ í•©ì‚°(ì˜µì…˜)
    """
    if 'ê±°ë˜ì²˜' not in ledger_df.columns or ledger_df['ê±°ë˜ì²˜'].nunique() < 1:
        return None

    df = ledger_df.copy()
    df['ì—°ì›”'] = df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
    pivot = df.pivot_table(index='ê±°ë˜ì²˜', columns='ì—°ì›”', values='ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’', aggfunc='sum').fillna(0)
    if pivot.empty:
        return None

    totals = pivot.sum(axis=1)
    if min_amount and min_amount > 0:
        above_idx = totals >= min_amount
    else:
        above_idx = totals >= 0  # ì „ë¶€

    pivot_above = pivot.loc[above_idx].copy()
    # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬(í•©ê³„ ê¸°ì¤€)
    pivot_above['_tot_'] = pivot_above.sum(axis=1)
    pivot_above = pivot_above.sort_values('_tot_', ascending=False).drop(columns=['_tot_'])

    # 'ê¸°íƒ€' í–‰ í•©ì‚°
    import pandas as pd
    pivot_final = pivot_above
    if include_others:
        below = pivot.loc[~above_idx]
        if not below.empty:
            etc_row = pd.DataFrame([below.sum(axis=0)], index=['ê¸°íƒ€'])
            pivot_final = pd.concat([pivot_above, etc_row], axis=0)

    import plotly.express as px
    fig = px.imshow(pivot_final, title="ê±°ë˜ì²˜ ì›”ë³„ í™œë™ íˆíŠ¸ë§µ", labels=dict(x="ì—°ì›”", y="ê±°ë˜ì²˜", color="ê±°ë˜ê¸ˆì•¡"))

    # ğŸ”¢ ì»¬ëŸ¬ë°”/íˆ´íŒ í¬ë§·
    fig.update_coloraxes(colorbar=dict(tickformat=',.0f'))
    fig.update_traces(hovertemplate='ì—°ì›”=%{x}<br>ê±°ë˜ì²˜=%{y}<br>ê±°ë˜ê¸ˆì•¡=%{z:,.0f} ì›<extra></extra>')
    return fig


def create_vendor_detail_figure(ledger_df: pd.DataFrame, vendor_name: str, all_months: List[str]):
    """íŠ¹ì • ê±°ë˜ì²˜ì˜ ì›”ë³„ ê±°ë˜ì•¡ì„ ê³„ì •ë³„ ëˆ„ì  ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ì „ì²´ ê¸°ê°„ Xì¶• ë³´ì¥)"""
    vendor_df = ledger_df[ledger_df['ê±°ë˜ì²˜'] == vendor_name].copy()

    vendor_df['ì—°ì›”'] = vendor_df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
    summary = vendor_df.groupby(['ì—°ì›”', 'ê³„ì •ëª…'], as_index=False)['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'].sum()

    unique_accounts = vendor_df['ê³„ì •ëª…'].unique()

    # ì¶• ì œëª© ì •ì˜
    axis_labels = {'ì—°ì›”': 'ê±°ë˜ì›”', 'ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’': 'ê±°ë˜ê¸ˆì•¡'}

    if len(unique_accounts) == 0:
        empty_df = pd.DataFrame({'ì—°ì›”': all_months, 'ê³„ì •ëª…': [None] * len(all_months), 'ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’': [0] * len(all_months)})
        fig = px.bar(empty_df, x='ì—°ì›”', y='ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’', labels=axis_labels)
    else:
        template_df = pd.DataFrame(list(product(all_months, unique_accounts)), columns=['ì—°ì›”', 'ê³„ì •ëª…'])
        merged_summary = pd.merge(template_df, summary, on=['ì—°ì›”', 'ê³„ì •ëª…'], how='left').fillna(0)
        fig = px.bar(
            merged_summary,
            x='ì—°ì›”', y='ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’', color='ê³„ì •ëª…',
            category_orders={'ì—°ì›”': all_months},
            labels=axis_labels,
        )

    fig.update_layout(
        barmode='stack',
        title=f"'{vendor_name}' ê±°ë˜ì²˜ ì›”ë³„/ê³„ì •ë³„ ìƒì„¸ ë‚´ì—­"
    )
    # ğŸ”¢ ì¶•/íˆ´íŒ í¬ë§·: ì²œë‹¨ìœ„ ì‰¼í‘œ, SI ì œê±°
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none')
    fig.update_traces(hovertemplate='ì—°ì›”=%{x}<br>ê¸ˆì•¡=%{y:,.0f} ì›<br>ê³„ì •ëª…=%{fullData.name}<extra></extra>')
    return fig


def run_vendor_module(lf: LedgerFrame, account_codes: List[str] | None = None,
                      min_amount: float = 0, include_others: bool = True) -> ModuleResult:
    """ê±°ë˜ì²˜ ëª¨ë“ˆ: ì„ íƒ ê³„ì • í•„í„° + ìµœì†Œ ê±°ë˜ê¸ˆì•¡ í•„í„°('ê¸°íƒ€' í•©ì‚°)."""
    df = lf.df
    use_df = df.copy()
    if account_codes:
        acs = [str(a) for a in account_codes]
        use_df = use_df[use_df['ê³„ì •ì½”ë“œ'].astype(str).isin(acs)]

    figures: Dict[str, Any] = {}
    warnings: List[str] = []

    pareto = create_pareto_figure(use_df, min_amount=min_amount, include_others=include_others)
    heatmap = create_vendor_heatmap(use_df, min_amount=min_amount, include_others=include_others)

    if pareto: figures['pareto'] = pareto
    else: warnings.append("Pareto ê·¸ë˜í”„ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±).")
    if heatmap: figures['heatmap'] = heatmap
    else: warnings.append("íˆíŠ¸ë§µ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±).")

    # ìš”ì•½ ì •ë³´
    cy = use_df[use_df['ì—°ë„'] == use_df['ì—°ë„'].max()]
    vendor_sum = cy.groupby('ê±°ë˜ì²˜')['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'].sum() if not cy.empty else pd.Series(dtype=float)
    n_above = int((vendor_sum >= min_amount).sum()) if not vendor_sum.empty else 0
    n_below = int((vendor_sum < min_amount).sum()) if not vendor_sum.empty else 0

    summary = {
        "filtered_accounts": [str(a) for a in account_codes] if account_codes else [],
        "min_amount": float(min_amount),
        "include_others": bool(include_others),
        "n_above_threshold": n_above,
        "n_below_threshold": n_below,
        "n_figures": len(figures),
        "period_tag_coverage": dict(use_df['period_tag'].value_counts()) if 'period_tag' in use_df.columns else {},
    }
    return ModuleResult(
        name="vendor",
        summary=summary,
        tables={},
        figures=figures,
        evidences=[],
        warnings=warnings
    )



==============================
ğŸ“„ FILE: analysis/__init__.py
==============================






==============================
ğŸ“„ FILE: infra/env_loader.py
==============================

from __future__ import annotations
import os, sys
from typing import Optional


def _read_kv_file(path: str) -> dict:
    # ë‹¨ìˆœ .env íŒŒì„œ (python-dotenv ì—†ì´ë„ ì‘ë™)
    data = {}
    if not os.path.exists(path):
        return data
    with open(path, "r", encoding="utf-8") as f:
        for raw in f:
            line = raw.strip()
            if not line or line.startswith("#"):
                continue
            if "=" not in line:
                continue
            k, v = line.split("=", 1)
            k = k.strip()
            v = v.strip().strip("\"'")  # ì–‘ìª½ ë”°ì˜´í‘œ ì œê±°
            data[k] = v
    return data


def _maybe_import_dotenv():
    try:
        from dotenv import load_dotenv  # type: ignore
        return load_dotenv
    except Exception:
        return None


# ë™ì˜ì–´ í‚¤ -> í‘œì¤€ í‚¤ ì •ê·œí™”
_OPENAI_ALIASES = [
    "OPENAI_API_KEY", "OPENAI_KEY", "OPENAI_TOKEN", "OPENAIAPIKEY", "OPENAI_APIKEY",
    # Azure/OpenAI ë³€í˜•ë“¤ (ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œë„ í—ˆìš©)
    "AZURE_OPENAI_API_KEY",
]


def _normalize_env(d: dict) -> None:
    # í‘œì¤€ í‚¤ê°€ ì—†ê³ , ë™ì˜ì–´ê°€ ìˆìœ¼ë©´ ëŒì–´ì™€ì„œ OPENAI_API_KEY ì„¸íŒ…
    if not d.get("OPENAI_API_KEY"):
        for k in _OPENAI_ALIASES:
            if k in d and d[k]:
                d["OPENAI_API_KEY"] = d[k]
                break


def ensure_api_keys_loaded() -> bool:
    # 1) python-dotenvê°€ ìˆìœ¼ë©´ ë¨¼ì € ì‹œë„
    load_dotenv = _maybe_import_dotenv()
    if load_dotenv:
        # ë‘ íŒŒì¼ì„ ëª¨ë‘ ì‹œë„(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì ìš©)
        for p in (".env", "API_KEY.env"):
            try:
                load_dotenv(dotenv_path=p, override=False)
            except Exception:
                pass

    # 2) ìˆ˜ë™ íŒŒì‹± (dotenvê°€ ì—†ê±°ë‚˜, ëª» ì½ì€ ê²½ìš° ëŒ€ë¹„)
    merged = {}
    for p in (".env", "API_KEY.env"):
        try:
            merged.update(_read_kv_file(p))
        except Exception:
            pass

    # 3) ë™ì˜ì–´ ì •ê·œí™” â†’ OPENAI_API_KEY
    _normalize_env(merged)

    # 4) í™˜ê²½ë³€ìˆ˜ì— ë°˜ì˜(ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì„¸íŒ…)
    for k, v in merged.items():
        if k not in os.environ and v:
            os.environ[k] = v

    ok = bool(os.environ.get("OPENAI_API_KEY") or os.environ.get("AZURE_OPENAI_API_KEY"))
    # ê°€ì‹œì  í”Œë˜ê·¸
    os.environ["LLM_AVAILABLE"] = "1" if ok else "0"
    return ok


def is_llm_ready() -> bool:
    return os.environ.get("LLM_AVAILABLE", "0") == "1"


def log_llm_status(logger=None):
    # í•œêµ­ì–´ ìƒíƒœ ë¡œê·¸
    if is_llm_ready():
        msg = "ğŸ”Œ OpenAI Key ê°ì§€: ì˜¨ë¼ì¸ LLM ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ì‚¬ìš©)"
    else:
        msg = "ğŸ”Œ OpenAI Key ì—†ìŒ: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ë¯¸ì‚¬ìš©)"
    if logger:
        try:
            logger.info(msg)
            return
        except Exception:
            pass
    print(msg, file=sys.stderr)


# ì•± ë¶€íŒ… ì‹œ ì‚¬ìš©í•  ì§„ì…ì  (importë§Œìœ¼ë¡œ ë¶€íŒ…ì´ˆê¸°í™”í•˜ê³  ì‹¶ì„ ë•Œ)
def boot():
    ensure_api_keys_loaded()
    log_llm_status()





==============================
ğŸ“„ FILE: services/cache.py
==============================

# services/cache.py  (ì „ì²´ êµì²´ë³¸)
# - ì„ë² ë”© ìºì‹œ(SQLite)
# - LLM ë§¤í•‘ ìºì‹œ(ìŠ¹ì¸/ë²„ì „)
# - ìºì‹œ ì •ë³´ í—¬í¼

from __future__ import annotations
import os, sqlite3, json, hashlib, threading, time
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from config import EMBED_CACHE_DIR

# ëª¨ë¸ë³„ DB íŒŒì¼ ì ‘ê·¼ ì‹œ ë ˆì´ìŠ¤ë¥¼ ë§‰ê¸° ìœ„í•œ ë½ ë§µ
_LOCKS: Dict[str, threading.Lock] = {}

def _model_dir(model: str) -> Path:
    # ëª¨ë¸ëª…ì„ ì•ˆì „í•œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜
    safe = model.replace("/", "_").replace(":", "_")
    d = Path(EMBED_CACHE_DIR) / safe
    d.mkdir(parents=True, exist_ok=True)
    return d

def _db_path(model: str) -> str:
    return str(_model_dir(model) / "embeddings.sqlite3")

def _sha1(s: str) -> str:
    # ë³´ì•ˆìš©ì´ ì•„ë‹Œ í‚¤ í•´ì‹±(ìºì‹œ í‚¤ìš©)
    return hashlib.sha1(s.encode("utf-8"), usedforsecurity=False).hexdigest()

def _get_lock(model: str) -> threading.Lock:
    if model not in _LOCKS:
        _LOCKS[model] = threading.Lock()
    return _LOCKS[model]

def _ensure_db(conn: sqlite3.Connection):
    # ê¸°ë³¸ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë³´ì¥
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS emb (
            k TEXT PRIMARY KEY,
            text TEXT,
            vec TEXT
        )
    """)
    conn.commit()

def _open(model: str) -> sqlite3.Connection:
    p = _db_path(model)
    conn = sqlite3.connect(p, timeout=30)
    _ensure_db(conn)
    return conn

def cache_get_many(model: str, texts: List[str]) -> Dict[str, List[float]]:
    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìºì‹œ ì¡°íšŒ
    if not texts:
        return {}
    keys = [(t, _sha1(f"{model}|{t}")) for t in texts]
    with _get_lock(model):
        conn = _open(model)
        try:
            cur = conn.cursor()
            qmarks = ",".join("?" for _ in keys)
            cur.execute(f"SELECT k, vec FROM emb WHERE k IN ({qmarks})", [k for _, k in keys])
            rows = {k: json.loads(vec) for (k, vec) in cur.fetchall()}
        finally:
            conn.close()
    out: Dict[str, List[float]] = {}
    for t, k in keys:
        if k in rows:
            out[t] = rows[k]
    return out

def cache_put_many(model: str, pairs: List[Tuple[str, List[float]]]) -> None:
    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸-ë²¡í„° ìŒì„ ìºì‹œì— ì €ì¥
    if not pairs:
        return
    records = [(_sha1(f"{model}|{t}"), t, json.dumps(vec)) for (t, vec) in pairs]
    with _get_lock(model):
        conn = _open(model)
        try:
            conn.executemany("INSERT OR REPLACE INTO emb(k,text,vec) VALUES (?,?,?)", records)
            conn.commit()
        finally:
            conn.close()

def get_or_embed_texts(
    texts: List[str],
    client,
    *,
    model: str,
    batch_size: int,
    timeout: int,
    max_retry: int,
) -> Dict[str, List[float]]:
    # í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ìºì‹œë¥¼ ìš°ì„  ì¡°íšŒí•˜ê³ , ëˆ„ë½ë¶„ë§Œ ì„ë² ë”© API í˜¸ì¶œ
    texts = list(dict.fromkeys([str(t) for t in texts]))
    cached = cache_get_many(model, texts)
    missing = [t for t in texts if t not in cached]
    if not missing:
        return cached
    out = dict(cached)
    for s in range(0, len(missing), batch_size):
        sub = missing[s:s+batch_size]
        last_err = None
        for attempt in range(max_retry):
            try:
                try:
                    resp = client.embeddings.create(model=model, input=sub, timeout=timeout)
                except TypeError:
                    # ì¼ë¶€ í´ë¼ì´ì–¸íŠ¸ëŠ” timeout íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                    resp = client.embeddings.create(model=model, input=sub)
                vecs = [d.embedding for d in resp.data]
                pairs = list(zip(sub, vecs))
                cache_put_many(model, pairs)
                out.update({sub[i]: vecs[i] for i in range(len(sub))})
                last_err = None
                break
            except Exception as e:
                last_err = e
        if last_err:
            # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ, ë§ˆì§€ë§‰ ì˜ˆì™¸ë¥¼ ì „íŒŒ
            raise last_err
    return out

# ===== LLM ë§¤í•‘ ìºì‹œ (ìŠ¹ì¸/ë²„ì „ ê³ ì •) =====
DEFAULT_CACHE_PATH = os.path.join(".cache", "llm_mappings.json")

class LLMMappingCache:
    # ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ìŠ¹ì¸/ë²„ì „ ê´€ë¦¬
    def __init__(self, path: str = DEFAULT_CACHE_PATH):
        self.path = path
        self._lock = threading.Lock()
        self._state: Dict[str, Any] = {"approved": {}, "proposed": {}, "versions": {}}
        self._load()

    def _load(self) -> None:
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        if os.path.exists(self.path):
            try:
                with open(self.path, "r", encoding="utf-8") as f:
                    self._state = json.load(f)
            except Exception:
                # íŒŒì† íŒŒì¼ì€ ì¡°ìš©íˆ ë¬´ì‹œ
                pass

    def _save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self._state, f, ensure_ascii=False, indent=2)

    def get_approved(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["approved"].get(key)
            return dict(item) if item else None

    def get_proposed(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["proposed"].get(key)
            return dict(item) if item else None

    def propose(self, key: str, value: Any, *, model: str, meta: Optional[Dict[str, Any]] = None) -> None:
        with self._lock:
            self._state["proposed"][key] = {
                "value": value,
                "model": model,
                "meta": meta or {},
                "ts": time.time(),
            }
            self._save()

    def approve(self, key: str, *, value_override: Any | None = None, note: str = "") -> Dict[str, Any]:
        # ì´ˆì•ˆì„ ìŠ¹ì¸í•˜ì—¬ ë²„ì „ì„ ì˜¬ë¦¬ê³  í™•ì •
        with self._lock:
            src = self._state["proposed"].get(key) or self._state["approved"].get(key)
            if not src:
                raise KeyError(f"No proposed/approved entry for key={key!r}")
            prev_ver = int(self._state["versions"].get(key, 0))
            new_ver = prev_ver + 1
            final_value = src["value"] if value_override is None else value_override
            rec = {
                "value": final_value,
                "version": f"v{new_ver}",
                "note": note,
                "approved_ts": time.time(),
                "model": src.get("model"),
                "meta": src.get("meta", {}),
            }
            self._state["approved"][key] = rec
            self._state["versions"][key] = new_ver
            if key in self._state["proposed"]:
                del self._state["proposed"][key]
            self._save()
            return dict(rec)

# ===== ì„ë² ë”© ìºì‹œ ì •ë³´ í—¬í¼ (app.py ì‚¬ì´ë“œë°” ë“±ì—ì„œ ì‚¬ìš©) =====
def get_cache_info(model: str) -> Dict[str, Any]:
    p = _db_path(model)
    info = {"model": model, "path": p, "exists": os.path.exists(p)}
    if not os.path.exists(p):
        return info
    try:
        conn = sqlite3.connect(p, timeout=10)
        _ensure_db(conn)
        try:
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) FROM emb")
            nrows = int(cur.fetchone()[0])
            size = os.path.getsize(p)
            info.update({"rows": nrows, "size_bytes": size})
        finally:
            conn.close()
    except Exception as e:
        info["error"] = str(e)
    return info



==============================
ğŸ“„ FILE: services/cycles_store.py
==============================

from __future__ import annotations
import json, os
from typing import Dict, List
from config import STANDARD_ACCOUNTING_CYCLES, CYCLES_USER_OVERRIDES_PATH


def _load_overrides() -> Dict[str, List[str]]:
    p = CYCLES_USER_OVERRIDES_PATH
    if not p or not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        return {str(k): [str(x) for x in v] for k, v in data.items() if isinstance(v, list)}
    except Exception:
        # ì˜ëª»ëœ íŒŒì¼/JSONì€ ë¬´ì‹œ
        return {}


def get_effective_cycles() -> Dict[str, List[str]]:
    # í”„ë¦¬ì…‹ì„ ë³µì‚¬í•˜ê³ , ë™ì¼ í‚¤ì˜ í•­ëª©ì€ ì‚¬ìš©ì ì„¤ì •ìœ¼ë¡œ ë®ì–´ì”€
    base = {k: list(v) for k, v in STANDARD_ACCOUNTING_CYCLES.items()}
    ov = _load_overrides()
    for k, v in ov.items():
        base[k] = list(v)
    return base




==============================
ğŸ“„ FILE: services/external.py
==============================




==============================
ğŸ“„ FILE: services/llm.py
==============================

"""
# services/llm.py
# - í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ì´ì–¸íŠ¸: í‚¤ê°€ ìˆìœ¼ë©´ OpenAI ì˜¨ë¼ì¸ ëª¨ë“œ, ì—†ìœ¼ë©´ ì˜¤í”„ë¼ì¸ ìŠ¤í…
"""

import os
from functools import lru_cache


def openai_available() -> bool:
    try:
        # ë¶€íŒ… ì‹œ env_loaderê°€ LLM_AVAILABLE í”Œë˜ê·¸ë¥¼ ì…‹ì—…
        from infra.env_loader import ensure_api_keys_loaded, is_llm_ready
        ensure_api_keys_loaded()
        return bool(is_llm_ready())
    except Exception:
        # ì§ì ‘ í™˜ê²½ë³€ìˆ˜ í™•ì¸ (í´ë°±)
        return bool(os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY"))


class _DummyEmbeddings:
    def create(self, *args, **kwargs):
        raise RuntimeError("Embeddings API not available in offline stub.")


class _DummyChat:
    class _Resp:
        class Choice:
            class Msg:
                content = ""
            message = Msg()
        choices = [Choice()]

    def completions(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")

    def completions_create(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")


class _DummyClient:
    embeddings = _DummyEmbeddings()
    chat = _DummyChat()


@lru_cache(maxsize=1)
def _openai_client():
    try:
        from openai import OpenAI  # type: ignore
    except Exception as e:
        return None
    # í‚¤ëŠ” env_loaderê°€ ì´ë¯¸ ì£¼ì…í•¨
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY")
    try:
        return OpenAI(api_key=api_key) if api_key else OpenAI()
    except Exception:
        return None


class LLMClient:
    def __init__(self, model: str | None = None, temperature: float | None = None, json_mode: bool | None = None):
        self._online = openai_available() and (_openai_client() is not None)
        self.client = _openai_client() if self._online else _DummyClient()
        # í˜¸ì¶œ í¸ì˜ íŒŒë¼ë¯¸í„° ì €ì¥(online generateì—ì„œ ì‚¬ìš©)
        self.model = model or os.getenv("LLM_MODEL", "gpt-4o")
        try:
            self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.2" if temperature is None else str(temperature)))
        except Exception:
            self.temperature = 0.2
        self.json_mode = True if (os.getenv("LLM_JSON_MODE", "true").lower() in ("1","true","yes")) else False

    def generate(self, system: str, user: str, tools=None, *, model: str | None = None, max_tokens: int | None = None, force_json: bool | None = None) -> str:
        if not self._online or self.client is None:
            raise RuntimeError("LLM not available: no API key or client. Run in offline mode.")
        try:
            use_model = model or self.model
            kwargs = dict(
                model=use_model,
                temperature=float(self.temperature),
                messages=[{"role":"system","content":system},{"role":"user","content":user}],
            )
            if max_tokens is not None:
                kwargs["max_tokens"] = int(max_tokens)
            if tools:
                kwargs["tools"] = tools
                try:
                    first_tool = tools[0]["function"]["name"]
                    if first_tool:
                        kwargs["tool_choice"] = {"type": "function", "function": {"name": first_tool}}
                except Exception:
                    pass
            else:
                use_force_json = self.json_mode if force_json is None else bool(force_json)
                if use_force_json:
                    kwargs["response_format"] = {"type": "json_object"}

            try:
                resp = self.client.chat.completions.create(**kwargs, timeout=60)
            except TypeError:
                resp = self.client.chat.completions.create(**kwargs)
            msg = resp.choices[0].message
            try:
                tool_calls = getattr(msg, "tool_calls", None)
                if tool_calls:
                    call = tool_calls[0]
                    args = getattr(call.function, "arguments", None)
                    return (args or "").strip()
            except Exception:
                pass
            return (msg.content or "").strip()
        except Exception as e:
            raise





==============================
ğŸ“„ FILE: services/__init__.py
==============================






==============================
ğŸ“„ FILE: tests/conftest.py
==============================

# tests/conftest.py
import os, sys
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)



==============================
ğŸ“„ FILE: tests/test_anomaly.py
==============================

# Z-Score ë° ìœ„í—˜ ì ìˆ˜ ë‹¨ì¡°ì„± í…ŒìŠ¤íŠ¸
import pandas as pd
from analysis.anomaly import calculate_grouped_stats_and_zscore, _risk_from

def test_zscore_and_risk_monotonic():
    df = pd.DataFrame({
        "ê³„ì •ì½”ë“œ": ["100"]*5 + ["200"]*5,
        "ì°¨ë³€":     [0,0,0,0,0] + [0,0,0,0,0],
        "ëŒ€ë³€":     [10,20,30,40,50] + [5,5,5,5,5],
    })
    out = calculate_grouped_stats_and_zscore(df, target_accounts=["100","200"])
    assert "Z-Score" in out.columns
    a1 = _risk_from(1.0, amount=1_000, pm=100_000)[-1]
    a2 = _risk_from(3.0, amount=1_000, pm=100_000)[-1]
    b1 = _risk_from(1.0, amount= 50_000, pm=100_000)[-1]
    b2 = _risk_from(1.0, amount=150_000, pm=100_000)[-1]
    assert a2 > a1
    assert b2 > b1





==============================
ğŸ“„ FILE: tests/test_assertion_matrix.py
==============================

from analysis.contracts import EvidenceDetail, ModuleResult
from analysis.assertion_risk import build_matrix
import pandas as pd


def _ev(row_id, acct, assertions, score):
    return EvidenceDetail(
        row_id=row_id, reason="t", anomaly_score=0.0, financial_impact=0.0,
        risk_score=score, is_key_item=False, impacted_assertions=assertions,
        links={"account_name": acct}
    )


def test_build_matrix_max_aggregation():
    mod = ModuleResult(
        name="anomaly",
        summary={}, tables={}, figures={},
        evidences=[
            _ev("r1", "ë§¤ì¶œ", ["A","E"], 0.40),
            _ev("r2", "ë§¤ì¶œ", ["E"],     0.65),
            _ev("r3", "ë§¤ì…", ["C"],     0.30),
        ],
        warnings=[]
    )
    mat, emap = build_matrix([mod])
    assert float(mat.loc["ë§¤ì¶œ","E"]) == 0.65   # ë™ì¼ ì…€ì€ ìµœëŒ€ê°’
    assert float(mat.loc["ë§¤ì¶œ","A"]) == 0.40
    assert float(mat.loc["ë§¤ì…","C"]) == 0.30
    assert ("ë§¤ì¶œ","E") in emap and set(emap[("ë§¤ì¶œ","E")]) == {"r1","r2"}




==============================
ğŸ“„ FILE: tests/test_evidence_schema.py
==============================

from dataclasses import asdict
from analysis.anomaly import run_anomaly_module
from analysis.contracts import LedgerFrame
import pandas as pd


def test_anomaly_emits_evidence_minimal():
    # ê°„ë‹¨ ê°€ì§œ DF
    df = pd.DataFrame({
        "row_id":["a","b","c"],
        "íšŒê³„ì¼ì": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03"]),
        "ê³„ì •ì½”ë“œ": ["400","400","400"],
        "ê³„ì •ëª…":   ["ë§¤ì¶œ","ë§¤ì¶œ","ë§¤ì¶œ"],
        "ì°¨ë³€": [0, 0, 0],
        "ëŒ€ë³€": [10_000_000, 100, 50],
    })
    lf = LedgerFrame(df=df, meta={})
    mod = run_anomaly_module(lf, target_accounts=["400"], topn=2, pm_value=500_000_000)
    assert isinstance(mod.evidences, list)
    assert len(mod.evidences) >= 1
    d = asdict(mod.evidences[0])
    for key in ["row_id","reason","anomaly_score","financial_impact","risk_score","is_key_item","impacted_assertions","links"]:
        assert key in d




==============================
ğŸ“„ FILE: tests/test_kdmeans_basic.py
==============================

# ê°„ë‹¨ ë‹¨ìœ„í…ŒìŠ¤íŠ¸: KDMeansê°€ ì˜ í•™ìŠµë˜ê³  ì†ì„±ë“¤ì´ ì±„ì›Œì§€ëŠ”ì§€ í™•ì¸
import numpy as np
from analysis.kdmeans_shim import HDBSCAN

def test_kdmeans_fixed_k():
    rng = np.random.default_rng(0)
    X = np.vstack([
        rng.normal(loc=[0,0], scale=0.1, size=(25,2)),
        rng.normal(loc=[3,3], scale=0.1, size=(25,2)),
    ])
    model = HDBSCAN(n_clusters=2, random_state=0).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ == 2
    assert set(model.labels_) == {0,1}
    assert np.allclose(model.probabilities_, 1.0)

def test_kdmeans_auto_k_runs():
    rng = np.random.default_rng(1)
    X = rng.normal(size=(200, 4))
    model = HDBSCAN(n_clusters=None, random_state=1).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ is not None




==============================
ğŸ“„ FILE: tests/test_report_units.py
==============================

# ë¦¬í¬íŠ¸ ë‚´ ì›(â‚©) ë‹¨ìœ„ ê°•ì œ ë³€í™˜ í…ŒìŠ¤íŠ¸
from analysis.report import _enforce_won_units

def test_unit_enforcement():
    s = "ì´ì•¡ì€ 3ì–µ 5,072ë§Œ ì›ì´ë©° ì´ì „ì—ëŠ” 2ì–µ ì›ì´ì—ˆë‹¤."
    out = _enforce_won_units(s)
    assert "350,720,000ì›" in out
    assert "200,000,000ì›" in out





==============================
ğŸ“„ FILE: tests/test_risk_boundaries.py
==============================

import math
import pytest
from analysis.anomaly import _risk_from


@pytest.mark.parametrize("z_abs", [0.0, 1.0, 3.0, 10.0])
@pytest.mark.parametrize("pm_value", [0.0, 1.0, 1e6, 1e12])
def test_risk_from_boundary_is_finite(z_abs, pm_value):
    a, f, k, score = _risk_from(z_abs=z_abs, amount=1_000_000, pm=pm_value)
    assert 0.0 <= score <= 1.0
    assert math.isfinite(score)


def test_risk_from_monotonic_in_z_when_pm_fixed():
    pm = 1e6
    s1 = _risk_from(0.5, amount=1_000_000, pm=pm)[-1]
    s2 = _risk_from(3.0, amount=1_000_000, pm=pm)[-1]
    s3 = _risk_from(10.0, amount=1_000_000, pm=pm)[-1]
    assert s1 <= s2 <= s3


@pytest.mark.parametrize("pm_lo,pm_hi", [(0.0, 1e9)])
def test_risk_from_non_decreasing_in_pm(pm_lo, pm_hi):
    z = 3.0
    slo = _risk_from(z, amount=1_000_000, pm=pm_lo)[-1]
    shi = _risk_from(z, amount=1_000_000, pm=pm_hi)[-1]
    assert slo <= shi




==============================
ğŸ“„ FILE: tests/test_risk_score.py
==============================

import math
from analysis.anomaly import _risk_from
from config import Z_SIGMOID_SCALE


def test_risk_from_basic():
    a, f, k, score = _risk_from(z_abs=3.0, amount=1_000_000_000, pm=500_000_000)
    exp_a = 1.0 / (1.0 + math.exp(-(3.0/float(Z_SIGMOID_SCALE or 1.0))))
    assert abs(a - exp_a) < 1e-9
    assert f == 1.0                 # PM ëŒ€ë¹„ ìº¡ 1
    assert k == 1.0                 # KIT
    # ê°€ì¤‘í•©: 0.5*a + 0.4*1 + 0.1*1
    expected = 0.5*a + 0.4 + 0.1
    assert abs(score - expected) < 1e-9


def test_risk_from_zero_pm_guard():
    a, f, k, score = _risk_from(z_abs=0.0, amount=0, pm=0)
    assert f == 0.0 and k == 0.0    # ë¶„ëª¨ ê°€ë“œ ë™ì‘
    assert 0.0 <= a <= 0.5          # z=0ì´ë©´ aëŠ” 0.5 ê·¼ì²˜




==============================
ğŸ“„ FILE: tests/test_risk_score_more.py
==============================

from analysis.anomaly import _risk_from
from analysis.anomaly import _assertions_for_row


def test_risk_from_none_and_negative_pm():
    for pm in (None, -1, -1000):
        a, f, k, score = _risk_from(z_abs=2.0, amount=1_000_000, pm=pm)
        assert f == 0.0 and k == 0.0
        assert 0.0 <= a <= 1.0
        assert 0.0 <= score <= 1.0

def test_assertions_mapping_rules():
    # í•­ìƒ A í¬í•¨
    assert "A" in _assertions_for_row(0.0)
    # í° ì–‘ì˜ ì´íƒˆ â†’ E í¬í•¨
    assert set(_assertions_for_row(+2.5)) >= {"A","E"}
    # í° ìŒì˜ ì´íƒˆ â†’ C í¬í•¨
    assert set(_assertions_for_row(-2.5)) >= {"A","C"}




==============================
ğŸ“„ FILE: tests/test_snapshot_core.py
==============================

import pandas as pd
from dataclasses import asdict
from analysis.contracts import LedgerFrame
from analysis.anomaly import run_anomaly_module
from analysis.assertion_risk import build_matrix


def _mini_df():
    df = pd.DataFrame({
        "row_id": ["file|L:2","file|L:3","file|L:4","file|L:5"],
        "íšŒê³„ì¼ì": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03","2024-01-04"]),
        "ê³„ì •ì½”ë“œ": ["101","101","201","201"],
        "ê³„ì •ëª…":   ["í˜„ê¸ˆ","í˜„ê¸ˆ","ë§¤ì¶œ","ë§¤ì¶œ"],
        "ì°¨ë³€": [0, 0, 0, 0],
        "ëŒ€ë³€": [5_000_000, 100, 50_000_000, 200],
    })
    return df


def test_snapshot_evidence_and_matrix_stable():
    lf = LedgerFrame(df=_mini_df(), meta={})
    mod = run_anomaly_module(lf, target_accounts=["101","201"], topn=10, pm_value=500_000_000)
    # Evidence ìŠ¤ëƒ…ìƒ·(í•µì‹¬ í•„ë“œë§Œ ë¹„êµ)
    snap = [{
        "row_id": e.row_id,
        "risk_score": round(e.risk_score, 6),
        "is_key_item": e.is_key_item,
        "assertions": tuple(e.impacted_assertions),
        "acct": e.links.get("account_name") or e.links.get("account_code")
    } for e in mod.evidences]
    # ê³ ì • ê¸°ëŒ€ê°’(ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜/PMì´ ë°”ë€Œë©´ ì‹¤íŒ¨í•˜ë„ë¡)
    assert any(s["acct"] == "ë§¤ì¶œ" for s in snap)
    # ë§¤íŠ¸ë¦­ìŠ¤ ìŠ¤ëƒ…ìƒ·
    mat, _ = build_matrix([mod])
    assert 0.0 <= float(mat.values.max()) <= 1.0




==============================
ğŸ“„ FILE: tests/test_timeseries_naive.py
==============================

# ì‹œê³„ì—´ ëª¨ë“ˆì˜ ê°„ë‹¨ ë°±ì—”ë“œ(EMA ë“±) ë™ì‘ì„± í…ŒìŠ¤íŠ¸
import pandas as pd
import numpy as np
from analysis.timeseries import run_timeseries_module, model_registry, z_and_risk, run_timeseries_for_account


def test_timeseries_naive_backend():
    df = pd.DataFrame({
        "account": ["A"]*7 + ["B"]*7,
        "date":    list(range(1,8)) + list(range(1,8)),
        "amount":  [10,11,10,12,11,10, 20] + [8,8,8,8,8,8, 5],
    })
    res = run_timeseries_module(df, account_col="account", date_col="date",
                                amount_col="amount", backend="ema", window=3)
    assert set(res["account"]) == {"A","B"}
    assert set(res["assertion"]).issubset({"E","C"})


def test_z_and_risk_basic():
    # í•œê¸€: 0 ì¤‘ì‹¬ ëŒ€ì¹­ ì”ì°¨ì— ëŒ€í•´ |z|ê°€ í¬ë©´ riskê°€ ì»¤ì§„ë‹¤
    resid = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])
    z, r = z_and_risk(resid)
    assert len(z) == len(resid)
    assert len(r) == len(resid)
    assert float(r[0]) > float(r[1])
    assert float(r[-1]) == float(r[0])


def test_model_registry_keys_present():
    reg = model_registry()
    for k in ["ma","ema","arima","prophet"]:
        assert k in reg
    assert reg["ma"] is True and reg["ema"] is True


def test_run_timeseries_for_account_dual():
    # í•œê¸€: 12ê°œì›” ìƒ˜í”Œë¡œ flow ëˆ„ì  balance ìƒì„±í•˜ì—¬ ë‘ íŠ¸ë™ ëª¨ë‘ ë°˜í™˜
    dates = pd.period_range("2024-01", periods=12, freq="M").to_timestamp()
    flow = np.linspace(100, 210, num=12)
    df = pd.DataFrame({"date": dates, "flow": flow})
    df["balance"] = df["flow"].cumsum()
    out = run_timeseries_for_account(df, account="ë§¤ì¶œì±„ê¶Œ", is_bs=True, flow_col="flow", balance_col="balance")
    assert set(out["measure"]).issuperset({"flow","balance"})
    assert set(out.columns) == {"date","account","measure","actual","predicted","error","z","risk","model"}





==============================
ğŸ“„ FILE: tests/test_zbins.py
==============================

import numpy as np, pandas as pd
from analysis.anomaly import _z_bins_025_sigma


def test_zbins_label_and_count():
    s = pd.Series(np.linspace(-4, 4, 101))
    df, order = _z_bins_025_sigma(s)
    assert len(df) == len(order) == 26      # í…Œì¼ í¬í•¨ 26ê°œ
    assert int(df["ê±´ìˆ˜"].sum()) == 101     # ì´í•© ë³´ì¡´




==============================
ğŸ“„ FILE: ui/inputs.py
==============================

import re
import streamlit as st

# KRW ì…ë ¥ ìœ„ì ¯(ì²œë‹¨ìœ„ ì‰¼í‘œ) - ì•ˆì •í˜•
# - ì‚¬ìš©ìëŠ” ììœ ë¡­ê²Œ íƒ€ì´í•‘(ì‰¼í‘œ/ê³µë°±/ë¬¸ì ì„ì—¬ë„ ë¬´ì‹œ)í•˜ê³ ,
#   í¬ì»¤ìŠ¤ ì•„ì›ƒ/ì—”í„° ì‹œì—ë§Œ ì •ê·œí™”(ìˆ«ìë§Œ ìœ ì§€ â†’ ì‰¼í‘œ í¬ë§·)í•©ë‹ˆë‹¤.
# - ì‹¤ì œ ìˆ«ìê°’ì€ session_state[key] (int)ë¡œ ë³´ê´€í•©ë‹ˆë‹¤.
# - í‘œì‹œìš© ë¬¸ìì—´ì€ session_state[f"{key}__txt"] ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

def _parse_krw_text(s: str) -> int:
    """ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ì•ˆì „í•˜ê²Œ intë¡œ ë³€í™˜(ìŒìˆ˜ ë°©ì–´, ê³µë€=0)."""
    if s is None:
        return 0
    s = str(s).replace(",", "").strip()
    s = re.sub(r"[^\d]", "", s)  # ìˆ«ì ì´ì™¸ ì œê±°
    if s == "":
        return 0
    try:
        return max(0, int(s))
    except Exception:
        return 0

def _fmt_krw(n: int) -> str:
    """ì •ìˆ˜ë¥¼ ì²œë‹¨ìœ„ ì‰¼í‘œ ë¬¸ìì—´ë¡œ í¬ë§·."""
    try:
        return f"{int(n):,}"
    except Exception:
        return "0"

def krw_input(label: str, key: str, default_value: int = 0, help_text: str = "") -> int:
    """
    KRW ì…ë ¥(ì²œë‹¨ìœ„ ì‰¼í‘œ) í†µí•© ìœ„ì ¯.
    - ìˆ«ì ìƒíƒœ: st.session_state[key] (int)
    - í‘œì‹œ ìƒíƒœ: st.session_state[f"{key}__txt"] (str, '1,234,567')
    - on_change ì‹œì—ë§Œ ì •ê·œí™”í•˜ì—¬ ì”ê³ ì¥(500,00 ë“±) ë°©ì§€
    """
    # ì´ˆê¸° ìƒíƒœ ë³´ì •
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if f"{key}__txt" not in st.session_state:
        st.session_state[f"{key}__txt"] = _fmt_krw(st.session_state[key])

    def _commit():
        """ì‚¬ìš©ì ì…ë ¥ ì™„ë£Œ(í¬ì»¤ìŠ¤ ì•„ì›ƒ/ì—”í„°) ì‹œ ìˆ«ì/ë¬¸ì ìƒíƒœ ë™ê¸°í™”."""
        raw = st.session_state.get(f"{key}__txt", "")
        val = _parse_krw_text(raw)
        st.session_state[key] = val
        st.session_state[f"{key}__txt"] = _fmt_krw(val)
        # Streamlitì€ on_change í›„ ìë™ rerun â†’ ê·¸ë˜í”„/í‘œ ê°±ì‹ ì— ì¶©ë¶„

    # í‘œì‹œ ì…ë ¥ì°½(íƒ€ì´í•‘ ì¤‘ì—ëŠ” í¬ë§· ê°•ì œí•˜ì§€ ì•ŠìŒ)
    st.text_input(
        label,
        key=f"{key}__txt",
        help=help_text,
        placeholder="ì˜ˆ: 500,000,000",
        on_change=_commit,
    )
    return int(st.session_state.get(key, int(default_value)))





==============================
ğŸ“„ FILE: ui/__init__.py
==============================

# íŒ¨í‚¤ì§€ ì´ˆê¸°í™” (ë¹„ì–´ìˆì–´ë„ ë¬´ë°©)




==============================
ğŸ“„ FILE: utils/drilldown.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional


def ensure_rowid(df: pd.DataFrame, id_col: str = "row_id") -> pd.DataFrame:
    """row_id ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜(ê³„ì•½ ì¤€ìˆ˜ëŠ” ìƒìœ„ ë‹¨ê³„ì—ì„œ)."""
    return df if id_col in df.columns else df


def attach_customdata(df: pd.DataFrame, cols: Iterable[str], id_col: str = "row_id"):
    """
    Plotlyì— ì˜¬ë¦´ customdata ë°°ì—´ ìƒì„±.
    ë°˜í™˜: (df, customdata(ndarray), header_labels(list))
    """
    import numpy as np
    use_cols = [c for c in cols if c in df.columns]
    if id_col not in use_cols and id_col in df.columns:
        use_cols = [id_col] + use_cols
    arr = df[use_cols].to_numpy()
    return df, np.asarray(arr), use_cols


def fmt_money(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return str(x)





==============================
ğŸ“„ FILE: utils/helpers.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional

def find_column_by_keyword(columns: Iterable[str], keyword: str) -> Optional[str]:
    """ì—´ ì´ë¦„ì—ì„œ keyword(ë¶€ë¶„ì¼ì¹˜, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)ë¥¼ ìš°ì„  íƒìƒ‰."""
    keyword = str(keyword or "").lower()
    cols = [str(c) for c in columns]
    # 1) ì™„ì „ ì¼ì¹˜ ìš°ì„ 
    for c in cols:
        if c.lower() == keyword:
            return c
    # 2) ë¶€ë¶„ ì¼ì¹˜
    for c in cols:
        if keyword in c.lower():
            return c
    return None

def add_provenance_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì—…ë¡œë“œ ì¶œì²˜ ì •ë³´ê°€ ì—†ì–´ë„ row_idë¥¼ ê°•ì œë¡œ ë¶€ì—¬."""
    out = df.copy()
    if "row_id" not in out.columns:
        out["row_id"] = out.reset_index().index.astype(str)
    return out

def add_period_tag(df: pd.DataFrame) -> pd.DataFrame:
    """ì—°ë„ ìµœëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ CY/PY/Other íƒœê·¸ë¥¼ ë¶€ì—¬."""
    out = df.copy()
    if "ì—°ë„" not in out.columns:
        out["period_tag"] = "Other"
        return out
    y_max = out["ì—°ë„"].max()
    out["period_tag"] = out["ì—°ë„"].apply(lambda y: "CY" if y == y_max else ("PY" if y == y_max - 1 else "Other"))
    return out




==============================
ğŸ“„ FILE: utils/viz.py
==============================

# utils/viz.py
# ëª©ì : Materiality(Performance Materiality, PM) ë³´ì¡°ì„ /ë°°ì§€ ì¶”ê°€ ìœ í‹¸
# - Plotly Figureì— ë¹¨ê°„ ì ì„ (ê°€ë¡œì„ ) + "PM=xxxì›" ë¼ë²¨ì„ ì•ˆì „í•˜ê²Œ ì¶”ê°€
# - í˜„ì¬ yì¶• ë²”ìœ„ì— PMì´ ì—†ìœ¼ë©´ ì¶•ì„ ìë™ í™•ì¥í•´ì„œ ì„ ì´ ë³´ì´ê²Œ í•¨
# - Pareto(ë³´ì¡°ì¶• ìˆìŒ)ì—ì„œë„ 1ì°¨ yì¶•ì— ì •í™•íˆ ê·¸ë ¤ì¤Œ

from __future__ import annotations
from typing import Optional
import math


def _is_plotly_fig(fig) -> bool:
    try:
        # ì§€ì—° ì„í¬íŠ¸ (í™˜ê²½ì— plotly ë¯¸ì„¤ì¹˜ì¼ ë•Œë„ í•¨ìˆ˜ ìì²´ëŠ” import ê°€ëŠ¥í•˜ë„ë¡)
        import plotly.graph_objects as go  # noqa: F401
        from plotly.graph_objs import Figure
        return isinstance(fig, Figure)
    except Exception:
        return False


def _get_primary_y_data_bounds(fig):
    """
    1ì°¨ yì¶• ë°ì´í„°ì˜ (min, max) ì¶”ì •.
    - secondary_y=Trueë¡œ ì˜¬ë¼ê°„ traceëŠ” ì œì™¸
    - trace.yê°€ ìˆ˜ì¹˜ ë°°ì—´ì¼ ë•Œë§Œ ì§‘ê³„
    """
    ymin, ymax = math.inf, -math.inf
    for tr in getattr(fig, "data", []):
        # ë³´ì¡°ì¶• ì—¬ë¶€: trace.yaxis ê°€ 'y2'/'y3'... ì´ë©´ ë³´ì¡°ì¶•
        yaxis = getattr(tr, "yaxis", "y")
        if yaxis and str(yaxis).lower() != "y":  # 'y2' ë“±ì€ ì œì™¸
            continue
        y = getattr(tr, "y", None)
        if y is None:
            continue
        try:
            for v in y:
                if v is None:
                    continue
                fv = float(v)
                if math.isfinite(fv):
                    ymin = min(ymin, fv)
                    ymax = max(ymax, fv)
        except Exception:
            # ìˆ«ì ë°°ì—´ì´ ì•„ë‹ˆë©´ ìŠ¤í‚µ
            continue
    if ymin is math.inf:  # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
        return (0.0, 0.0)
    return (ymin, ymax)


def _ensure_y_contains(fig, y_value: float, pad_ratio: float = 0.05):
    """
    y_valueê°€ yì¶• ë²”ìœ„ì— í¬í•¨ë˜ë„ë¡ ë ˆì´ì•„ì›ƒì„ ì¡°ì •.
    - ê¸°ì¡´ auto-rangeë¼ë„ PMì´ ì¶• ë°–ì´ë©´ ê°•ì œë¡œ range ë¶€ì—¬
    - pad_ratioë§Œí¼ ì—¬ìœ ë¥¼ ë‘¬ì„œ ë¼ë²¨ì´ ì˜ë¦¬ì§€ ì•Šê²Œ í•¨
    """
    if not math.isfinite(y_value):
        return
    # í˜„ì¬ 1ì°¨ yì¶• ë°ì´í„° ë²”ìœ„ ì¶”ì •
    ymin_data, ymax_data = _get_primary_y_data_bounds(fig)
    # ë°ì´í„°ê°€ ì „ë¶€ ìŒìˆ˜ì´ê±°ë‚˜ ì „ë¶€ ì–‘ìˆ˜ì¼ ìˆ˜ ìˆìŒ â†’ PMì´ ë” í° ìª½ì— ìˆìœ¼ë©´ í™•ì¥
    base_min = min(0.0, ymin_data) if math.isfinite(ymin_data) else 0.0
    base_max = max(0.0, ymax_data) if math.isfinite(ymax_data) else 0.0
    tgt_min = min(base_min, y_value)
    tgt_max = max(base_max, y_value)
    if tgt_min == tgt_max:
        # ì™„ì „ í‰í‰í•˜ë©´ ì‚´ì§ í­ ì¶”ê°€
        span = abs(y_value) if y_value != 0 else 1.0
        tgt_min -= span * 0.5
        tgt_max += span * 0.5
    # ì—¬ìœ  íŒ¨ë”©
    span = (tgt_max - tgt_min) or 1.0
    pad = span * float(pad_ratio)
    final_min = tgt_min - pad
    final_max = tgt_max + pad
    # yaxisëŠ” ë ˆì´ì•„ì›ƒ í‚¤ 'yaxis' (ì„œë¸Œí”Œë¡¯ ì•„ë‹Œ ê¸°ë³¸ ë„ë©´ ê¸°ì¤€)
    if "yaxis" not in fig.layout:
        fig.update_layout(yaxis=dict(range=[final_min, final_max]))
    else:
        fig.layout.yaxis.update(range=[final_min, final_max])


def add_materiality_threshold(fig, pm_value: Optional[float], *, label: bool = True):
    """
    Plotly Figureì— PM ê°€ë¡œ ì ì„  + ë¼ë²¨ ì¶”ê°€.
    - pm_valueê°€ None/0/ìŒìˆ˜ë©´ ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    - Pareto(ë³´ì¡°ì¶•)ë„ 1ì°¨ yì¶•ì— ë¼ì¸ì„ ê·¸ë¦¼ (yref='y')
    - ì¶• ë²”ìœ„ë¥¼ ìë™ í™•ì¥í•´ì„œ í•­ìƒ ë³´ì´ê²Œ í•¨
    ë°˜í™˜: ë™ì¼ Figure (in-place ìˆ˜ì • í›„)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    # yì¶• ë²”ìœ„ì— PMì´ í¬í•¨ë˜ë„ë¡ ë¨¼ì € ë³´ì¥
    _ensure_y_contains(fig, pm, pad_ratio=0.08)

    # ì ì„  ë¼ì¸ ì¶”ê°€
    # xref='paper'ë¡œ 0~1 ì „í­ì— ê±¸ì³ ìˆ˜í‰ì„ , yref='y'ë¡œ 1ì°¨ yì¶• ê¸°ì¤€ ê³ ì •
    line_shape = dict(
        type="line",
        xref="paper", x0=0, x1=1,
        yref="y",     y0=pm, y1=pm,
        line=dict(color="red", width=2, dash="dot"),
        layer="above"
    )
    shapes = list(fig.layout.shapes) if getattr(fig.layout, "shapes", None) else []
    shapes.append(line_shape)
    fig.update_layout(shapes=shapes)

    # ë¼ë²¨(ì˜¤ë¥¸ìª½ ë)
    if label:
        annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
        annotations.append(dict(
            x=1.0, xref="paper",
            y=pm, yref="y",
            xanchor="left", yanchor="bottom",
            text=f"PM {pm:,.0f}ì›",
            showarrow=False,
            font=dict(color="red", size=11),
            bgcolor="rgba(255,255,255,0.6)",
            bordercolor="red",
            borderwidth=0.5,
            align="left"
        ))
        fig.update_layout(annotations=annotations)

    return fig


def add_pm_badge(fig, pm_value: Optional[float], *, text: str | None = None):
    """
    Heatmapì²˜ëŸ¼ ì„ ì„ ê¸‹ê¸° ì• ë§¤í•œ ê·¸ë˜í”„ì— ìš°ì¸¡ ìƒë‹¨ ë°°ì§€ ì¶”ê°€.
    ë°˜í™˜: ë™ì¼ Figure (in-place)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    label = text or f"PM {pm:,.0f}ì›"
    annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
    annotations.append(dict(
        x=0.995, xref="paper",
        y=0.995, yref="paper",
        xanchor="right", yanchor="top",
        text=label,
        showarrow=False,
        font=dict(color="red", size=11),
        bgcolor="rgba(255,255,255,0.6)",
        bordercolor="red",
        borderwidth=0.5,
        align="right"
    ))
    fig.update_layout(annotations=annotations)
    return fig





==============================
ğŸ“„ FILE: utils/__init__.py
==============================




==============================
ğŸ“„ FILE: viz/guides.py
==============================

# Materiality ê°€ì´ë“œë¼ì¸(ë¶‰ì€ ì ì„ ) ìœ í‹¸
# - matplotlib/plotly ëª¨ë‘ ì§€ì›. ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ì¶° í˜¸ì¶œë§Œ ë¶™ì´ë©´ ë¨.

def add_materiality_lines_matplotlib(ax, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # yì¶• ê¸°ì¤€ ìˆ˜í‰ì„ 
    if y_threshold is not None and ax is not None:
        ax.axhline(y_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)  # ë¶‰ì€ ì ì„ 
        ax.text(ax.get_xlim()[0], y_threshold, f"{label_prefix}: {y_threshold:,.0f}",
                va="bottom", ha="left", fontsize=9, color="red", alpha=0.9)
    # xì¶• ê¸°ì¤€ ìˆ˜ì§ì„ 
    if x_threshold is not None and ax is not None:
        ax.axvline(x_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)
        ax.text(x_threshold, ax.get_ylim()[1], f"{label_prefix}: {x_threshold:,.0f}",
                va="top", ha="right", fontsize=9, color="red", alpha=0.9)


def add_materiality_lines_plotly(fig, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # Plotly Figureì— ê°€ì´ë“œë¼ì¸ ì¶”ê°€
    if fig is None:
        return fig
    if y_threshold is not None:
        try:
            fig.add_hline(y=y_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(xref="paper", x=0.0, y=y_threshold, yref="y",
                               text=f"{label_prefix}: {y_threshold:,.0f}",
                               showarrow=False, align="left", yanchor="bottom", font=dict(color="red", size=10))
        except Exception:
            pass
    if x_threshold is not None:
        try:
            fig.add_vline(x=x_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(yref="paper", y=1.0, x=x_threshold, xref="x",
                               text=f"{label_prefix}: {x_threshold:,.0f}",
                               showarrow=False, align="right", xanchor="right", font=dict(color="red", size=10))
        except Exception:
            pass
    return fig




