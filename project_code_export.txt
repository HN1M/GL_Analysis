
==============================
📄 FILE: app.py
==============================

# app_v0.17.py (거래처 상세 분석 오류 수정)
# --- BEGIN: LLM 키 부팅 보장 ---
try:
    from infra.env_loader import boot as _llm_boot
    _llm_boot()  # 키 로드 + 상태 로그
except Exception as _e:
    # 최악의 경우에도 앱은 뜨게 하고, 상태를 stderr로만 알림
    import sys
    print(f"[env_loader] 초기화 실패: {_e}", file=sys.stderr)
# --- END: LLM 키 부팅 보장 ---

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
from utils.helpers import find_column_by_keyword, add_provenance_columns, add_period_tag
from analysis.integrity import analyze_reconciliation
from analysis.contracts import LedgerFrame, ModuleResult
from analysis.trend import create_monthly_trend_figure, run_trend_module
from analysis.anomaly import run_anomaly_module, compute_amount_columns
from analysis.correlation import run_correlation_module
from analysis.vendor import (
    create_pareto_figure,
    create_vendor_heatmap,
    create_vendor_detail_figure,
    run_vendor_module,
)
from analysis.report import generate_rag_context, run_final_analysis, build_methodology_note
from analysis.embedding import (
    ensure_rich_embedding_text,
    perform_embedding_and_clustering,
    perform_embedding_only,
)
from analysis.anomaly import calculate_grouped_stats_and_zscore
from services.llm import LLMClient
from config import EMB_USE_LARGE_DEFAULT, HDBSCAN_RESCUE_TAU
try:
    from config import PM_DEFAULT, PROVISIONAL_RULE_NAME, provisional_risk_formula_str
except Exception:
    PM_DEFAULT = 500_000_000
from utils.viz import add_materiality_threshold, add_pm_badge
from analysis.assertion_risk import build_matrix

# --- KRW 입력(천단위 콤마) 유틸: 콜백 기반으로 안정화 ---
def _krw_input(label: str, key: str, *, default_value: int, help_text: str = "") -> int:
    """
    한국 원화 입력 위젯(천단위 콤마). 핵심 규칙:
    1) 위젯 키(pm_value__txt 등)를 런 루프에서 직접 대입하지 않는다.
    2) 콤마 재포맷은 on_change 콜백 안에서만 수행한다.
    3) 분석에 쓰는 정수 값은 st.session_state[key]에 보관한다.
    """
    txt_key = f"{key}__txt"  # 실제 text_input 위젯이 바인딩되는 키

    # 초기 셋업: 숫자/문자 상태를 위젯 생성 전에 준비
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if txt_key not in st.session_state:
        st.session_state[txt_key] = f"{int(st.session_state[key]):,}"

    # 콜백: 포커스 아웃/Enter 시 콤마 포맷을 적용하고 숫자 상태를 동기화
    def _on_blur_format():
        raw_now = st.session_state.get(txt_key, "")
        digits = re.sub(r"[^\d]", "", str(raw_now or ""))
        val = int(digits) if digits else 0
        if val < 0:
            val = 0
        st.session_state[key] = int(val)            # 분석에 쓰는 정수 상태
        st.session_state[txt_key] = f"{int(val):,}"  # 위젯 표시 텍스트(콤마)

    # 위젯 생성
    raw = st.text_input(
        label,
        value=st.session_state[txt_key],
        key=txt_key,
        help=help_text,
        placeholder="예: 500,000,000",
        on_change=_on_blur_format,
    )

    # 라이브 타이핑 동안에도 그래프가 즉시 반영되도록 정수 상태만 업데이트(위젯 키는 건드리지 않음)
    digits_live = re.sub(r"[^\d]", "", str(raw or ""))
    live_val = int(digits_live) if digits_live else 0
    if live_val < 0:
        live_val = 0
    st.session_state[key] = int(live_val)

    return int(st.session_state[key])


# --- 3. UI 부분 ---
st.set_page_config(page_title="AI 분석 시스템 v0.18", layout="wide")
st.title("AI 분석 시스템 v0.18: 최종 개선 🏗️")
st.markdown("---")

for key in ['mapping_confirmed', 'analysis_done']:
    if key not in st.session_state:
        st.session_state[key] = False


# (removed) number_input 기반 대체 구현: 쉼표 미표시·키 충돌 유발 가능성 → 단일 구현으로 통일

with st.sidebar:
    st.header("1. 데이터 준비")
    uploaded_file = st.file_uploader("분석할 엑셀 파일을 올려주세요.", type=["xlsx", "xlsm"])
    if 'last_file' not in st.session_state or st.session_state.last_file != uploaded_file:
        st.session_state.mapping_confirmed = False
        st.session_state.analysis_done = False
        st.session_state.last_file = uploaded_file

    st.markdown("---")
    st.header("2. 분석 기간")
    default_scope = st.session_state.get("period_scope", "당기")
    st.session_state.period_scope = st.radio(
        "분석 스코프(트렌드 제외):",
        options=["당기", "당기+전기"],
        index=["당기","당기+전기"].index(default_scope),
        horizontal=True,
        help="상관/거래처/이상치 모듈에 적용됩니다. 트렌드는 설계상 CY vs PY 비교 유지."
    )
    st.markdown("---")
    st.header("3. Embedding / Clustering")
    st.session_state.use_large_embedding = st.toggle(
        "Use Large Embedding (cost ↑)",
        value=st.session_state.get("use_large_embedding", EMB_USE_LARGE_DEFAULT),
        help="Large model improves semantics but is slower and more expensive."
    )
    st.session_state.rescue_tau = st.slider(
        "Noise rescue τ (cosine)",
        min_value=0.60, max_value=0.90, step=0.01,
        value=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
        help="Reassign -1 (noise) to nearest cluster if similarity ≥ τ."
    )
    st.markdown("---")
    st.header("4. Materiality")
    pm_val = _krw_input(
        "Performance Materiality (KRW)",
        key="pm_value",
        default_value=PM_DEFAULT,
        help_text="Used for KIT (PM exceed) and integrated risk scoring."
    )
    st.caption("ⓘ The PM threshold is drawn as a red dotted line on applicable charts. "
               "Y-axis scaling may change to accommodate this line.")

    # 🧹 캐시 관리
    with st.expander("🧹 캐시 관리", expanded=False):
        if st.button("임베딩 캐시 비우기"):
            import shutil
            from services.cache import _model_dir
            for m in ["text-embedding-3-small", "text-embedding-3-large"]:
                try:
                    shutil.rmtree(_model_dir(m), ignore_errors=True)
                except Exception as e:
                    st.warning(f"{m} 삭제 실패: {e}")
            st.success("임베딩 캐시 삭제 완료")

        if st.button("데이터 캐시 비우기"):
            st.cache_data.clear()
            st.success("Streamlit 데이터 캐시 삭제 완료")

        if st.button("캐시 정보 보기"):
            from services.cache import get_cache_info
            try:
                st.write(get_cache_info("text-embedding-3-small"))
                st.write(get_cache_info("text-embedding-3-large"))
            except Exception as e:
                st.info(f"정보 조회 실패: {e}")


@st.cache_data(show_spinner=False)
def _read_excel(_file, sheet_name=None):
    return pd.read_excel(_file, sheet_name=sheet_name)


@st.cache_data(show_spinner=False)
def _read_xls(_file):
    # pickle 직렬화 가능한 타입만 캐시 → 시트명 리스트로 반환
    return pd.ExcelFile(_file).sheet_names

# (removed duplicated definition) _krw_input — 위의 단일 버전만 유지

def _apply_scope(df: pd.DataFrame, scope: str) -> pd.DataFrame:
    """스코프 적용 시 결측 컬럼 방어: 'period_tag' 미존재면 원본 반환.
    df.get('period_tag','')가 문자열을 반환할 경우 .eq 호출 AttributeError를 방지한다.
    """
    if df is None or df.empty or 'period_tag' not in df.columns:
        return df
    if scope == "당기":
        return df[df['period_tag'].eq('CY')]
    if scope == "당기+전기":
        return df[df['period_tag'].isin(['CY', 'PY'])]
    return df

def _lf_by_scope() -> LedgerFrame:
    """상관/거래처/이상치에서 사용할 스코프 적용 LedgerFrame."""
    hist = st.session_state.get('lf_hist')
    scope = st.session_state.get('period_scope', '당기')
    if hist is None:
        return None
    return LedgerFrame(df=_apply_scope(hist.df, scope), meta=hist.meta)

# (removed) 구버전 텍스트입력 + ±step / ✖reset 변형들 — 사용자 요청으로 버튼류 삭제 및 단일화


if uploaded_file is not None:
    if not st.session_state.mapping_confirmed:
        # ... 컬럼 매핑 UI ...
        try:
            st.info("2단계: 엑셀의 컬럼을 분석 표준 필드에 맞게 지정해주세요.")
            sheet_names = _read_xls(uploaded_file)
            first_ledger_sheet = next((s for s in sheet_names if 'ledger' in s.lower()), None)
            if first_ledger_sheet is None:
                st.error("오류: 'Ledger' 시트를 찾을 수 없습니다.")
                st.stop()
            ledger_cols = _read_excel(uploaded_file, sheet_name=first_ledger_sheet).columns.tolist()
            ledger_map = {}
            st.markdown("#### **Ledger 시트** 항목 매핑")
            cols = st.columns(6)
            ledger_fields = {'회계일자': '일자', '계정코드': '계정코드', '거래처': '거래처', '적요': '적요', '차변': '차변', '대변': '대변'}
            for i, (key, keyword) in enumerate(ledger_fields.items()):
                with cols[i]:
                    is_optional = key == '거래처'
                    default_col = find_column_by_keyword(ledger_cols, keyword)
                    options = ['선택 안 함'] + ledger_cols if is_optional else ledger_cols
                    default_index = options.index(default_col) if default_col in options else 0
                    ledger_map[key] = st.selectbox(f"**'{key}'** 필드 선택", options, index=default_index, key=f"map_ledger_{key}")
            st.markdown("---")
            st.markdown("#### **Master 시트** 항목 매핑")
            master_cols = _read_excel(uploaded_file, sheet_name='Master').columns.tolist()
            master_map = {}
            cols = st.columns(7)
            master_fields = {'계정코드': '계정코드', '계정명': '계정명', 'BS/PL': 'BS/PL', '차변/대변': '차변/대변', '당기말잔액': '당기말', '전기말잔액': '전기말', '전전기말잔액': '전전기말'}
            for i, (key, keyword) in enumerate(master_fields.items()):
                with cols[i]:
                    default_col = find_column_by_keyword(master_cols, keyword)
                    default_index = master_cols.index(default_col) if default_col in master_cols else 0
                    master_map[key] = st.selectbox(f"**'{key}'** 필드 선택", master_cols, index=default_index, key=f"map_master_{key}")
            if st.button("✅ 매핑 확인 및 데이터 처리", type="primary"):
                st.session_state.ledger_map = ledger_map
                st.session_state.master_map = master_map
                st.session_state.mapping_confirmed = True
                st.rerun()
        except Exception as e:
            st.error(f"엑셀 파일의 컬럼을 읽는 중 오류가 발생했습니다: {e}")

    else:  # 매핑 확인 후
        try:
            ledger_map, master_map = st.session_state.ledger_map, st.session_state.master_map
            master_df = _read_excel(uploaded_file, sheet_name='Master')
            sheet_names = _read_xls(uploaded_file)
            ledger_sheets = [s for s in sheet_names if 'ledger' in s.lower()]
            all_parts = []
            for s in ledger_sheets:
                part = _read_excel(uploaded_file, sheet_name=s)
                part['source_sheet'] = s
                part = add_provenance_columns(part)
                all_parts.append(part)
            ledger_df = pd.concat(all_parts, ignore_index=True)
            # row_id: 파일명|시트:행  (세션/재실행에도 안정)
            try:
                base = Path(getattr(uploaded_file, "name", "uploaded.xlsx")).stem
                if 'row_id' in ledger_df.columns:
                    ledger_df['row_id'] = base + "|" + ledger_df['row_id'].astype(str)
            except Exception:
                pass
            ledger_df.rename(columns={v: k for k, v in ledger_map.items() if v != '선택 안 함'}, inplace=True)
            master_df.rename(columns={v: k for k, v in master_map.items()}, inplace=True)

            # 🔧 병합 전에 타입/포맷을 먼저 통일
            for df_ in [ledger_df, master_df]:
                if '계정코드' in df_.columns:
                    df_['계정코드'] = (
                        df_['계정코드']
                        .astype(str)
                        .str.replace(r'\.0$', '', regex=True)
                        .str.strip()
                    )

            master_essentials = master_df[['계정코드', '계정명']].drop_duplicates()
            ledger_df = pd.merge(ledger_df, master_essentials, on='계정코드', how='left')
            ledger_df['계정명'] = ledger_df['계정명'].fillna('미지정 계정')

            ledger_df['회계일자'] = pd.to_datetime(ledger_df['회계일자'], errors='coerce')
            ledger_df.dropna(subset=['회계일자'], inplace=True)
            for col in ['차변', '대변']:
                ledger_df[col] = pd.to_numeric(ledger_df[col], errors='coerce').fillna(0)
            for col in ['당기말잔액', '전기말잔액', '전전기말잔액']:
                if col in master_df.columns:
                    master_df[col] = pd.to_numeric(master_df[col], errors='coerce').fillna(0)
                else:
                    master_df[col] = 0
            ledger_df['거래금액'] = ledger_df['차변'] - ledger_df['대변']
            ledger_df['거래금액_절대값'] = abs(ledger_df['거래금액'])
            ledger_df['연도'] = ledger_df['회계일자'].dt.year
            ledger_df['월'] = ledger_df['회계일자'].dt.month
            # ✅ 분석 규칙: 계정 서브셋 분석 시에도 전체 히스토리를 확보하기 위한 편의 파생
            ledger_df['연월'] = ledger_df['회계일자'].dt.to_period('M').astype(str)
            # ✅ period_tag 추가(CY/PY/Other)
            ledger_df = add_period_tag(ledger_df)
            if '거래처' not in ledger_df.columns:
                ledger_df['거래처'] = '정보 없음'
            ledger_df['거래처'] = ledger_df['거래처'].fillna('정보 없음').astype(str)

            if st.button("🚀 전체 분석 실행", type="primary"):
                with st.spinner('데이터를 분석 중입니다...'):
                    # ✅ 정합성은 사용자 기간 선택과 무관하게 전체 기준으로 계산
                    st.session_state.recon_status, st.session_state.recon_df = analyze_reconciliation(ledger_df, master_df)
                    # ✅ 표준 LedgerFrame 구성(정합성은 항상 전체 기준: DF_hist)
                    lf_hist = LedgerFrame(df=ledger_df, meta={
                        "file_name": getattr(uploaded_file, "name", "uploaded.xlsx"),
                        "master_df": master_df,
                    })
                    # 초기엔 focus=hist (후속 단계에서 사용자 필터 연결)
                    lf_focus = lf_hist

                    st.session_state.master_df = master_df
                    st.session_state.ledger_df = ledger_df
                    st.session_state.lf_hist = lf_hist
                    st.session_state.lf_focus = lf_focus
                    st.session_state.analysis_done = True
                st.rerun()

            if st.session_state.analysis_done:
                st.success("✅ 분석이 완료되었습니다. 아래 탭에서 결과를 확인하세요.")
                with st.expander("🔍 빠른 진단(데이터 품질 체크)", expanded=False):
                    df = st.session_state.ledger_df.copy()
                    issues = []

                    invalid_date = int(df['회계일자'].isna().sum())
                    if invalid_date > 0:
                        issues.append(f"❗ 유효하지 않은 날짜(NaT): {invalid_date:,}건")

                    if '거래처' in df.columns:
                        missing_vendor = int((df['거래처'].isna() | (df['거래처'] == '정보 없음')).sum())
                        if missing_vendor > 0:
                            issues.append(f"ℹ️ 거래처 정보 없음/결측: {missing_vendor:,}건")

                    zero_abs = int((df['거래금액_절대값'] == 0).sum())
                    issues.append(f"ℹ️ 금액 절대값이 0인 전표: {zero_abs:,}건")

                    unlinked = int(df['계정명'].eq('미지정 계정').sum())
                    if unlinked > 0:
                        issues.append(f"❗ Master와 매칭되지 않은 전표(계정명 미지정): {unlinked:,}건")

                    st.write("**체크 결과**")
                    if issues:
                        for line in issues:
                            st.write("- " + line)
                    else:
                        st.success("문제 없이 깔끔합니다!")
                tab1, tab2, tab3, tab4, tab_ts, tab5, tab6 = st.tabs(["📈 대시보드", "🌊 데이터 무결성 및 흐름", "🏢 거래처 심층 분석", "🔬 이상 패턴 탐지", "📉 시계열/예측", "⚠️ 위험 평가", "🤖 AI 리포트"])

                with tab1:  # ...
                    st.header("핵심 요약 대시보드")
                    recon_status, ledger_df_res = st.session_state.recon_status, st.session_state.ledger_df
                    st.subheader("데이터 현황")
                    kpi_cols = st.columns(3)
                    kpi_cols[0].metric("총 거래 건수", f"{len(ledger_df_res):,} 건")
                    kpi_cols[1].metric("총 거래 금액 (절대값)", f"{st.session_state.ledger_df['거래금액_절대값'].sum():,.0f} 원")
                    kpi_cols[2].metric("분석 대상 계정 수", f"{ledger_df_res['계정코드'].nunique()} 개")
                    st.subheader("데이터 무결성")
                    if recon_status == 'Pass':
                        st.success("✅ 데이터 정합성: 모든 계정 일치")
                    elif recon_status == 'Warning':
                        st.warning("⚠️ 데이터 정합성: 일부 계정에서 사소한 차이 발견")
                    else:
                        st.error("🚨 데이터 정합성: 일부 계정에서 중대한 차이 발견")
                with tab2:  # ...
                    st.header("데이터 무결성 및 흐름")
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')}")
                    st.subheader("1. 데이터 정합성 검증 결과")
                    status, result_df = st.session_state.recon_status, st.session_state.recon_df
                    if status == "Pass":
                        st.success("✅ 모든 계정의 데이터가 일치합니다.")
                    elif status == "Warning":
                        st.warning("⚠️ 일부 계정에서 사소한 차이가 발견되었습니다.")
                    else:
                        st.error("🚨 일부 계정에서 중대한 차이가 발견되었습니다.")

                    def highlight_status(row):
                        if row.상태 == 'Fail':
                            return ['background-color: #ffcccc'] * len(row)
                        elif row.상태 == 'Warning':
                            return ['background-color: #fff0cc'] * len(row)
                        return [''] * len(row)

                    format_dict = {col: '{:,.0f}' for col in result_df.select_dtypes(include=np.number).columns}
                    st.dataframe(result_df.style.apply(highlight_status, axis=1).format(format_dict), use_container_width=True)
                    st.markdown("---")
                    st.subheader("2. 계정별 월별 추이 (PY vs CY)")
                    # ✅ 자동 추천 제거: 사용자가 계정을 선택한 경우에만 그래프 렌더
                    account_list = st.session_state.master_df['계정명'].unique()
                    selected_accounts = st.multiselect(
                        "분석할 계정을 선택하세요 (1개 이상 필수)",
                        account_list, default=[]
                    )
                    if not selected_accounts:
                        st.info("계정을 1개 이상 선택하면 월별 추이 그래프가 표시됩니다.")
                    else:
                        lf_use = st.session_state.get('lf_focus') or st.session_state.get('lf_hist')
                        # 선택된 계정명을 계정코드로 변환
                        mdf = st.session_state.master_df
                        accounts_codes = (
                            mdf[mdf['계정명'].isin(selected_accounts)]['계정코드']
                            .astype(str)
                            .tolist()
                        )
                        mod = run_trend_module(lf_use, accounts=accounts_codes)
                        for w in mod.warnings:
                            st.warning(w)
                        if mod.figures:
                            for title, fig in mod.figures.items():
                                # PM 임계선(항상 표시; 범위 밖이면 자동 확장)
                                st.plotly_chart(
                                    add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT))),
                                    use_container_width=True
                                )
                        else:
                            st.info("표시할 추이 그래프가 없습니다.")

                    st.markdown("---")
                    st.subheader("3. 계정 간 상관 히트맵")
                    # ✅ 버튼 없이 즉시 렌더: 계정 2개 이상 선택 + 임계치 슬라이더 제공
                    corr_accounts = st.multiselect(
                        "상관 분석 대상 계정(2개 이상 선택)",
                        account_list,
                        default=selected_accounts,
                        help="선택한 계정들 간 월별 흐름의 피어슨 상관을 계산합니다."
                    )
                    corr_thr = st.slider(
                        "상관 임계치(강한 상관쌍 표 전용)",
                        min_value=0.50, max_value=0.95, step=0.05, value=0.70,
                        help="절대값 기준 임계치 이상인 계정쌍만 표에 표시합니다."
                    )
                    if len(corr_accounts) < 2:
                        st.info("계정을 **2개 이상** 선택하면 히트맵이 표시됩니다.")
                    else:
                        lf_use = _lf_by_scope()
                        mdf = st.session_state.master_df
                        codes = mdf[mdf['계정명'].isin(corr_accounts)]['계정코드'].astype(str).tolist()
                        cmod = run_correlation_module(lf_use, accounts=codes, corr_threshold=float(corr_thr))
                        for w in cmod.warnings:
                            st.warning(w)
                        if cmod.figures:
                            st.plotly_chart(cmod.figures['heatmap'], use_container_width=True)
                        if 'strong_pairs' in cmod.tables and not cmod.tables['strong_pairs'].empty:
                            st.markdown("**임계치 이상 상관쌍**")
                            st.dataframe(cmod.tables['strong_pairs'], use_container_width=True)
                        if 'excluded_accounts' in cmod.tables and not cmod.tables['excluded_accounts'].empty:
                            with st.expander("제외된 계정 보기(변동없음/활동월 부족)", expanded=False):
                                st.dataframe(cmod.tables['excluded_accounts'], use_container_width=True)

                with tab3:
                    st.header("거래처 심층 분석")
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')}")

                    st.subheader("거래처 집중도 및 활동성 (계정별)")
                    master_df_res = st.session_state.master_df
                    account_list_vendor = master_df_res['계정명'].unique()
                    selected_accounts_vendor = st.multiselect("분석할 계정(들)을 선택하세요.", account_list_vendor, default=[])

                    # 🔧 최소 거래금액(연간, CY) 필터 — KRW 입력(커밋 시 쉼표 정규화)
                    min_amount_vendor = _krw_input(
                        "최소 거래금액(연간, CY) 필터",
                        key="vendor_min_amount",
                        default_value=0,
                        help_text="CY 기준 거래금액 합계가 이 값 미만인 거래처는 '기타'로 합산됩니다."
                    )
                    include_others_vendor = st.checkbox("나머지는 '기타'로 합산", value=True)

                    if selected_accounts_vendor:
                        selected_codes = (
                            master_df_res[master_df_res['계정명'].isin(selected_accounts_vendor)]['계정코드']
                            .astype(str)
                            .tolist()
                        )
                        lf_use = _lf_by_scope()
                        vmod = run_vendor_module(
                            lf_use,
                            account_codes=selected_codes,
                            min_amount=float(min_amount_vendor),
                            include_others=bool(include_others_vendor),
                        )
                        if vmod.figures:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'pareto' in vmod.figures:
                                    figp = vmod.figures['pareto']
                                    figp = add_materiality_threshold(figp, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figp, use_container_width=True)
                            with col2:
                                if 'heatmap' in vmod.figures:
                                    figh = add_pm_badge(vmod.figures['heatmap'], float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figh, use_container_width=True)
                        else:
                            st.warning("선택하신 계정에는 분석할 거래처 데이터가 부족합니다.")
                        for w in vmod.warnings:
                            st.warning(w)
                    else:
                        st.info("계정을 선택하면 해당 계정의 거래처 집중도 및 활동성 분석을 볼 수 있습니다.")

                    st.markdown("---")
                    st.subheader("거래처별 세부 분석 (전체 계정)")
                    full_ledger_df = st.session_state.ledger_df
                    vendor_list = sorted(full_ledger_df[full_ledger_df['거래처'] != '정보 없음']['거래처'].unique())

                    if len(vendor_list) > 0:
                        options = ['선택하세요...'] + vendor_list
                        selected_vendor = st.selectbox("상세 분석할 거래처를 선택하세요.", options, index=0)

                        if selected_vendor != '선택하세요...':
                            all_months_in_data = pd.period_range(
                                start=full_ledger_df['회계일자'].min(),
                                end=full_ledger_df['회계일자'].max(),
                                freq='M'
                            ).strftime('%Y-%m').tolist()
                            detail_fig = create_vendor_detail_figure(full_ledger_df, selected_vendor, all_months_in_data)
                            # PM line on vendor detail (stacked bars)
                            try:
                                detail_fig = add_materiality_threshold(detail_fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                            except Exception:
                                pass
                            if detail_fig:
                                st.plotly_chart(detail_fig, use_container_width=True)
                    else:
                        st.info("분석할 거래처 데이터가 없습니다.")

                with tab4:
                    st.header("이상 패턴 탐지 (v1: Z-Score)")
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')}")
                    mdf = st.session_state.master_df
                    acct_names = mdf['계정명'].unique()
                    pick = st.multiselect("대상 계정 선택(미선택 시 자동 추천)", acct_names, default=[])
                    topn = st.slider("표시 개수(상위 |Z|)", min_value=10, max_value=500, value=20, step=10)
                    if st.button("이상치 분석 실행"):
                        lf_use = _lf_by_scope()
                        codes = None
                        if pick:
                            codes = mdf[mdf['계정명'].isin(pick)]['계정코드'].astype(str).tolist()
                        amod = run_anomaly_module(lf_use, target_accounts=codes, topn=topn, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                        for w in amod.warnings: st.warning(w)
                        if 'anomaly_top' in amod.tables:
                            _tbl = amod.tables['anomaly_top'].copy()
                            fmt = {}
                            if '발생액' in _tbl.columns: fmt['발생액'] = '{:,.0f}'
                            if 'Z-Score' in _tbl.columns: fmt['Z-Score'] = '{:.2f}'
                            st.dataframe(_tbl.style.format(fmt), use_container_width=True)
                        if 'zscore_hist' in amod.figures:
                            st.plotly_chart(amod.figures['zscore_hist'], use_container_width=True)

                with tab_ts:
                    st.header("시계열 예측(MoR) — 마지막 포인트 요약 + 라인차트")
                    st.caption("※ 기본값은 '월별 발생액(Δ잔액)'이며, BS 계정은 잔액(balance) 기준도 병행합니다.")
                    with st.expander("📘 해석 가이드", expanded=False):
                        st.markdown(
                            "- **error = 실제 − 예측** (양수면 예상보다 큼)\n"
                            "- **z**: error가 과거 변동성(σ) 대비 몇 σ인지 (±2 주의, ±3 이례)\n"
                            "- **risk**: |z|, PM 대비 비중, KIT 여부를 결합한 0~1 점수"
                        )
                        st.caption("금액 단위가 큰 계정은 금액 자체보다 z 크기를 우선적으로 보세요.")
                    lf_use = _lf_by_scope()
                    mdf = st.session_state.master_df
                    dfm = lf_use.df.copy()
                    dfm['연월'] = dfm['회계일자'].dt.to_period('M').dt.to_timestamp('M')
                    agg = (dfm.groupby(['계정명','연월'])['거래금액'].sum()
                               .reset_index().rename(columns={'계정명':'account','연월':'date','거래금액':'amount'}))
                    pick_accounts_ts = st.multiselect("대상 계정", sorted(agg['account'].unique()), default=[], key="ts_accounts")
                    use_ts = agg if not pick_accounts_ts else agg[agg['account'].isin(pick_accounts_ts)]
                    from analysis.timeseries import run_timeseries_module as _ts_run
                    res = _ts_run(use_ts.rename(columns={'account':'account','date':'date','amount':'amount'}),
                                  account_col='account', date_col='date', amount_col='amount',
                                  pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                    if not res.empty:
                        out = res.copy()
                        out = out.rename(columns={'account':'계정'})
                        for c in ['actual','predicted','error','z','risk']:
                            out[c] = pd.to_numeric(out[c], errors='coerce')
                        _disp = out[['date','계정','actual','predicted','error','z','risk']].rename(columns={
                            'predicted': '예상 발생액(월 합계)',
                            'error': '차이(실제-예상)'
                        })
                        st.caption("MoR(최적 모델) 기준. BS는 balance 기준도 병행 계산합니다(요약 테이블에는 flow가 기본).")
                        st.dataframe(_disp.style.format({
                            'actual':'{:,.0f}', '예상 발생액(월 합계)':'{:,.0f}', '차이(실제-예상)':'{:,.0f}', 'z':'{:+.2f}', 'risk':'{:.2f}'
                        }), use_container_width=True)
                    else:
                        st.info("예측을 표시할 충분한 월별 데이터가 없습니다.")
                with tab5:
                    st.header("계정 × 주장(CEAVOP) 위험 평가")
                    # 잠정 기준 안내 배지
                    try:
                        st.info(f"ⓘ 통합 위험점수는 {PROVISIONAL_RULE_NAME}에 따라 {provisional_risk_formula_str()}로 계산되었습니다.")
                        st.caption("ⓘ CEAVOP 주장은 기본 규칙(예: 예측 상회→E, 하회→C)에 따라 자동 제안되었으며, 전문가의 검토가 필요합니다.")
                    except Exception:
                        pass
                    st.caption(f"🔎 현재 스코프: {st.session_state.get('period_scope','당기')} · PM={float(st.session_state.get('pm_value', PM_DEFAULT)):,.0f}원")
                    # ✅ 한글 가이드: 좌/우 2열 레이아웃
                    g_left, g_right = st.columns([0.48, 0.52])
                    with g_left:
                        st.markdown("#### 어떻게 읽나요?")
                        st.markdown(
                            "- 히트맵의 각 셀은 **계정 × 주장** 조합에 대한 통합 위험점수(0~1)의 *최대값*입니다.\n"
                            "- 위험점수는 **|Z-Score|**, **PM 대비 금액비율**, **Key Item(PM 초과)** 여부를 결합합니다.\n"
                            "- 좌측 사이드바의 **Performance Materiality(PM)** 를 조정하면 KIT 플래그와 히트맵 강도가 함께 변합니다."
                        )
                        st.markdown("#### CEAVOP(주장) 간단 해설")
                        st.info(
                            "C(완전성): 누락 없이 모두 반영되었는가?\n\n"
                            "E(존재): 기록된 거래가 실제 존재하는가?\n\n"
                            "A(정확성): 금액/계산이 정확한가?\n\n"
                            "V(평가·배분): 적절한 평가·배분이 되었는가?\n\n"
                            "O(발생): 발생사실/권리·의무가 타당한가?\n\n"
                            "P(표시·공시): 적절히 분류·표시·공시되었는가?"
                        )
                    with g_right:
                        st.markdown("#### 히트맵")
                    lf_use = _lf_by_scope()
                    # 전체 스코프 기준으로 이상치 모듈을 실행(리스크 에비던스 확보)
                    amod_full = run_anomaly_module(lf_use, target_accounts=None, topn=200, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))

                    # --- [ADD] 타임시리즈 Evidence 생성 & 결합 ---
                    from analysis.timeseries import run_timeseries_module
                    from analysis.contracts import EvidenceDetail, ModuleResult
                    from analysis.anomaly import _risk_from  # anomaly_score 정합성 유지용

                    pm_cur = float(st.session_state.get("pm_value", PM_DEFAULT))

                    # 1) 월 시계열 집계(계정 × 월, 금액=거래금액 합계)
                    ts_base = lf_use.df.copy()
                    ts_base['연월'] = ts_base['회계일자'].dt.to_period('M')
                    ts_monthly = (
                        ts_base.groupby(['계정코드','계정명','연월'], as_index=False)['거래금액']
                               .sum()
                    )
                    # run_timeseries_module는 account/date/amount 3컬럼만 사용 → code|name로 메타 보존
                    ts_monthly['account'] = ts_monthly.apply(lambda r: f"{str(r['계정코드'])}|{str(r['계정명'])}", axis=1)
                    ts_monthly['date'] = ts_monthly['연월'].dt.to_timestamp('M')
                    ts_monthly['amount'] = ts_monthly['거래금액']

                    def _ts_adapter(r: dict) -> EvidenceDetail:
                        # r: {'account','date','amount','predicted','error','z','z_abs','assertion','risk',...}
                        acc_key = str(r.get('account', ''))
                        if '|' in acc_key:
                            acc_code, acc_name = acc_key.split('|', 1)
                        else:
                            acc_code, acc_name = acc_key, acc_key
                        dt = r.get('date')
                        yyyymm = dt.strftime('%Y-%m') if hasattr(dt, 'strftime') else str(dt)
                        a, f, k, score = _risk_from(float(r.get('z_abs', 0.0)), float(r.get('amount', 0.0)), pm_cur)
                        return EvidenceDetail(
                            row_id=f"TS::{acc_code}::{yyyymm}",
                            reason=f"예측 대비 {'상회' if float(r.get('error',0))>0 else '하회'}: z={float(r.get('z',0)):+.2f}",
                            anomaly_score=float(a),
                            financial_impact=abs(float(r.get('amount', 0.0))),
                            risk_score=float(r.get('risk', score)),
                            is_key_item=bool(abs(float(r.get('amount',0.0))) >= pm_cur),
                            measure="flow",
                            sign_rule="assets/expenses↑=+, liabilities/equity↑=-",
                            impacted_assertions=sorted({ "A", str(r.get('assertion','A')) }),
                            links={"account_code": str(acc_code), "account_name": str(acc_name), "period_tag": "CY"}
                        )

                    # 2) EvidenceDetail 리스트 생성
                    ts_evidences = run_timeseries_module(
                        ts_monthly[['account','date','amount']],
                        evidence_adapter=_ts_adapter,
                    )

                    ts_mod = ModuleResult(
                        name="timeseries",
                        summary={"n_rows": len(ts_monthly), "n_evidences": len(ts_evidences)},
                        tables={},
                        figures={},
                        evidences=ts_evidences,
                        warnings=[]
                    )

                    # 3) 위험 매트릭스: 이상치 + 예측 Evidence 동시 반영
                    mat, emap = build_matrix([amod_full, ts_mod])
                    if mat.empty:
                        st.info("위험 매트릭스를 생성할 Evidence가 없습니다.")
                    else:
                        import plotly.express as px
                        fig = px.imshow(mat, aspect='auto', origin='upper',
                                        title="계정 × 주장 위험 히트맵 (max risk_score, 0~1)",
                                        labels=dict(x="Assertion", y="Account", color="Risk"))
                        fig.update_coloraxes(cmin=0, cmax=1)
                        st.plotly_chart(fig, use_container_width=True)
                        with st.expander("수치 테이블 보기", expanded=False):
                            st.dataframe(mat, use_container_width=True)
                        # 🔽 전체 Evidence CSV 내보내기 (행렬 근거 전부)
                        from dataclasses import asdict
                        all_evs = (amod_full.evidences or []) + (ts_mod.evidences or [])
                        if all_evs:
                            ev_all_df = pd.DataFrame([asdict(e) for e in all_evs])
                            # impacted_assertions 리스트 문자열화
                            if 'impacted_assertions' in ev_all_df.columns:
                                ev_all_df['impacted_assertions'] = ev_all_df['impacted_assertions'].apply(
                                    lambda xs: ",".join(xs) if isinstance(xs, list) else str(xs)
                                )
                            # links 평탄화
                            if 'links' in ev_all_df.columns:
                                _lnk = pd.json_normalize(ev_all_df['links']).add_prefix('links.')
                                ev_all_df = pd.concat([ev_all_df.drop(columns=['links']), _lnk], axis=1)
                            # 컬럼 순서 고정
                            _ORDER = ['row_id','risk_score','anomaly_score','financial_impact','is_key_item',
                                      'impacted_assertions','links.account_code','links.account_name','links.period_tag','reason']
                            for col in _ORDER:
                                if col not in ev_all_df.columns:
                                    ev_all_df[col] = ""
                            ev_all_df = ev_all_df[_ORDER]
                            st.download_button(
                                "📥 Evidence 전체 CSV 다운로드",
                                ev_all_df.to_csv(index=False).encode('utf-8-sig'),
                                file_name="evidence_all.csv",
                                mime="text/csv"
                            )

                        # 드릴다운: 계정/주장 선택 → 근거 표시
                        st.subheader("🔎 드릴다운: 특정 셀의 근거(Evidence)")
                        acct = st.selectbox("계정(행)", ["선택하세요..."] + mat.index.tolist(), index=0, help="조사할 계정(행)을 선택하세요.", key="risk_dd_account")
                        asrt = st.selectbox("주장(열)", ["선택하세요..."] + list(mat.columns), index=0, help="관리자의 주장(CEAVOP) 중에서 선택하세요.", key="risk_dd_assertion")
                        # 선택한 주장에 대한 짧은 설명
                        _asrt_help = {
                            "C":"완전성", "E":"존재", "A":"정확성", "V":"평가·배분",
                            "O":"발생", "P":"표시·공시"
                        }
                        if asrt in _asrt_help:
                            st.caption(f"선택한 주장 설명: **{asrt} – {_asrt_help[asrt]}**")
                        if acct != "선택하세요..." and asrt != "선택하세요...":
                            from dataclasses import asdict
                            ev_all = st.session_state.get('amod_full_evidences') or amod_full.evidences
                            st.session_state['amod_full_evidences'] = ev_all
                            def _match_ev(e, acct_name, asrt_code):
                                name_ok = (e.links.get("account_name") == acct_name) or (e.links.get("account_code") == acct_name)
                                asrt_ok = asrt_code in (e.impacted_assertions or [])
                                return bool(name_ok and asrt_ok)
                            direct_hits = [asdict(e) for e in ev_all if _match_ev(e, acct, asrt)]
                            row_ids = emap.get((acct, asrt), [])
                            by_id_hits = [asdict(e) for e in ev_all if str(e.row_id) in row_ids]
                            rows = direct_hits or by_id_hits
                            ev_df = pd.DataFrame(rows)
                            if not ev_df.empty:
                                ev_df['impacted_assertions'] = ev_df['impacted_assertions'].apply(lambda xs: ",".join(xs) if isinstance(xs, list) else str(xs))
                                show_cols = ['row_id','risk_score','is_key_item','anomaly_score','financial_impact','impacted_assertions','reason']
                                st.dataframe(ev_df[show_cols].sort_values('risk_score', ascending=False), use_container_width=True)
                            else:
                                st.info("해당 셀에서 표시할 Evidence 레코드를 찾지 못했습니다. (키 미스매치 방지 로직 적용 완료)")

                        # 예측 Evidence 요약(듀얼 기준 + MoR)
                        with st.expander("🔮 예측 기반 Evidence(요약) — 계정별 마지막 포인트", expanded=False):
                            def _acc_label(k: str) -> str:
                                return (k.split('|',1)[1] if '|' in str(k) else str(k))
                            # 계정별 월 집계
                            base = ts_monthly[['account','date','amount']].copy()
                            base = base.rename(columns={'amount':'flow'})
                            base['balance'] = base.groupby('account')['flow'].cumsum()
                            # BS/PL 매핑
                            bs_map = st.session_state.master_df[['계정코드','계정명','BS/PL']].drop_duplicates()
                            bs_map['key'] = bs_map['계정코드'].astype(str) + '|' + bs_map['계정명'].astype(str)
                            bs_flag = bs_map.set_index('key')['BS/PL'].map(lambda x: str(x).upper()=='BS').to_dict()
                            from analysis.timeseries import run_timeseries_for_account
                            rows_ts = []
                            for acc, g in base.groupby('account'):
                                is_bs = bool(bs_flag.get(str(acc), True))  # 정보없으면 True로 보수적 처리
                                out = run_timeseries_for_account(
                                    g[['date','flow','balance']], _acc_label(str(acc)),
                                    is_bs=is_bs, flow_col='flow', balance_col='balance',
                                    pm_value=float(st.session_state.get("pm_value", PM_DEFAULT))
                                )
                                if not out.empty:
                                    rows_ts.append(out)
                            # 보기 범위 토글은 항상 노출
                            scope = st.selectbox("보기 범위", options=["flow","balance","both"], index=2, key="ts_scope_main")
                            if rows_ts:
                                df_ts = pd.concat(rows_ts, ignore_index=True)
                                st.caption("BS 계정은 잔액·발생액 기준을 병행 계산합니다. 표시 기준은 위 토글을 따릅니다.")
                                st.caption("본 예측은 **PY+CY 연속 월**(보간 없음)로 학습하고 MoR(최적 모델)을 사용합니다. error는 z와 함께 해석하세요.")
                                df_view = df_ts if scope == "both" else df_ts[df_ts['measure'] == scope]
                                st.dataframe(df_view[['date','account','measure','actual','predicted','error','z','risk','model']], use_container_width=True)

                                # === (교체) 계정/기준 선택 라인/쌍차트 ===
                                import plotly.graph_objects as go

                                def _make_ts_fig(df_hist: pd.DataFrame, measure: str, title: str):
                                    """EMA 기반 예측선을 같이 그려주는 간단 라인차트(실선=actual, 점선=pred)."""
                                    s = df_hist[['date', measure]].rename(columns={measure: 'actual'}).sort_values('date').copy()
                                    if s.empty:
                                        return None
                                    # EMA 예측선(shift 1)
                                    s['predicted'] = s['actual'].ewm(span=6, adjust=False).mean().shift(1)
                                    fig = go.Figure()
                                    fig.add_trace(go.Scatter(x=s['date'], y=s['actual'], mode='lines', name='actual'))
                                    fig.add_trace(go.Scatter(x=s['date'], y=s['predicted'], mode='lines', name='predicted', line=dict(dash='dot')))
                                    fig.update_layout(title=title, xaxis_title='month', yaxis_title=measure)
                                    try:
                                        return add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    except Exception:
                                        return fig

                                # ── 선택 계정
                                sel_acc = st.selectbox("계정 선택", sorted(df_ts["account"].unique()), key="ts_plot_acc")

                                # 히스토리(월별 flow/balance 재구성)
                                hist_base = use_ts.copy()  # use_ts는 위에서 만든 monthly agg (account,date,amount)
                                # 계정별로 flow/balance 동시 구성
                                hist_base = hist_base.rename(columns={'amount':'flow'})
                                hist_base['balance'] = hist_base.sort_values('date').groupby('account')['flow'].cumsum()

                                cur_hist = hist_base[hist_base['account'] == sel_acc].copy()
                                if cur_hist.empty:
                                    st.info("선택 계정의 월별 데이터가 없습니다.")
                                else:
                                    # BS 여부 판단 (Master의 BS/PL 활용)
                                    _mdf = st.session_state.master_df[['계정코드','계정명','BS/PL']].drop_duplicates()
                                    # 계정명이 같은 항목을 찾아 BS/PL 확인 (없으면 PL 취급)
                                    try:
                                        is_bs = bool(_mdf[_mdf['계정명'] == sel_acc]['BS/PL'].astype(str).str.upper().eq('BS').any())
                                    except Exception:
                                        is_bs = False

                                    pair = st.toggle("쌍차트 보기(Flow+Balance)", value=is_bs, disabled=not is_bs)
                                    if pair and is_bs:
                                        c1, c2 = st.columns(2)
                                        with c1:
                                            f1 = _make_ts_fig(cur_hist, 'flow', f"{sel_acc} — Flow (actual vs MoR)")
                                            if f1: st.plotly_chart(f1, use_container_width=True)
                                        with c2:
                                            f2 = _make_ts_fig(cur_hist, 'balance', f"{sel_acc} — Balance (actual vs MoR)")
                                            if f2: st.plotly_chart(f2, use_container_width=True)
                                    else:
                                        measure = st.radio("기준(Measure)", ["flow","balance"], horizontal=True, index=0 if not is_bs else 0,
                                                           help="BS가 아닌 계정은 balance가 의미 없을 수 있습니다.")
                                        fig = _make_ts_fig(cur_hist, measure, f"{sel_acc} — {measure.title()} (actual vs MoR)")
                                        if fig: st.plotly_chart(fig, use_container_width=True)

                                # CSV 내보내기(항상 두 기준 포함)
                                st.download_button(
                                    "📥 예측 요약 CSV 다운로드(듀얼기준)",
                                    data=df_ts.to_csv(index=False).encode('utf-8-sig'),
                                    file_name="evidence_timeseries_dual.csv",
                                    mime="text/csv",
                                    key="ts_csv_dl"
                                )
                            else:
                                st.info("최근 포인트 기준 예측 이탈 Evidence가 없습니다.")

                        # (중복) 드릴다운 블럭 제거 — 위에 이미 1회 존재

                with tab6:
                    st.header("AI 리포트 및 채팅")
                    # LLM 키 미가용이어도 오프라인 리포트 모드로 생성 가능
                    LLM_OK = False
                    try:
                        from services.llm import openai_available
                        LLM_OK = bool(openai_available())
                    except Exception:
                        LLM_OK = False
                    if not LLM_OK:
                        st.info("🔌 OpenAI Key 없음: 오프라인 리포트 모드로 생성합니다. (클러스터/요약 LLM 미사용)")
                    rendered_report = False

                    # === 모델/토큰 옵션 UI ===
                    colm1, colm2, colm3 = st.columns([1,1,1])
                    with colm1:
                        llm_model_choice = st.selectbox(
                            "LLM 모델", options=["gpt-5", "gpt-4o"], index=1,
                            help="gpt-5 미가용 시 자동으로 gpt-4o로 대체하세요(코드에서 예외 처리)."
                        )
                    with colm2:
                        desired_tokens = st.number_input(
                            "보고서 최대 출력 토큰", min_value=512, max_value=32000, value=16000, step=512,
                            help="실제 전송값은 모델 컨텍스트와 입력 토큰을 고려해 안전 클램프됩니다."
                        )
                    with colm3:
                        st.caption("금액·포맷은 코드에서 강제됩니다.")

                    # --- 입력 영역 ---
                    mdf = st.session_state.master_df
                    ldf = st.session_state.ledger_df

                    # ① 계정 선택(필수) — 자동 추천 제거
                    acct_names_all = sorted(mdf['계정명'].dropna().unique().tolist())
                    pick_accounts = st.multiselect(
                        "보고서 대상 계정(들)을 선택하세요. (최소 1개)",
                        options=acct_names_all,
                        default=[]
                    )
                    # ② 옵션 제거: 항상 수행 플래그
                    opt_knn_evidence = True
                    opt_patterns = True
                    opt_patterns_py = True

                    # ③ 사용자 메모(선택)
                    manual_ctx = st.text_area(
                        "보고서에 추가할 메모/주의사항(선택)",
                        placeholder="예: 5~7월 대형 캠페인 집행 영향, 3분기부터 단가 인상 예정 등"
                    )

                    # ④ 선택 계정코드 매핑
                    pick_codes = (
                        mdf[mdf['계정명'].isin(pick_accounts)]['계정코드']
                        .astype(str).tolist()
                    )

                    colA, colB, colC = st.columns([1,1,1])
                    with colA: st.write("선택 계정코드:", ", ".join(pick_codes) if pick_codes else "-")
                    with colB: st.write("기준 연도(CY):", int(ldf['연도'].max()))
                    with colC: st.write("보고서 기준:", "Current Year GL")

                    # 버튼은 계정 미선택 시 비활성화
                    btn = st.button("📝 보고서 생성", type="primary", disabled=(len(pick_codes) == 0))
                    if len(pick_codes) == 0:
                        st.info("계정 1개 이상 선택 시 버튼이 활성화됩니다.")

                    if btn:
                        import time
                        from analysis.anomaly import compute_amount_columns
                        from analysis.embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
                        from analysis.report import generate_rag_context, run_final_analysis, build_methodology_note, run_offline_fallback_report
                        from services.llm import LLMClient
                        from analysis.anomaly import ensure_zscore

                        t0 = time.perf_counter()
                        with st.status("보고서 준비 중...", expanded=True) as s:
                            # Step 1) 데이터 슬라이싱
                            s.write("① 스코프 적용 및 데이터 슬라이싱(CY/PY)…")
                            cur_year = ldf['연도'].max()
                            df_cy = ldf[(ldf['period_tag'] == 'CY') & (ldf['계정코드'].astype(str).isin(pick_codes))].copy()
                            df_py = ldf[(ldf['period_tag'] == 'PY') & (ldf['계정코드'].astype(str).isin(pick_codes))].copy()
                            s.write(f"    └ CY {len(df_cy):,}건 / PY {len(df_py):,}건")

                            # Step 2) 필수 파생(발생액/순액)
                            s.write("② 금액 파생 컬럼 생성(발생액/순액)…")
                            df_cy = compute_amount_columns(df_cy)
                            df_py = compute_amount_columns(df_py)

                            # Step 3) (선택) 패턴요약: 임베딩/클러스터링 (LLM 사용 가능 시에만)
                            cl_ok = False
                            if LLM_OK and opt_patterns and not df_cy.empty:
                                s.write("③ 임베딩·클러스터링 실행(선택)…")
                                # 입력 텍스트 풍부화 + 임베딩 + HDBSCAN (최대 N 제한으로 안전가드)
                                df_cy_small = df_cy.copy()
                                max_rows = 8000
                                if len(df_cy_small) > max_rows:
                                    df_cy_small = df_cy_small.sample(max_rows, random_state=42)
                                    s.write(f"    └ 데이터가 많아 {max_rows:,}건으로 샘플링")
                                df_cy_small = ensure_rich_embedding_text(df_cy_small)
                                try:
                                    emb_client = LLMClient().client  # OpenAI 클라이언트 객체
                                    # LLM naming is mandatory for the report
                                    df_clu, ok = perform_embedding_and_clustering(
                                        df_cy_small, emb_client,
                                        name_with_llm=True, must_name_with_llm=True,
                                        use_large=bool(st.session_state.get("use_large_embedding", False)),
                                        rescue_tau=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
                                    )
                                    if ok:
                                        # unify near-duplicate names using LLM
                                        from analysis.embedding import unify_cluster_names_with_llm, unify_cluster_labels_llm
                                        df_clu, name_map = unify_cluster_names_with_llm(df_clu, emb_client)
                                        # 추가 LLM 라벨 통합(JSON 매핑 방식) — CY의 cluster_group은 유지
                                        try:
                                            raw_map = unify_cluster_labels_llm(df_clu['cluster_name'].dropna().unique().tolist(), emb_client)
                                            if raw_map:
                                                df_clu['cluster_name'] = df_clu['cluster_name'].map(lambda x: raw_map.get(str(x), x))
                                                # ❗ cluster_group는 unify_cluster_names_with_llm()이 정한 canonical을 유지
                                        except Exception:
                                            pass
                                        # 간단 요약(상위 5개)
                                        topc = (df_clu.groupby('cluster_group')['발생액']
                                                .agg(['count','sum']).sort_values('sum', ascending=False).head(5))
                                        s.write("    └ 클러스터 상위 5개 요약:")
                                        st.dataframe(
                                            topc.rename(columns={'count':'건수','sum':'발생액합계'})
                                                .style.format({'발생액합계':'{:,.0f}'}),
                                            use_container_width=True
                                        )
                                        # Quality telemetry
                                        try:
                                            n = int(len(df_clu))
                                            noise_rate = float((df_clu['cluster_id'] == -1).mean()) if n else 0.0
                                            n_clusters = int(df_clu.loc[df_clu['cluster_id'] != -1, 'cluster_id'].nunique())
                                            if n_clusters > 0:
                                                avg_size = float(df_clu[df_clu['cluster_id'] != -1].groupby('cluster_id').size().mean())
                                            else:
                                                avg_size = 0.0
                                            rescue_rate = float(df_clu.get('rescued', False).mean()) if 'rescued' in df_clu.columns else 0.0
                                            model_used = df_clu.attrs.get('embedding_model', 'unknown')
                                            umap_on = bool(df_clu.attrs.get('umap_used', False))
                                            s.write(
                                                f"    └ Quality: N={n:,}, noise={noise_rate*100:.1f}%, "
                                                f"clusters={n_clusters}, avg_size={avg_size:.1f}, rescued={rescue_rate*100:.1f}%"
                                            )
                                            s.write(
                                                f"    └ Model/UMAP: {model_used} | UMAP={'on' if umap_on else 'off'} | τ={float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)):.2f}"
                                            )
                                            # Persist metrics for dashboard card
                                            st.session_state['cluster_quality'] = {
                                                "N": n,
                                                "noise_rate": noise_rate,
                                                "n_clusters": n_clusters,
                                                "avg_size": avg_size,
                                                "rescued_rate": rescue_rate,
                                                "model": model_used,
                                                "umap": umap_on,
                                                "tau": float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)),
                                            }
                                        except Exception:
                                            pass
                                        # 보고서 컨텍스트에 반영: group/label 동시 부착
                                        df_cy = df_cy.merge(
                                            df_clu[['row_id','cluster_id','cluster_name','cluster_group']],
                                            on='row_id', how='left'
                                        )
                                        # 필요 시 vector도 함께 병합 가능:
                                        # df_cy = df_cy.merge(df_clu[['row_id','vector']], on='row_id', how='left')
                                        # (현재는 perform_embedding_only 단계에서 CY/PY df에 vector가 직접 부여됨)
                                        # --- PY clustering and alignment (optional) ---
                                        if opt_patterns_py and not df_py.empty:
                                            try:
                                                from analysis.embedding import cluster_year, align_yearly_clusters, unify_cluster_labels_llm
                                                # sampling guard similar to CY
                                                df_py_small = df_py.copy()
                                                max_rows = 8000
                                                if len(df_py_small) > max_rows:
                                                    df_py_small = df_py_small.sample(max_rows, random_state=42)
                                                    s.write(f"    └ PY 데이터가 많아 {max_rows:,}건으로 샘플링")
                                                df_py_clu = cluster_year(df_py_small, emb_client)
                                                # push back columns to df_py via row_id if available
                                                if not df_py_clu.empty and 'row_id' in df_py.columns:
                                                    df_py = df_py.merge(df_py_clu, on='row_id', how='left', suffixes=("", "_pyclu"))
                                                # alignment: map PY cluster IDs to CY
                                                if 'cluster_id' in df_py_clu.columns:
                                                    mapping = align_yearly_clusters(df_clu, df_py_clu, sim_threshold=0.70)
                                                    # cluster_id → (aligned_cy_cluster, aligned_sim)
                                                    cy_id_to_name = df_clu.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()
                                                    def _get_pair(cid):
                                                        try:
                                                            if pd.isna(cid):
                                                                return (np.nan, np.nan)
                                                            cid_int = int(cid)
                                                            return mapping.get(cid_int, (np.nan, np.nan))
                                                        except Exception:
                                                            return (np.nan, np.nan)
                                                    if 'cluster_id' in df_py.columns:
                                                        pairs = df_py['cluster_id'].map(_get_pair)
                                                        df_py[['aligned_cy_cluster', 'aligned_sim']] = pd.DataFrame(pairs.tolist(), index=df_py.index)
                                                        # 이름은 CY의 이름으로 정렬(가능한 경우)
                                                        df_py['cluster_name'] = df_py['aligned_cy_cluster'].map(cy_id_to_name).fillna(df_py.get('cluster_name'))
                                                # final unification over union of names — CY의 cluster_group 불변, PY는 표시명/그룹을 canonical로 정렬
                                                try:
                                                    all_names = pd.Series([], dtype=object)
                                                    if 'cluster_name' in df_cy.columns:
                                                        all_names = pd.concat([all_names, df_cy['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    if 'cluster_name' in df_py.columns:
                                                        all_names = pd.concat([all_names, df_py['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    all_names = all_names.dropna().unique().tolist()
                                                    canon = unify_cluster_labels_llm(all_names, emb_client)
                                                    if canon:
                                                        if 'cluster_name' in df_cy.columns:
                                                            df_cy['cluster_name'] = df_cy['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_name' in df_py.columns:
                                                            df_py['cluster_name'] = df_py['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_group' in df_py.columns:
                                                            df_py['cluster_group'] = df_py['cluster_name']
                                                except Exception:
                                                    pass
                                            except Exception as e:
                                                s.write(f"    └ PY 클러스터링/정렬 스킵: {e}")
                                        # 컨텍스트에 별도 노트는 추가하지 않음
                                        cl_ok = True
                                    else:
                                        s.write("    └ LLM 클러스터 이름 생성 실패 또는 결과 없음 → 보고서 생성 요건 미충족")
                                except Exception as e:
                                    s.write(f"    └ 임베딩/클러스터링 실패: {e}")
                            else:
                                s.write("③ 임베딩·클러스터링: LLM 미가용 또는 옵션 비활성 → 스킵")

                            # Step 3-1) (옵션 A) 근거 인용(KNN)용 임베딩만 수행 (LLM 가능 시)
                            if LLM_OK and opt_knn_evidence:
                                s.write("③-1 근거 인용용 임베딩(CY/PY)…")
                                from analysis.embedding import perform_embedding_only, ensure_rich_embedding_text
                                emb_client2 = LLMClient().client
                                df_cy = ensure_rich_embedding_text(df_cy)
                                df_py = ensure_rich_embedding_text(df_py)
                                df_cy = perform_embedding_only(
                                    df_cy, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False))
                                )
                                df_py = perform_embedding_only(
                                    df_py, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False))
                                )
                            elif not LLM_OK:
                                s.write("③-1 근거 인용 임베딩: LLM 미가용 → 스킵")

                            # Step 3-2) Z-Score: 반드시 존재해야 함
                            s.write("③-2 Z-Score 계산/검증…")
                            df_cy, z_ok = ensure_zscore(df_cy, pick_codes)
                            df_py, _    = ensure_zscore(df_py, pick_codes)  # 전기에도 Z-Score 계산(컨텍스트에서 사용)
                            if not z_ok:
                                s.write("    └ Z-Score 미계산 또는 전부 결측")

                            # ✅ 게이트 완화: Z-Score만 확보되면 보고서 진행.
                            #    (클러스터 실패 시 관련 섹션은 자동 축약/생략)
                            if not z_ok:
                                st.error("보고서 생성 중단: Z-Score 없음.")
                                s.update(label="보고서 요건 미충족", state="error")
                                st.stop()
                            if not cl_ok:
                                s.write("    └ 클러스터링 결과 없음 → 리포트에서 클러스터 섹션은 생략/축약됩니다.")

                            # Step 4) 컨텍스트 생성 + 방법론 노트
                            s.write("④ 컨텍스트 텍스트 구성…")
                            ctx = generate_rag_context(
                                mdf, df_cy, df_py,
                                account_codes=pick_codes,
                                manual_context=manual_ctx,
                                include_risk_summary=True,
                                pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                            )
                            note = build_methodology_note(report_accounts=pick_codes)

                            # Step 5) LLM 호출 전 점검(길이/토큰)
                            s.write("⑤ LLM 프롬프트 점검…")
                            prompt_len = len(ctx) + len(note)
                            s.write(f"    └ 컨텍스트 길이: {prompt_len:,} chars")
                            try:
                                import tiktoken
                                enc = tiktoken.get_encoding("cl100k_base")
                                est_tokens = len(enc.encode(ctx)) + len(enc.encode(note))
                                s.write(f"    └ 예상 토큰 수: ~{est_tokens:,} tokens")
                            except Exception:
                                s.write("    └ tiktoken 미설치: 토큰 추정 생략")

                            # Step 6) 보고서 생성: LLM 가능하면 시도, 실패/불가 시 오프라인 폴백
                            final_report = None
                            if LLM_OK:
                                s.write("⑥ LLM 요약 생성 호출…")
                                try:
                                    t_llm0 = time.perf_counter()
                                    final_report = run_final_analysis(
                                        context=ctx + "\n" + note,
                                        account_codes=pick_codes,
                                        model=llm_model_choice,
                                        max_tokens=int(desired_tokens),
                                    )
                                    s.write(f"    └ LLM 완료 (경과 {time.perf_counter()-t_llm0:.1f}s)")
                                except Exception as e:
                                    s.write(f"    └ LLM 실패: {e} → 오프라인 폴백으로 전환")

                            if final_report is None:
                                s.write("⑥-폴백: 오프라인 리포트 생성…")
                                final_report = run_offline_fallback_report(
                                    current_df=df_cy,
                                    previous_df=df_py,
                                    account_codes=pick_codes,
                                    pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                                )

                            s.update(label="보고서 준비 완료", state="complete")

                            # 결과 출력 및 세션 보존
                            st.session_state['last_report'] = final_report
                            st.session_state['last_context'] = ctx + "\n" + note
                            st.session_state['last_dfcy'] = df_cy
                            st.session_state['last_dfpy'] = df_py

                            st.success("보고서가 생성되었습니다.")
                            st.markdown("### 📄 AI 요약 보고서")
                            st.markdown(final_report)

                        with st.expander("🔎 근거 컨텍스트(LLM 입력)", expanded=False):
                            st.text(st.session_state['last_context'])

                        # ZIP 단일 다운로드 + RAW 미리보기
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['회계일자','계정코드','계정명','거래처','적요','발생액','순액','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()

                        raw_df = _build_raw_evidence(st.session_state['last_dfcy'])
                        st.markdown("#### 📑 근거: 선택 계정 원장(RAW) + 클러스터")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'발생액':'{:,.0f}','순액':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("표시할 RAW가 없습니다.")

                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "📥 보고서+근거 다운로드(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_current"  # 고유 키(현재 결과)
                        )

                        st.caption(f"⏱ 총 소요: {time.perf_counter()-t0:.1f}s")
                        rendered_report = True

                    # === 캐시된 이전 결과 렌더(버튼 미클릭 시에만) ===
                    if st.session_state.get('last_report') and not btn:
                        st.success("보고서가 준비되어 있습니다.")
                        st.markdown("### 📄 AI 요약 보고서")
                        st.markdown(st.session_state['last_report'])
                        with st.expander("🔎 근거 컨텍스트(LLM 입력)", expanded=False):
                            st.text(st.session_state['last_context'])
                        # RAW 미리보기 + ZIP 버튼 재출력
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['회계일자','계정코드','계정명','거래처','적요','발생액','순액','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()
                        raw_df = _build_raw_evidence(st.session_state.get('last_dfcy', pd.DataFrame()))
                        st.markdown("#### 📑 근거: 선택 계정 원장(RAW) + 클러스터")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'발생액':'{:,.0f}','순액':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("표시할 RAW가 없습니다.")
                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "📥 보고서+근거 다운로드(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_cached"  # 고유 키(캐시 결과)
                        )
                        # Cluster quality card (if available)
                        cq = st.session_state.get("cluster_quality")
                        if cq:
                            st.markdown("---")
                            st.subheader("클러스터 품질 요약")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("Noise rate", f"{cq['noise_rate']*100:.1f}%")
                            c2.metric("#Clusters", f"{cq['n_clusters']}")
                            c3.metric("Avg size", f"{cq['avg_size']:.1f}")
                            c4.metric("Rescued", f"{cq['rescued_rate']*100:.1f}%")
                            st.caption(f"Model: {cq['model']} | UMAP: {'on' if cq['umap'] else 'off'} | τ={cq['tau']:.2f} | N={cq['N']:,}")
        except Exception as e:
            st.error(f"데이터 처리 중 오류가 발생했습니다: {e}")
            if st.button("매핑 단계로 돌아가기"):
                st.session_state.mapping_confirmed = False
                st.rerun()
else:
    st.info("⬅️ 왼쪽 사이드바에서 분석할 엑셀 파일을 업로드해주세요.")





==============================
📄 FILE: config.py
==============================

LLM_MODEL = "gpt-4o"
LLM_TEMPERATURE = 0.2
LLM_JSON_MODE = True
PM_DEFAULT = 500_000_000  # Project-wide Performance Materiality (KRW)

EMBED_BATCH_SIZE = 256
EMBED_CACHE_DIR = ".cache/embeddings"

# 훈님 결정 반영 ✅
SHAP_TOP_N_PER_ACCOUNT_DEFAULT = 25   # 사용자 UI에서 20~30 범위 선택 가능
CYCLE_RECOMMENDER = "llm_only"        # LLM 100% 자동 추천
PM_DEFAULT = PM_DEFAULT              # (kept above; single source of truth)

# ---- NEW: Embedding & Clustering defaults ----
# Embedding model switch (Small by default; Large improves semantics at higher cost)
EMB_MODEL_SMALL = "text-embedding-3-small"
EMB_MODEL_LARGE = "text-embedding-3-large"
EMB_USE_LARGE_DEFAULT = False           # UI/auto-upscale can override per run

# UMAP threshold (apply UMAP → HDBSCAN only when N is large)
UMAP_APPLY_THRESHOLD = 8000             # set 0/None to disable
UMAP_N_COMPONENTS = 20
UMAP_N_NEIGHBORS = 30
UMAP_MIN_DIST = 0.0

# HDBSCAN noise-rescue cosine threshold
HDBSCAN_RESCUE_TAU = 0.75               # 0.72~0.78 usually works well

# Adaptive clustering knobs (computed per run, not static)
# min_cluster_size = max(8, int(sqrt(N))); min_samples = max(2, int(0.5 * min_cluster_size))

# ===== NEW: Materiality & Risk Weights =====
# === 통합 리스크 가중치(고정값; v2.0-RC 동결) ===
# 점수 = 0.5*A(|Z|정규화) + 0.4*F(PM 대비 비율 capped 1) + 0.1*K(PM 초과=1)
RISK_WEIGHT_A = 0.5
RISK_WEIGHT_F = 0.4
RISK_WEIGHT_K = 0.1

# --- NEW: Z-Score → sigmoid 스케일 조정 (로드맵 호환)
# anomaly_score = sigmoid(|Z| / Z_SIGMOID_DIVISOR)
# 로드맵 권고: 3.0 (과도 포화 완화)
Z_SIGMOID_DIVISOR = 3.0
Z_SIGMOID_SCALE = Z_SIGMOID_DIVISOR  # 하위호환

# --- 표준 회계 사이클 (STANDARD_ACCOUNTING_CYCLES) ---
# 키: 사이클 식별자, 값: 해당 사이클에 매핑될 가능성이 높은 계정명 키워드(부분일치)
# *한국어/영문 혼용. 필요 시 프로젝트 도메인에 맞춰 보강하세요.
STANDARD_ACCOUNTING_CYCLES = {
    "Cash": ["현금", "예금", "단기금융", "Cash", "Bank"],
    "Revenue": ["매출", "판매수익", "Sales", "Revenue"],
    "Receivables": ["매출채권", "외상매출금", "미수금", "Receivable", "A/R"],
    "Inventory": ["재고", "상품", "제품", "원재료", "재공품", "Inventory"],
    "Payables": ["매입채무", "외상매입금", "미지급금", "Payable", "A/P"],
    "Expenses": ["복리후생비", "급여", "임차료", "접대비", "감가상각비", "비용", "Expense"],
    "FixedAssets": ["유형자산", "감가상각누계", "기계장치", "건물", "비품", "PPE", "Fixed Asset"],
    "Equity": ["자본금", "이익잉여금", "자본잉여금", "Equity", "Capital"],
}

# --- Provisional rule naming (도메인 합의 전) ---
PROVISIONAL_RULE_VERSION = "v1.0"
PROVISIONAL_RULE_NAME = f"잠정 기준({PROVISIONAL_RULE_VERSION})"

def provisional_risk_formula_str() -> str:
    """UI/리포트 안내문에 쓰일 가중치 요약 문자열을 동적으로 생성"""
    a = int(RISK_WEIGHT_A * 100)
    f = int(RISK_WEIGHT_F * 100)
    k = int(RISK_WEIGHT_K * 100)
    return f"통계적 이상({a}%) + 재무적 영향({f}%) + KIT 여부({k}%)"

# 리포트(최종본) 포함 조건 노브 (기본: 포함 안 함)
INCLUDE_RISK_MATRIX_SUMMARY_IN_FINAL = False
# ‘상위 N’ 결과가 이 값 미만이면 최종본에 생략 (근거 컨텍스트엔 유지)
RISK_MATRIX_SECTION_MIN_ITEMS = 3

# --- TimeSeries forecast knobs ---
FORECAST_MIN_POINTS = 8         # Prophet/ARIMA 사용 권장 최소 길이(권고치)
ARIMA_DEFAULT_ORDER = (1,1,1)

# --- User overrides for STANDARD_ACCOUNTING_CYCLES ---
CYCLES_USER_OVERRIDES_PATH = ".cache/cycles_overrides.json"



==============================
📄 FILE: analysis/anomaly.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import List, Optional
from utils.helpers import find_column_by_keyword


def compute_amount_columns(df: pd.DataFrame) -> pd.DataFrame:
    """발생액(절대 규모) / 순액(차-대) 계산."""
    dcol = find_column_by_keyword(df.columns, '차변')
    ccol = find_column_by_keyword(df.columns, '대변')
    df = df.copy()
    if not dcol or not ccol:
        df['발생액'] = 0.0; df['순액'] = 0.0; df['거래금액'] = 0.0
        return df
    d = pd.to_numeric(df[dcol], errors='coerce').fillna(0.0)
    c = pd.to_numeric(df[ccol], errors='coerce').fillna(0.0)
    row_amt = np.where((d > 0) & (c == 0), d,
              np.where((c > 0) & (d == 0), c,
              np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
    df['발생액'] = row_amt
    df['순액']  = d - c
    df['거래금액'] = df['순액']
    return df


def calculate_grouped_stats_and_zscore(df: pd.DataFrame, target_accounts: List[str], data_type: str = "당기") -> pd.DataFrame:
    """선택 계정 그룹의 발생액 분포 기준 Z-Score 산출."""
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    df = compute_amount_columns(df.copy())
    if not acct_col:
        df['Z-Score'] = 0.0
        return df
    is_target = df[acct_col].astype(str).isin([str(x) for x in target_accounts])
    tgt = df.loc[is_target, '발생액'].astype(float)
    df['Z-Score'] = 0.0
    if tgt.empty:
        return df
    mu = float(tgt.mean()); std = float(tgt.std(ddof=1))
    if std and std > 0:
        df.loc[is_target, 'Z-Score'] = (df.loc[is_target, '발생액'] - mu) / std
    else:
        med = float(tgt.median()); mad = float((np.abs(tgt - med)).median())
        df.loc[is_target, 'Z-Score'] = 0.0 if mad == 0 else 0.6745 * (df.loc[is_target, '발생액'] - med) / mad
    return df

# --- NEW: ensure_zscore ---
def ensure_zscore(df: pd.DataFrame, account_codes: List[str]):
    """
    Recompute Z-Score for the given account subset and return (df, ok).
    ok=True only if Z-Score column exists and has at least one non-null value.
    """
    df2 = calculate_grouped_stats_and_zscore(df.copy(), target_accounts=[str(x) for x in account_codes] if account_codes else [])
    z = df2.get('Z-Score')
    ok = (z is not None) and (z.notna().any())
    return df2, bool(ok)




# === (ADD) v0.18: ModuleResult 러너 ===
from analysis.contracts import ModuleResult, EvidenceDetail
from config import PM_DEFAULT, RISK_WEIGHT_A, RISK_WEIGHT_F, RISK_WEIGHT_K, Z_SIGMOID_SCALE, Z_SIGMOID_DIVISOR
import plotly.express as px
import numpy as np
import pandas as pd


def _z_bins_025_sigma(series: pd.Series):
    """0.25σ 간격 bin (±3σ 테일 포함)."""
    # 경계에 +3.0 포함(+inf 테일) → 총 bin 수 = 24(코어) + 2(테일) = 26
    edges = [-np.inf] + [round(x, 2) for x in np.arange(-3.0, 3.0 + 0.25, 0.25)] + [np.inf]
    core_lefts = [x for x in np.arange(-3.0, 3.0, 0.25)]  # 24개
    labels_mid = [f"{a:.2f}~{a+0.25:.2f}σ" for a in core_lefts]
    labels = ["≤-3σ"] + labels_mid + ["≥3σ"]               # 26개
    cats = pd.cut(
        series.astype(float),
        bins=edges,
        labels=labels,
        right=False,
        include_lowest=True,
    )
    # 빈 구간도 0으로 채워 순서 유지
    counts = cats.value_counts(sort=False).reindex(labels, fill_value=0)
    out = pd.DataFrame({"구간": labels, "건수": counts.values})
    order = labels
    return out, order


def _sigmoid(x: float) -> float:
    import math
    return 1.0 / (1.0 + math.exp(-x))


def _risk_from(z_abs: float, amount: float, pm: float):
    """리스크 점수 구성요소 계산.
    - a: 시그모이드 정규화된 이탈 강도(|Z|/scale). scale은 설정값.
    - f: PM 대비 금액비율(0~1로 캡). PM이 0/음수면 0으로 강제.
    - k: Key Item 플래그(PM 초과시 1). PM이 0/음수면 0으로 강제.
    """
    # 우선순위: Z_SIGMOID_DIVISOR(신규 노브) > Z_SIGMOID_SCALE(구명). 기본 1.0
    div = None
    try:
        div = float(Z_SIGMOID_DIVISOR)
    except Exception:
        div = None
    if not div or div <= 0:
        try:
            div = float(Z_SIGMOID_SCALE)
        except Exception:
            div = 1.0
    if not div or div <= 0:
        div = 1.0
    a = _sigmoid(float(abs(z_abs)) / float(div))      # anomaly_score
    # PM 가드: pm<=0이면 f=0, k=0
    if pm is None or float(pm) <= 0:
        f = 0.0
        k = 0.0
    else:
        f = min(1.0, abs(float(amount)) / float(pm))  # PM ratio (capped at 1)
        k = 1.0 if abs(float(amount)) >= float(pm) else 0.0
    score = RISK_WEIGHT_A * a + RISK_WEIGHT_F * f + RISK_WEIGHT_K * k
    return a, f, k, score


def _assertions_for_row(z_val: float) -> List[str]:
    # 기본 규칙: A는 항상 포함. 음의 큰 이탈(C), 양의 큰 이탈(E)을 보강.
    out = {"A"}
    try:
        if float(z_val) <= -2.0:
            out.add("C")
        if float(z_val) >=  2.0:
            out.add("E")
    except Exception:
        pass
    return sorted(out)


def run_anomaly_module(lf, target_accounts=None, topn=20, pm_value: Optional[float] = None):
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    if not acct_col:
        return ModuleResult("anomaly", {}, {}, {}, [], ["계정코드 컬럼을 찾지 못했습니다."])

    # 대상 계정 서브셋
    if target_accounts:
        codes = [str(x) for x in target_accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    # Z-Score 계산
    df = calculate_grouped_stats_and_zscore(df, target_accounts=df[acct_col].astype(str).unique().tolist())
    if '회계일자' in df.columns:
        df['연월'] = df['회계일자'].dt.to_period('M').astype(str)

    # 이상치 플래그 (±3σ)
    df['is_outlier'] = df['Z-Score'].abs() >= 3

    # 이상치 후보 테이블 (절댓값 기준 상위)
    out_cols = [c for c in ['row_id','회계일자','연월','계정코드','계정명','거래처','적요','발생액','Z-Score'] if c in df.columns]
    cand = (df.assign(absz=df['Z-Score'].abs())
              .sort_values('absz', ascending=False)
              .drop(columns=['absz'])
              .head(int(topn)))
    table = cand[out_cols + (['is_outlier'] if 'is_outlier' in cand.columns and 'is_outlier' not in out_cols else [])] if out_cols else cand

    # === EvidenceDetail 생성 (KIT + |Z| 기준) ===
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    ev_rows: List[EvidenceDetail] = []
    # 증거 채집 대상: (1) PM 초과 or (2) |Z|>=2.5 or (3) 상위 topn
    mask_key = df['발생액'].abs() >= pm if '발생액' in df.columns else pd.Series(False, index=df.index)
    mask_z   = df['Z-Score'].abs() >= 2.5 if 'Z-Score' in df.columns else pd.Series(False, index=df.index)
    idx_sel  = set(df.index[mask_key | mask_z].tolist()) | set(table.index.tolist())
    sub = df.loc[sorted(idx_sel)].copy() if len(idx_sel)>0 else df.head(0).copy()
    for _, r in sub.iterrows():
        z  = float(r.get('Z-Score', 0.0)) if pd.notna(r.get('Z-Score', np.nan)) else 0.0
        za = abs(z)
        amt = float(r.get('발생액', 0.0))
        a, f, k, score = _risk_from(za, amt, pm)
        ev_rows.append(EvidenceDetail(
            row_id=str(r.get('row_id','')),
            reason=f"|Z|={za:.2f}",
            anomaly_score=float(a),
            financial_impact=abs(amt),
            risk_score=float(score),
            is_key_item=bool(abs(amt) >= pm),
            impacted_assertions=_assertions_for_row(z),
            links={
                "account_code": str(r.get('계정코드','')),
                "account_name": str(r.get('계정명','')),
                "period_tag": str(r.get('period_tag','')),
            }
        ))

    # step-σ bin 분포 막대
    figures = {}
    try:
        dist_df, order = _z_bins_025_sigma(df['Z-Score'])
        total_n = int(len(df))
        outlier_rate = float((df['Z-Score'].abs() >= 3).mean() * 100) if total_n else 0.0
        title = f"Z-Score 분포 (0.25σ bin, ±3σ 집계) — N={total_n:,}, outlier≈{outlier_rate:.1f}%"
        fig = px.bar(dist_df, x='구간', y='건수', title=title)
        fig.update_yaxes(separatethousands=True)
        fig.update_layout(bargap=0.10)
        figures = {"zscore_hist": fig}
    except Exception:
        pass

    summary = {
        "n_rows": int(len(df)),
        "n_candidates": int(len(table)),
        "accounts": sorted(df[acct_col].astype(str).unique().tolist()),
        "period_tag_coverage": dict(df.get('period_tag', pd.Series(dtype=str)).value_counts()) if 'period_tag' in df.columns else {}
    }
    # Evidence 미리보기 테이블(선택)
    try:
        import pandas as _pd
        ev_tbl = _pd.DataFrame([{
            "row_id": e.row_id,
            "계정코드": e.links.get("account_code",""),
            "계정명":   e.links.get("account_name",""),
            "risk_score": e.risk_score,
            "is_key_item": e.is_key_item,
            "impacted": ",".join(e.impacted_assertions),
            "reason": e.reason,
        } for e in ev_rows]).sort_values("risk_score", ascending=False).head(100)
    except Exception:
        ev_tbl = None

    return ModuleResult(
        name="anomaly",
        summary=summary,
        tables={"anomaly_top": table, **({"evidence_preview": ev_tbl} if ev_tbl is not None else {})},
        figures=figures,
        evidences=ev_rows,
        warnings=[]
    )


==============================
📄 FILE: analysis/assertion_risk.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from analysis.contracts import ModuleResult, EvidenceDetail, ASSERTIONS


HEATMAP_BS_RISK = "max"  # or "balance_only" / "weighted"


def _agg_bs_risk(rows: pd.DataFrame) -> float:
    """BS 셀 위험도 집계 규칙(기본 max).
    - EvidenceDetail.measure가 제공되는 경우에만 적용 가능.
    - 'weighted'는 balance 0.6, flow 0.4 가중.
    """
    if rows is None or rows.empty:
        return 0.0
    if HEATMAP_BS_RISK == "balance_only":
        r = rows.loc[rows.get("measure", pd.Series()).eq("balance"), "risk_score"]
        return float(r.max() if not r.empty else rows["risk_score"].max())
    if HEATMAP_BS_RISK == "weighted":
        w = rows.get("measure", pd.Series(index=rows.index)).map({"balance": 0.6, "flow": 0.4}).fillna(0.5)
        try:
            return float(np.average(rows["risk_score"].astype(float), weights=w))
        except Exception:
            return float(rows["risk_score"].max())
    return float(rows["risk_score"].max())


def build_matrix(modules: List[ModuleResult]):
    """
    모듈 EvidenceDetail → (계정 × 주장) 최대 risk_score 매트릭스 + 드릴다운 맵
    반환: (matrix_df[account_name x ASSERTIONS], evidence_map[(acct, asrt)] -> [row_id...])
    """
    bucket_rows: Dict[Tuple[str,str], List[Dict]] = {}
    emap: Dict[Tuple[str,str], List[str]] = {}
    accts: set[str] = set()

    for mod in modules:
        for ev in (mod.evidences or []):
            acct = ev.links.get("account_name") or ev.links.get("account_code") or "UNMAPPED"
            accts.add(acct)
            for a in (ev.impacted_assertions or []):
                key = (acct, a)
                bucket_rows.setdefault(key, []).append({
                    "risk_score": float(ev.risk_score),
                    "measure": getattr(ev, "measure", None)
                })
                emap.setdefault(key, []).append(str(ev.row_id))

    idx = sorted(accts)
    mat = pd.DataFrame(index=idx, columns=ASSERTIONS, data=0.0)
    for (acct, asrt), rows in bucket_rows.items():
        df = pd.DataFrame(rows)
        mat.loc[acct, asrt] = _agg_bs_risk(df) if not df.empty else 0.0
    return mat.fillna(0.0), emap





==============================
📄 FILE: analysis/contracts.py
==============================

from dataclasses import dataclass, field
import pandas as pd
from typing import Dict, List, Any, Optional, Literal
# --- New: Measure 타입 힌트("flow" 또는 "balance") ---
Measure = Literal["flow", "balance"]


@dataclass(frozen=True)
class LedgerFrame:
    df: pd.DataFrame
    meta: Dict[str, Any]  # 예: {"company": "...", "file_name": "...", "uploaded_at": ...}

# CEAVOP assertions
ASSERTIONS = ["C","E","A","V","O","P"]

@dataclass(frozen=True)
class EvidenceDetail:
    row_id: str
    reason: str                  # e.g., "|Z|=3.1 (CY group mean-based)"
    anomaly_score: float         # 0~1 normalized
    financial_impact: float      # KRW absolute amount
    risk_score: float            # integrated score
    is_key_item: bool            # PM exceed flag
    # --- NEW: measurement basis and sign rule ---
    measure: Measure = "flow"     # "flow"(월별 발생액, Δ잔액/순액) 또는 "balance"
    sign_rule: str = "assets/expenses↑=+, liabilities/equity↑=-"
    # --- NEW: 시계열 예측 메타 (옵셔널) ---
    model: Optional[str] = None           # 사용된 모델명 (예: EMA/MA/ARIMA/Prophet)
    window_policy: Optional[str] = None   # 예: "PY+CY"
    data_span: Optional[str] = None       # 예: "YYYY-MM ~ YYYY-MM"
    train_months: Optional[int] = None    # 학습 월 수
    horizon: Optional[int] = None         # 예측 수평(월)
    basis_note: Optional[str] = None      # 예: "BS는 잔액·발생액 병렬 계산"
    extra: Optional[Dict[str, Any]] = field(default_factory=dict)
    impacted_assertions: List[str] = field(default_factory=list)  # e.g., ["A","C"]
    links: Dict[str, Any] = field(default_factory=dict)           # e.g., {"account_code": "...", "account_name": "..."}

@dataclass(frozen=True)
class ModuleResult:
    name: str
    summary: Dict[str, Any]             # LLM 입력용 핵심 수치/지표
    tables: Dict[str, pd.DataFrame]
    figures: Dict[str, Any]             # plotly Figure
    evidences: List[EvidenceDetail]     # structured evidences
    warnings: List[str]


# 공개 API 명시(스키마 고정에 도움)
__all__ = [
    "LedgerFrame", "EvidenceDetail", "ModuleResult", "ASSERTIONS"
]




==============================
📄 FILE: analysis/correlation.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
import plotly.express as px
from typing import List, Dict, Any, Tuple, Optional
import re
try:
    from services.cycles_store import get_effective_cycles
except Exception:
    # 폴백: config의 STANDARD_ACCOUNTING_CYCLES 사용
    try:
        from config import STANDARD_ACCOUNTING_CYCLES as _STD_CYCLES
    except Exception:
        _STD_CYCLES = {}
    def get_effective_cycles():  # type: ignore
        return dict(_STD_CYCLES)
from analysis.contracts import LedgerFrame, ModuleResult
from utils.helpers import find_column_by_keyword


def _monthly_pivot(df: pd.DataFrame, acct_col: str) -> pd.DataFrame:
    """계정코드×연월 피벗(거래금액 합계). PL/BS 모두 월 흐름 기준."""
    if '회계일자' not in df.columns:
        raise ValueError("회계일자 필요")
    g = (df.assign(연월=df['회계일자'].dt.to_period('M').astype(str))
           .groupby([acct_col, '연월'])['거래금액'].sum()
           .unstack('연월', fill_value=0.0)
           .sort_index())
    return g
 
def _filter_accounts_for_corr(piv: pd.DataFrame, min_active_months: int = 6) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    - Drop accounts with zero variance across months (std == 0) OR
      with insufficient active months (abs(value)>0 in fewer than min_active_months months).
    - Return filtered pivot and an exclusions dataframe with reasons.
    """
    if piv.empty:
        return piv, pd.DataFrame(columns=['계정코드','사유','활동월수','표준편차'])
    std = piv.std(axis=1)
    active = (piv.abs() > 0).sum(axis=1)
    reason = []
    idx = piv.index.astype(str)
    keep = (std > 0) & (active >= int(min_active_months))
    for code, s, a, k in zip(idx, std, active, keep):
        if k:
            continue
        r = []
        if s == 0:
            r.append("변동없음(표준편차 0)")
        if a < int(min_active_months):
            r.append(f"활동 월 부족(<{int(min_active_months)})")
        reason.append((code, " & ".join(r) if r else "제외", int(a), float(s)))
    excluded = pd.DataFrame(reason, columns=['계정코드','사유','활동월수','표준편차'])
    return piv.loc[keep], excluded


def _infer_cycle(account_name: str) -> Optional[str]:
    """
    STANDARD_ACCOUNTING_CYCLES 기반의 간단한 키워드 매핑.
    가장 먼저 매칭되는 사이클을 반환(우선순위: dict 정의 순서).
    """
    name = str(account_name or "").lower()
    for cycle, keywords in get_effective_cycles().items():
        for kw in keywords:
            if kw and re.search(re.escape(str(kw).lower()), name):
                return cycle
    return None


def map_accounts_to_cycles(accounts: List[str]) -> Dict[str, Optional[str]]:
    """배치 매핑: 계정명 리스트 → {계정명: 사이클(or None)}"""
    return {acc: _infer_cycle(acc) for acc in accounts}


def run_correlation_module(lf: LedgerFrame, accounts: List[str] | None = None, corr_threshold: float = 0.7, min_active_months: int = 6) -> ModuleResult:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, '계정코드')
    if not acct_col:
        return ModuleResult("correlation", {}, {}, {}, [], ["계정코드 컬럼을 찾지 못했습니다."])

    # 대상 계정 필터
    if accounts:
        codes = [str(a) for a in accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    if df.empty:
        return ModuleResult("correlation", {}, {}, {}, [], ["선택된 데이터가 없습니다."])

    piv = _monthly_pivot(df, acct_col)
    piv_f, excluded = _filter_accounts_for_corr(piv, min_active_months=min_active_months)
    if piv_f.shape[0] < 2:
        warn = "상관을 계산할 계정이 2개 미만입니다."
        if not excluded.empty:
            warn += f" (제외된 계정 {len(excluded)}개: 변동없음/활동월 부족)"
        return ModuleResult("correlation", {}, {"excluded_accounts": excluded}, {}, [], [warn])

    corr = piv_f.T.corr(method='pearson')  # 계정×계정
    fig = px.imshow(corr, text_auto=False, title="계정 간 월별 상관 히트맵", labels=dict(x="계정코드", y="계정코드", color="상관계수"), aspect='auto')
    fig.update_coloraxes(cmin=-1, cmax=1)
    fig.update_xaxes(type='category')
    fig.update_yaxes(type='category')

    # 임계 상관쌍 테이블
    pairs: List[Tuple[str,str,float]] = []
    idx = corr.index.astype(str).tolist()
    for i in range(len(idx)):
        for j in range(i+1, len(idx)):
            r = float(corr.iloc[i, j])
            if abs(r) >= corr_threshold:
                pairs.append((idx[i], idx[j], r))
    pairs_df = pd.DataFrame(pairs, columns=['계정코드_A','계정코드_B','상관계수']).sort_values('상관계수', ascending=False)

    # 사이클 매핑 요약(계정명 필요하므로 별도 표에서는 계정명 매핑 필요 시 upstream에서 처리)
    summary = {
        "n_accounts": int(corr.shape[0]),
        "n_pairs_over_threshold": int(len(pairs_df)),
        "corr_threshold": float(corr_threshold)
    }
    return ModuleResult(
        name="correlation",
        summary=summary,
        tables={"strong_pairs": pairs_df, "corr_matrix": corr, "excluded_accounts": excluded},
        figures={"heatmap": fig},
        evidences=[],
        warnings=([f"제외된 계정 {len(excluded)}개(변동없음/활동월 부족)."] if not excluded.empty else [])
    )





==============================
📄 FILE: analysis/embedding.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
import time
from typing import Dict, List, Optional, Tuple
# --- KDMeans 기반 HDBSCAN 대체 사용 ---
from analysis.kdmeans_shim import HDBSCAN   # (주의) 실제로는 KMeans 기반
_HAS_HDBSCAN = True
# ---------------------------------------

from utils.helpers import find_column_by_keyword
from services.cache import get_or_embed_texts
from config import (
    EMB_MODEL_SMALL, EMB_MODEL_LARGE, EMB_USE_LARGE_DEFAULT,
    UMAP_APPLY_THRESHOLD, UMAP_N_COMPONENTS, UMAP_N_NEIGHBORS, UMAP_MIN_DIST,
    HDBSCAN_RESCUE_TAU,
)

# Embedding call defaults (can be overridden via pick_emb_model / params)
EMB_BATCH_SIZE = 128
EMB_TIMEOUT = 60
EMB_MAX_RETRY = 4
EMB_TRUNC_CHARS = 2000


def embed_texts_batched(
    texts: List[str],
    client,
    model: str,
    batch_size: int = EMB_BATCH_SIZE,
    timeout: int = EMB_TIMEOUT,
    max_retry: int = EMB_MAX_RETRY,
    trunc_chars: int = EMB_TRUNC_CHARS,
) -> Dict[str, List[float]]:
    """배치 임베딩 유틸. {원본문자열: 벡터} 반환."""
    if not texts:
        return {}
    san = []
    for t in texts:
        s = t if isinstance(t, str) else str(t)
        san.append(s[:trunc_chars] if trunc_chars and len(s) > trunc_chars else s)

    # Use persistent cache (SQLite per model)
    return get_or_embed_texts(
        san, client, model=model, batch_size=batch_size, timeout=timeout, max_retry=max_retry
    )


def _clean_text_series(s: pd.Series) -> pd.Series:
    """Lightweight denoising: collapse long numbers, squeeze spaces, trim."""
    s = s.astype(str)
    s = s.str.replace(r"\d{8,}", "#NUM", regex=True)
    s = s.str.replace(r"\s+", " ", regex=True).str.strip()
    return s

def ensure_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure df['embedding_text'] exists (desc+vendor) and is cleaned."""
    if 'embedding_text' not in df.columns:
        desc = df['적요'].fillna('').astype(str) if '적요' in df.columns else ''
        cp   = df['거래처'].fillna('').astype(str) if '거래처' in df.columns else ''
        df['embedding_text'] = desc + " (거래처: " + cp + ")"
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def _amount_bucket(a: float) -> str:
    a = float(abs(a))
    if a < 1_000_000:   return "1백만 미만"
    if a < 10_000_000:  return "1천만 미만"
    if a < 100_000_000: return "1억원 미만"
    if a < 500_000_000: return "5억원 미만"
    if a < 1_000_000_000:return "10억원 미만"
    if a < 5_000_000_000:return "50억원 미만"
    return "50억원 이상"


def ensure_rich_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """적요+거래처+월+금액구간+차/대 성격을 조합해 임베딩 텍스트 생성."""
    # 발생액/순액은 anomaly.compute_amount_columns를 쓰면 순환 import가 생김 → 최소 필드만 계산
    def _compute_amount_cols(_df: pd.DataFrame) -> pd.DataFrame:
        dcol = find_column_by_keyword(_df.columns, '차변')
        ccol = find_column_by_keyword(_df.columns, '대변')
        if not dcol or not ccol:
            _df['발생액'] = 0.0; _df['순액'] = 0.0
            return _df
        d = pd.to_numeric(_df[dcol], errors='coerce').fillna(0.0)
        c = pd.to_numeric(_df[ccol], errors='coerce').fillna(0.0)
        row_amt = np.where((d > 0) & (c == 0), d,
                  np.where((c > 0) & (d == 0), c,
                  np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
        _df['발생액'] = row_amt
        _df['순액']  = d - c
        return _df

    df = _compute_amount_cols(df.copy())
    month = df['회계일자'].dt.month.fillna(0).astype(int).astype(str).str.zfill(2) if '회계일자' in df.columns else "00"
    amtbin = df['발생액'].apply(_amount_bucket)
    sign   = np.where(df['순액'] >= 0, "차변성", "대변성")
    desc = df['적요'].fillna('').astype(str) if '적요' in df.columns else ''
    cp   = df['거래처'].fillna('').astype(str) if '거래처' in df.columns else ''
    df['embedding_text'] = desc + " | 거래처:" + cp + " | 월:" + month + " | 금액구간:" + amtbin + " | 성격:" + sign
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def perform_embedding_only(df: pd.DataFrame, client, text_col: str = 'embedding_text', *, use_large: bool|None=None) -> pd.DataFrame:
    """df[text_col]을 배치 임베딩해서 df['vector'] 추가"""
    if df.empty: return df
    if text_col not in df.columns:
        raise ValueError(f"임베딩 텍스트 컬럼 '{text_col}'이 없습니다.")
    uniq = df[text_col].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(uniq, client, model=model)
    df = df.copy()
    df['vector'] = df[text_col].astype(str).map(mapping)
    # 누락 보강 시도
    if df['vector'].isna().any():
        miss = df.loc[df['vector'].isna(), text_col].astype(str).unique().tolist()
        if miss:
            fb = embed_texts_batched(miss, client, model=model)
            df.loc[df['vector'].isna(), 'vector'] = df.loc[df['vector'].isna(), text_col].astype(str).map(fb)
    return df


def _l2_normalize(X: np.ndarray) -> np.ndarray:
    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)

def _adaptive_hdbscan(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    n = int(X.shape[0])
    model = HDBSCAN(
        n_clusters=None,           # 자동 k 선택(실루엣 기반)
        min_cluster_size=max(8, int(np.sqrt(max(2, n)))),  # 너무 많은 군집 방지
        max_k=None,                # 필요시 상한 지정 가능
        k_search="silhouette",     # 휴리스틱 대신 실루엣 기반
        sample_size=2000,
        random_state=42,
        n_init="auto",
    )
    model.fit(X)
    labels = model.labels_.astype(int)
    try:
        probs = model.probabilities_.astype(float)
    except Exception:
        probs = np.ones(shape=(n,), dtype=float)
    return labels, probs

def _optional_umap(X: np.ndarray, enabled: Optional[bool] = None) -> Tuple[np.ndarray, bool]:
    """Dimensionality reduction control.
    - enabled=True: always try UMAP; on failure return (X, False)
    - enabled=False: skip → (X, False)
    - enabled=None: apply only if dataset size >= UMAP_APPLY_THRESHOLD
    Returns (X_or_reduced, used_flag).
    """
    if enabled is False:
        return X, False
    force = enabled is True
    try:
        thr = int(UMAP_APPLY_THRESHOLD) if UMAP_APPLY_THRESHOLD else None
    except Exception:
        thr = None
    if force or (thr and X.shape[0] >= thr):
        try:
            import umap
            reducer = umap.UMAP(
                n_components=int(UMAP_N_COMPONENTS),
                n_neighbors=int(UMAP_N_NEIGHBORS),
                min_dist=float(UMAP_MIN_DIST),
                random_state=42,
                metric="euclidean",
            )
            return reducer.fit_transform(X), True
        except Exception:
            return X, False
    return X, False

def _rescue_noise(df: pd.DataFrame, tau: float = HDBSCAN_RESCUE_TAU) -> pd.DataFrame:
    # KDMeans는 노이즈(-1) 라벨이 없으므로 구조적 리스큐 불필요
    return df

def pick_emb_model(use_large: bool|None=None) -> str:
    """Select embedding model (small/large)."""
    flag = EMB_USE_LARGE_DEFAULT if use_large is None else bool(use_large)
    return EMB_MODEL_LARGE if flag else EMB_MODEL_SMALL


def postprocess_cluster_names(df: pd.DataFrame) -> pd.DataFrame:
    """(간소화) LLM이 준 cluster_name을 그대로 유지한다. 태그/접미사 미부여."""
    return df


def _llm_cluster_name(client, model: str, samples_desc: list[str], samples_vendor: list[str]) -> str | None:
    import textwrap
    prompt = textwrap.dedent(f"""
    너는 회계 감사 보조 AI다. 아래 거래 샘플을 보고 이 그룹의 성격을 가장 잘 드러내는
    한국어 **클러스터 이름 1개만**, 10자 내외로 제시해라.
    숫자/기호/따옴표/접두사 금지. 예: 직원 급여, 통신비, 임원 복지.

    [거래 적요 샘플]
    - {'\n- '.join(samples_desc) if samples_desc else '(샘플 부족)'}

    [주요 거래처 샘플]
    - {'\n- '.join(samples_vendor) if samples_vendor else '(샘플 부족)'}

    정답:
    """).strip()
    try:
        resp = client.chat.completions.create(
            model=model, messages=[{"role":"user","content":prompt}], temperature=0
        )
        cand = (resp.choices[0].message.content or "").strip().splitlines()[0].strip(" \"'[]()")
        if not cand:
            return None
        bad = {"클러스터","unknown","이름없음","미정","기타"}
        if cand.lower() in bad or cand.startswith("클러스터"):
            return None
        return cand[:20]
    except Exception:
        return None

def perform_embedding_and_clustering(
    df: pd.DataFrame,
    client,
    name_with_llm: bool = True,
    must_name_with_llm: bool = False,
    llm_model: str = "gpt-4o-mini",
    *,
    use_large: bool|None = None,
    rescue_tau: float = HDBSCAN_RESCUE_TAU,
    umap_enabled: bool|None = None,   # None => use config threshold
):
    """
    Embedding + (optional UMAP) + L2-normalized Euclidean HDBSCAN + noise rescue + (LLM naming).
    Returns: (df, ok)
    ok=False if: no vectors, or LLM naming required but missing/failed.
    """
    df = ensure_embedding_text(df.copy())
    uniq = df['embedding_text'].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(uniq, client, model=model)
    df['vector'] = df['embedding_text'].astype(str).map(mapping)
    # keep only valid vectors
    mask = df['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)
    df = df.loc[mask].copy()
    if df.empty:
        return None, False

    X = np.vstack(df['vector'].values).astype(float)
    # Optional UMAP if dataset large (threshold controlled by config)
    X, umap_used = _optional_umap(X, enabled=umap_enabled)
    # L2 normalize and cluster with Euclidean (≈ cosine)
    Xn = _l2_normalize(X)
    labels, probs = _adaptive_hdbscan(Xn)
    df['cluster_id'] = labels
    df['cluster_prob'] = probs
    # telemetry attrs
    try:
        df.attrs['embedding_model'] = model
        df.attrs['umap_used'] = bool(umap_used)
        df.attrs['rescue_tau'] = float(rescue_tau) if rescue_tau is not None else None
    except Exception:
        pass

    # --- LLM cluster naming (with graceful fallback names) ---
    labels_uniq = sorted(pd.Series(labels).unique())
    names = {}
    if name_with_llm and hasattr(client, "chat"):
        for cid in labels_uniq:
            if cid == -1:
                names[cid] = "클러스터 노이즈(-1)"
                continue
            sub = df[df['cluster_id'] == cid]
            descs = sub['적요'].dropna().astype(str).unique().tolist()[:5] if '적요' in sub.columns else []
            vendors = sub['거래처'].dropna().astype(str).unique().tolist()[:5] if '거래처' in sub.columns else []
            cand = _llm_cluster_name(client, llm_model, descs, vendors)
            # fallback rule-based name if LLM failed
            if not cand or cand == "이름 생성 실패":
                # heuristic: frequent vendor or keyword + amount tag
                amt_tag = "규모 중간"
                try:
                    abs_amt = sub.get('발생액', pd.Series(dtype=float)).abs().median()
                    if float(abs_amt) >= 1e8: amt_tag = "1억원 이상"
                    elif float(abs_amt) >= 1e7: amt_tag = "1천만~1억"
                except Exception:
                    pass
                top_vendor = sub.get('거래처', pd.Series(dtype=str)).value_counts().index.tolist()
                vname = top_vendor[0] if top_vendor else "일반"
                cand = f"{vname} 중심({amt_tag})"
            names[cid] = cand
    else:
        for cid in labels_uniq:
            names[cid] = "클러스터 노이즈(-1)" if cid == -1 else "이름 생성 실패"

    df['cluster_name'] = df['cluster_id'].map(names)
    df = postprocess_cluster_names(df)

    # --- Noise rescue: reassign -1 to nearest centroid if cosine >= tau ---
    if rescue_tau and float(rescue_tau) > 0:
        df = _rescue_noise(df, tau=float(rescue_tau))

    # gate: if must_name_with_llm, all non-noise clusters must have valid names
    if must_name_with_llm:
        non_noise = df[df['cluster_id'] != -1]
        has_any = not non_noise.empty
        invalid = non_noise['cluster_name'].isna() | non_noise['cluster_name'].astype(str).str.contains("^이름 생성 실패|^클러스터\\s", regex=True)
        if (not has_any) or bool(invalid.any()):
            return df, False

    # default reporting group equals the (validated) cluster_name; may be unified later
    df['cluster_group'] = df['cluster_name']
    return df, True



# --- NEW: LLM synonym grouping for cluster names ---
def _cosine_sim_matrix(vecs: list[list[float]]):
    import numpy as np
    V = np.asarray(vecs, dtype=float)
    if V.ndim != 2 or V.shape[0] == 0:
        return np.zeros((0, 0))
    Vn = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    return Vn @ Vn.T


def unify_cluster_names_with_llm(
    df: pd.DataFrame,
    client,
    sim_threshold: float = 0.90,
    emb_model: str = EMB_MODEL_SMALL,
    llm_model: str = "gpt-4o-mini",
):
    """
    Collapse clusters with effectively identical names.
    Strategy:
      1) Embed unique names (excluding noise), preselect candidate pairs via cosine >= sim_threshold.
      2) Ask LLM YES/NO if two names are synonyms for accounting transaction categories.
      3) Union-Find merge; choose canonical = most frequent name in df (fallback shortest).
    Returns: (df_with_cluster_group, mapping{name->canonical})
    """
    import numpy as np
    import itertools
    base = df.copy()
    if 'cluster_name' not in base.columns:
        base['cluster_group'] = base.get('cluster_name', None)
        return base, {}
    names = (
        base.loc[base['cluster_id'] != -1, 'cluster_name']
        .dropna().astype(str).unique().tolist()
    )
    if not names:
        base['cluster_group'] = base['cluster_name']
        return base, {}

    # Embedding prefilter
    name2vec = embed_texts_batched(names, client, model=emb_model)
    ordered = [n for n in names if n in name2vec]
    vecs = [name2vec[n] for n in ordered]
    S = _cosine_sim_matrix(vecs)

    # Union-Find
    parent = {n: n for n in ordered}
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    # LLM pair confirmation
    for i, j in itertools.combinations(range(len(ordered)), 2):
        if S[i, j] < float(sim_threshold):
            continue
        a, b = ordered[i], ordered[j]
        try:
            q = (
                "너는 회계 감사 보조 AI다. 다음 두 표현이 '회계 거래 카테고리' 이름으로서 "
                "사실상 같은 의미인지 YES/NO로만 답하라.\n"
                f"A: {a}\nB: {b}\n정답:"
            )
            resp = client.chat.completions.create(
                model=llm_model,
                messages=[{"role": "user", "content": q}],
                temperature=0
            )
            ans = (resp.choices[0].message.content or "").strip().split()[0].upper()
            if ans.startswith("Y"):  # YES
                union(a, b)
        except Exception:
            # On failure, skip merging
            pass

    # Build groups
    groups = {}
    for n in ordered:
        r = find(n)
        groups.setdefault(r, []).append(n)

    # Choose canonical per group
    freq = base['cluster_name'].value_counts().to_dict()
    mapping = {}
    for root, members in groups.items():
        cand = sorted(members, key=lambda x: (-freq.get(x, 0), len(x)))[0]
        for m in members:
            mapping[m] = cand

    base['cluster_group'] = base['cluster_name'].map(lambda x: mapping.get(x, x))
    return base, mapping


# --- NEW: Utilities for PY→CY mapping and label unification ---
def _cosine(a, b):
    import numpy as np
    if a is None or b is None:
        return np.nan
    a = np.asarray(a)
    b = np.asarray(b)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    return float(np.dot(a, b) / denom) if denom else np.nan


def map_previous_to_current_clusters(df_cur: pd.DataFrame, df_prev: pd.DataFrame) -> pd.DataFrame:
    """
    전기 전표를 당기 클러스터 센트로이드에 최근접 배정하여 (mapped_cluster_id/name, mapped_sim) 부여.
    - 노이즈(-1) 센트로이드는 제외
    - 반환: prev_df(with mapped_cluster_id, mapped_cluster_name, mapped_sim)
    """
    import numpy as np
    import pandas as pd
    need_cols = ['cluster_id', 'cluster_name', 'vector']
    if any(c not in df_cur.columns for c in need_cols) or 'vector' not in df_prev.columns:
        return df_prev.copy()
    cur = df_cur[df_cur['cluster_id'] != -1].copy()
    if cur.empty:
        return df_prev.copy()
    # 센트로이드 계산
    cents = (
        cur.groupby(['cluster_id', 'cluster_name'])['vector']
           .apply(lambda s: np.mean(np.vstack(list(s)), axis=0))
           .reset_index()
    )
    prev = df_prev.copy()

    def _pick(row: pd.Series) -> pd.Series:
        v = row.get('vector', None)
        if v is None:
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        sims = cents['vector'].apply(lambda c: _cosine(v, c))
        if len(sims) == 0 or sims.isna().all():
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        idx = int(sims.idxmax())
        return pd.Series({
            'mapped_cluster_id': int(cents.loc[idx, 'cluster_id']),
            'mapped_cluster_name': cents.loc[idx, 'cluster_name'],
            'mapped_sim': float(sims.max()) if not np.isnan(sims.max()) else np.nan,
        })

    prev[['mapped_cluster_id', 'mapped_cluster_name', 'mapped_sim']] = prev.apply(_pick, axis=1)
    return prev


def unify_cluster_labels_llm(names, client) -> dict:
    """
    유사 의미의 한글 클러스터명을 LLM으로 묶어 canonical name 매핑을 리턴.
    입력: 예) ["경비 관리","경비 처리","관리 경비", ...]
    출력: {"경비 관리":"경비 관리","경비 처리":"경비 관리", ...}
    """
    uniq = sorted([n for n in set([str(x) for x in names]) if n and n.lower() != 'nan'])
    if not uniq:
        return {}
    prompt = (
        "다음 한국어 클러스터 이름들을 의미가 같은 것끼리 묶어 하나의 대표명으로 통합하세요.\n"
        "규칙: 1) 가장 일반적/짧은 표현을 대표명으로, 2) JSON 객체로만 응답, 3) 형식: {원래명:대표명, ...}.\n"
        f"목록: {uniq}"
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        import json
        txt = (resp.choices[0].message.content or "").strip()
        mapping = json.loads(txt)
        if isinstance(mapping, dict):
            return mapping
    except Exception:
        pass
    return {n: n for n in uniq}


# --- NEW: Yearly clustering helpers and alignment ---
def cluster_year(df: pd.DataFrame, client) -> pd.DataFrame:
    """
    당기/전기 등 입력 df에 대해 풍부 임베딩 텍스트를 보장하고 HDBSCAN+LLM 네이밍을 실행.
    반환: ['row_id','cluster_id','cluster_name','cluster_prob','vector']가 포함된 DataFrame(부분집합 가능).
    입력이 비어있으면 빈 DataFrame 반환.
    """
    if df is None or df.empty:
        return pd.DataFrame()
    from .embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
    df_in = ensure_rich_embedding_text(df.copy())
    df_out, ok = perform_embedding_and_clustering(df_in, client, name_with_llm=True, must_name_with_llm=False)
    if not ok or df_out is None:
        return pd.DataFrame()
    keep = [c for c in ['row_id','cluster_id','cluster_name','cluster_prob','vector'] if c in df_out.columns]
    return df_out[keep].copy()


def compute_centroids(df: pd.DataFrame) -> pd.DataFrame:
    """
    ['cluster_id','vector']를 갖는 df에서 클러스터별 센트로이드 계산(-1 제외).
    'cluster_name'이 있으면 함께 유지.
    반환: columns=['cluster_id','cluster_name','vector']
    """
    import numpy as np
    import pandas as pd
    need = ['cluster_id','vector']
    if df is None or df.empty or any(c not in df.columns for c in need):
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    base = df[df['cluster_id'] != -1].copy()
    if base.empty:
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    def _mean_stack(s):
        try:
            return np.mean(np.vstack(list(s)), axis=0)
        except Exception:
            return None
    cents = base.groupby('cluster_id')['vector'].apply(_mean_stack).reset_index()
    if 'cluster_name' in base.columns:
        name_map = base.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name']
        cents['cluster_name'] = cents['cluster_id'].map(name_map)
    else:
        cents['cluster_name'] = None
    # re-order columns
    cents = cents[['cluster_id','cluster_name','vector']]
    # drop rows with invalid vectors
    cents = cents[cents['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)]
    return cents.reset_index(drop=True)


def align_yearly_clusters(df_cy: pd.DataFrame, df_py: pd.DataFrame, sim_threshold: float = 0.70) -> dict:
    """
    CY/PY 센트로이드 코사인 유사도 행렬 기반 Hungarian 매칭(cost=1-sim).
    반환: {py_cluster_id: (cy_cluster_id, sim)} (임계치 미만은 값 None)
    """
    import numpy as np
    py_c = compute_centroids(df_py)
    cy_c = compute_centroids(df_cy)
    if py_c.empty or cy_c.empty:
        return {}
    # build similarity matrix
    py_vecs = list(py_c['vector'].values)
    cy_vecs = list(cy_c['vector'].values)
    S_py = _cosine_sim_matrix(py_vecs)
    S_cy = _cosine_sim_matrix(cy_vecs)
    # We need PY x CY sims; compute directly
    # Efficient: normalize and dot
    import numpy as np
    def _norm(V):
        V = np.asarray([np.asarray(v, dtype=float) for v in V], dtype=float)
        return V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    Npy = _norm(py_vecs)
    Ncy = _norm(cy_vecs)
    sim = Npy @ Ncy.T  # shape: [n_py, n_cy]
    # Hungarian matching on cost = 1 - sim
    try:
        from scipy.optimize import linear_sum_assignment
        cost = 1.0 - sim
        row_ind, col_ind = linear_sum_assignment(cost)
    except Exception:
        # Fallback: greedy matching by highest sim without replacement
        pairs = []
        used_py = set(); used_cy = set()
        # flatten and sort
        flat = [
            (i, j, float(sim[i, j]))
            for i in range(sim.shape[0])
            for j in range(sim.shape[1])
        ]
        flat.sort(key=lambda x: x[2], reverse=True)
        for i, j, s in flat:
            if i in used_py or j in used_cy:
                continue
            pairs.append((i, j))
            used_py.add(i); used_cy.add(j)
        row_ind = np.array([p[0] for p in pairs], dtype=int)
        col_ind = np.array([p[1] for p in pairs], dtype=int)
    mapping: dict[int, tuple[int, float] | None] = {}
    for k in range(len(row_ind)):
        i = int(row_ind[k]); j = int(col_ind[k])
        s = float(sim[i, j])
        py_id = int(py_c.loc[i, 'cluster_id'])
        cy_id = int(cy_c.loc[j, 'cluster_id'])
        if s >= float(sim_threshold):
            mapping[py_id] = (cy_id, s)
        else:
            mapping[py_id] = None
    # Ensure all PY clusters are present in mapping
    for py_id in py_c['cluster_id'].tolist():
        if py_id not in mapping:
            mapping[py_id] = None
    return mapping




==============================
📄 FILE: analysis/evidence.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import Optional


def build_knn_index(prev_df: pd.DataFrame):
    """prev_df['vector']로 KNN 생성."""
    if 'vector' not in prev_df.columns or prev_df['vector'].isna().any():
        raise ValueError("build_knn_index: prev_df에 'vector' 필요")
    from sklearn.neighbors import NearestNeighbors
    X = np.vstack(prev_df['vector'].values)
    knn = NearestNeighbors(metric='cosine', n_neighbors=min(10, len(X))).fit(X)
    return knn, X


def cluster_centroid_vector(cluster_df: pd.DataFrame):
    if 'vector' not in cluster_df.columns or cluster_df.empty:
        return None
    return np.mean(np.vstack(cluster_df['vector'].values), axis=0)


def retrieve_similar_from_previous(prev_df, prev_knn, prev_X, query_vec, topk=5, dedup_by_vendor=True, min_sim=0.7):
    if query_vec is None or prev_X is None or len(prev_X) == 0:
        return pd.DataFrame()
    dist, idx = prev_knn.kneighbors([query_vec], n_neighbors=min(max(10, topk*3), len(prev_X)))
    cands = prev_df.iloc[idx[0]].copy()
    cands['similarity'] = (1 - dist[0])
    cands = cands[cands['similarity'] >= min_sim]
    if dedup_by_vendor and '거래처' in cands.columns:
        cands = cands.sort_values('similarity', ascending=False).drop_duplicates('거래처', keep='first')
    cands = cands.sort_values('similarity', ascending=False).head(topk)
    cols = ['회계일자','계정코드','거래처','적요','발생액','similarity']
    for c in cols:
        if c not in cands.columns: cands[c] = np.nan
    return cands[cols]


def build_cluster_evidence_block(current_df: pd.DataFrame, previous_df: pd.DataFrame,
                                 topk: int = 3, restrict_same_months: bool = True, min_sim: float = 0.7,
                                 dedup_by_vendor: bool = True) -> str:
    if any(col not in current_df.columns for col in ['cluster_id','vector']):
        return "\n\n## 근거 인용(전기 유사 거래)\n- 현재 데이터에 클러스터/벡터가 없어 근거를 생성할 수 없습니다."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## 근거 인용(전기 유사 거래)\n- 전기 데이터 임베딩이 없어 근거를 생성할 수 없습니다."
    def _ok_vec(v):
        return v is not None and isinstance(v, (list, tuple, np.ndarray)) and len(v) > 0
    lines = ["\n\n## 근거 인용(전기 유사 거래)"]
    for cid in sorted(current_df['cluster_id'].unique()):
        cur_c = current_df[current_df['cluster_id'] == cid]
        if cur_c.empty: continue
        cname = cur_c['cluster_name'].iloc[0] if 'cluster_name' in cur_c.columns else str(cid)
        lines.append(f"[클러스터 #{cid} | {cname}]")
        prev_subset = previous_df.copy()
        if restrict_same_months and '회계일자' in cur_c.columns and cur_c['회계일자'].notna().any():
            months = set(cur_c['회계일자'].dt.month.dropna().unique().tolist())
            filtered = previous_df[previous_df['회계일자'].dt.month.isin(months)]
            prev_subset = filtered if not filtered.empty else previous_df
        if 'vector' in prev_subset.columns:
            prev_subset = prev_subset[prev_subset['vector'].apply(_ok_vec)].copy()
        if prev_subset.empty:
            lines.append("    └ 전기 유사 벡터 없음"); continue
        try:
            knn, X = build_knn_index(prev_subset)
        except Exception as e:
            lines.append(f"    └ 인덱스 생성 실패: {e}"); continue
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_subset, knn, X, qv, topk=topk, dedup_by_vendor=dedup_by_vendor, min_sim=min_sim)
        if ev.empty:
            lines.append("    └ 유사 전표: 없음")
        else:
            def _fmt_date(x): 
                try: return x.strftime('%Y-%m-%d') if pd.notna(x) else ""
                except: return ""
            def _fmt_money(x):
                try: return f"{int(x):,}원"
                except: return str(x)
            def _fmt_sim(s):
                try: return f"{float(s):.2f}"
                except: return "N/A"
            for rank, (_, r) in enumerate(ev.sort_values('similarity', ascending=False).iterrows(), 1):
                lines.append(f"    {rank}) {_fmt_date(r['회계일자'])} | {str(r['거래처'])} | {_fmt_money(r['발생액'])} | sim {_fmt_sim(r['similarity'])}")
    return "\n".join(lines)



def build_transaction_evidence_block(current_df, previous_df, topn=10, per_tx_topk=3, min_sim=0.8):
    import numpy as np, pandas as pd
    def _ok_vec(v): return isinstance(v, (list, tuple, np.ndarray)) and len(v)>0
    if current_df.empty or 'vector' not in current_df.columns: 
        return "\n\n## 거래별 근거\n- 현재 데이터에 벡터가 없어 근거를 생성할 수 없습니다."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## 거래별 근거\n- 전기 데이터 임베딩이 없어 근거를 생성할 수 없습니다."

    cur = current_df.copy()
    if 'Z-Score' in cur.columns and cur['Z-Score'].notna().any():
        order_idx = cur['Z-Score'].abs().sort_values(ascending=False).index
    else:
        # Z-Score 미시행 시 발생액 상위
        amt = cur.get('발생액', pd.Series(dtype=float))
        order_idx = amt.sort_values(ascending=False).index
    cur = cur.reindex(order_idx).head(int(topn))

    # 전기 벡터 유효성 필터
    prev = previous_df.copy()
    prev = prev[prev['vector'].apply(_ok_vec)]
    if prev.empty:
        return "\n\n## 거래별 근거\n- 전기 데이터 벡터가 유효하지 않습니다."

    from .evidence import build_knn_index, retrieve_similar_from_previous
    try:
        knn, X = build_knn_index(prev)
    except Exception:
        return "\n\n## 거래별 근거\n- 전기 KNN 인덱스 생성 실패."

    lines = [f"\n\n## 거래별 근거 (상위 {len(cur)}건)"]
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        if qv is None: continue
        # 동월 우선
        psub = prev
        if '회계일자' in r and pd.notna(r['회계일자']):
            m = r['회계일자'].month
            cand = prev[prev['회계일자'].dt.month == m]
            if not cand.empty: psub = cand
            knn, X = build_knn_index(psub)
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=int(per_tx_topk), dedup_by_vendor=True, min_sim=float(min_sim))
        dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
        amt = r.get('발생액', 0.0); z = r.get('Z-Score', np.nan)
        ztxt = f" | Z={z:+.2f}" if not pd.isna(z) else ""
        lines.append(f"[{i}] {dt} | 거래처:{r.get('거래처','')} | 금액:{int(amt):,}원{ztxt}")
        if ev.empty:
            lines.append("    └ 유사 전표: 없음")
        else:
            lines.append(f"    └ 전기 유사 Top {len(ev)}")
            for _, rr in ev.iterrows():
                d2 = rr['회계일자'].strftime('%Y-%m-%d') if pd.notna(rr['회계일자']) else ''
                lines.append(f"       • {d2} | {rr['거래처']} | {int(rr['발생액']):,}원 | sim {rr['similarity']:.2f}")
    return "\n".join(lines)

# --- NEW: Structured evidence blocks for the redesigned context ---
def build_current_cluster_block(current_df: pd.DataFrame) -> str:
    """
    ## 당기 클러스터 및 금액
    - One bullet per cluster_group: total absolute amount, count, and ONE example voucher.
    """
    import pandas as pd
    if current_df.empty or 'cluster_group' not in current_df.columns:
        return "\n\n## 당기 클러스터 및 금액\n- (클러스터 결과 없음)"
    lines = ["\n\n## 당기 클러스터 및 금액"]
    grp = current_df.copy()
    grp['abs_amt'] = grp.get('발생액', pd.Series(dtype=float)).abs()
    for name, cdf in grp.groupby('cluster_group', dropna=False):
        tot = cdf['abs_amt'].sum()
        cnt = len(cdf)
        ex = cdf.sort_values('abs_amt', ascending=False).head(1).iloc[0]
        dt = ex['회계일자'].strftime('%Y-%m-%d') if '회계일자' in ex and pd.notna(ex['회계일자']) else ''
        vend = ex.get('거래처', '')
        amt = int(ex.get('발생액', 0.0))
        lines.append(f"- [{name}] 건수 {cnt}건, 규모(절대값) {tot:,.0f}원")
        lines.append(f"  · 예시: {dt} | {vend} | {amt:,.0f}원")
    return "\n".join(lines)

def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float = 0.70) -> str:
    """
    ## 전기 클러스터 및 금액
    Project PY vouchers onto CY cluster centroids; report total abs amount, avg similarity, and ONE example.
    """
    import pandas as pd
    import numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous, cluster_centroid_vector
    if current_df.empty or previous_df.empty or 'vector' not in previous_df.columns or 'cluster_group' not in current_df.columns:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 데이터/벡터/클러스터 정보 없음)"
    lines = ["\n\n## 전기 클러스터 및 금액"]
    prev_ok = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)]
    if prev_ok.empty:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 유효 벡터 없음)"
    try:
        knn, X = build_knn_index(prev_ok)
    except Exception:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 KNN 인덱스 생성 실패)"
    for name, cur_c in current_df.groupby('cluster_group', dropna=False):
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_ok, knn, X, qv, topk=10, dedup_by_vendor=True, min_sim=float(min_sim))
        if ev.empty:
            lines.append(f"- [{name}] 유사 전표 없음")
            continue
        ev['abs_amt'] = ev.get('발생액', pd.Series(dtype=float)).abs()
        tot = ev['abs_amt'].sum()
        avg_sim = ev['similarity'].mean()
        ex = ev.sort_values('similarity', ascending=False).head(1).iloc[0]
        dt = ex['회계일자'].strftime('%Y-%m-%d') if pd.notna(ex['회계일자']) else ''
        lines.append(f"- [{name}] 규모(절대값) {tot:,.0f}원, 평균 유사도 {avg_sim:.2f}")
        lines.append(f"  · 예시: {dt} | {ex['거래처']} | {int(ex['발생액']):,}원 | sim {ex['similarity']:.2f}")
    return "\n".join(lines)

def build_zscore_top5_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## Z-score 기준 TOP5 전표
    List top |Z| vouchers with one counterpart from PY (same-month preferred), no row-id.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous
    if current_df.empty or 'Z-Score' not in current_df.columns:
        return "\n\n## Z-score 기준 TOP5 전표\n- (Z-Score 미계산)"
    cur = current_df.copy()
    order = cur['Z-Score'].abs().sort_values(ascending=False).index
    cur = cur.reindex(order).head(int(topn))
    lines = [f"\n\n## Z-score 기준 TOP5 전표"]
    if previous_df.empty or 'vector' not in previous_df.columns:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    # KNN on PY (same-month preferred)
    prev = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if prev.empty:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    knn_all, X_all = build_knn_index(prev)
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
        head = f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}"
        if qv is None:
            lines.append(head)
            continue
        psub = prev
        if '회계일자' in r and pd.notna(r['회계일자']):
            m = r['회계일자'].month
            cand = prev[prev['회계일자'].dt.month == m]
            if not cand.empty:
                psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  · 전기 대응: 없음")
        else:
            rr = ev.iloc[0]
            d2 = rr['회계일자'].strftime('%Y-%m-%d') if pd.notna(rr['회계일자']) else ''
            lines.append(f"  · 전기 대응: {d2} | {rr['거래처']} | {int(rr['발생액']):,}원 | sim {rr['similarity']:.2f}")
    return "\n".join(lines)


# --- NEW: 전기 기준 TOP5 블록 ---
def build_zscore_top5_block_for_py(previous_df: pd.DataFrame, current_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## 전기 Z-score 기준 TOP5 전표
    전기 데이터를 기준으로 |Z| 상위 5건을 나열하고, 가능한 경우 당기 대응 1건을 함께 표시.
    previous_df에 Z-Score가 있어야 한다.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous

    if previous_df.empty or 'Z-Score' not in previous_df.columns:
        return "\n\n## 전기 Z-score 기준 TOP5 전표\n- (전기 Z-Score 미계산)"

    prev = previous_df.copy()
    order = prev['Z-Score'].abs().sort_values(ascending=False).index
    prev = prev.reindex(order).head(int(topn))

    lines = [f"\n\n## 전기 Z-score 기준 TOP5 전표"]

    if current_df.empty or 'vector' not in current_df.columns:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    cur_ok = current_df[current_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if cur_ok.empty:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    knn_all, X_all = build_knn_index(cur_ok)

    for i, (_, r) in enumerate(prev.iterrows(), 1):
        dt = r['회계일자'].strftime('%Y-%m-%d') if '회계일자' in r and pd.notna(r['회계일자']) else ''
        head = f"- [{i}] {dt} | {r.get('거래처','')} | {int(r.get('발생액',0)):,.0f}원 | Z={float(r.get('Z-Score',0)):+.2f}"

        qv = r.get('vector', None)
        if qv is None:
            lines.append(head); continue

        psub = cur_ok
        if '회계일자' in r and pd.notna(r['회계일자']):
            m = r['회계일자'].month
            cand = cur_ok[cur_ok['회계일자'].dt.month == m]
            if not cand.empty: psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all

        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  · 당기 대응: 없음")
        else:
            rr = ev.iloc[0]
            d2 = rr['회계일자'].strftime('%Y-%m-%d') if pd.notna(rr['회계일자']) else ''
            lines.append(f"  · 당기 대응: {d2} | {rr['거래처']} | {int(rr['발생액']):,}원 | sim {rr['similarity']:.2f}")

    return "\n".join(lines)


==============================
📄 FILE: analysis/integrity.py
==============================

import pandas as pd


def analyze_reconciliation(ledger_df: pd.DataFrame, master_df: pd.DataFrame):
    """Master와 Ledger 데이터 간의 정합성을 검증합니다.

    반환값:
    - overall_status: "Pass" | "Warning" | "Fail"
    - 결과 DataFrame
    """
    results, overall_status = [], "Pass"
    cy_ledger_df = ledger_df[ledger_df['연도'] == ledger_df['연도'].max()]
    for _, master_row in master_df.iterrows():
        account_code = master_row['계정코드']
        bspl = master_row.get('BS/PL', 'PL').upper()
        bop = master_row.get('전기말잔액', 0)
        eop_master = master_row.get('당기말잔액', 0)

        net_change_gl = cy_ledger_df[cy_ledger_df['계정코드'] == account_code]['거래금액'].sum()
        eop_gl = (bop + net_change_gl) if bspl == 'BS' else net_change_gl

        difference = eop_master - eop_gl
        diff_pct = abs(difference) / max(abs(eop_master), 1)
        status = "Fail" if diff_pct > 0.001 else "Warning" if abs(difference) > 0 else "Pass"
        if status == "Fail":
            overall_status = "Fail"
        elif status == "Warning" and overall_status == "Pass":
            overall_status = "Warning"

        results.append({
            '계정코드': account_code,
            '계정명': master_row.get('계정명', ''),
            '구분': bspl,
            '기초잔액(Master)': bop,
            '당기증감액(Ledger)': net_change_gl,
            '계산된 기말잔액(GL)': eop_gl,
            '기말잔액(Master)': eop_master,
            '차이': difference,
            '상태': status
        })
    return overall_status, pd.DataFrame(results)




==============================
📄 FILE: analysis/kdmeans_shim.py
==============================

from __future__ import annotations
from typing import Optional, Sequence
import numpy as np

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
except Exception as e:
    raise ImportError("scikit-learn이 필요합니다. `pip install scikit-learn`") from e


class HDBSCAN:
    """
    KDMeans: KMeans를 사용하되 HDBSCAN의 최소 속성 인터페이스를 흉내냄.
    - fit(X): labels_, probabilities_ 설정
    - labels_: np.ndarray[int], [0..k-1]
    - probabilities_: np.ndarray[float], 0~1 (KDMeans에서는 전부 1.0로 설정)
    매개변수:
      - n_clusters: 고정 k (None이면 자동 선택)
      - min_cluster_size: k 상한을 계산하기 위한 힌트(너무 많은 군집 방지)
      - max_k: 자동 선택 시 k 상한(기본: 데이터 크기와 min_cluster_size로 유도)
      - k_search: "silhouette" | "heuristic"
      - sample_size: 자동 선택 시 실루엣 계산에 사용할 샘플 크기(기본 2000)
      - random_state: 재현성
      - n_init: KMeans 초기화 횟수(또는 "auto")
    """
    def __init__(
        self,
        n_clusters: Optional[int] = None,
        min_cluster_size: int = 8,
        max_k: Optional[int] = None,
        k_search: str = "silhouette",
        sample_size: int = 2000,
        random_state: int = 42,
        n_init: str | int = "auto",
    ):
        self.n_clusters = n_clusters
        self.min_cluster_size = max(2, int(min_cluster_size))
        self.max_k = max_k
        self.k_search = k_search
        self.sample_size = int(sample_size)
        self.random_state = int(random_state)
        self.n_init = n_init

        # 학습 후 속성(HDBSCAN 호환)
        self.labels_: Optional[np.ndarray] = None
        self.probabilities_: Optional[np.ndarray] = None
        # 추가 텔레메트리
        self.chosen_k_: Optional[int] = None
        self.silhouette_: Optional[float] = None

    # --- 내부: k 후보 산정 ---
    def _candidate_ks(self, n: int) -> Sequence[int]:
        if n < 2:
            return [1]
        base = max(2, int(np.sqrt(n)))
        # 최소 크기 제약 기반 상한
        max_by_min = max(2, n // self.min_cluster_size)
        # 외부 상한 적용
        if self.max_k is not None:
            max_by_min = min(max_by_min, int(self.max_k))
        # 지나치게 큰 k는 계산 비용 이슈 → 실무적으로 캡
        hard_cap = 24 if n >= 1200 else 12
        k_hi = max(2, min(max_by_min, hard_cap))

        ks = {2, 3, 5, base - 1, base, base + 1, int(np.log2(n)) + 1, k_hi}
        ks = {int(k) for k in ks if 2 <= int(k) <= k_hi}
        return sorted(ks)

    # --- 내부: 샘플링 ---
    def _sample(self, X: np.ndarray) -> np.ndarray:
        n = X.shape[0]
        if n <= self.sample_size:
            return X
        rng = np.random.default_rng(self.random_state)
        idx = rng.choice(n, size=self.sample_size, replace=False)
        return X[idx]

    # --- 내부: k 자동 선택 (실루엣) ---
    def _choose_k(self, X: np.ndarray) -> int:
        n = X.shape[0]
        if n < 2:
            return 1
        if self.n_clusters is not None:
            return max(1, int(self.n_clusters))

        # 후보 목록
        ks = self._candidate_ks(n)
        if len(ks) == 0:
            return max(2, int(np.sqrt(n)))

        if self.k_search != "silhouette":
            # 휴리스틱: √n에 가장 가까운 값
            base = max(2, int(np.sqrt(n)))
            return min(ks, key=lambda k: abs(k - base))

        Xs = self._sample(X)
        best_k, best_s = None, -1.0

        for k in ks:
            if k >= len(Xs):   # 샘플보다 큰 k 불가
                continue
            try:
                km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
                labels = km.fit_predict(Xs)
                # 모든 라벨이 하나면 실루엣 계산 불가
                if len(set(labels)) < 2:
                    continue
                s = silhouette_score(Xs, labels, metric="euclidean")
                if s > best_s:
                    best_k, best_s = int(k), float(s)
            except Exception:
                continue

        if best_k is None:
            # 폴백: √n 인근
            base = max(2, int(np.sqrt(n)))
            best_k = min(ks, key=lambda k: abs(k - base))
            best_s = float("nan")

        self.silhouette_ = best_s
        return int(best_k)

    # --- 공개 API ---
    def fit(self, X: np.ndarray):
        X = np.asarray(X, dtype=float)
        if X.ndim != 2 or X.shape[0] < 1:
            raise ValueError("X must be 2D array with at least 1 row")

        k = self._choose_k(X)
        self.chosen_k_ = k

        km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
        labels = km.fit_predict(X)

        # HDBSCAN 호환 속성 부여
        self.labels_ = labels.astype(int)
        self.probabilities_ = np.ones(shape=(X.shape[0],), dtype=float)
        return self

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        return self.fit(X).labels_





==============================
📄 FILE: analysis/report.py
==============================

from __future__ import annotations
import pandas as pd
from typing import List
from .evidence import (
    build_current_cluster_block,
    # build_previous_projection_block,  # 파일 하단 로컬 정의 사용
    build_zscore_top5_block,
    build_zscore_top5_block_for_py,
)
from .timeseries import run_timeseries_module
from .embedding import map_previous_to_current_clusters
import numpy as np
from config import PM_DEFAULT
import re
import json
import time


def _fmt_money(x):
    try:
        return f"{float(x):,.0f}원"
    except Exception:
        return str(x)


# --- 단위 강제 후처리: 억/만 → 원 단위 ---
_NUM = r'(?:\d{1,3}(?:,\d{3})*|\d+)'


def _to_int(s):
    return int(str(s).replace(',', ''))


def _replace_korean_units(m):
    # 케이스: "3억 5,072만 원" / "54억 1,444만 원" / "2억 원" / "370만 원"
    eok = m.group('eok')
    man = m.group('man')
    won = m.group('won')
    total = 0
    if eok:
        total += _to_int(eok) * 100_000_000
    if man:
        total += _to_int(man) * 10_000
    if won:
        total += _to_int(won)
    return f"{total:,.0f}원"


def _enforce_won_units(text: str) -> str:
    # 1) 억/만/원 혼합을 원 단위로 치환
    pat = re.compile(
        rf'(?:(?P<eok>{_NUM})\s*억)?\s*(?:(?P<man>{_NUM})\s*만)?\s*(?:(?P<won>{_NUM})\s*원)?'
        r'(?!\s*단위)', flags=re.IGNORECASE)

    def _smart_sub(s):
        out = []
        last = 0
        for m in pat.finditer(s):
            # 의미 없는 빈 매칭 방지: 억/만이 없으면 스킵(이미 원 단위일 가능성)
            if not any(m.group(g) for g in ('eok', 'man')):
                continue
            out.append(s[last:m.start()])
            out.append(_replace_korean_units(m))
            last = m.end()
        out.append(s[last:])
        return ''.join(out)

    return _smart_sub(text)


def _boldify_bracket_headers(text: str) -> str:
    # [요약], [주요 거래], [결론], [용어 설명] → **[...]**\n
    text = re.sub(r'^\[(요약|주요 거래|결론|용어 설명)\]\s*', r'**[\1]**\n', text, flags=re.MULTILINE)
    return text


def _safe_load(s: str):
    """엄격한 JSON 로더: 코드 펜스 제거 후 strict json.loads.
    주변 텍스트/마크다운 허용하지 않음.
    """
    text = (s or "").strip()
    # 시작 펜스 제거
    text = re.sub(r"^\s*```(?:json|JSON)?\s*\n", "", text)
    # 끝 펜스 제거
    text = re.sub(r"\n\s*```\s*$", "", text)
    text = text.strip()
    return json.loads(text)


def generate_rag_context(master_df: pd.DataFrame, current_df: pd.DataFrame, previous_df: pd.DataFrame,
                         account_codes: List[str], manual_context: str = "",
                         include_risk_summary: bool = False, pm_value: float | None = None) -> str:
    acc_info = master_df[master_df['계정코드'].astype(str).isin(account_codes)]
    acc_names = ", ".join(acc_info['계정명'].unique().tolist())
    master_summary = f"- 분석 대상 계정 그룹: {acc_names} ({', '.join(account_codes)})"
    if not acc_info.empty:
        acct_type = acc_info.iloc[0].get('BS/PL', 'PL')
        has_dates_cur = ('회계일자' in current_df.columns) and current_df['회계일자'].notna().any()
        has_dates_prev = ('회계일자' in previous_df.columns) and previous_df['회계일자'].notna().any()
        if str(acct_type).upper() == 'PL' and has_dates_cur:
            min_date = current_df['회계일자'].min(); max_date = current_df['회계일자'].max()
            if has_dates_prev:
                # 래핑 구간(예: 11~2월) 오판 방지: 연-월 Period로 비교
                cur_months = current_df['회계일자'].dt.to_period('M')
                prev_months = previous_df['회계일자'].dt.to_period('M')
                mask = prev_months.isin(cur_months.unique())
                prev_f = previous_df.loc[mask]
            else:
                prev_f = previous_df.copy()
            # Net first (순액: 차-대), absolute as reference (규모(절대값))
            cur_net = current_df.get('순액', pd.Series(dtype=float)).sum()
            prev_net = prev_f.get('순액', pd.Series(dtype=float)).sum()
            cur_abs = current_df.get('발생액', pd.Series(dtype=float)).sum()
            prev_abs = prev_f.get('발생액', pd.Series(dtype=float)).sum()
            var = cur_net - prev_net
            var_pct = (var / prev_net * 100) if prev_net not in (0, 0.0) else float('inf')
            period = f"{min_date.strftime('%m월')}~{max_date.strftime('%m월')}"
            master_summary += (
                f"\n- 당기 **순액(차-대)** 합계 ({period}): {cur_net:,.0f}원"
                f" | 전기 동기간 순액: {prev_net:,.0f}원 | 순액 증감: {var:,.0f}원 ({var_pct:+.2f}%)"
                f"\n- (참고) **규모(절대값)** 발생액: 당기 {cur_abs:,.0f}원 | 전기 {prev_abs:,.0f}원"
                f" | 차이: {cur_abs - prev_abs:,.0f}원"
            )
        else:
            cur_bal = acc_info.get('당기말잔액', pd.Series(dtype=float)).sum()
            prior_bal = acc_info.get('전기말잔액', pd.Series(dtype=float)).sum()
            var = cur_bal - prior_bal
            var_pct = (var / prior_bal * 100) if prior_bal not in (0, 0.0) else float('inf')
            master_summary += f"\n- 당기말 잔액(합산): {cur_bal:,.0f}원 | 전기말 잔액(합산): {prior_bal:,.0f}원 | 증감: {var:,.0f}원 ({var_pct:+.2f}%)"

    manual_summary = f"\n\n## 사용자 제공 추가 정보\n{manual_context}" if manual_context and not manual_context.isspace() else ""

    # --- New context layout ---
    sec_info = f"## 분석대상 계정정보\n{master_summary}{manual_summary}"
    sec_cur = build_current_cluster_block(current_df)
    # Prior-year: 전표 전체를 CY 센트로이드에 최근접 매핑하여 합산하는 증거 블록 사용
    sec_prev = build_previous_projection_block(current_df, previous_df)
    sec_top5_cy = build_zscore_top5_block(current_df, previous_df, topn=5)
    sec_top5_py = build_zscore_top5_block_for_py(previous_df, current_df, topn=5)
    sec_ts = build_timeseries_summary_block(current_df)

    # --- NEW: 위험 매트릭스 요약(선택)
    sec_risk = ""
    if include_risk_summary:
        try:
            sec_risk = _build_risk_matrix_section(current_df, pm_value=pm_value)
        except Exception:
            sec_risk = ""

    parts = [sec_info, sec_cur, sec_prev, sec_ts, sec_top5_cy, sec_top5_py]
    if sec_risk:
        parts.insert(2, sec_risk)  # 정보→(위험)→클러스터 순
    return "\n".join(parts)


def build_timeseries_summary_block(current_df: pd.DataFrame, topn: int = 5) -> str:
    """
    ## 예측 이탈 요약
    계정별 월별 합계를 기반으로 마지막 포인트의 예측 대비 이탈을 요약.
    """
    if current_df is None or current_df.empty or '회계일자' not in current_df.columns:
        return "\n\n## 예측 이탈 요약\n- (데이터 없음)"
    df = current_df.copy()
    if '계정명' not in df.columns:
        return "\n\n## 예측 이탈 요약\n- (계정명이 필요합니다)"
    df['연월'] = df['회계일자'].dt.to_period('M')
    m = (df.groupby(['계정명','연월'], as_index=False)['거래금액'].sum())
    m['account'] = m['계정명']
    m['date'] = m['연월'].dt.to_timestamp('M')
    m['amount'] = m['거래금액']

    rows = run_timeseries_module(m[['account','date','amount']])
    if rows is None or rows.empty:
        return "\n\n## 예측 이탈 요약\n- (유의미한 이탈 없음)"

    rows = rows.sort_values('risk', ascending=False).head(int(topn))
    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime('%Y-%m-%d') if _pd.notna(x) else ""
        except:
            return ""
    lines = [
        "\n\n## 예측 이탈 요약",
        "※ 기본은 '월별 발생액(Δ잔액/flow)'. BS 계정은 **balance** 기준도 내부 평가하며, 아래 표기는 MoR과 z·risk를 함께 보여줍니다."
    ]
    for _, r in rows.iterrows():
        lines.append(
            f"- [{_fmt_dt(r['date'])}] {r['account']} ({r.get('measure','flow')}, MoR={r.get('model','-')})"
            f" | 실제 {r['actual']:,.0f}원 vs 예측 {r['predicted']:,.0f}원"
            f" → {'상회' if float(r['error'])>0 else '하회'} | z={float(r['z']):+.2f} | risk={float(r['risk']):.2f}"
        )
    return "\n".join(lines)


def _build_risk_matrix_section(current_df: pd.DataFrame, pm_value: float | None = None) -> str:
    """
    당기 데이터로 이상치 모듈을 한 번 돌려 (계정×주장) 위험 매트릭스 상위 셀을 요약.
    """
    from .anomaly import run_anomaly_module
    from .assertion_risk import build_matrix
    from config import PM_DEFAULT

    if current_df is None or current_df.empty:
        return "\n\n## 위험 매트릭스 요약\n- (데이터 없음)"

    # 간이 LedgerFrame 구성
    from analysis.contracts import LedgerFrame
    lf = LedgerFrame(df=current_df.copy(), meta={})

    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    mod = run_anomaly_module(lf, target_accounts=None, topn=200, pm_value=pm)
    mat, _ = build_matrix([mod])
    if mat.empty:
        return "\n\n## 위험 매트릭스 요약\n- (생성된 Evidence 없음)"

    # 상위 10 셀 추출
    flat = (
        mat.stack()
          .rename("risk")
          .reset_index()
          .sort_values("risk", ascending=False)
    )
    top = flat[flat["risk"] > 0].head(10)
    if top.empty:
        return "\n\n## 위험 매트릭스 요약\n- (유의미한 위험값 없음)"

    lines = ["\n\n## 위험 매트릭스 요약 (상위 10)"]
    for _, r in top.iterrows():
        acct = str(r["level_0"]); asrt = str(r["level_1"]); val = float(r["risk"])
        lines.append(f"- {acct} × {asrt} ⇒ {val:.2f}")
    return "\n".join(lines)


def build_methodology_note(report_accounts=None) -> str:
    lines = [
        "\n\n## 분석 기준(알림)",
        "- 이번 분석은 UI에서 선택된 계정 기준으로 산출되었습니다.",
        "- 요약 수치: **순액(차-대)** 기준. (발생액=규모(절대값)은 참고용)",
        "- Z-Score: 선택 계정들의 **발생액(절대값)** 분포 기준.",
        "- 유사도/근거: **적요+거래처** 임베딩 후 코사인 유사도(전기 동월 우선).",
        "- '클러스터 노이즈(-1)'는 의미가 충분히 모이지 않아 자동으로 묶이지 않은 산발적 거래 묶음입니다.",
    ]
    return "\n".join(lines)


# LLM 보고서는 서비스 계층(stub) 사용 권장
from services.llm import LLMClient


def _format_from_json(obj: dict) -> str:
    """
    단순 스키마(JSON) → 최종 마크다운.
    - key_transactions: LLM이 작성한 전체 섹션 마크다운을 그대로 사용
    - glossary: 필수 항목 보강(없을 경우 기본 정의 추가)
    """
    summary = (obj.get("summary") or "").strip()
    kt_val = obj.get("key_transactions")
    # 과거 호환(오브젝트가 오면 텍스트로 변환 시도)
    if isinstance(kt_val, dict):
        parts = []
        for k, v in kt_val.items():
            if isinstance(v, str):
                parts.append(v.strip())
        key_tx_md = "\n\n".join(p for p in parts if p)
    else:
        key_tx_md = (kt_val or "").strip()

    conclusion = (obj.get("conclusion") or "").strip()
    glossary_list = obj.get("glossary") or []

    # 필수 용어 보강: Z-Score, 클러스터 노이즈(-1)
    g_text = " ".join(map(str, glossary_list))
    need_z = ("z-score" not in g_text.lower()) and ("Z-Score" not in g_text)
    need_noise = ("클러스터 노이즈" not in g_text)
    if need_z:
        glossary_list.append("Z-Score: 표본의 값이 평균에서 몇 개의 표준편차만큼 떨어져 있는지 나타내는 지표(‘표준편차의 배수’). |Z|가 클수록 이례적임.")
    if need_noise:
        glossary_list.append("클러스터 노이즈(-1): 의미가 충분히 모이지 않아 자동으로 묶이지 않은 산발적 거래 묶음.")
    glossary = "\n".join(f"- {str(x)}" for x in glossary_list)

    md = (
        f"**[요약]**\n{summary}\n\n"
        f"**[주요 거래]**\n{key_tx_md}\n\n"
        f"**[결론]**\n{conclusion}\n\n"
        f"**[용어 설명]**\n{glossary}"
    )
    return md


# --- 전기 전체 매핑 합산 방식으로 교체: 이전 전표를 CY 클러스터에 최근접 매핑 후 합산 ---
def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float | None = None) -> str:
    """
    Project all PY vouchers onto CY cluster centroids and aggregate absolute amounts by the CY cluster_group.
    - No similarity computation is shown or used for filtering.
    - Output contains only total absolute amount and ONE example voucher (no sim).
    """
    import pandas as pd
    if current_df is None or previous_df is None or current_df.empty or previous_df.empty:
        return "\n\n## 전기 클러스터 및 금액\n- (전기 데이터 없음)"
    need_cur = {'cluster_id','cluster_name','vector'}
    if not need_cur.issubset(current_df.columns) or 'vector' not in previous_df.columns:
        return "\n\n## 전기 클러스터 및 금액\n- (클러스터/벡터 정보 부족)"

    prev_m = map_previous_to_current_clusters(current_df, previous_df)
    if prev_m is None or prev_m.empty or 'mapped_cluster_id' not in prev_m.columns:
        return "\n\n## 전기 클러스터 및 금액\n- (매핑 실패)"

    if 'cluster_group' in current_df.columns:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_group'].to_dict()
    else:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()

    prev_m = prev_m.copy()
    prev_m['mapped_group'] = prev_m['mapped_cluster_id'].map(id2group)
    prev_m['abs_amt'] = prev_m.get('발생액', pd.Series(dtype=float)).abs()

    agg = (
        prev_m.groupby('mapped_group', dropna=False)
              .agg(규모=('abs_amt','sum'))
              .reset_index()
              .sort_values('규모', ascending=False)
    )

    lines = ["\n\n## 전기 클러스터 및 금액"]
    for _, row in agg.iterrows():
        g = row['mapped_group'] if pd.notna(row['mapped_group']) else '(미매핑)'
        tot = row['규모']
        sub = prev_m[prev_m['mapped_group'] == row['mapped_group']]
        if not sub.empty:
            ex = sub.sort_values('abs_amt', ascending=False).head(1).iloc[0]
            raw_dt = ex.get('회계일자', None)
            if pd.notna(raw_dt):
                try:
                    _dt = pd.to_datetime(raw_dt, errors='coerce')
                    dt = _dt.strftime('%Y-%m-%d') if pd.notna(_dt) else ''
                except Exception:
                    dt = ''
            else:
                dt = ''
            lines.append(f"- [{g}] 규모(절대값) {tot:,.0f}원")
            lines.append(f"  · 예시: {dt} | {ex.get('거래처','')} | {int(ex.get('발생액',0)):,.0f}원")
        else:
            lines.append(f"- [{g}] 규모(절대값) {tot:,.0f}원")
    return "\n".join(lines)


def run_final_analysis(context: str, account_codes: list[str], *, model: str | None = None, max_tokens: int | None = 16000) -> str:
    system = (
        "You are a CPA. Do all hidden reasoning internally and output ONLY the JSON object in the EXACT schema below. "
        "Language: Korean (ko-KR) for every natural-language value.\n"
        "Schema: {"
        '"summary": str,'
        '"key_transactions": str,'
        '"conclusion": str,'
        '"glossary": [str]'
        "}\n"
        "Authoring rules:\n"
        "• Monetary values MUST be formatted in KRW like '1,234원' (never 억/만원).\n"
        "• [요약]은 계정군 수준의 변동과 규모를 한 문장으로 명료히.\n"
        "• [주요 거래]는 전체 서술을 네가 설계하되, 컨텍스트(CY/PY 클러스터, 매핑, Z-score 상위 항목)를 근거로 구성 비중/이상치/전기 대응관계를 자연스럽게 녹여라. 필요하면 불릿·소제목을 임의로 사용해 가독성을 높여라.\n"
        "• [결론]은 원인·리스크·통제·액션아이템 중심으로 실무적 제안 위주로 작성한다.\n"
        "• [용어 설명]에는 반드시 다음 두 항목이 포함되도록 한다:   1) '클러스터 노이즈(-1)' 정의, 2) 'Z-Score'가 ‘평균에서 몇 표준편차’인지의 직관적 의미.\n"
        "Compliance: Output MUST be the JSON object itself, with no markdown/code-fences or extra text."
    )
    user = (
        f"Target accounts: {', '.join(account_codes)}\n"
        f"{context}\n"
        "Return ONLY the JSON per schema via function call."
    )

    tool_schema = {
        "type": "function",
        "function": {
            "name": "emit_report",
            "description": "Return the report strictly in the fixed JSON schema.",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "key_transactions": {"type": "string"},
                    "conclusion": {"type": "string"},
                    "glossary": {"type": "array","items":{"type":"string"}}
                },
                "required": ["summary","key_transactions","conclusion","glossary"]
            }
        }
    }

    llm = LLMClient()
    max_retries = 2
    last_err = None

    for attempt in range(max_retries + 1):
        try:
            raw = llm.generate(
                system=system, user=user, model=model,
                max_tokens=max_tokens, tools=[tool_schema], force_json=False
            )
            obj = _safe_load(raw)
            text = _format_from_json(obj)
            return _enforce_won_units(text)
        except Exception as e:
            last_err = e
            if attempt < max_retries:
                time.sleep(1.0)
            continue

    raise ValueError(f"LLM failed to produce valid JSON report after retries. Details: {last_err}")


# --- NEW: LLM 미사용/실패 시 폴백 리포트 (순수 로컬 계산) ---
def run_offline_fallback_report(current_df: pd.DataFrame,
                                previous_df: pd.DataFrame,
                                account_codes: list[str],
                                pm_value: float | None = None) -> str:
    """
    외부 LLM을 전혀 사용하지 않고 간단 보고서를 생성한다.
    - 요약: CY/PY 순액/규모 비교
    - 주요 거래: |Z| Top 5 (가능하면 Z 기준, 없으면 발생액 상위)
    - 결론: KIT(≥PM) 건수/비중 및 리스크 주의
    - 용어: Z-Score, Key Item 기본 정의
    """
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    cur = current_df.copy()
    prev = previous_df.copy()

    def _safe_sum(df, col): 
        return float(df.get(col, pd.Series(dtype=float)).sum()) if not df.empty else 0.0

    cur_net  = _safe_sum(cur, "순액")
    prev_net = _safe_sum(prev, "순액")
    cur_abs  = _safe_sum(cur, "발생액")
    prev_abs = _safe_sum(prev, "발생액")

    var_net = cur_net - prev_net
    var_abs = cur_abs - prev_abs
    var_pct = (var_net / prev_net * 100.0) if prev_net not in (0, 0.0) else float("inf")

    # Top 5: Z-Score 우선, 없으면 발생액 상위
    top_df = cur.copy()
    if "Z-Score" in top_df.columns and top_df["Z-Score"].notna().any():
        top_df = top_df.reindex(top_df["Z-Score"].abs().sort_values(ascending=False).index)
    else:
        top_df = top_df.reindex(top_df.get("발생액", pd.Series(dtype=float)).abs().sort_values(ascending=False).index)
    top_df = top_df.head(5)

    # KIT 집계(절대발생액 기준)
    kit_mask = top_df.get("발생액", pd.Series(dtype=float)).abs() >= pm if not top_df.empty else pd.Series([], dtype=bool)
    kit_cnt  = int(kit_mask.sum()) if not top_df.empty else 0

    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime("%Y-%m-%d") if _pd.notna(x) else ""
        except Exception:
            return ""

    # Compose sections (간단 Markdown)
    summary = (
        f"선택 계정({', '.join(account_codes)}) 기준으로 당기 **순액** {cur_net:,.0f}원,"
        f" 전기 {prev_net:,.0f}원 → 증감 {var_net:,.0f}원 ({var_pct:+.2f}%).\n"
        f"(참고) **규모(발생액 절대값)** 당기 {cur_abs:,.0f}원, 전기 {prev_abs:,.0f}원 → 차이 {var_abs:,.0f}원."
    )

    kt_lines = []
    if not top_df.empty:
        for i, (_, r) in enumerate(top_df.iterrows(), 1):
            dt = _fmt_dt(r.get("회계일자"))
            vend = str(r.get("거래처", "") or "")
            amt = float(r.get("발생액", 0.0))
            z   = r.get("Z-Score", np.nan)
            ztxt = f" | Z={float(z):+.2f}" if not pd.isna(z) else ""
            kt_lines.append(f"- [{i}] {dt} | {vend} | {amt:,.0f}원{ztxt}")
    key_tx = "\n".join(kt_lines) if kt_lines else "- 상위 항목을 산출할 데이터가 없습니다."

    conclusion = (
        f"PM {pm:,.0f}원 기준 **Key Item(KIT)** 후보는 상위 리스트 중 {kit_cnt}건입니다. "
        "Z-Score가 큰 항목은 적요·거래처 등 근거 확인과 원인 파악이 필요합니다. "
        "주요 변동은 월별 추이/상관 분석과 함께 교차검토하는 것을 권장합니다."
    )
    glossary = (
        "- Z-Score: 표본 값이 평균에서 몇 표준편차만큼 떨어져 있는지 나타내는 지표(‘표준편차의 배수’). |Z|가 클수록 이례적.\n"
        "- Key Item(KIT): 단일 항목 절대금액이 PM 이상인 전표."
    )
    return (
        f"**[요약]**\n{summary}\n\n"
        f"**[주요 거래]**\n{key_tx}\n\n"
        f"**[결론]**\n{conclusion}\n\n"
        f"**[용어 설명]**\n{glossary}"
    )




==============================
📄 FILE: analysis/summarization.py
==============================




==============================
📄 FILE: analysis/timeseries.py
==============================

# timeseries.py
# v3 — Compact TS module with PY+CY window, MoR(EMA/MA/ARIMA/Prophet), dual-basis(flow/balance)
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple, Callable
import math
import numpy as np
import pandas as pd

# -------- Optional config / anomaly imports with safe fallbacks --------
try:
    from config import PM_DEFAULT as _PM_DEFAULT
except Exception:
    _PM_DEFAULT = 0.7
try:
    from config import FORECAST_MIN_POINTS as _FORECAST_MIN_POINTS
except Exception:
    _FORECAST_MIN_POINTS = 8
try:
    from config import ARIMA_DEFAULT_ORDER as _ARIMA_DEFAULT_ORDER
except Exception:
    _ARIMA_DEFAULT_ORDER = (1, 1, 1)

def _risk_from_fallback(z_abs: float, amount: float, pm: float) -> float:
    # simple logistic mapping as a fallback
    return float(1.0 / (1.0 + math.exp(-abs(z_abs))))

try:
    from analysis.anomaly import _risk_from as _RISK_EXTERNAL  # type: ignore
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        try:
            r = _RISK_EXTERNAL(z_abs, amount=amount, pm=float(pm))
            return float(r[-1] if isinstance(r, (list, tuple)) else r)
        except Exception:
            return _risk_from_fallback(z_abs, amount, pm)
except Exception:
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        return _risk_from_fallback(z_abs, amount, pm)

# ----------------------------- Utilities ------------------------------
def _to_month_period_index(dates: pd.Series) -> pd.PeriodIndex:
    return pd.to_datetime(dates).dt.to_period("M")

def _longest_contiguous_month_run(periods: pd.PeriodIndex) -> pd.PeriodIndex:
    if len(periods) <= 1: return periods
    p = pd.PeriodIndex(np.unique(np.asarray(periods)), freq="M")
    best_s = best_e = cur_s = 0
    for i in range(1, len(p)):
        if (p[i] - p[i-1]).n != 1:
            if i-1 - cur_s > best_e - best_s:
                best_s, best_e = cur_s, i-1
            cur_s = i
    if len(p)-1 - cur_s > best_e - best_s:
        best_s, best_e = cur_s, len(p)-1
    return p[best_s:best_e+1]

def _smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    denom = np.maximum(1e-12, np.abs(yt) + np.abs(yp))
    return float(np.mean(200.0 * np.abs(yt - yp) / denom))

def _std_last(x: np.ndarray, w: int = 6) -> float:
    if len(x) < 2: return 0.0
    s = np.std(x[-min(w, len(x)):], ddof=1) if len(x) > 1 else 0.0
    return float(s if math.isfinite(s) else 0.0)

def z_and_risk(residuals: np.ndarray, pm: float = _PM_DEFAULT) -> Tuple[np.ndarray, np.ndarray]:
    """잔차 시퀀스에 대해 표준화 z와 위험도 배열을 반환.
    테스트 호환을 위해 간단한 정규화와 |z|→risk 매핑을 사용.
    """
    r = np.asarray(residuals, dtype=float)
    if r.size <= 1:
        z = np.zeros_like(r)
    else:
        sd = float(np.std(r, ddof=1))
        z = (r / sd) if sd > 0 else np.zeros_like(r)
    risk_vals = np.array([_risk_score(abs(float(zi)), amount=1.0, pm=float(pm)) for zi in z], dtype=float)
    return z, risk_vals

def _has_seasonality(y: pd.Series) -> bool:
    y = pd.Series(y, dtype=float)
    if len(y) < 12: return False
    ac = np.abs(np.fft.rfft((y - y.mean()).values))
    core = ac[2:] if len(ac) > 2 else ac
    return bool(core.size and (core.max() / (core.mean() + 1e-9) > 5.0))

# --------------------------- Model backends ---------------------------
def _model_registry() -> Dict[str, bool]:
    ok_arima = ok_prophet = False
    try:
        import statsmodels.api as _  # noqa
        ok_arima = True
    except Exception:
        pass
    try:
        from prophet import Prophet as _  # noqa
        ok_prophet = True
    except Exception:
        pass
    return {"ema": True, "ma": True, "arima": ok_arima, "prophet": ok_prophet}

def model_registry() -> Dict[str, bool]:
    """공개 API: 사용 가능한 백엔드 레지스트리 반환."""
    return _model_registry()

# EMA
def _fit_ema(y: pd.Series, alpha: float = 0.3) -> Dict[str, Any]:
    return {"alpha": float(alpha), "y": y}

def _pred_ema(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]
    alpha = float(m["alpha"])
    pred = y.ewm(alpha=alpha, adjust=False).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# MA
def _fit_ma(y: pd.Series, window: int = 6) -> Dict[str, Any]:
    return {"window": int(window), "y": y}

def _pred_ma(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]; w = int(m["window"])
    pred = y.rolling(w, min_periods=1).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# ARIMA
def _fit_arima(y: pd.Series, order: Tuple[int,int,int] = _ARIMA_DEFAULT_ORDER):
    import statsmodels.api as sm
    return sm.tsa.ARIMA(y, order=tuple(order)).fit()

def _pred_arima(m, steps: Optional[int] = None) -> np.ndarray:
    if steps is None:
        fv = pd.Series(m.fittedvalues).shift(1).fillna(method="bfill")
        return fv.values
    return np.asarray(m.forecast(steps=int(steps)))

# Prophet
def _fit_prophet(y: pd.Series):
    from prophet import Prophet
    df = pd.DataFrame({"ds": y.index.to_timestamp(), "y": y.values})
    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    m.fit(df)
    return {"m": m, "idx": y.index}

def _pred_prophet(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    model = m["m"]; idx: pd.PeriodIndex = m["idx"]
    if steps is None:
        fit = model.predict(pd.DataFrame({"ds": idx.to_timestamp()}))["yhat"].values
        return np.roll(fit, 1)  # 1-step ahead approx.
    last = idx[-1].to_timestamp()
    future = pd.date_range(last + pd.offsets.MonthBegin(1), periods=int(steps), freq="MS")
    return model.predict(pd.DataFrame({"ds": future}))["yhat"].values

# -------------------- Model selection (MoR) via rolling CV -------------
def _rolling_origin_cv(
    y: pd.Series,
    fit_fn, pred_fn,
    k: int = 3, min_train: int = 6
) -> float:
    y = y.dropna(); n = len(y)
    if n < max(min_train + k, 8):
        m = fit_fn(y); yhat = pred_fn(m)
        yhat = np.asarray(yhat)[:n] if yhat is not None else np.repeat(y.iloc[:1].values, n)
        return _smape(y.values, yhat)
    step = max((n - min_train) // (k + 1), 1)
    scores = []
    for i in range(min_train, n, step):
        tr = y.iloc[:i]; te = y.iloc[i:i+step]
        if te.empty: break
        m = fit_fn(tr); yh = pred_fn(m, steps=len(te))
        scores.append(_smape(te.values, np.asarray(yh)[:len(te)]))
    return float(np.mean(scores)) if scores else 999.0

def _choose_model(y: pd.Series, measure: str) -> Tuple[str, np.ndarray]:
    reg = _model_registry()
    cands: List[Tuple[str, np.ndarray, Any, Any]] = []
    # always EMA/MA
    m_ema = _fit_ema(y); yhat_ema = _pred_ema(m_ema); cands.append(("EMA", yhat_ema, _fit_ema, _pred_ema))
    m_ma  = _fit_ma(y);  yhat_ma  = _pred_ma(m_ma);   cands.append(("MA",  yhat_ma,  _fit_ma,  _pred_ma))
    # ARIMA
    if reg["arima"]:
        try:
            m = _fit_arima(y); yhat = _pred_arima(m); cands.append(("ARIMA", yhat, _fit_arima, _pred_arima))
        except Exception:
            pass
    # Prophet: only for flow, enough data & seasonal
    if measure == "flow" and reg["prophet"] and len(y) >= 12 and _has_seasonality(y):
        try:
            m = _fit_prophet(y); yhat = _pred_prophet(m); cands.append(("Prophet", yhat, _fit_prophet, _pred_prophet))
        except Exception:
            pass
    # pick by CV
    scores = [(nm, _rolling_origin_cv(y, fit, pred)) for (nm, _, fit, pred) in cands]
    best = min(scores, key=lambda x: x[1])[0] if scores else "EMA"
    # return best in-sample prediction
    if best == "EMA": return "EMA", yhat_ema
    if best == "MA":  return "MA",  yhat_ma
    if best == "ARIMA":
        try: return "ARIMA", _pred_arima(_fit_arima(y))
        except Exception: return "EMA", yhat_ema
    if best == "Prophet":
        try: return "Prophet", _pred_prophet(_fit_prophet(y))
        except Exception: return "EMA", yhat_ema
    return "EMA", yhat_ema

# --------------------------- Core predictors --------------------------
def _prepare_monthly(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=[date_col]).copy()
    p = _to_month_period_index(df[date_col])
    df2 = df.copy()
    df2["_p"] = p
    df2 = df2.dropna(subset=["_p"]).sort_values("_p")
    run = _longest_contiguous_month_run(df2["_p"])
    return df2[df2["_p"].isin(run)].reset_index(drop=True)

def _one_track_lastrow(
    monthly: pd.DataFrame,
    value_col: str,
    measure: str,
    pm_value: float
) -> Optional[Dict[str, Any]]:
    df = _prepare_monthly(monthly, "date")
    if df.empty or value_col not in df.columns: return None
    y = pd.Series(df[value_col].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2: return None
    model, yhat = _choose_model(y, measure=measure)
    resid = y.values - yhat
    error_last = float(resid[-1])
    sigma = _std_last(resid, w=6)
    z = float(error_last / sigma) if sigma > 0 else 0.0
    risk = _risk_score(abs(z), amount=float(y.iloc[-1]), pm=float(pm_value))
    return {
        "date": df["_p"].iloc[-1].to_timestamp(),
        "measure": measure,
        "actual": float(y.iloc[-1]),
        "predicted": float(yhat[-1]),
        "error": float(error_last),
        "z": float(z),
        "risk": float(risk),
        "model": model,
    }

# ------------------------------- API ----------------------------------
def run_timeseries_for_account(
    monthly: pd.DataFrame,
    account: str,
    is_bs: bool,
    flow_col: str = "flow",
    balance_col: Optional[str] = None,
    allow_prophet: bool = True,   # kept for backward compatibility (no-op switch)
    pm_value: float = _PM_DEFAULT,
    **kwargs: Any,                # absorb legacy args safely
) -> pd.DataFrame:
    """
    단일 계정의 월별 데이터에서 마지막 포인트를 평가.
    - BS 계정: flow/balance 2행(해당 시 존재) 반환
    - PL 계정: flow 1행 반환
    반환 컬럼: ["date","account","measure","actual","predicted","error","z","risk","model"]
    """
    rows: List[Dict[str, Any]] = []
    # flow
    if flow_col in monthly.columns:
        r = _one_track_lastrow(monthly.rename(columns={flow_col: "val"}), "val", "flow", pm_value)
        if r: rows.append(r)
    # balance
    if is_bs:
        if balance_col and (balance_col in monthly.columns):
            r = _one_track_lastrow(monthly.rename(columns={balance_col: "val"}), "val", "balance", pm_value)
            if r: rows.append(r)
        else:
            if flow_col in monthly.columns:
                tmp = monthly[["date", flow_col]].copy()
                tmp["val"] = tmp[flow_col].astype(float).cumsum()
                r = _one_track_lastrow(tmp[["date","val"]], "val", "balance", pm_value)
                if r: rows.append(r)
    out = pd.DataFrame(rows)
    if not out.empty:
        out["account"] = account
        out = out[["date","account","measure","actual","predicted","error","z","risk","model"]]
        out = out.sort_values(["account","measure","date"]).reset_index(drop=True)
    else:
        out = pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"])
    return out

def run_timeseries_module(
    df: pd.DataFrame,
    *,
    account_col: str = "account",
    date_col: str = "date",
    amount_col: str = "amount",
    pm_value: float = _PM_DEFAULT,
    make_balance: bool = False,  # 기본값 False로 변경: 필요 시 balance 구성
    output: str = "all",        # "all" | "flow" | "balance"
    evidence_adapter: Optional[Callable[[Dict[str, Any]], Any]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    집계형: 계정별 월합계(amount)만 주어진 경우.
    기본: flow만 계산. make_balance=True일 때 balance(누적합)도 함께 계산.
    output으로 최종 반환 필터링 가능("flow"/"balance").
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    work = df[[account_col, date_col, amount_col]].copy()
    work.columns = ["account","date","amount"]
    work = work.sort_values(["account","date"])
    all_rows: List[pd.DataFrame] = []
    for acc, g in work.groupby("account", dropna=False):
        mon = g[["date","amount"]].rename(columns={"amount":"flow"}).copy()
        if make_balance:
            mon["balance"] = mon["flow"].astype(float).cumsum()
        out = run_timeseries_for_account(mon, str(acc), is_bs=make_balance, flow_col="flow",
                                         balance_col=("balance" if make_balance else None),
                                         pm_value=float(pm_value))
        if not out.empty:
            # CEAVOP 제안(간단 규칙): error>0 → E(존재), error<=0 → C(완전성)
            try:
                out["assertion"] = out["error"].map(lambda e: "E" if float(e) > 0 else "C")
            except Exception:
                out["assertion"] = "E"
        if output in ("flow", "balance") and not out.empty:
            out = out[out["measure"] == output]
        all_rows.append(out)
    result = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    if evidence_adapter is not None and not result.empty:
        rows = []
        for r in result.to_dict(orient="records"):
            d = dict(r)
            if "amount" not in d:
                d["amount"] = float(d.get("actual", 0.0))
            if "z_abs" not in d:
                try:
                    d["z_abs"] = abs(float(d.get("z", 0.0)))
                except Exception:
                    d["z_abs"] = 0.0
            if "assertion" not in d:
                try:
                    d["assertion"] = "E" if float(d.get("error", 0.0)) > 0 else "C"
                except Exception:
                    d["assertion"] = "E"
            rows.append(evidence_adapter(d))
        return rows  # type: ignore[return-value]
    return result

def run_timeseries_module_with_flag(
    df: pd.DataFrame,
    *,
    account_col: str = "account",
    date_col: str = "date",
    amount_col: str = "amount",
    is_bs_col: str = "is_bs",
    pm_value: float = _PM_DEFAULT,
) -> pd.DataFrame:
    """
    혼합 데이터셋에서 계정별 BS 여부에 따라 듀얼(Flow+Balance) 또는 단일(Flow)로 처리.
    - is_bs=True: flow + balance(누적합) 계산
    - is_bs=False: flow만 계산
    반환 컬럼: ["account","date","measure","actual","predicted","error","z","risk","model"]
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    work = df[[account_col, date_col, amount_col, is_bs_col]].copy()
    work.columns = ["account","date","amount","is_bs"]
    work = work.sort_values(["account","date"])
    outs: List[pd.DataFrame] = []
    for acc, g in work.groupby("account", dropna=False):
        is_bs = bool(g["is_bs"].iloc[-1])
        mon = g[["date","amount"]].rename(columns={"amount":"flow"}).copy()
        balance_col = None
        if is_bs:
            mon["balance"] = mon["flow"].astype(float).cumsum()
            balance_col = "balance"
        out = run_timeseries_for_account(
            mon, str(acc), is_bs=is_bs, flow_col="flow",
            balance_col=balance_col, pm_value=float(pm_value)
        )
        if not out.empty:
            try:
                out["assertion"] = out["error"].map(lambda e: "E" if float(e) > 0 else "C")
            except Exception:
                out["assertion"] = "E"
        outs.append(out)
    return pd.concat(outs, ignore_index=True) if outs else pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])



==============================
📄 FILE: analysis/trend.py
==============================

import pandas as pd
import plotly.express as px
from typing import List, Dict, Any
from analysis.contracts import LedgerFrame, ModuleResult


def create_monthly_trend_figure(ledger_df: pd.DataFrame, master_df: pd.DataFrame, account_code: str, account_name: str):
    """BS/PL, 차/대변 성격을 반영하여 월별 추이 그래프를 생성합니다."""
    mrow = master_df[master_df['계정코드'] == account_code]
    if mrow.empty:
        return None  # 안전 가드
    master_row = mrow.iloc[0]
    bspl = master_row.get('BS/PL', 'PL').upper()
    nature = master_row.get('차변/대변', '차변').strip()
    sign = -1.0 if '대변' in nature else 1.0

    current_year = ledger_df['연도'].max()
    df_filtered = ledger_df[(ledger_df['계정코드'] == account_code) & (ledger_df['연도'].isin([current_year, current_year - 1]))]
    months = list(range(1, 13))
    plot_df_list = []

    if bspl == 'BS':
        bop_cy = master_row.get('전기말잔액', 0)
        bop_py = master_row.get('전전기말잔액', 0)
        for year, bop, year_label in [(current_year, bop_cy, 'CY'), (current_year - 1, bop_py, 'PY')]:
            monthly_flow = df_filtered[df_filtered['연도'] == year].groupby('월')['거래금액'].sum()
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(monthly_flow)
            monthly_balance = bop + monthly_series.cumsum()
            plot_df_list.append(pd.DataFrame({'월': months, '금액': monthly_balance.values * sign, '구분': year_label}))
        title_suffix = "월별 잔액 추이 (BS)"
    else:
        monthly_sum = df_filtered.groupby(['연도', '월'])['거래금액'].sum().reset_index()
        for year, year_label in [(current_year, 'CY'), (current_year - 1, 'PY')]:
            year_data = monthly_sum[monthly_sum['연도'] == year]
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(year_data.set_index('월')['거래금액'])
            plot_df_list.append(pd.DataFrame({'월': months, '금액': monthly_series.values * sign, '구분': year_label}))
        title_suffix = "월별 발생액 추이 (PL)"

    if not plot_df_list:
        return None

    plot_df = pd.concat(plot_df_list)
    fig = px.bar(
        plot_df,
        x='월', y='금액', color='구분', barmode='group',
        title=f"'{account_name}' ({account_code}) {title_suffix}",
        labels={'월': '월', '금액': '금액', '구분': '연도'},
        color_discrete_map={'PY': '#a9a9a9', 'CY': '#1f77b4'}
    )
    fig.update_xaxes(dtick=1)
    # 🔢 축/툴팁 포맷: 천단위 쉼표, SI 단위 제거
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none')
    fig.update_traces(hovertemplate='월=%{x}<br>금액=%{y:,.0f} 원<br>구분=%{fullData.name}<extra></extra>')
    return fig


# (제거됨) 자동 추천 로직: 사용자가 명시적으로 선택한 계정만 사용


def run_trend_module(lf: LedgerFrame, accounts: List[str] | None = None) -> ModuleResult:
    """월별 추이 모듈: 사용자가 선택한 계정만 그린다(자동 추천 없음)."""
    df = lf.df
    master_df = lf.meta.get("master_df")
    if master_df is None:
        return ModuleResult(
            name="trend",
            summary={},
            tables={},
            figures={},
            evidences=[],
            warnings=["Master DF가 없습니다."]
        )

    # ✅ 자동 추천 제거: 계정이 명시적으로 주어지지 않으면 빈 결과 반환
    if not accounts:
        return ModuleResult(
            name="trend",
            summary={"picked_accounts": [], "n_figures": 0, "period_tag_coverage": {}},
            tables={},
            figures={},
            evidences=[],
            warnings=["계정이 선택되지 않았습니다. (자동 추천 비활성화)"]
        )

    acc_codes = [str(a) for a in accounts]
    figures: Dict[str, Any] = {}
    warns: List[str] = []
    for code in acc_codes:
        m = master_df[master_df['계정코드'].astype(str) == code]
        if m.empty:
            warns.append(f"계정코드 {code}가 Master에 없습니다.")
            continue
        name = m.iloc[0].get('계정명', str(code))
        fig = create_monthly_trend_figure(df, master_df, code, name)
        if fig:
            figures[f"{code}:{name}"] = fig
        else:
            warns.append(f"{name}({code}) 그림 생성 불가(데이터 부족).")

    summary = {
        "picked_accounts": acc_codes,
        "n_figures": len(figures),
        "period_tag_coverage": dict(df['period_tag'].value_counts()) if 'period_tag' in df.columns else {},
    }

    return ModuleResult(
        name="trend",
        summary=summary,
        tables={},
        figures=figures,
        evidences=[],
        warnings=warns
    )




==============================
📄 FILE: analysis/vendor.py
==============================

from typing import List, Dict, Any
import pandas as pd
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from itertools import product
from analysis.contracts import LedgerFrame, ModuleResult


def create_pareto_figure(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True):
    """거래처별 거래금액 파레토 차트.
    - min_amount 이상인 거래처만 개별 표기
    - 나머지는 '기타'로 합산(옵션)
    """
    cy_df = ledger_df[ledger_df['연도'] == ledger_df['연도'].max()]
    if '거래처' not in cy_df.columns or cy_df['거래처'].nunique() < 1:
        return None

    vendor_sum = cy_df.groupby('거래처')['거래금액_절대값'].sum()
    if vendor_sum.empty:
        return None

    if min_amount and min_amount > 0:
        above = vendor_sum[vendor_sum >= min_amount].sort_values(ascending=False)
    else:
        above = vendor_sum.sort_values(ascending=False)

    # '기타' 합산
    etc_sum = float(vendor_sum.sum() - above.sum())
    series = above
    if include_others and etc_sum > 0:
        import pandas as pd
        series = pd.concat([above, pd.Series({'기타': etc_sum})])

    # 누적 비율(표시된 막대 기준; '기타' 포함 시 100%로 수렴)
    cum_ratio = series.cumsum() / series.sum() * 100.0

    from plotly.subplots import make_subplots
    import plotly.graph_objects as go
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(go.Bar(x=series.index, y=series.values, name='거래 금액'), secondary_y=False)
    fig.add_trace(go.Scatter(x=series.index, y=cum_ratio.values, name='누적 비율(%)', mode='lines+markers'), secondary_y=True)
    fig.update_layout(title='거래처 집중도 분석 (Pareto)', yaxis_title='금액', yaxis2_title='누적 비율(%)')

    # 🔢 축/툴팁 포맷
    # 좌측 금액축: 천단위, SI 제거
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none', secondary_y=False)
    # 우측 %축
    fig.update_yaxes(tickformat='.1f', ticksuffix='%', secondary_y=True)

    # 툴팁 포맷
    fig.update_traces(hovertemplate='%{x}<br>%{y:,.0f} 원<extra></extra>', selector=dict(type='bar'))
    fig.update_traces(hovertemplate='%{x}<br>누적 비율=%{y:.1f}%<extra></extra>', selector=dict(type='scatter'))

    return fig


def create_vendor_heatmap(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True):
    """거래처별 월별 활동 히트맵.
    - min_amount 이상인 거래처만 개별 표기
    - 나머지는 '기타'로 월별 합산(옵션)
    """
    if '거래처' not in ledger_df.columns or ledger_df['거래처'].nunique() < 1:
        return None

    df = ledger_df.copy()
    df['연월'] = df['회계일자'].dt.to_period('M').astype(str)
    pivot = df.pivot_table(index='거래처', columns='연월', values='거래금액_절대값', aggfunc='sum').fillna(0)
    if pivot.empty:
        return None

    totals = pivot.sum(axis=1)
    if min_amount and min_amount > 0:
        above_idx = totals >= min_amount
    else:
        above_idx = totals >= 0  # 전부

    pivot_above = pivot.loc[above_idx].copy()
    # 내림차순 정렬(합계 기준)
    pivot_above['_tot_'] = pivot_above.sum(axis=1)
    pivot_above = pivot_above.sort_values('_tot_', ascending=False).drop(columns=['_tot_'])

    # '기타' 행 합산
    import pandas as pd
    pivot_final = pivot_above
    if include_others:
        below = pivot.loc[~above_idx]
        if not below.empty:
            etc_row = pd.DataFrame([below.sum(axis=0)], index=['기타'])
            pivot_final = pd.concat([pivot_above, etc_row], axis=0)

    import plotly.express as px
    fig = px.imshow(pivot_final, title="거래처 월별 활동 히트맵", labels=dict(x="연월", y="거래처", color="거래금액"))

    # 🔢 컬러바/툴팁 포맷
    fig.update_coloraxes(colorbar=dict(tickformat=',.0f'))
    fig.update_traces(hovertemplate='연월=%{x}<br>거래처=%{y}<br>거래금액=%{z:,.0f} 원<extra></extra>')
    return fig


def create_vendor_detail_figure(ledger_df: pd.DataFrame, vendor_name: str, all_months: List[str]):
    """특정 거래처의 월별 거래액을 계정별 누적 막대그래프로 생성합니다. (전체 기간 X축 보장)"""
    vendor_df = ledger_df[ledger_df['거래처'] == vendor_name].copy()

    vendor_df['연월'] = vendor_df['회계일자'].dt.to_period('M').astype(str)
    summary = vendor_df.groupby(['연월', '계정명'], as_index=False)['거래금액_절대값'].sum()

    unique_accounts = vendor_df['계정명'].unique()

    # 축 제목 정의
    axis_labels = {'연월': '거래월', '거래금액_절대값': '거래금액'}

    if len(unique_accounts) == 0:
        empty_df = pd.DataFrame({'연월': all_months, '계정명': [None] * len(all_months), '거래금액_절대값': [0] * len(all_months)})
        fig = px.bar(empty_df, x='연월', y='거래금액_절대값', labels=axis_labels)
    else:
        template_df = pd.DataFrame(list(product(all_months, unique_accounts)), columns=['연월', '계정명'])
        merged_summary = pd.merge(template_df, summary, on=['연월', '계정명'], how='left').fillna(0)
        fig = px.bar(
            merged_summary,
            x='연월', y='거래금액_절대값', color='계정명',
            category_orders={'연월': all_months},
            labels=axis_labels,
        )

    fig.update_layout(
        barmode='stack',
        title=f"'{vendor_name}' 거래처 월별/계정별 상세 내역"
    )
    # 🔢 축/툴팁 포맷: 천단위 쉼표, SI 제거
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none')
    fig.update_traces(hovertemplate='연월=%{x}<br>금액=%{y:,.0f} 원<br>계정명=%{fullData.name}<extra></extra>')
    return fig


def run_vendor_module(lf: LedgerFrame, account_codes: List[str] | None = None,
                      min_amount: float = 0, include_others: bool = True) -> ModuleResult:
    """거래처 모듈: 선택 계정 필터 + 최소 거래금액 필터('기타' 합산)."""
    df = lf.df
    use_df = df.copy()
    if account_codes:
        acs = [str(a) for a in account_codes]
        use_df = use_df[use_df['계정코드'].astype(str).isin(acs)]

    figures: Dict[str, Any] = {}
    warnings: List[str] = []

    pareto = create_pareto_figure(use_df, min_amount=min_amount, include_others=include_others)
    heatmap = create_vendor_heatmap(use_df, min_amount=min_amount, include_others=include_others)

    if pareto: figures['pareto'] = pareto
    else: warnings.append("Pareto 그래프 생성 불가(데이터 부족).")
    if heatmap: figures['heatmap'] = heatmap
    else: warnings.append("히트맵 생성 불가(데이터 부족).")

    # 요약 정보
    cy = use_df[use_df['연도'] == use_df['연도'].max()]
    vendor_sum = cy.groupby('거래처')['거래금액_절대값'].sum() if not cy.empty else pd.Series(dtype=float)
    n_above = int((vendor_sum >= min_amount).sum()) if not vendor_sum.empty else 0
    n_below = int((vendor_sum < min_amount).sum()) if not vendor_sum.empty else 0

    summary = {
        "filtered_accounts": [str(a) for a in account_codes] if account_codes else [],
        "min_amount": float(min_amount),
        "include_others": bool(include_others),
        "n_above_threshold": n_above,
        "n_below_threshold": n_below,
        "n_figures": len(figures),
        "period_tag_coverage": dict(use_df['period_tag'].value_counts()) if 'period_tag' in use_df.columns else {},
    }
    return ModuleResult(
        name="vendor",
        summary=summary,
        tables={},
        figures=figures,
        evidences=[],
        warnings=warnings
    )



==============================
📄 FILE: analysis/__init__.py
==============================






==============================
📄 FILE: infra/env_loader.py
==============================

from __future__ import annotations
import os, sys
from typing import Optional


def _read_kv_file(path: str) -> dict:
    # 단순 .env 파서 (python-dotenv 없이도 작동)
    data = {}
    if not os.path.exists(path):
        return data
    with open(path, "r", encoding="utf-8") as f:
        for raw in f:
            line = raw.strip()
            if not line or line.startswith("#"):
                continue
            if "=" not in line:
                continue
            k, v = line.split("=", 1)
            k = k.strip()
            v = v.strip().strip("\"'")  # 양쪽 따옴표 제거
            data[k] = v
    return data


def _maybe_import_dotenv():
    try:
        from dotenv import load_dotenv  # type: ignore
        return load_dotenv
    except Exception:
        return None


# 동의어 키 -> 표준 키 정규화
_OPENAI_ALIASES = [
    "OPENAI_API_KEY", "OPENAI_KEY", "OPENAI_TOKEN", "OPENAIAPIKEY", "OPENAI_APIKEY",
    # Azure/OpenAI 변형들 (있으면 그대로도 허용)
    "AZURE_OPENAI_API_KEY",
]


def _normalize_env(d: dict) -> None:
    # 표준 키가 없고, 동의어가 있으면 끌어와서 OPENAI_API_KEY 세팅
    if not d.get("OPENAI_API_KEY"):
        for k in _OPENAI_ALIASES:
            if k in d and d[k]:
                d["OPENAI_API_KEY"] = d[k]
                break


def ensure_api_keys_loaded() -> bool:
    # 1) python-dotenv가 있으면 먼저 시도
    load_dotenv = _maybe_import_dotenv()
    if load_dotenv:
        # 두 파일을 모두 시도(존재하는 것만 적용)
        for p in (".env", "API_KEY.env"):
            try:
                load_dotenv(dotenv_path=p, override=False)
            except Exception:
                pass

    # 2) 수동 파싱 (dotenv가 없거나, 못 읽은 경우 대비)
    merged = {}
    for p in (".env", "API_KEY.env"):
        try:
            merged.update(_read_kv_file(p))
        except Exception:
            pass

    # 3) 동의어 정규화 → OPENAI_API_KEY
    _normalize_env(merged)

    # 4) 환경변수에 반영(존재하지 않는 경우에만 세팅)
    for k, v in merged.items():
        if k not in os.environ and v:
            os.environ[k] = v

    ok = bool(os.environ.get("OPENAI_API_KEY") or os.environ.get("AZURE_OPENAI_API_KEY"))
    # 가시적 플래그
    os.environ["LLM_AVAILABLE"] = "1" if ok else "0"
    return ok


def is_llm_ready() -> bool:
    return os.environ.get("LLM_AVAILABLE", "0") == "1"


def log_llm_status(logger=None):
    # 한국어 상태 로그
    if is_llm_ready():
        msg = "🔌 OpenAI Key 감지: 온라인 LLM 모드로 생성합니다. (클러스터/요약 LLM 사용)"
    else:
        msg = "🔌 OpenAI Key 없음: 오프라인 리포트 모드로 생성합니다. (클러스터/요약 LLM 미사용)"
    if logger:
        try:
            logger.info(msg)
            return
        except Exception:
            pass
    print(msg, file=sys.stderr)


# 앱 부팅 시 사용할 진입점 (import만으로 부팅초기화하고 싶을 때)
def boot():
    ensure_api_keys_loaded()
    log_llm_status()





==============================
📄 FILE: services/cache.py
==============================

# services/cache.py  (전체 교체본)
# - 임베딩 캐시(SQLite)
# - LLM 매핑 캐시(승인/버전)
# - 캐시 정보 헬퍼

from __future__ import annotations
import os, sqlite3, json, hashlib, threading, time
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from config import EMBED_CACHE_DIR

# 모델별 DB 파일 접근 시 레이스를 막기 위한 락 맵
_LOCKS: Dict[str, threading.Lock] = {}

def _model_dir(model: str) -> Path:
    # 모델명을 안전한 폴더명으로 변환
    safe = model.replace("/", "_").replace(":", "_")
    d = Path(EMBED_CACHE_DIR) / safe
    d.mkdir(parents=True, exist_ok=True)
    return d

def _db_path(model: str) -> str:
    return str(_model_dir(model) / "embeddings.sqlite3")

def _sha1(s: str) -> str:
    # 보안용이 아닌 키 해싱(캐시 키용)
    return hashlib.sha1(s.encode("utf-8"), usedforsecurity=False).hexdigest()

def _get_lock(model: str) -> threading.Lock:
    if model not in _LOCKS:
        _LOCKS[model] = threading.Lock()
    return _LOCKS[model]

def _ensure_db(conn: sqlite3.Connection):
    # 기본 테이블 스키마 보장
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS emb (
            k TEXT PRIMARY KEY,
            text TEXT,
            vec TEXT
        )
    """)
    conn.commit()

def _open(model: str) -> sqlite3.Connection:
    p = _db_path(model)
    conn = sqlite3.connect(p, timeout=30)
    _ensure_db(conn)
    return conn

def cache_get_many(model: str, texts: List[str]) -> Dict[str, List[float]]:
    # 여러 텍스트에 대한 캐시 조회
    if not texts:
        return {}
    keys = [(t, _sha1(f"{model}|{t}")) for t in texts]
    with _get_lock(model):
        conn = _open(model)
        try:
            cur = conn.cursor()
            qmarks = ",".join("?" for _ in keys)
            cur.execute(f"SELECT k, vec FROM emb WHERE k IN ({qmarks})", [k for _, k in keys])
            rows = {k: json.loads(vec) for (k, vec) in cur.fetchall()}
        finally:
            conn.close()
    out: Dict[str, List[float]] = {}
    for t, k in keys:
        if k in rows:
            out[t] = rows[k]
    return out

def cache_put_many(model: str, pairs: List[Tuple[str, List[float]]]) -> None:
    # 여러 텍스트-벡터 쌍을 캐시에 저장
    if not pairs:
        return
    records = [(_sha1(f"{model}|{t}"), t, json.dumps(vec)) for (t, vec) in pairs]
    with _get_lock(model):
        conn = _open(model)
        try:
            conn.executemany("INSERT OR REPLACE INTO emb(k,text,vec) VALUES (?,?,?)", records)
            conn.commit()
        finally:
            conn.close()

def get_or_embed_texts(
    texts: List[str],
    client,
    *,
    model: str,
    batch_size: int,
    timeout: int,
    max_retry: int,
) -> Dict[str, List[float]]:
    # 텍스트 목록에 대해 캐시를 우선 조회하고, 누락분만 임베딩 API 호출
    texts = list(dict.fromkeys([str(t) for t in texts]))
    cached = cache_get_many(model, texts)
    missing = [t for t in texts if t not in cached]
    if not missing:
        return cached
    out = dict(cached)
    for s in range(0, len(missing), batch_size):
        sub = missing[s:s+batch_size]
        last_err = None
        for attempt in range(max_retry):
            try:
                try:
                    resp = client.embeddings.create(model=model, input=sub, timeout=timeout)
                except TypeError:
                    # 일부 클라이언트는 timeout 파라미터를 지원하지 않음
                    resp = client.embeddings.create(model=model, input=sub)
                vecs = [d.embedding for d in resp.data]
                pairs = list(zip(sub, vecs))
                cache_put_many(model, pairs)
                out.update({sub[i]: vecs[i] for i in range(len(sub))})
                last_err = None
                break
            except Exception as e:
                last_err = e
        if last_err:
            # 재시도 실패 시, 마지막 예외를 전파
            raise last_err
    return out

# ===== LLM 매핑 캐시 (승인/버전 고정) =====
DEFAULT_CACHE_PATH = os.path.join(".cache", "llm_mappings.json")

class LLMMappingCache:
    # 간단한 파일 기반 승인/버전 관리
    def __init__(self, path: str = DEFAULT_CACHE_PATH):
        self.path = path
        self._lock = threading.Lock()
        self._state: Dict[str, Any] = {"approved": {}, "proposed": {}, "versions": {}}
        self._load()

    def _load(self) -> None:
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        if os.path.exists(self.path):
            try:
                with open(self.path, "r", encoding="utf-8") as f:
                    self._state = json.load(f)
            except Exception:
                # 파손 파일은 조용히 무시
                pass

    def _save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self._state, f, ensure_ascii=False, indent=2)

    def get_approved(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["approved"].get(key)
            return dict(item) if item else None

    def get_proposed(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["proposed"].get(key)
            return dict(item) if item else None

    def propose(self, key: str, value: Any, *, model: str, meta: Optional[Dict[str, Any]] = None) -> None:
        with self._lock:
            self._state["proposed"][key] = {
                "value": value,
                "model": model,
                "meta": meta or {},
                "ts": time.time(),
            }
            self._save()

    def approve(self, key: str, *, value_override: Any | None = None, note: str = "") -> Dict[str, Any]:
        # 초안을 승인하여 버전을 올리고 확정
        with self._lock:
            src = self._state["proposed"].get(key) or self._state["approved"].get(key)
            if not src:
                raise KeyError(f"No proposed/approved entry for key={key!r}")
            prev_ver = int(self._state["versions"].get(key, 0))
            new_ver = prev_ver + 1
            final_value = src["value"] if value_override is None else value_override
            rec = {
                "value": final_value,
                "version": f"v{new_ver}",
                "note": note,
                "approved_ts": time.time(),
                "model": src.get("model"),
                "meta": src.get("meta", {}),
            }
            self._state["approved"][key] = rec
            self._state["versions"][key] = new_ver
            if key in self._state["proposed"]:
                del self._state["proposed"][key]
            self._save()
            return dict(rec)

# ===== 임베딩 캐시 정보 헬퍼 (app.py 사이드바 등에서 사용) =====
def get_cache_info(model: str) -> Dict[str, Any]:
    p = _db_path(model)
    info = {"model": model, "path": p, "exists": os.path.exists(p)}
    if not os.path.exists(p):
        return info
    try:
        conn = sqlite3.connect(p, timeout=10)
        _ensure_db(conn)
        try:
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) FROM emb")
            nrows = int(cur.fetchone()[0])
            size = os.path.getsize(p)
            info.update({"rows": nrows, "size_bytes": size})
        finally:
            conn.close()
    except Exception as e:
        info["error"] = str(e)
    return info



==============================
📄 FILE: services/cycles_store.py
==============================

from __future__ import annotations
import json, os
from typing import Dict, List
from config import STANDARD_ACCOUNTING_CYCLES, CYCLES_USER_OVERRIDES_PATH


def _load_overrides() -> Dict[str, List[str]]:
    p = CYCLES_USER_OVERRIDES_PATH
    if not p or not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        return {str(k): [str(x) for x in v] for k, v in data.items() if isinstance(v, list)}
    except Exception:
        # 잘못된 파일/JSON은 무시
        return {}


def get_effective_cycles() -> Dict[str, List[str]]:
    # 프리셋을 복사하고, 동일 키의 항목은 사용자 설정으로 덮어씀
    base = {k: list(v) for k, v in STANDARD_ACCOUNTING_CYCLES.items()}
    ov = _load_overrides()
    for k, v in ov.items():
        base[k] = list(v)
    return base




==============================
📄 FILE: services/external.py
==============================




==============================
📄 FILE: services/llm.py
==============================

"""
# services/llm.py
# - 하이브리드 클라이언트: 키가 있으면 OpenAI 온라인 모드, 없으면 오프라인 스텁
"""

import os
from functools import lru_cache


def openai_available() -> bool:
    try:
        # 부팅 시 env_loader가 LLM_AVAILABLE 플래그를 셋업
        from infra.env_loader import ensure_api_keys_loaded, is_llm_ready
        ensure_api_keys_loaded()
        return bool(is_llm_ready())
    except Exception:
        # 직접 환경변수 확인 (폴백)
        return bool(os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY"))


class _DummyEmbeddings:
    def create(self, *args, **kwargs):
        raise RuntimeError("Embeddings API not available in offline stub.")


class _DummyChat:
    class _Resp:
        class Choice:
            class Msg:
                content = ""
            message = Msg()
        choices = [Choice()]

    def completions(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")

    def completions_create(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")


class _DummyClient:
    embeddings = _DummyEmbeddings()
    chat = _DummyChat()


@lru_cache(maxsize=1)
def _openai_client():
    try:
        from openai import OpenAI  # type: ignore
    except Exception as e:
        return None
    # 키는 env_loader가 이미 주입함
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY")
    try:
        return OpenAI(api_key=api_key) if api_key else OpenAI()
    except Exception:
        return None


class LLMClient:
    def __init__(self, model: str | None = None, temperature: float | None = None, json_mode: bool | None = None):
        self._online = openai_available() and (_openai_client() is not None)
        self.client = _openai_client() if self._online else _DummyClient()
        # 호출 편의 파라미터 저장(online generate에서 사용)
        self.model = model or os.getenv("LLM_MODEL", "gpt-4o")
        try:
            self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.2" if temperature is None else str(temperature)))
        except Exception:
            self.temperature = 0.2
        self.json_mode = True if (os.getenv("LLM_JSON_MODE", "true").lower() in ("1","true","yes")) else False

    def generate(self, system: str, user: str, tools=None, *, model: str | None = None, max_tokens: int | None = None, force_json: bool | None = None) -> str:
        if not self._online or self.client is None:
            raise RuntimeError("LLM not available: no API key or client. Run in offline mode.")
        try:
            use_model = model or self.model
            kwargs = dict(
                model=use_model,
                temperature=float(self.temperature),
                messages=[{"role":"system","content":system},{"role":"user","content":user}],
            )
            if max_tokens is not None:
                kwargs["max_tokens"] = int(max_tokens)
            if tools:
                kwargs["tools"] = tools
                try:
                    first_tool = tools[0]["function"]["name"]
                    if first_tool:
                        kwargs["tool_choice"] = {"type": "function", "function": {"name": first_tool}}
                except Exception:
                    pass
            else:
                use_force_json = self.json_mode if force_json is None else bool(force_json)
                if use_force_json:
                    kwargs["response_format"] = {"type": "json_object"}

            try:
                resp = self.client.chat.completions.create(**kwargs, timeout=60)
            except TypeError:
                resp = self.client.chat.completions.create(**kwargs)
            msg = resp.choices[0].message
            try:
                tool_calls = getattr(msg, "tool_calls", None)
                if tool_calls:
                    call = tool_calls[0]
                    args = getattr(call.function, "arguments", None)
                    return (args or "").strip()
            except Exception:
                pass
            return (msg.content or "").strip()
        except Exception as e:
            raise





==============================
📄 FILE: services/__init__.py
==============================






==============================
📄 FILE: tests/conftest.py
==============================

# tests/conftest.py
import os, sys
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)



==============================
📄 FILE: tests/test_anomaly.py
==============================

# Z-Score 및 위험 점수 단조성 테스트
import pandas as pd
from analysis.anomaly import calculate_grouped_stats_and_zscore, _risk_from

def test_zscore_and_risk_monotonic():
    df = pd.DataFrame({
        "계정코드": ["100"]*5 + ["200"]*5,
        "차변":     [0,0,0,0,0] + [0,0,0,0,0],
        "대변":     [10,20,30,40,50] + [5,5,5,5,5],
    })
    out = calculate_grouped_stats_and_zscore(df, target_accounts=["100","200"])
    assert "Z-Score" in out.columns
    a1 = _risk_from(1.0, amount=1_000, pm=100_000)[-1]
    a2 = _risk_from(3.0, amount=1_000, pm=100_000)[-1]
    b1 = _risk_from(1.0, amount= 50_000, pm=100_000)[-1]
    b2 = _risk_from(1.0, amount=150_000, pm=100_000)[-1]
    assert a2 > a1
    assert b2 > b1





==============================
📄 FILE: tests/test_assertion_matrix.py
==============================

from analysis.contracts import EvidenceDetail, ModuleResult
from analysis.assertion_risk import build_matrix
import pandas as pd


def _ev(row_id, acct, assertions, score):
    return EvidenceDetail(
        row_id=row_id, reason="t", anomaly_score=0.0, financial_impact=0.0,
        risk_score=score, is_key_item=False, impacted_assertions=assertions,
        links={"account_name": acct}
    )


def test_build_matrix_max_aggregation():
    mod = ModuleResult(
        name="anomaly",
        summary={}, tables={}, figures={},
        evidences=[
            _ev("r1", "매출", ["A","E"], 0.40),
            _ev("r2", "매출", ["E"],     0.65),
            _ev("r3", "매입", ["C"],     0.30),
        ],
        warnings=[]
    )
    mat, emap = build_matrix([mod])
    assert float(mat.loc["매출","E"]) == 0.65   # 동일 셀은 최대값
    assert float(mat.loc["매출","A"]) == 0.40
    assert float(mat.loc["매입","C"]) == 0.30
    assert ("매출","E") in emap and set(emap[("매출","E")]) == {"r1","r2"}




==============================
📄 FILE: tests/test_evidence_schema.py
==============================

from dataclasses import asdict
from analysis.anomaly import run_anomaly_module
from analysis.contracts import LedgerFrame
import pandas as pd


def test_anomaly_emits_evidence_minimal():
    # 간단 가짜 DF
    df = pd.DataFrame({
        "row_id":["a","b","c"],
        "회계일자": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03"]),
        "계정코드": ["400","400","400"],
        "계정명":   ["매출","매출","매출"],
        "차변": [0, 0, 0],
        "대변": [10_000_000, 100, 50],
    })
    lf = LedgerFrame(df=df, meta={})
    mod = run_anomaly_module(lf, target_accounts=["400"], topn=2, pm_value=500_000_000)
    assert isinstance(mod.evidences, list)
    assert len(mod.evidences) >= 1
    d = asdict(mod.evidences[0])
    for key in ["row_id","reason","anomaly_score","financial_impact","risk_score","is_key_item","impacted_assertions","links"]:
        assert key in d




==============================
📄 FILE: tests/test_kdmeans_basic.py
==============================

# 간단 단위테스트: KDMeans가 잘 학습되고 속성들이 채워지는지 확인
import numpy as np
from analysis.kdmeans_shim import HDBSCAN

def test_kdmeans_fixed_k():
    rng = np.random.default_rng(0)
    X = np.vstack([
        rng.normal(loc=[0,0], scale=0.1, size=(25,2)),
        rng.normal(loc=[3,3], scale=0.1, size=(25,2)),
    ])
    model = HDBSCAN(n_clusters=2, random_state=0).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ == 2
    assert set(model.labels_) == {0,1}
    assert np.allclose(model.probabilities_, 1.0)

def test_kdmeans_auto_k_runs():
    rng = np.random.default_rng(1)
    X = rng.normal(size=(200, 4))
    model = HDBSCAN(n_clusters=None, random_state=1).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ is not None




==============================
📄 FILE: tests/test_report_units.py
==============================

# 리포트 내 원(₩) 단위 강제 변환 테스트
from analysis.report import _enforce_won_units

def test_unit_enforcement():
    s = "총액은 3억 5,072만 원이며 이전에는 2억 원이었다."
    out = _enforce_won_units(s)
    assert "350,720,000원" in out
    assert "200,000,000원" in out





==============================
📄 FILE: tests/test_risk_boundaries.py
==============================

import math
import pytest
from analysis.anomaly import _risk_from


@pytest.mark.parametrize("z_abs", [0.0, 1.0, 3.0, 10.0])
@pytest.mark.parametrize("pm_value", [0.0, 1.0, 1e6, 1e12])
def test_risk_from_boundary_is_finite(z_abs, pm_value):
    a, f, k, score = _risk_from(z_abs=z_abs, amount=1_000_000, pm=pm_value)
    assert 0.0 <= score <= 1.0
    assert math.isfinite(score)


def test_risk_from_monotonic_in_z_when_pm_fixed():
    pm = 1e6
    s1 = _risk_from(0.5, amount=1_000_000, pm=pm)[-1]
    s2 = _risk_from(3.0, amount=1_000_000, pm=pm)[-1]
    s3 = _risk_from(10.0, amount=1_000_000, pm=pm)[-1]
    assert s1 <= s2 <= s3


@pytest.mark.parametrize("pm_lo,pm_hi", [(0.0, 1e9)])
def test_risk_from_non_decreasing_in_pm(pm_lo, pm_hi):
    z = 3.0
    slo = _risk_from(z, amount=1_000_000, pm=pm_lo)[-1]
    shi = _risk_from(z, amount=1_000_000, pm=pm_hi)[-1]
    assert slo <= shi




==============================
📄 FILE: tests/test_risk_score.py
==============================

import math
from analysis.anomaly import _risk_from
from config import Z_SIGMOID_SCALE


def test_risk_from_basic():
    a, f, k, score = _risk_from(z_abs=3.0, amount=1_000_000_000, pm=500_000_000)
    exp_a = 1.0 / (1.0 + math.exp(-(3.0/float(Z_SIGMOID_SCALE or 1.0))))
    assert abs(a - exp_a) < 1e-9
    assert f == 1.0                 # PM 대비 캡 1
    assert k == 1.0                 # KIT
    # 가중합: 0.5*a + 0.4*1 + 0.1*1
    expected = 0.5*a + 0.4 + 0.1
    assert abs(score - expected) < 1e-9


def test_risk_from_zero_pm_guard():
    a, f, k, score = _risk_from(z_abs=0.0, amount=0, pm=0)
    assert f == 0.0 and k == 0.0    # 분모 가드 동작
    assert 0.0 <= a <= 0.5          # z=0이면 a는 0.5 근처




==============================
📄 FILE: tests/test_risk_score_more.py
==============================

from analysis.anomaly import _risk_from
from analysis.anomaly import _assertions_for_row


def test_risk_from_none_and_negative_pm():
    for pm in (None, -1, -1000):
        a, f, k, score = _risk_from(z_abs=2.0, amount=1_000_000, pm=pm)
        assert f == 0.0 and k == 0.0
        assert 0.0 <= a <= 1.0
        assert 0.0 <= score <= 1.0

def test_assertions_mapping_rules():
    # 항상 A 포함
    assert "A" in _assertions_for_row(0.0)
    # 큰 양의 이탈 → E 포함
    assert set(_assertions_for_row(+2.5)) >= {"A","E"}
    # 큰 음의 이탈 → C 포함
    assert set(_assertions_for_row(-2.5)) >= {"A","C"}




==============================
📄 FILE: tests/test_snapshot_core.py
==============================

import pandas as pd
from dataclasses import asdict
from analysis.contracts import LedgerFrame
from analysis.anomaly import run_anomaly_module
from analysis.assertion_risk import build_matrix


def _mini_df():
    df = pd.DataFrame({
        "row_id": ["file|L:2","file|L:3","file|L:4","file|L:5"],
        "회계일자": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03","2024-01-04"]),
        "계정코드": ["101","101","201","201"],
        "계정명":   ["현금","현금","매출","매출"],
        "차변": [0, 0, 0, 0],
        "대변": [5_000_000, 100, 50_000_000, 200],
    })
    return df


def test_snapshot_evidence_and_matrix_stable():
    lf = LedgerFrame(df=_mini_df(), meta={})
    mod = run_anomaly_module(lf, target_accounts=["101","201"], topn=10, pm_value=500_000_000)
    # Evidence 스냅샷(핵심 필드만 비교)
    snap = [{
        "row_id": e.row_id,
        "risk_score": round(e.risk_score, 6),
        "is_key_item": e.is_key_item,
        "assertions": tuple(e.impacted_assertions),
        "acct": e.links.get("account_name") or e.links.get("account_code")
    } for e in mod.evidences]
    # 고정 기대값(리스크 가중치/PM이 바뀌면 실패하도록)
    assert any(s["acct"] == "매출" for s in snap)
    # 매트릭스 스냅샷
    mat, _ = build_matrix([mod])
    assert 0.0 <= float(mat.values.max()) <= 1.0




==============================
📄 FILE: tests/test_timeseries_naive.py
==============================

# 시계열 모듈의 간단 백엔드(EMA 등) 동작성 테스트
import pandas as pd
import numpy as np
from analysis.timeseries import run_timeseries_module, model_registry, z_and_risk, run_timeseries_for_account


def test_timeseries_naive_backend():
    df = pd.DataFrame({
        "account": ["A"]*7 + ["B"]*7,
        "date":    list(range(1,8)) + list(range(1,8)),
        "amount":  [10,11,10,12,11,10, 20] + [8,8,8,8,8,8, 5],
    })
    res = run_timeseries_module(df, account_col="account", date_col="date",
                                amount_col="amount", backend="ema", window=3)
    assert set(res["account"]) == {"A","B"}
    assert set(res["assertion"]).issubset({"E","C"})


def test_z_and_risk_basic():
    # 한글: 0 중심 대칭 잔차에 대해 |z|가 크면 risk가 커진다
    resid = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])
    z, r = z_and_risk(resid)
    assert len(z) == len(resid)
    assert len(r) == len(resid)
    assert float(r[0]) > float(r[1])
    assert float(r[-1]) == float(r[0])


def test_model_registry_keys_present():
    reg = model_registry()
    for k in ["ma","ema","arima","prophet"]:
        assert k in reg
    assert reg["ma"] is True and reg["ema"] is True


def test_run_timeseries_for_account_dual():
    # 한글: 12개월 샘플로 flow 누적 balance 생성하여 두 트랙 모두 반환
    dates = pd.period_range("2024-01", periods=12, freq="M").to_timestamp()
    flow = np.linspace(100, 210, num=12)
    df = pd.DataFrame({"date": dates, "flow": flow})
    df["balance"] = df["flow"].cumsum()
    out = run_timeseries_for_account(df, account="매출채권", is_bs=True, flow_col="flow", balance_col="balance")
    assert set(out["measure"]).issuperset({"flow","balance"})
    assert set(out.columns) == {"date","account","measure","actual","predicted","error","z","risk","model"}





==============================
📄 FILE: tests/test_zbins.py
==============================

import numpy as np, pandas as pd
from analysis.anomaly import _z_bins_025_sigma


def test_zbins_label_and_count():
    s = pd.Series(np.linspace(-4, 4, 101))
    df, order = _z_bins_025_sigma(s)
    assert len(df) == len(order) == 26      # 테일 포함 26개
    assert int(df["건수"].sum()) == 101     # 총합 보존




==============================
📄 FILE: ui/inputs.py
==============================

import re
import streamlit as st

# KRW 입력 위젯(천단위 쉼표) - 안정형
# - 사용자는 자유롭게 타이핑(쉼표/공백/문자 섞여도 무시)하고,
#   포커스 아웃/엔터 시에만 정규화(숫자만 유지 → 쉼표 포맷)합니다.
# - 실제 숫자값은 session_state[key] (int)로 보관합니다.
# - 표시용 문자열은 session_state[f"{key}__txt"] 로 관리합니다.

def _parse_krw_text(s: str) -> int:
    """문자열에서 숫자만 추출하여 안전하게 int로 변환(음수 방어, 공란=0)."""
    if s is None:
        return 0
    s = str(s).replace(",", "").strip()
    s = re.sub(r"[^\d]", "", s)  # 숫자 이외 제거
    if s == "":
        return 0
    try:
        return max(0, int(s))
    except Exception:
        return 0

def _fmt_krw(n: int) -> str:
    """정수를 천단위 쉼표 문자열로 포맷."""
    try:
        return f"{int(n):,}"
    except Exception:
        return "0"

def krw_input(label: str, key: str, default_value: int = 0, help_text: str = "") -> int:
    """
    KRW 입력(천단위 쉼표) 통합 위젯.
    - 숫자 상태: st.session_state[key] (int)
    - 표시 상태: st.session_state[f"{key}__txt"] (str, '1,234,567')
    - on_change 시에만 정규화하여 잔고장(500,00 등) 방지
    """
    # 초기 상태 보정
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if f"{key}__txt" not in st.session_state:
        st.session_state[f"{key}__txt"] = _fmt_krw(st.session_state[key])

    def _commit():
        """사용자 입력 완료(포커스 아웃/엔터) 시 숫자/문자 상태 동기화."""
        raw = st.session_state.get(f"{key}__txt", "")
        val = _parse_krw_text(raw)
        st.session_state[key] = val
        st.session_state[f"{key}__txt"] = _fmt_krw(val)
        # Streamlit은 on_change 후 자동 rerun → 그래프/표 갱신에 충분

    # 표시 입력창(타이핑 중에는 포맷 강제하지 않음)
    st.text_input(
        label,
        key=f"{key}__txt",
        help=help_text,
        placeholder="예: 500,000,000",
        on_change=_commit,
    )
    return int(st.session_state.get(key, int(default_value)))





==============================
📄 FILE: ui/__init__.py
==============================

# 패키지 초기화 (비어있어도 무방)




==============================
📄 FILE: utils/drilldown.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional


def ensure_rowid(df: pd.DataFrame, id_col: str = "row_id") -> pd.DataFrame:
    """row_id 컬럼이 없으면 생성하지 않고 그대로 반환(계약 준수는 상위 단계에서)."""
    return df if id_col in df.columns else df


def attach_customdata(df: pd.DataFrame, cols: Iterable[str], id_col: str = "row_id"):
    """
    Plotly에 올릴 customdata 배열 생성.
    반환: (df, customdata(ndarray), header_labels(list))
    """
    import numpy as np
    use_cols = [c for c in cols if c in df.columns]
    if id_col not in use_cols and id_col in df.columns:
        use_cols = [id_col] + use_cols
    arr = df[use_cols].to_numpy()
    return df, np.asarray(arr), use_cols


def fmt_money(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return str(x)





==============================
📄 FILE: utils/helpers.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional

def find_column_by_keyword(columns: Iterable[str], keyword: str) -> Optional[str]:
    """열 이름에서 keyword(부분일치, 대소문자 무시)를 우선 탐색."""
    keyword = str(keyword or "").lower()
    cols = [str(c) for c in columns]
    # 1) 완전 일치 우선
    for c in cols:
        if c.lower() == keyword:
            return c
    # 2) 부분 일치
    for c in cols:
        if keyword in c.lower():
            return c
    return None

def add_provenance_columns(df: pd.DataFrame) -> pd.DataFrame:
    """업로드 출처 정보가 없어도 row_id를 강제로 부여."""
    out = df.copy()
    if "row_id" not in out.columns:
        out["row_id"] = out.reset_index().index.astype(str)
    return out

def add_period_tag(df: pd.DataFrame) -> pd.DataFrame:
    """연도 최대값 기준으로 CY/PY/Other 태그를 부여."""
    out = df.copy()
    if "연도" not in out.columns:
        out["period_tag"] = "Other"
        return out
    y_max = out["연도"].max()
    out["period_tag"] = out["연도"].apply(lambda y: "CY" if y == y_max else ("PY" if y == y_max - 1 else "Other"))
    return out




==============================
📄 FILE: utils/viz.py
==============================

# utils/viz.py
# 목적: Materiality(Performance Materiality, PM) 보조선/배지 추가 유틸
# - Plotly Figure에 빨간 점선(가로선) + "PM=xxx원" 라벨을 안전하게 추가
# - 현재 y축 범위에 PM이 없으면 축을 자동 확장해서 선이 보이게 함
# - Pareto(보조축 있음)에서도 1차 y축에 정확히 그려줌

from __future__ import annotations
from typing import Optional
import math


def _is_plotly_fig(fig) -> bool:
    try:
        # 지연 임포트 (환경에 plotly 미설치일 때도 함수 자체는 import 가능하도록)
        import plotly.graph_objects as go  # noqa: F401
        from plotly.graph_objs import Figure
        return isinstance(fig, Figure)
    except Exception:
        return False


def _get_primary_y_data_bounds(fig):
    """
    1차 y축 데이터의 (min, max) 추정.
    - secondary_y=True로 올라간 trace는 제외
    - trace.y가 수치 배열일 때만 집계
    """
    ymin, ymax = math.inf, -math.inf
    for tr in getattr(fig, "data", []):
        # 보조축 여부: trace.yaxis 가 'y2'/'y3'... 이면 보조축
        yaxis = getattr(tr, "yaxis", "y")
        if yaxis and str(yaxis).lower() != "y":  # 'y2' 등은 제외
            continue
        y = getattr(tr, "y", None)
        if y is None:
            continue
        try:
            for v in y:
                if v is None:
                    continue
                fv = float(v)
                if math.isfinite(fv):
                    ymin = min(ymin, fv)
                    ymax = max(ymax, fv)
        except Exception:
            # 숫자 배열이 아니면 스킵
            continue
    if ymin is math.inf:  # 데이터가 비어있는 경우
        return (0.0, 0.0)
    return (ymin, ymax)


def _ensure_y_contains(fig, y_value: float, pad_ratio: float = 0.05):
    """
    y_value가 y축 범위에 포함되도록 레이아웃을 조정.
    - 기존 auto-range라도 PM이 축 밖이면 강제로 range 부여
    - pad_ratio만큼 여유를 둬서 라벨이 잘리지 않게 함
    """
    if not math.isfinite(y_value):
        return
    # 현재 1차 y축 데이터 범위 추정
    ymin_data, ymax_data = _get_primary_y_data_bounds(fig)
    # 데이터가 전부 음수이거나 전부 양수일 수 있음 → PM이 더 큰 쪽에 있으면 확장
    base_min = min(0.0, ymin_data) if math.isfinite(ymin_data) else 0.0
    base_max = max(0.0, ymax_data) if math.isfinite(ymax_data) else 0.0
    tgt_min = min(base_min, y_value)
    tgt_max = max(base_max, y_value)
    if tgt_min == tgt_max:
        # 완전 평평하면 살짝 폭 추가
        span = abs(y_value) if y_value != 0 else 1.0
        tgt_min -= span * 0.5
        tgt_max += span * 0.5
    # 여유 패딩
    span = (tgt_max - tgt_min) or 1.0
    pad = span * float(pad_ratio)
    final_min = tgt_min - pad
    final_max = tgt_max + pad
    # yaxis는 레이아웃 키 'yaxis' (서브플롯 아닌 기본 도면 기준)
    if "yaxis" not in fig.layout:
        fig.update_layout(yaxis=dict(range=[final_min, final_max]))
    else:
        fig.layout.yaxis.update(range=[final_min, final_max])


def add_materiality_threshold(fig, pm_value: Optional[float], *, label: bool = True):
    """
    Plotly Figure에 PM 가로 점선 + 라벨 추가.
    - pm_value가 None/0/음수면 아무 것도 하지 않음
    - Pareto(보조축)도 1차 y축에 라인을 그림 (yref='y')
    - 축 범위를 자동 확장해서 항상 보이게 함
    반환: 동일 Figure (in-place 수정 후)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    # y축 범위에 PM이 포함되도록 먼저 보장
    _ensure_y_contains(fig, pm, pad_ratio=0.08)

    # 점선 라인 추가
    # xref='paper'로 0~1 전폭에 걸쳐 수평선, yref='y'로 1차 y축 기준 고정
    line_shape = dict(
        type="line",
        xref="paper", x0=0, x1=1,
        yref="y",     y0=pm, y1=pm,
        line=dict(color="red", width=2, dash="dot"),
        layer="above"
    )
    shapes = list(fig.layout.shapes) if getattr(fig.layout, "shapes", None) else []
    shapes.append(line_shape)
    fig.update_layout(shapes=shapes)

    # 라벨(오른쪽 끝)
    if label:
        annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
        annotations.append(dict(
            x=1.0, xref="paper",
            y=pm, yref="y",
            xanchor="left", yanchor="bottom",
            text=f"PM {pm:,.0f}원",
            showarrow=False,
            font=dict(color="red", size=11),
            bgcolor="rgba(255,255,255,0.6)",
            bordercolor="red",
            borderwidth=0.5,
            align="left"
        ))
        fig.update_layout(annotations=annotations)

    return fig


def add_pm_badge(fig, pm_value: Optional[float], *, text: str | None = None):
    """
    Heatmap처럼 선을 긋기 애매한 그래프에 우측 상단 배지 추가.
    반환: 동일 Figure (in-place)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    label = text or f"PM {pm:,.0f}원"
    annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
    annotations.append(dict(
        x=0.995, xref="paper",
        y=0.995, yref="paper",
        xanchor="right", yanchor="top",
        text=label,
        showarrow=False,
        font=dict(color="red", size=11),
        bgcolor="rgba(255,255,255,0.6)",
        bordercolor="red",
        borderwidth=0.5,
        align="right"
    ))
    fig.update_layout(annotations=annotations)
    return fig





==============================
📄 FILE: utils/__init__.py
==============================




==============================
📄 FILE: viz/guides.py
==============================

# Materiality 가이드라인(붉은 점선) 유틸
# - matplotlib/plotly 모두 지원. 사용하는 라이브러리에 맞춰 호출만 붙이면 됨.

def add_materiality_lines_matplotlib(ax, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # y축 기준 수평선
    if y_threshold is not None and ax is not None:
        ax.axhline(y_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)  # 붉은 점선
        ax.text(ax.get_xlim()[0], y_threshold, f"{label_prefix}: {y_threshold:,.0f}",
                va="bottom", ha="left", fontsize=9, color="red", alpha=0.9)
    # x축 기준 수직선
    if x_threshold is not None and ax is not None:
        ax.axvline(x_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)
        ax.text(x_threshold, ax.get_ylim()[1], f"{label_prefix}: {x_threshold:,.0f}",
                va="top", ha="right", fontsize=9, color="red", alpha=0.9)


def add_materiality_lines_plotly(fig, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # Plotly Figure에 가이드라인 추가
    if fig is None:
        return fig
    if y_threshold is not None:
        try:
            fig.add_hline(y=y_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(xref="paper", x=0.0, y=y_threshold, yref="y",
                               text=f"{label_prefix}: {y_threshold:,.0f}",
                               showarrow=False, align="left", yanchor="bottom", font=dict(color="red", size=10))
        except Exception:
            pass
    if x_threshold is not None:
        try:
            fig.add_vline(x=x_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(yref="paper", y=1.0, x=x_threshold, xref="x",
                               text=f"{label_prefix}: {x_threshold:,.0f}",
                               showarrow=False, align="right", xanchor="right", font=dict(color="red", size=10))
        except Exception:
            pass
    return fig




