
==============================
ğŸ“„ FILE: app.py
==============================

# app_v0.17.py (ê±°ë˜ì²˜ ìƒì„¸ ë¶„ì„ ì˜¤ë¥˜ ìˆ˜ì •)
# --- BEGIN: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---
try:
    from infra.env_loader import boot as _llm_boot
    _llm_boot()  # í‚¤ ë¡œë“œ + ìƒíƒœ ë¡œê·¸
except Exception as _e:
    # ìµœì•…ì˜ ê²½ìš°ì—ë„ ì•±ì€ ëœ¨ê²Œ í•˜ê³ , ìƒíƒœë¥¼ stderrë¡œë§Œ ì•Œë¦¼
    import sys
    print(f"[env_loader] ì´ˆê¸°í™” ì‹¤íŒ¨: {_e}", file=sys.stderr)
# --- END: LLM í‚¤ ë¶€íŒ… ë³´ì¥ ---

import streamlit as st
import pandas as pd
import numpy as np
import re
from pathlib import Path
import plotly.graph_objects as go
from utils.helpers import find_column_by_keyword, add_provenance_columns, add_period_tag
from analysis.integrity import analyze_reconciliation, run_integrity_module
from analysis.contracts import LedgerFrame, ModuleResult
from analysis.trend import create_monthly_trend_figure, run_trend_module
from analysis.timeseries import (
    run_timeseries_module,          # â† ë³´ê³ ì„œ íƒ­ì—ì„œ ê³„ì† ì‚¬ìš©
    create_timeseries_figure        # â† ê·¸ë˜í”„ ë Œë” ê·¸ëŒ€ë¡œ ì‚¬ìš©
)
from analysis.ts_v2 import (
    run_timeseries_minimal,
    compute_series_stats,     # â† NEW
    build_anomaly_table,      # â† NEW
    add_future_shading        # â† NEW (ì‹œê° ìŒì˜)
)
from analysis.aggregation import aggregate_monthly, month_end_00
from analysis.anomaly import run_anomaly_module, compute_amount_columns
from analysis.correlation import run_correlation_module
from analysis.vendor import (
    create_pareto_figure,
    create_vendor_heatmap,
    create_vendor_detail_figure,
    run_vendor_module,
)
from analysis.report import run_final_analysis, build_methodology_note
from analysis.embedding import (
    ensure_rich_embedding_text,
    perform_embedding_and_clustering,
    perform_embedding_only,
    unify_cluster_names_with_llm,
)
from analysis.anomaly import calculate_grouped_stats_and_zscore
from services.llm import LLMClient
from services.cache import get_or_embed_texts
import services.cycles_store as cyc
from config import EMB_USE_LARGE_DEFAULT, HDBSCAN_RESCUE_TAU, EMB_MODEL_SMALL
try:
    from config import PM_DEFAULT
except Exception:
    PM_DEFAULT = 500_000_000
from utils.viz import add_materiality_threshold, add_pm_badge
from services.cluster_naming import (
    make_synonym_confirm_fn,
    unify_cluster_labels_llm,
)

# --- KRW ì…ë ¥(ì²œë‹¨ìœ„ ì½¤ë§ˆ) ìœ í‹¸: ì½œë°± ê¸°ë°˜ìœ¼ë¡œ ì•ˆì •í™” ---
def _krw_input(label: str, key: str, *, default_value: int, help_text: str = "") -> int:
    """
    í•œêµ­ ì›í™” ì…ë ¥ ìœ„ì ¯(ì²œë‹¨ìœ„ ì½¤ë§ˆ). í•µì‹¬ ê·œì¹™:
    1) ìœ„ì ¯ í‚¤(pm_value__txt ë“±)ë¥¼ ëŸ° ë£¨í”„ì—ì„œ ì§ì ‘ ëŒ€ì…í•˜ì§€ ì•ŠëŠ”ë‹¤.
    2) ì½¤ë§ˆ ì¬í¬ë§·ì€ on_change ì½œë°± ì•ˆì—ì„œë§Œ ìˆ˜í–‰í•œë‹¤.
    3) ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ê°’ì€ st.session_state[key]ì— ë³´ê´€í•œë‹¤.
    """
    txt_key = f"{key}__txt"  # ì‹¤ì œ text_input ìœ„ì ¯ì´ ë°”ì¸ë”©ë˜ëŠ” í‚¤

    # ì´ˆê¸° ì…‹ì—…: ìˆ«ì/ë¬¸ì ìƒíƒœë¥¼ ìœ„ì ¯ ìƒì„± ì „ì— ì¤€ë¹„
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if txt_key not in st.session_state:
        st.session_state[txt_key] = f"{int(st.session_state[key]):,}"

    # ì½œë°±: í¬ì»¤ìŠ¤ ì•„ì›ƒ/Enter ì‹œ ì½¤ë§ˆ í¬ë§·ì„ ì ìš©í•˜ê³  ìˆ«ì ìƒíƒœë¥¼ ë™ê¸°í™”
    def _on_blur_format():
        raw_now = st.session_state.get(txt_key, "")
        digits = re.sub(r"[^\d]", "", str(raw_now or ""))
        val = int(digits) if digits else 0
        if val < 0:
            val = 0
        st.session_state[key] = int(val)            # ë¶„ì„ì— ì“°ëŠ” ì •ìˆ˜ ìƒíƒœ
        st.session_state[txt_key] = f"{int(val):,}"  # ìœ„ì ¯ í‘œì‹œ í…ìŠ¤íŠ¸(ì½¤ë§ˆ)

    # ìœ„ì ¯ ìƒì„±
    raw = st.text_input(
        label,
        value=st.session_state[txt_key],
        key=txt_key,
        help=help_text,
        placeholder="ì˜ˆ: 500,000,000",
        on_change=_on_blur_format,
    )

    # ë¼ì´ë¸Œ íƒ€ì´í•‘ ë™ì•ˆì—ë„ ê·¸ë˜í”„ê°€ ì¦‰ì‹œ ë°˜ì˜ë˜ë„ë¡ ì •ìˆ˜ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸(ìœ„ì ¯ í‚¤ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    digits_live = re.sub(r"[^\d]", "", str(raw or ""))
    live_val = int(digits_live) if digits_live else 0
    if live_val < 0:
        live_val = 0
    st.session_state[key] = int(live_val)

    return int(st.session_state[key])


# ì‚¬ì´í´ í”„ë¦¬ì…‹ì„ ê³„ì • ì„ íƒê¸°ë¡œ ì£¼ì…í•˜ëŠ” í—¬í¼
def _apply_cycles_to_picker(*, upload_id: str, cycles_state_key: str, accounts_state_key: str, master_df: pd.DataFrame):
    """ì„ íƒëœ ì‚¬ì´í´ì˜ ê³„ì •ë“¤ì„ ê³„ì • ë©€í‹°ì…€ë ‰íŠ¸ì— í•©ì³ ë„£ì–´ì¤€ë‹¤."""
    cycles_map = cyc.get_effective_cycles(upload_id)
    chosen_cycles = st.session_state.get(cycles_state_key, []) or []
    # ì§€ì›: KO ë¼ë²¨ ë˜ëŠ” ì½”ë“œ ë¼ë²¨ â€” ê³µì‹ KO ë¼ë²¨ ì§‘í•© ê¸°ì¤€ìœ¼ë¡œ íŒë³„
    KO_LABELS = set(cyc.CYCLE_KO.values())
    if chosen_cycles and all(lbl in KO_LABELS for lbl in chosen_cycles):
        codes = cyc.accounts_for_cycles_ko(cycles_map, chosen_cycles)
    else:
        codes = cyc.accounts_for_cycles(cycles_map, chosen_cycles)
    names = (master_df[master_df['ê³„ì •ì½”ë“œ'].astype(str).isin(codes)]['ê³„ì •ëª…']
                .dropna().astype(str).unique().tolist())
    cur = set(st.session_state.get(accounts_state_key, []) or [])
    st.session_state[accounts_state_key] = sorted(cur.union(names))


# --- NEW: Correlation UI helpers (DRY) ---
from analysis.correlation import (
    run_correlation_module,
    run_correlation_focus_module as run_corr_focus,
    friendly_correlation_explainer,
    suggest_anchor_accounts,
)
from config import CORR_THRESHOLD_DEFAULT, CORR_MIN_ACTIVE_MONTHS_DEFAULT

def _render_corr_basic_tab(*, upload_id: str):
    """
    ê¸°ë³¸ ìƒê´€ê´€ê³„ ë¶„ì„(íˆíŠ¸ë§µ/ê°•í•œ ìƒê´€ìŒ/ì œì™¸ê³„ì •)ì„ ë Œë”í•©ë‹ˆë‹¤.
    - ê¸°ì¡´ 'ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„' íƒ­ì˜ êµ¬í˜„ì„ ê·¸ëŒ€ë¡œ ì˜®ê²¨, ìƒê´€ íƒ­ì˜ 'ê¸°ë³¸' ì„œë¸Œíƒ­ì—ì„œ ì‚¬ìš©.
    - state keyëŠ” 'corr_basic_*' ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ì¶©ëŒ ë°©ì§€.
    """
    import services.cycles_store as cyc
    mdf = st.session_state.master_df
    acct_names = sorted(mdf['ê³„ì •ëª…'].dropna().astype(str).unique().tolist())
    st.subheader("ê³„ì • ê°„ ìƒê´€ íˆíŠ¸ë§µ(ê¸°ë³¸)")
    # ë²„í¼ ì ìš©(ìœ„ì ¯ ìƒì„± ì „)
    if st.session_state.get("corr_basic_accounts_needs_update") and st.session_state.get("corr_basic_accounts_buf"):
        st.session_state["corr_basic_accounts"] = list(st.session_state["corr_basic_accounts_buf"])  
        st.session_state["corr_basic_accounts_needs_update"] = False

    # ëŒ€ìƒ ê³„ì • ì„ íƒ ìœ„ì ¯
    picked_accounts = st.multiselect(
        "ìƒê´€ ë¶„ì„ ëŒ€ìƒ ê³„ì •(2ê°œ ì´ìƒ ì„ íƒ)",
        acct_names,
        default=[],
        help="ì„ íƒí•œ ê³„ì •ë“¤ ê°„ ì›”ë³„ íë¦„ì˜ í”¼ì–´ìŠ¨ ìƒê´€ì„ ê³„ì‚°í•©ë‹ˆë‹¤.",
        key="corr_basic_accounts"
    )

    # ì‚¬ì´í´ í”„ë¦¬ì…‹(ëŒ€ìƒ ê³„ì • ì„ íƒ í•˜ë‹¨)
    cycles_map_now = cyc.get_effective_cycles(upload_id)
    if cycles_map_now:
        picked_cycles = st.multiselect(
            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ", list(cyc.CYCLE_KO.values()),
            default=[], key="corr_basic_cycles"
        )
        if st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_corr_basic"):
            mapping = cyc.get_effective_cycles(upload_id)
            codes = cyc.accounts_for_cycles_ko(mapping, picked_cycles)
            code_to_name = (
                mdf[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']].assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str)).drop_duplicates()
                  .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict()
            )
            cur_set = set(st.session_state.get("corr_basic_accounts", []))
            cur_set.update({code_to_name.get(c, c) for c in codes})
            st.session_state["corr_basic_accounts_buf"] = sorted(cur_set)
            st.session_state["corr_basic_accounts_needs_update"] = True
            st.rerun()
    corr_thr = st.slider(
        "ìƒê´€ ì„ê³„ì¹˜(ê°•í•œ ìƒê´€ìŒ í‘œ ì „ìš©)",
        min_value=0.50, max_value=0.95, step=0.05, value=0.70,
        help="ì ˆëŒ€ê°’ ê¸°ì¤€ ì„ê³„ì¹˜ ì´ìƒì¸ ê³„ì •ìŒë§Œ í‘œì— í‘œì‹œí•©ë‹ˆë‹¤.",
        key="corr_basic_thr"
    )

    if len(picked_accounts) < 2:
        st.info("ê³„ì •ì„ **2ê°œ ì´ìƒ** ì„ íƒí•˜ë©´ íˆíŠ¸ë§µì´ í‘œì‹œë©ë‹ˆë‹¤.")
        return

    # ìŠ¤ì½”í”„ ì ìš©ëœ LedgerFrameì„ ì¬ì‚¬ìš©
    lf_use = _lf_by_scope()

    # ê³„ì •ëª… â†’ ì½”ë“œ
    codes = (
        mdf[mdf['ê³„ì •ëª…'].isin(picked_accounts)]['ê³„ì •ì½”ë“œ']
        .astype(str).tolist()
    )
    cmod = run_correlation_module(
        lf_use,
        accounts=codes,
        corr_threshold=float(corr_thr),
        cycles_map=cyc.get_effective_cycles(upload_id),
    )
    _push_module(cmod)
    for w in cmod.warnings:
        st.warning(w)

    # íˆíŠ¸ë§µ(+í˜¸ë²„ ê³„ì •ëª…)
    if 'heatmap' in cmod.figures:
        fig = cmod.figures['heatmap']
        try:
            name_map = dict(zip(
                mdf["ê³„ì •ì½”ë“œ"].astype(str),
                mdf["ê³„ì •ëª…"].astype(str)
            ))
            tr = fig.data[0]
            x_codes = list(map(str, getattr(tr, 'x', [])))
            y_codes = list(map(str, getattr(tr, 'y', [])))
            x_names = [name_map.get(c, c) for c in x_codes]
            y_names = [name_map.get(c, c) for c in y_codes]
            tr.update(x=x_names, y=y_names)
            fig.update_traces(hovertemplate="ê³„ì •: %{y} Ã— %{x}<br>ìƒê´€ê³„ìˆ˜: %{z:.3f}<extra></extra>")
        except Exception:
            pass
        st.plotly_chart(fig, use_container_width=True, key=f"corr_basic_heatmap_{'_'.join(codes)}_{int(corr_thr*100)}")

    # ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ
    if 'strong_pairs' in cmod.tables and not cmod.tables['strong_pairs'].empty:
        st.markdown("**ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ**")
        st.dataframe(cmod.tables['strong_pairs'], use_container_width=True, height=320)

    # ì œì™¸ëœ ê³„ì •
    if 'excluded_accounts' in cmod.tables and not cmod.tables['excluded_accounts'].empty:
        with st.expander("ì œì™¸ëœ ê³„ì • ë³´ê¸°(ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)", expanded=False):
            exc = cmod.tables['excluded_accounts'].copy()
            if 'ê³„ì •ì½”ë“œ' in exc.columns:
                name_map = dict(zip(
                    mdf["ê³„ì •ì½”ë“œ"].astype(str),
                    mdf["ê³„ì •ëª…"].astype(str)
                ))
                exc['ê³„ì •ì½”ë“œ'] = exc['ê³„ì •ì½”ë“œ'].astype(str)
                exc['ê³„ì •ëª…'] = exc['ê³„ì •ì½”ë“œ'].map(name_map)
                cols = ['ê³„ì •ëª…', 'ê³„ì •ì½”ë“œ'] + [c for c in exc.columns if c not in ('ê³„ì •ëª…','ê³„ì •ì½”ë“œ')]
                exc = exc[cols]
            st.dataframe(exc, use_container_width=True)


def _render_corr_advanced_tab(*, upload_id: str):
    """
    ê³ ê¸‰ ìƒê´€ê´€ê³„ ë¶„ì„(ë°©ë²•/ì‹œì°¨/ë¡¤ë§ ì•ˆì •ì„± ë“±)ì„ ë Œë”í•©ë‹ˆë‹¤.
    - ê¸°ì¡´ 'ìƒê´€ê´€ê³„(ê³ ê¸‰)' íƒ­ì˜ ì½”ë“œë¥¼ ì„œë¸Œíƒ­ìš© í•¨ìˆ˜ë¡œ ëª¨ë“ˆí™”.
    - state keyëŠ” ê¸°ì¡´ 'corr_adv_*' ìœ ì§€(í˜¸í™˜).
    """
    import services.cycles_store as cyc
    st.subheader("ê³ ê¸‰ ìƒê´€ê´€ê³„")
    lf_adv = _lf_by_scope()  # ìŠ¤ì½”í”„ ì¼ê´€ì„± ìœ ì§€
    if lf_adv is None:
        st.info("ì›ì¥ì„ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        return

    mdf_adv = st.session_state.master_df
    acct_names_adv = sorted(mdf_adv['ê³„ì •ëª…'].dropna().astype(str).unique().tolist())
    # ë²„í¼ ì ìš©(ìœ„ì ¯ ìƒì„± ì „)
    if st.session_state.get("corr_adv_accounts_needs_update") and st.session_state.get("corr_adv_accounts_buf"):
        st.session_state["corr_adv_accounts"] = list(st.session_state["corr_adv_accounts_buf"])  
        st.session_state["corr_adv_accounts_needs_update"] = False

    # ëŒ€ìƒ ê³„ì • ì„ íƒ
    picked_accounts_adv = st.multiselect("ë¶„ì„ ê³„ì •(ë‹¤ì¤‘ ì„ íƒ)", options=acct_names_adv, key="corr_adv_accounts")
    
    # ì‚¬ì´í´ í”„ë¦¬ì…‹(ëŒ€ìƒ ê³„ì • ì„ íƒ í•˜ë‹¨)
    picked_cycles_adv = st.multiselect("ì‚¬ì´í´ í”„ë¦¬ì…‹(ì„ íƒ ì‹œ ê³„ì • ìë™ ë°˜ì˜)", options=list(cyc.CYCLE_KO.values()), key="corr_adv_cycles")
    if st.button("í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_preset_corr_adv"):
        mapping = cyc.get_effective_cycles(upload_id)
        codes = cyc.accounts_for_cycles_ko(mapping, picked_cycles_adv)
        code_to_name = (
            mdf_adv[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']].assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str)).drop_duplicates()
                .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict()
        )
        cur_set = set(st.session_state.get("corr_adv_accounts", []))
        cur_set.update({code_to_name.get(c, c) for c in codes})
        st.session_state["corr_adv_accounts_buf"] = sorted(cur_set)
        st.session_state["corr_adv_accounts_needs_update"] = True
        st.rerun()

    method = st.selectbox("ìƒê´€ ë°©ì‹", ["pearson", "spearman", "kendall"], index=0, key="corr_adv_method")
    corr_threshold = st.slider("ì„ê³„ì¹˜(|r|)", 0.1, 0.95, 0.70, 0.05, key="corr_adv_thr")
    c1, c2 = st.columns(2)
    with c1:
        max_lag = st.slider("ìµœëŒ€ ì‹œì°¨(ê°œì›”)", 0, 12, 6, 1, key="corr_adv_maxlag")
    with c2:
        rolling_window = st.slider("ë¡¤ë§ ìœˆë„ìš°(ê°œì›”)", 3, 24, 6, 1, key="corr_adv_rollwin")

    if st.button("ë¶„ì„ ì‹¤í–‰", key="run_corr_adv"):
        try:
            from analysis.corr_advanced import run_corr_advanced as run_corr_adv
            # âœ… UI(ê³„ì •ëª…) â†’ ì½”ë“œ ë³€í™˜
            _names = st.session_state.get("corr_adv_accounts", picked_accounts_adv) or []
            _codes = (
                mdf_adv[mdf_adv['ê³„ì •ëª…'].isin(_names)]['ê³„ì •ì½”ë“œ']
                .astype(str).drop_duplicates().tolist()
            )
            mr = run_corr_adv(
                lf_adv,
                _codes,
                method=st.session_state.get("corr_adv_method", "pearson"),
                corr_threshold=float(st.session_state.get("corr_adv_thr", 0.70)),
                max_lag=int(st.session_state.get("corr_adv_maxlag", 6)),
                rolling_window=int(st.session_state.get("corr_adv_rollwin", 6)),
            )
            st.subheader("íˆíŠ¸ë§µ")
            if "heatmap" in mr.figures:
                st.plotly_chart(mr.figures["heatmap"], use_container_width=True)
            if "strong_pairs" in mr.tables:
                st.subheader("ì„ê³„ì¹˜ ì´ìƒ ìƒê´€ìŒ")
                st.dataframe(mr.tables["strong_pairs"], use_container_width=True)
            if "lagged_pairs" in mr.tables:
                st.subheader("ìµœì  ì‹œì°¨ ìƒê´€(Top)")
                st.dataframe(mr.tables["lagged_pairs"], use_container_width=True)
            if "rolling_stability" in mr.tables:
                st.subheader("ë¡¤ë§ ì•ˆì •ì„±(ë³€ë™ì„± ë‚®ì€ ìˆœ)")
                st.dataframe(mr.tables["rolling_stability"], use_container_width=True)
        except Exception as _e:
            st.warning(f"ê³ ê¸‰ ìƒê´€ ë¶„ì„ ì‹¤íŒ¨: {_e}")

# --- NEW: Focus Tab (ë‹¨ì¼ê³„ì •) ---
def _render_corr_focus_tab(*, upload_id: str):
    import services.cycles_store as cyc
    from analysis.correlation import run_correlation_focus_module
    st.subheader("ë‹¨ì¼ ê³„ì •(í¬ì»¤ìŠ¤) ìƒê´€")
    lf_use = _lf_by_scope()
    if lf_use is None or lf_use.df.empty:
        st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return
    mdf = st.session_state.master_df
    acct_names = sorted(mdf['ê³„ì •ëª…'].dropna().astype(str).unique().tolist())
    col1, col2 = st.columns([2,1])
    with col1:
        focus_name = st.selectbox("í¬ì»¤ìŠ¤ ê³„ì •(1ê°œ)", options=["ì„ íƒí•˜ì„¸ìš”..."]+acct_names, index=0, key="corr_focus_name")
    with col2:
        within = st.checkbox("ë™ì¼ ì‚¬ì´í´ ë‚´ì—ì„œë§Œ", value=True, help="ì„ íƒ ì‹œ ê°™ì€ ì‚¬ì´í´ì— ì†í•œ ê³„ì •ë“¤ë§Œ ë¹„êµí•©ë‹ˆë‹¤.", key="corr_focus_within")
    if focus_name and focus_name != "ì„ íƒí•˜ì„¸ìš”...":
        code = (mdf[mdf['ê³„ì •ëª…']==focus_name]['ê³„ì •ì½”ë“œ'].astype(str).head(1).tolist() or [""])[0]
        mapping = cyc.get_effective_cycles(upload_id)
        mr = run_correlation_focus_module(lf_use, focus_account=focus_name, cycles_map=mapping, within_same_cycle=bool(within))
        _push_module(mr)
        for w in mr.warnings: st.warning(w)
        if "bar" in mr.figures: st.plotly_chart(mr.figures["bar"], use_container_width=True)
        if "focus_corr" in mr.tables and not mr.tables["focus_corr"].empty:
            st.dataframe(mr.tables["focus_corr"], use_container_width=True, height=_auto_table_height(mr.tables["focus_corr"]))


# --- 3. UI ë¶€ë¶„ ---
st.set_page_config(page_title="AI ë¶„ì„ ì‹œìŠ¤í…œ v0.18", layout="wide")
st.title("í›ˆ's GLë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("---")

for key in ['mapping_confirmed', 'analysis_done']:
    if key not in st.session_state:
        st.session_state[key] = False

# --- NEW: ëª¨ë“ˆ ê²°ê³¼ ìˆ˜ì§‘ìš© ì»¨í…Œì´ë„ˆ ---
if 'modules' not in st.session_state:
    st.session_state['modules'] = {}

def _push_module(mod: ModuleResult):
    """ModuleResultë¥¼ ì„¸ì…˜ì— ìˆ˜ì§‘(ë™ëª… ëª¨ë“ˆì€ ìµœì‹ ìœ¼ë¡œ êµì²´)."""
    try:
        if mod and getattr(mod, "name", None):
            st.session_state['modules'][str(mod.name)] = mod
    except Exception:
        pass


# (removed) number_input ê¸°ë°˜ ëŒ€ì²´ êµ¬í˜„: ì‰¼í‘œ ë¯¸í‘œì‹œÂ·í‚¤ ì¶©ëŒ ìœ ë°œ ê°€ëŠ¥ì„± â†’ ë‹¨ì¼ êµ¬í˜„ìœ¼ë¡œ í†µì¼

with st.sidebar:
    st.header("1. ë°ì´í„° ì¤€ë¹„")
    uploaded_file = st.file_uploader("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”.", type=["xlsx", "xlsm"])
    if 'last_file' not in st.session_state or st.session_state.last_file != uploaded_file:
        st.session_state.mapping_confirmed = False
        st.session_state.analysis_done = False
        st.session_state.last_file = uploaded_file

    st.markdown("---")
    st.header("2. ë¶„ì„ ê¸°ê°„")
    default_scope = st.session_state.get("period_scope", "ë‹¹ê¸°")
    st.session_state.period_scope = st.radio(
        "ë¶„ì„ ìŠ¤ì½”í”„(íŠ¸ë Œë“œ ì œì™¸):",
        options=["ë‹¹ê¸°", "ë‹¹ê¸°+ì „ê¸°"],
        index=["ë‹¹ê¸°","ë‹¹ê¸°+ì „ê¸°"].index(default_scope),
        horizontal=True,
        help="ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ ëª¨ë“ˆì— ì ìš©ë©ë‹ˆë‹¤. íŠ¸ë Œë“œëŠ” ì„¤ê³„ìƒ CY vs PY ë¹„êµ ìœ ì§€."
    )
    st.markdown("---")
    st.header("3. Embedding / Clustering")
    st.session_state.use_large_embedding = st.toggle(
        "Use Large Embedding (cost â†‘)",
        value=st.session_state.get("use_large_embedding", EMB_USE_LARGE_DEFAULT),
        help="Large model improves semantics but is slower and more expensive."
    )
    st.session_state.rescue_tau = st.slider(
        "Noise rescue Ï„ (cosine)",
        min_value=0.60, max_value=0.90, step=0.01,
        value=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
        help="Reassign -1 (noise) to nearest cluster if similarity â‰¥ Ï„."
    )
    st.markdown("---")
    st.header("4. Materiality")
    pm_val = _krw_input(
        "Performance Materiality (KRW)",
        key="pm_value",
        default_value=PM_DEFAULT,
        help_text="Used for KIT (PM exceed) and integrated risk scoring."
    )
    st.caption("â“˜ The PM threshold is drawn as a red dotted line on applicable charts. "
               "Y-axis scaling may change to accommodate this line.")

    # ğŸ§¹ ìºì‹œ ê´€ë¦¬
    with st.expander("ğŸ§¹ ìºì‹œ ê´€ë¦¬", expanded=False):
        if st.button("ì„ë² ë”© ìºì‹œ ë¹„ìš°ê¸°"):
            import shutil
            from services.cache import _model_dir
            for m in ["text-embedding-3-small", "text-embedding-3-large"]:
                try:
                    shutil.rmtree(_model_dir(m), ignore_errors=True)
                except Exception as e:
                    st.warning(f"{m} ì‚­ì œ ì‹¤íŒ¨: {e}")
            st.success("ì„ë² ë”© ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ë°ì´í„° ìºì‹œ ë¹„ìš°ê¸°"):
            st.cache_data.clear()
            st.success("Streamlit ë°ì´í„° ìºì‹œ ì‚­ì œ ì™„ë£Œ")

        if st.button("ìºì‹œ ì •ë³´ ë³´ê¸°"):
            from services.cache import get_cache_info
            try:
                st.write(get_cache_info("text-embedding-3-small"))
                st.write(get_cache_info("text-embedding-3-large"))
            except Exception as e:
                st.info(f"ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")


@st.cache_data(show_spinner=False)
def _read_excel(_file, sheet_name=None):
    return pd.read_excel(_file, sheet_name=sheet_name)


@st.cache_data(show_spinner=False)
def _read_xls(_file):
    # pickle ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ë§Œ ìºì‹œ â†’ ì‹œíŠ¸ëª… ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    return pd.ExcelFile(_file).sheet_names

# (removed duplicated definition) _krw_input â€” ìœ„ì˜ ë‹¨ì¼ ë²„ì „ë§Œ ìœ ì§€

def _apply_scope(df: pd.DataFrame, scope: str) -> pd.DataFrame:
    """ìŠ¤ì½”í”„ ì ìš© ì‹œ ê²°ì¸¡ ì»¬ëŸ¼ ë°©ì–´: 'period_tag' ë¯¸ì¡´ì¬ë©´ ì›ë³¸ ë°˜í™˜.
    df.get('period_tag','')ê°€ ë¬¸ìì—´ì„ ë°˜í™˜í•  ê²½ìš° .eq í˜¸ì¶œ AttributeErrorë¥¼ ë°©ì§€í•œë‹¤.
    """
    if df is None or df.empty or 'period_tag' not in df.columns:
        return df
    if scope == "ë‹¹ê¸°":
        return df[df['period_tag'].eq('CY')]
    if scope == "ë‹¹ê¸°+ì „ê¸°":
        return df[df['period_tag'].isin(['CY', 'PY'])]
    return df

# === ê³µìš©: í‘œ ë†’ì´ ìë™ ì œí•œ(í–‰ ìˆ˜ ê¸°ë°˜) ===
def _auto_table_height(df: pd.DataFrame, max_rows: int = 8,
                       row_px: int = 28, header_px: int = 38, pad_px: int = 12) -> int:
    """
    í‘œ ë†’ì´ë¥¼ 'í‘œì‹œ í–‰ ìˆ˜' ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•´ ë„˜ê¸°ê¸° ìœ„í•œ ìœ í‹¸.
    - max_rows: ìµœëŒ€ í‘œì‹œ í–‰ìˆ˜
    - ì‹¤íŒ¨ ì‹œ 300pxë¡œ í´ë°±
    """
    try:
        n = int(min(max(len(df), 1), max_rows))
        return int(header_px + n * row_px + pad_px)
    except Exception:
        return 300

def _lf_by_scope() -> LedgerFrame:
    """ìƒê´€/ê±°ë˜ì²˜/ì´ìƒì¹˜ì—ì„œ ì‚¬ìš©í•  ìŠ¤ì½”í”„ ì ìš© LedgerFrame."""
    hist = st.session_state.get('lf_hist')
    scope = st.session_state.get('period_scope', 'ë‹¹ê¸°')
    if hist is None:
        return None
    return LedgerFrame(df=_apply_scope(hist.df, scope), meta=hist.meta)

# (removed) êµ¬ë²„ì „ í…ìŠ¤íŠ¸ì…ë ¥ + Â±step / âœ–reset ë³€í˜•ë“¤ â€” ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ë²„íŠ¼ë¥˜ ì‚­ì œ ë° ë‹¨ì¼í™”


if uploaded_file is not None:
    if not st.session_state.mapping_confirmed:
        # ... ì»¬ëŸ¼ ë§¤í•‘ UI ...
        try:
            st.info("2ë‹¨ê³„: ì—‘ì…€ì˜ ì»¬ëŸ¼ì„ ë¶„ì„ í‘œì¤€ í•„ë“œì— ë§ê²Œ ì§€ì •í•´ì£¼ì„¸ìš”.")
            sheet_names = _read_xls(uploaded_file)
            first_ledger_sheet = next((s for s in sheet_names if 'ledger' in s.lower()), None)
            if first_ledger_sheet is None:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            ledger_cols = _read_excel(uploaded_file, sheet_name=first_ledger_sheet).columns.tolist()
            ledger_map = {}
            st.markdown("#### **Ledger ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            cols = st.columns(6)
            ledger_fields = {'íšŒê³„ì¼ì': 'ì¼ì', 'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê±°ë˜ì²˜': 'ê±°ë˜ì²˜', 'ì ìš”': 'ì ìš”', 'ì°¨ë³€': 'ì°¨ë³€', 'ëŒ€ë³€': 'ëŒ€ë³€'}
            for i, (key, keyword) in enumerate(ledger_fields.items()):
                with cols[i]:
                    is_optional = key == 'ê±°ë˜ì²˜'
                    default_col = find_column_by_keyword(ledger_cols, keyword)
                    options = ['ì„ íƒ ì•ˆ í•¨'] + ledger_cols if is_optional else ledger_cols
                    default_index = options.index(default_col) if default_col in options else 0
                    ledger_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", options, index=default_index, key=f"map_ledger_{key}")
            st.markdown("---")
            st.markdown("#### **Master ì‹œíŠ¸** í•­ëª© ë§¤í•‘")
            master_cols = _read_excel(uploaded_file, sheet_name='Master').columns.tolist()
            master_map = {}
            cols = st.columns(7)
            master_fields = {'ê³„ì •ì½”ë“œ': 'ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…': 'ê³„ì •ëª…', 'BS/PL': 'BS/PL', 'ì°¨ë³€/ëŒ€ë³€': 'ì°¨ë³€/ëŒ€ë³€', 'ë‹¹ê¸°ë§ì”ì•¡': 'ë‹¹ê¸°ë§', 'ì „ê¸°ë§ì”ì•¡': 'ì „ê¸°ë§', 'ì „ì „ê¸°ë§ì”ì•¡': 'ì „ì „ê¸°ë§'}
            for i, (key, keyword) in enumerate(master_fields.items()):
                with cols[i]:
                    default_col = find_column_by_keyword(master_cols, keyword)
                    default_index = master_cols.index(default_col) if default_col in master_cols else 0
                    master_map[key] = st.selectbox(f"**'{key}'** í•„ë“œ ì„ íƒ", master_cols, index=default_index, key=f"map_master_{key}")
            if st.button("âœ… ë§¤í•‘ í™•ì¸ ë° ë°ì´í„° ì²˜ë¦¬", type="primary"):
                st.session_state.ledger_map = ledger_map
                st.session_state.master_map = master_map
                st.session_state.mapping_confirmed = True
                st.rerun()
        except Exception as e:
            st.error(f"ì—‘ì…€ íŒŒì¼ì˜ ì»¬ëŸ¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    else:  # ë§¤í•‘ í™•ì¸ í›„
        try:
            ledger_map, master_map = st.session_state.ledger_map, st.session_state.master_map
            master_df = _read_excel(uploaded_file, sheet_name='Master')
            sheet_names = _read_xls(uploaded_file)
            ledger_sheets = [s for s in sheet_names if 'ledger' in s.lower()]
            if not ledger_sheets:
                st.error("ì˜¤ë¥˜: 'Ledger' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
            all_parts = []
            for s in ledger_sheets:
                part = _read_excel(uploaded_file, sheet_name=s)
                part['source_sheet'] = s
                part = add_provenance_columns(part)
                all_parts.append(part)
            ledger_df = pd.concat(all_parts, ignore_index=True)
            # row_id: íŒŒì¼ëª…|ì‹œíŠ¸:í–‰  (ì„¸ì…˜/ì¬ì‹¤í–‰ì—ë„ ì•ˆì •)
            try:
                base = Path(getattr(uploaded_file, "name", "uploaded.xlsx")).stem
                if 'row_id' in ledger_df.columns:
                    ledger_df['row_id'] = base + "|" + ledger_df['row_id'].astype(str)
            except Exception:
                pass
            ledger_df.rename(columns={v: k for k, v in ledger_map.items() if v != 'ì„ íƒ ì•ˆ í•¨'}, inplace=True)
            master_df.rename(columns={v: k for k, v in master_map.items()}, inplace=True)

            # ğŸ”§ ë³‘í•© ì „ì— íƒ€ì…/í¬ë§·ì„ ë¨¼ì € í†µì¼
            for df_ in [ledger_df, master_df]:
                if 'ê³„ì •ì½”ë“œ' in df_.columns:
                    df_['ê³„ì •ì½”ë“œ'] = (
                        df_['ê³„ì •ì½”ë“œ']
                        .astype(str)
                        .str.replace(r'\.0$', '', regex=True)
                        .str.strip()
                    )

            master_essentials = master_df[['ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…']].drop_duplicates()
            ledger_df = pd.merge(ledger_df, master_essentials, on='ê³„ì •ì½”ë“œ', how='left')
            ledger_df['ê³„ì •ëª…'] = ledger_df['ê³„ì •ëª…'].fillna('ë¯¸ì§€ì • ê³„ì •')

            ledger_df['íšŒê³„ì¼ì'] = pd.to_datetime(ledger_df['íšŒê³„ì¼ì'], errors='coerce')
            ledger_df.dropna(subset=['íšŒê³„ì¼ì'], inplace=True)
            for col in ['ì°¨ë³€', 'ëŒ€ë³€']:
                ledger_df[col] = pd.to_numeric(ledger_df[col], errors='coerce').fillna(0)
            for col in ['ë‹¹ê¸°ë§ì”ì•¡', 'ì „ê¸°ë§ì”ì•¡', 'ì „ì „ê¸°ë§ì”ì•¡']:
                if col in master_df.columns:
                    master_df[col] = pd.to_numeric(master_df[col], errors='coerce').fillna(0)
                else:
                    master_df[col] = 0
            ledger_df['ê±°ë˜ê¸ˆì•¡'] = ledger_df['ì°¨ë³€'] - ledger_df['ëŒ€ë³€']
            ledger_df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] = abs(ledger_df['ê±°ë˜ê¸ˆì•¡'])
            ledger_df['ì—°ë„'] = ledger_df['íšŒê³„ì¼ì'].dt.year
            ledger_df['ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.month
            # âœ… ë¶„ì„ ê·œì¹™: ê³„ì • ì„œë¸Œì…‹ ë¶„ì„ ì‹œì—ë„ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ í™•ë³´í•˜ê¸° ìœ„í•œ í¸ì˜ íŒŒìƒ
            ledger_df['ì—°ì›”'] = ledger_df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
            # âœ… period_tag ì¶”ê°€(CY/PY/Other)
            ledger_df = add_period_tag(ledger_df)
            if 'ê±°ë˜ì²˜' not in ledger_df.columns:
                ledger_df['ê±°ë˜ì²˜'] = 'ì •ë³´ ì—†ìŒ'
            ledger_df['ê±°ë˜ì²˜'] = ledger_df['ê±°ë˜ì²˜'].fillna('ì •ë³´ ì—†ìŒ').astype(str)

            if st.button("ğŸš€ ì „ì²´ ë¶„ì„ ì‹¤í–‰", type="primary"):
                with st.spinner('ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
                    # âœ… ì •í•©ì„±ì€ ì‚¬ìš©ì ê¸°ê°„ ì„ íƒê³¼ ë¬´ê´€í•˜ê²Œ ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
                    st.session_state.recon_status, st.session_state.recon_df = analyze_reconciliation(ledger_df, master_df)
                    # âœ… í‘œì¤€ LedgerFrame êµ¬ì„±(ì •í•©ì„±ì€ í•­ìƒ ì „ì²´ ê¸°ì¤€: DF_hist)
                    lf_hist = LedgerFrame(df=ledger_df, meta={
                        "file_name": getattr(uploaded_file, "name", "uploaded.xlsx"),
                        "master_df": master_df,
                    })
                    # ì´ˆê¸°ì—” focus=hist (í›„ì† ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì í•„í„° ì—°ê²°)
                    lf_focus = lf_hist

                    st.session_state.master_df = master_df
                    st.session_state.ledger_df = ledger_df
                    st.session_state.lf_hist = lf_hist
                    st.session_state.lf_focus = lf_focus
                    st.session_state.analysis_done = True
                st.rerun()

            if st.session_state.analysis_done:
                st.success("âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                # --- ê³„ì •â†’ì‚¬ì´í´ ë§¤í•‘ ê²€í† /ìˆ˜ì • ---
                upload_id = getattr(uploaded_file, "name", "uploaded.xlsx")
                # ì—…ë¡œë“œ ì§í›„ 1íšŒ: í”„ë¦¬ì…‹ ì—†ìœ¼ë©´ ë£°ë² ì´ìŠ¤ ìë™ ìƒì„±
                names_dict = (
                    master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']]
                        .drop_duplicates()
                        .assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str))
                        .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict()
                )
                if not cyc.get_effective_cycles(upload_id):
                    cyc.build_cycles_preset(upload_id, names_dict, use_llm=False)

                with st.expander("ğŸ§­ ê³„ì • â†’ ì‚¬ì´í´ ë§¤í•‘ ê²€í† /ìˆ˜ì •", expanded=False):
                    cur_map = cyc.get_effective_cycles(upload_id)
                    map_df = master_df[['ê³„ì •ì½”ë“œ','ê³„ì •ëª…']].drop_duplicates().copy()
                    map_df['ê³„ì •ì½”ë“œ'] = map_df['ê³„ì •ì½”ë“œ'].astype(str)
                    map_df['ì‚¬ì´í´(í‘œì‹œ)'] = map_df['ê³„ì •ì½”ë“œ'].map(lambda c: cyc.code_to_ko(cur_map.get(c, 'Other')))

                    st.caption("ì‚¬ì´í´ ë¼ë²¨ì„ ìˆ˜ì •í•œ ë’¤ ì €ì¥ì„ ëˆ„ë¥´ì„¸ìš”. (í‘œì‹œëŠ” í•œê¸€, ë‚´ë¶€ëŠ” ì½”ë“œë¡œ ì €ì¥ë©ë‹ˆë‹¤)")
                    edited = st.data_editor(
                        map_df, hide_index=True, use_container_width=True,
                        column_config={
                            "ì‚¬ì´í´(í‘œì‹œ)": st.column_config.SelectboxColumn(
                                options=list(cyc.CYCLE_KO.values()), required=True
                            )
                        }
                    )

                    c1, c2, c3 = st.columns(3)
                    with c1:
                        if st.button("ğŸ’¾ ë§¤í•‘ ì €ì¥", type="primary", key="btn_save_cycles_map"):
                            new_map_codes = {
                                str(r['ê³„ì •ì½”ë“œ']): cyc.ko_to_code(r['ì‚¬ì´í´(í‘œì‹œ)'])
                                for _, r in edited.iterrows()
                            }
                            cyc.set_cycles_map(upload_id, new_map_codes)
                            st.success(f"ì €ì¥ë¨: {len(new_map_codes):,}ê°œ ê³„ì •")
                    with c2:
                        if st.button("ğŸ¤– LLM ì¶”ì²œ ë³‘í•©", help="ë£°ë² ì´ìŠ¤ ê²°ê³¼ ìœ„ì— LLM ì œì•ˆì„ ë®ì–´ì”Œì›ë‹ˆë‹¤", key="btn_merge_llm_cycles"):
                            cyc.build_cycles_preset(upload_id, names_dict, use_llm=True)
                            st.success("LLM ì¶”ì²œì„ ë³‘í•©í–ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                    with c3:
                        if st.button("â†º ë£°ë² ì´ìŠ¤ë¡œ ì´ˆê¸°í™”", key="btn_reset_rule_cycles"):
                            cyc.build_cycles_preset(upload_id, names_dict, use_llm=False)
                            st.success("ë£°ë² ì´ìŠ¤ë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                with st.expander("ğŸ” ë¹ ë¥¸ ì§„ë‹¨(ë°ì´í„° í’ˆì§ˆ ì²´í¬)", expanded=False):
                    df = st.session_state.ledger_df.copy()
                    issues = []

                    invalid_date = int(df['íšŒê³„ì¼ì'].isna().sum())
                    if invalid_date > 0:
                        issues.append(f"â— ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ(NaT): {invalid_date:,}ê±´")

                    if 'ê±°ë˜ì²˜' in df.columns:
                        missing_vendor = int((df['ê±°ë˜ì²˜'].isna() | (df['ê±°ë˜ì²˜'] == 'ì •ë³´ ì—†ìŒ')).sum())
                        if missing_vendor > 0:
                            issues.append(f"â„¹ï¸ ê±°ë˜ì²˜ ì •ë³´ ì—†ìŒ/ê²°ì¸¡: {missing_vendor:,}ê±´")

                    zero_abs = int((df['ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’'] == 0).sum())
                    issues.append(f"â„¹ï¸ ê¸ˆì•¡ ì ˆëŒ€ê°’ì´ 0ì¸ ì „í‘œ: {zero_abs:,}ê±´")

                    unlinked = int(df['ê³„ì •ëª…'].eq('ë¯¸ì§€ì • ê³„ì •').sum())
                    if unlinked > 0:
                        issues.append(f"â— Masterì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì „í‘œ(ê³„ì •ëª… ë¯¸ì§€ì •): {unlinked:,}ê±´")

                    st.write("**ì²´í¬ ê²°ê³¼**")
                    if issues:
                        for line in issues:
                            st.write("- " + line)
                    else:
                        st.success("ë¬¸ì œ ì—†ì´ ê¹”ë”í•©ë‹ˆë‹¤!")
                tab_integrity, tab_vendor, tab_anomaly, tab_ts, tab_report, tab_corr = st.tabs(["ğŸŒŠ ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„", "ğŸ¢ ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„", "ğŸ”¬ ì´ìƒ íŒ¨í„´ íƒì§€", "ğŸ“‰ ì‹œê³„ì—´ ì˜ˆì¸¡", "ğŸ§  ë¶„ì„ ì¢…í•© ëŒ€ì‹œë³´ë“œ", "ğŸ“Š ìƒê´€ê´€ê³„"])

                # (ì´ì „ ë²„ì „) ëŒ€ì‹œë³´ë“œ íƒ­ì€ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì œê±°ë¨
                with tab_integrity:  # ...
                    st.header("ë°ì´í„° ë¬´ê²°ì„± ë° íë¦„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    st.subheader("1. ë°ì´í„° ì •í•©ì„± ê²€ì¦ ê²°ê³¼")
                    mod = st.session_state.get('modules', {}).get('integrity')
                    result_df = (getattr(mod, 'tables', {}) or {}).get('reconciliation') if mod else st.session_state.get('recon_df')
                    
                    # === í‘œ ë°ì´í„°ì™€ ì§ì ‘ ì—°ë™ëœ ë°°ë„ˆ ë¡œì§ ===
                    def render_integrity_banner(df):
                        if df is None or df.empty:
                            st.info("ê²€ì¦í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            return
                        
                        # 1) 'ì°¨ì´'ë¥¼ ì•ˆì „í•˜ê²Œ ìˆ˜ì¹˜í™”
                        if "ì°¨ì´" in df.columns:
                            diff = pd.to_numeric(df["ì°¨ì´"].astype(str).str.replace(",", ""), errors="coerce").fillna(0)
                        else:
                            diff = pd.Series([0] * len(df))
                        
                        # 2) í—ˆìš© ì˜¤ì°¨(1ì›) ì´í•˜ë¥¼ 0ìœ¼ë¡œ ê°„ì£¼
                        tol = 1.0
                        fails_mask = diff.abs() > tol
                        
                        # 3) 'ìƒíƒœ'ê°€ ìˆìœ¼ë©´ êµì°¨ í™•ì¸(ë°©ì–´ì )
                        if "ìƒíƒœ" in df.columns:
                            fails_mask = fails_mask | df["ìƒíƒœ"].astype(str).str.lower().eq("fail")
                        
                        n_fail = int(fails_mask.sum())
                        
                        if n_fail > 0:
                            max_gap = float(diff.abs().max())
                            st.warning(f"âŒ ë¶ˆì¼ì¹˜ ê³„ì • {n_fail}ê±´ ë°œê²¬ Â· ìµœëŒ€ ì°¨ì´: {max_gap:,.0f}")
                        else:
                            st.success("âœ… ëª¨ë“  ê³„ì •ì˜ ë°ì´í„°ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤.")
                    
                    render_integrity_banner(result_df)

                    def highlight_status(row):
                        if row.ìƒíƒœ == 'Fail':
                            return ['background-color: #ffcccc'] * len(row)
                        elif row.ìƒíƒœ == 'Warning':
                            return ['background-color: #fff0cc'] * len(row)
                        return [''] * len(row)

                    format_dict = {col: '{:,.0f}' for col in result_df.select_dtypes(include=np.number).columns}
                    st.dataframe(result_df.style.apply(highlight_status, axis=1).format(format_dict), use_container_width=True)
                    st.markdown("---")
                    st.subheader("2. ê³„ì •ë³„ ì›”ë³„ ì¶”ì´ (PY vs CY)")
                    # âœ… ìë™ ì¶”ì²œ ì œê±°: ì‚¬ìš©ìê°€ ê³„ì •ì„ ì„ íƒí•œ ê²½ìš°ì—ë§Œ ê·¸ë˜í”„ ë Œë”
                    account_list = st.session_state.master_df['ê³„ì •ëª…'].unique()
                    selected_accounts = st.multiselect(
                        "ë¶„ì„í•  ê³„ì •ì„ ì„ íƒí•˜ì„¸ìš” (1ê°œ ì´ìƒ í•„ìˆ˜)",
                        account_list, default=[],
                        key="trend_accounts_pick"
                    )
                    # â–¼ ì‚¬ì´í´ í”„ë¦¬ì…‹(ì„ íƒ ì‹œ ìœ„ ë©€í‹°ì…€ë ‰íŠ¸ì— ê³„ì • ìë™ ë°˜ì˜)
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ(ì„ íƒí•˜ë©´ ìœ„ ê³„ì • ëª©ë¡ì— ìë™ ë°˜ì˜)",
                            list(cyc.CYCLE_KO.values()),
                            default=[], key="trend_cycles_pick"
                        )
                        st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_trend", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="trend_cycles_pick",
                                              accounts_state_key="trend_accounts_pick",
                                              master_df=st.session_state.master_df))
                    if not selected_accounts:
                        st.info("ê³„ì •ì„ 1ê°œ ì´ìƒ ì„ íƒí•˜ë©´ ì›”ë³„ ì¶”ì´ ê·¸ë˜í”„ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
                    else:
                        lf_use = st.session_state.get('lf_focus') or st.session_state.get('lf_hist')
                        # ì„ íƒëœ ê³„ì •ëª…ì„ ê³„ì •ì½”ë“œë¡œ ë³€í™˜
                        mdf = st.session_state.master_df
                        accounts_codes = (
                            mdf[mdf['ê³„ì •ëª…'].isin(selected_accounts)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        mod = run_trend_module(lf_use, accounts=accounts_codes)
                        _push_module(mod)
                        for w in mod.warnings:
                            st.warning(w)
                        if mod.figures:
                            for title, fig in mod.figures.items():
                                # PM ì„ê³„ì„ (í•­ìƒ í‘œì‹œ; ë²”ìœ„ ë°–ì´ë©´ ìë™ í™•ì¥)
                                st.plotly_chart(
                                    add_materiality_threshold(fig, float(st.session_state.get("pm_value", PM_DEFAULT))),
                                    use_container_width=True,
                                    key=f"trend_{title}"
                                )
                        else:
                            st.info("í‘œì‹œí•  ì¶”ì´ ê·¸ë˜í”„ê°€ ì—†ìŠµë‹ˆë‹¤.")



                with tab_vendor:
                    st.header("ê±°ë˜ì²˜ ì‹¬ì¸µ ë¶„ì„")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")

                    st.subheader("ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± (ê³„ì •ë³„)")
                    master_df_res = st.session_state.master_df
                    account_list_vendor = master_df_res['ê³„ì •ëª…'].unique()
                    selected_accounts_vendor = st.multiselect("ë¶„ì„í•  ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”.", account_list_vendor, default=[], key="vendor_accounts_pick")
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles_vendor = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ", list(cyc.CYCLE_KO.values()),
                            default=[], key="vendor_cycles_pick"
                        )
                        st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_vendor", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="vendor_cycles_pick",
                                              accounts_state_key="vendor_accounts_pick",
                                              master_df=st.session_state.master_df))

                    # ğŸ”§ ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„° â€” KRW ì…ë ¥(ì»¤ë°‹ ì‹œ ì‰¼í‘œ ì •ê·œí™”)
                    min_amount_vendor = _krw_input(
                        "ìµœì†Œ ê±°ë˜ê¸ˆì•¡(ì—°ê°„, CY) í•„í„°",
                        key="vendor_min_amount",
                        default_value=0,
                        help_text="CY ê¸°ì¤€ ê±°ë˜ê¸ˆì•¡ í•©ê³„ê°€ ì´ ê°’ ë¯¸ë§Œì¸ ê±°ë˜ì²˜ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°ë©ë‹ˆë‹¤."
                    )
                    include_others_vendor = st.checkbox("ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°", value=True)

                    if selected_accounts_vendor:
                        selected_codes = (
                            master_df_res[master_df_res['ê³„ì •ëª…'].isin(selected_accounts_vendor)]['ê³„ì •ì½”ë“œ']
                            .astype(str)
                            .tolist()
                        )
                        lf_use = _lf_by_scope()
                        vmod = run_vendor_module(
                            lf_use,
                            account_codes=selected_codes,
                            min_amount=float(min_amount_vendor),
                            include_others=bool(include_others_vendor),
                        )
                        _push_module(vmod)
                        if vmod.figures:
                            col1, col2 = st.columns(2)
                            with col1:
                                if 'pareto' in vmod.figures:
                                    figp = vmod.figures['pareto']
                                    figp = add_materiality_threshold(figp, float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figp, use_container_width=True, key=f"vendor_pareto_{'_'.join(selected_accounts_vendor) or 'all'}")
                            with col2:
                                if 'heatmap' in vmod.figures:
                                    figh = add_pm_badge(vmod.figures['heatmap'], float(st.session_state.get("pm_value", PM_DEFAULT)))
                                    st.plotly_chart(figh, use_container_width=True, key=f"vendor_heatmap_{'_'.join(selected_accounts_vendor) or 'all'}")
                        else:
                            st.warning("ì„ íƒí•˜ì‹  ê³„ì •ì—ëŠ” ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                        for w in vmod.warnings:
                            st.warning(w)
                    else:
                        st.info("ê³„ì •ì„ ì„ íƒí•˜ë©´ í•´ë‹¹ ê³„ì •ì˜ ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë° í™œë™ì„± ë¶„ì„ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.subheader("ê±°ë˜ì²˜ë³„ ì„¸ë¶€ ë¶„ì„ (ì „ì²´ ê³„ì •)")
                    full_ledger_df = st.session_state.ledger_df
                    vendor_list = sorted(full_ledger_df[full_ledger_df['ê±°ë˜ì²˜'] != 'ì •ë³´ ì—†ìŒ']['ê±°ë˜ì²˜'].unique())

                    if len(vendor_list) > 0:
                        options = ['ì„ íƒí•˜ì„¸ìš”...'] + vendor_list
                        selected_vendor = st.selectbox("ìƒì„¸ ë¶„ì„í•  ê±°ë˜ì²˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.", options, index=0)

                        if selected_vendor != 'ì„ íƒí•˜ì„¸ìš”...':
                            all_months_in_data = pd.period_range(
                                start=full_ledger_df['íšŒê³„ì¼ì'].min(),
                                end=full_ledger_df['íšŒê³„ì¼ì'].max(),
                                freq='M'
                            ).strftime('%Y-%m').tolist()
                            detail_fig = create_vendor_detail_figure(full_ledger_df, selected_vendor, all_months_in_data)
                            # PM line on vendor detail (stacked bars)
                            try:
                                detail_fig = add_materiality_threshold(detail_fig, float(st.session_state.get("pm_value", PM_DEFAULT)))
                            except Exception:
                                pass
                            if detail_fig:
                                st.plotly_chart(detail_fig, use_container_width=True, key=f"vendor_detail_{selected_vendor}")
                    else:
                        st.info("ë¶„ì„í•  ê±°ë˜ì²˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                with tab_anomaly:
                    st.header("ì´ìƒ íŒ¨í„´ íƒì§€")
                    st.caption(f"ğŸ” í˜„ì¬ ìŠ¤ì½”í”„: {st.session_state.get('period_scope','ë‹¹ê¸°')}")
                    mdf = st.session_state.master_df
                    acct_names = mdf['ê³„ì •ëª…'].unique()
                    pick = st.multiselect("ëŒ€ìƒ ê³„ì • ì„ íƒ(ë¯¸ì„ íƒ ì‹œ ìë™ ì¶”ì²œ)", acct_names, default=[])
                    topn = st.slider("í‘œì‹œ ê°œìˆ˜(ìƒìœ„ |Z|)", min_value=10, max_value=500, value=20, step=10)
                    if st.button("ì´ìƒì¹˜ ë¶„ì„ ì‹¤í–‰"):
                        lf_use = _lf_by_scope()
                        codes = None
                        if pick:
                            codes = mdf[mdf['ê³„ì •ëª…'].isin(pick)]['ê³„ì •ì½”ë“œ'].astype(str).tolist()
                        amod = run_anomaly_module(lf_use, target_accounts=codes, topn=topn, pm_value=float(st.session_state.get("pm_value", PM_DEFAULT)))
                        _push_module(amod)
                        for w in amod.warnings: st.warning(w)
                        if 'anomaly_top' in amod.tables:
                            _tbl = amod.tables['anomaly_top'].copy()
                            fmt = {}
                            if 'ë°œìƒì•¡' in _tbl.columns: fmt['ë°œìƒì•¡'] = '{:,.0f}'
                            if 'Z-Score' in _tbl.columns: fmt['Z-Score'] = '{:.2f}'
                            st.dataframe(_tbl.style.format(fmt), use_container_width=True)
                        if 'zscore_hist' in amod.figures:
                            st.plotly_chart(amod.figures['zscore_hist'], use_container_width=True, key="anomaly_hist")

                with tab_ts:
                    st.header("ì‹œê³„ì—´ ì˜ˆì¸¡")
                    with st.expander("ğŸ§­ í•´ì„ ê°€ì´ë“œ", expanded=False, icon=":material/help:"):
                        st.markdown(
                            """
### ìš©ì–´
- **z(í‘œì¤€í™” ì§€ìˆ˜)**: `z = (ì‹¤ì¸¡ âˆ’ ì˜ˆì¸¡) / Ïƒ`  
  - ì›”ë³„ ì˜ˆì¸¡ ì”ì°¨(ì‹¤ì¸¡-ì˜ˆì¸¡)ë¥¼ í‘œì¤€í™”í•œ ì§€ìˆ˜ì…ë‹ˆë‹¤. **ì´ìƒ íŒ¨í„´ íƒì§€ì˜ Z-Scoreì™€ ë‹¤ë¥¸ ê°œë…ì…ë‹ˆë‹¤.**
  - |z|â‰ˆ2ëŠ” **ì´ë¡€ì **, |z|â‰ˆ3ì€ **ë§¤ìš° ì´ë¡€ì **ì…ë‹ˆë‹¤.  
- **Ïƒ(í‘œì¤€í¸ì°¨) ì§‘ê³„**: ìµœê·¼ *k=6ê°œì›”* ì”ì°¨ì˜ í‘œì¤€í¸ì°¨ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ì§§ìœ¼ë©´ ì‹œì‘~í˜„ì¬ê¹Œì§€ì˜ **expanding Ïƒ**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  
- **ìœ„í—˜ë„(0~1)** = `min(1, 0.5Â·|z|/3 + 0.3Â·PMëŒ€ë¹„ + 0.2Â·KIT)`  
  - PMëŒ€ë¹„ = `min(1, |ì‹¤ì¸¡âˆ’ì˜ˆì¸¡| / PM)`,  **KIT** = PM ì´ˆê³¼ ì—¬ë¶€(True/False)
- **Flow / Balance**: *Flow*ëŠ” **ì›” ë°œìƒì•¡(Î”ì”ì•¡)**, *Balance*ëŠ” **ì›”ë§ ì”ì•¡**ì…ë‹ˆë‹¤. *(BS ê³„ì •ì€ Balance ê¸°ì¤€ë„ ë³‘í–‰ ê³„ì‚°í•©ë‹ˆë‹¤.)*
- **ì •ìƒì„±**: ì‹œê³„ì—´ì˜ í‰ê· /ë¶„ì‚°ì´ ì‹œê°„ì— ë”°ë¼ **ì•ˆ ë³€í•¨**(ARIMAê°€ íŠ¹íˆ ì„ í˜¸).
- **MAE**: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨(ì› ë‹¨ìœ„). **ì‘ì„ìˆ˜ë¡ ì •í™•**.
- **MAPE**: ìƒëŒ€ ì˜¤ì°¨(%). **ê·œëª¨ ë‹¤ë¥¸ ê³„ì • ë¹„êµ**ì— ìœ ìš©.
- **AIC/BIC**: ëª¨ë¸ ë³µì¡ë„ê¹Œì§€ ê³ ë ¤í•œ **ì •ë³´ëŸ‰ ì§€í‘œ**. **ì‘ì„ìˆ˜ë¡ ìš°ìˆ˜**.

### ì°¨íŠ¸ ì½ê¸°
- ì‹¤ì„ =ì‹¤ì¸¡, ì ì„ =ì˜ˆì¸¡(**MoR**: EMA/MA/ARIMA/Prophet ì¤‘ ìë™ ì„ íƒ)  
- íšŒìƒ‰ ì ì„ : **ì—°(êµµê²Œ)** / **ë¶„ê¸°(ì–‡ê²Œ)** ê²½ê³„ì„ , ë¶‰ì€ ì ì„ : **PM ê¸°ì¤€ì„ **

### ì‚¬ìš©í•œ ì˜ˆì¸¡ëª¨ë¸
- **MA(ì´ë™í‰ê· )**: ìµœê·¼ *n*ê°œì›” **ë‹¨ìˆœ í‰ê· **. **ì§§ì€ ë°ì´í„°/ë³€ë™ ì™„ë§Œ**í•  ë•Œ ì•ˆì •ì .
- **EMA(ì§€ìˆ˜ì´ë™í‰ê· )**: **ìµœê·¼ê°’ ê°€ì¤‘** í‰ê· . **ìµœê·¼ ì¶”ì„¸ ë°˜ì˜**ì´ í•„ìš”í•  ë•Œ ìœ ë¦¬.
- **ARIMA(p,d,q)**: **ìê¸°ìƒê´€** ê¸°ë°˜. **ê³„ì ˆì„±ì´ ì•½(ë˜ëŠ” ì œê±° ê°€ëŠ¥)**í•˜ê³  **ë°ì´í„°ê°€ ì¶©ë¶„**í•  ë•Œ ê°•í•¨.
- **Prophet**: **ì—°/ë¶„ê¸° ê³„ì ˆì„±Â·íœ´ì¼íš¨ê³¼**ê°€ ëšœë ·í•  ë•Œ ì í•©(ì´ìƒì¹˜ì— ë¹„êµì  ê²¬ê³ ).

> :blue[**ëª¨ë¸ì€ ê³„ì •Ã—ê¸°ì¤€(Flow/Balance)ë³„ë¡œ êµì°¨ê²€ì¦ ì˜¤ì°¨(MAPE/MAE)ì™€ (ê°€ëŠ¥í•˜ë©´) ì •ë³´ëŸ‰(AIC/BIC)ì„ ì¢…í•©í•´ ìë™ ì„ íƒë©ë‹ˆë‹¤.**]
"""
                        )

                    # 0) ì›ì¥/ì„¸ì…˜ í™•ë³´
                    master_df: pd.DataFrame = st.session_state.get("master_df", pd.DataFrame())
                    if master_df.empty:
                        st.info("ì›ì¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    # --- state bootstrap --- (ìœ„ì ¯ ìƒì„± ì „ì— ì‹¤í–‰)
                    st.session_state.setdefault("ts_accounts_names", [])
                    st.session_state.setdefault("ts_cycles_ko", [])
                    st.session_state.setdefault("ts_acc_buffer", None)
                    st.session_state.setdefault("ts_acc_needs_update", False)

                    # --- preset ì£¼ì… í›…: rerun ì§í›„, ë©€í‹°ì…€ë ‰íŠ¸ ìƒì„± 'ì´ì „'ì— 1íšŒ ì£¼ì… ---
                    if st.session_state.ts_acc_needs_update and st.session_state.ts_acc_buffer is not None:
                        st.session_state.ts_accounts_names = st.session_state.ts_acc_buffer
                        st.session_state.ts_acc_needs_update = False

                    upload_id = getattr(uploaded_file, 'name', '_default')

                    # ë‘ ì»¨í…Œì´ë„ˆë¡œ ì‹œê° ìˆœì„œëŠ” ìœ ì§€(ê³„ì • ìœ„, í”„ë¦¬ì…‹ ì•„ë˜) + ì½”ë“œ ìˆœì„œ ì œì–´
                    box_accounts = st.container()
                    box_preset = st.container()

                    # Helper: í”„ë¦¬ì…‹(KO ë¼ë²¨) â†’ ê³„ì •ëª… ë¦¬ìŠ¤íŠ¸ë¡œ í™•ì¥
                    def expand_cycles_to_account_names(*, upload_id: str, cycles_ko: list[str], master_df: pd.DataFrame) -> list[str]:
                        try:
                            mapping = cyc.get_effective_cycles(upload_id)
                            codes = cyc.accounts_for_cycles_ko(mapping, cycles_ko)
                            df_map = master_df[["ê³„ì •ì½”ë“œ","ê³„ì •ëª…"]].dropna().copy()
                            df_map["ê³„ì •ì½”ë“œ"] = df_map["ê³„ì •ì½”ë“œ"].astype(str)
                            code_to_name = df_map.drop_duplicates("ê³„ì •ì½”ë“œ").set_index("ê³„ì •ì½”ë“œ")["ê³„ì •ëª…"].astype(str).to_dict()
                            names = [code_to_name.get(str(c), str(c)) for c in codes]
                            # ìœ ë‹ˆí¬+ìˆœì„œë³´ì¡´
                            return list(dict.fromkeys([n for n in names if n]))
                        except Exception:
                            return []

                    # (ì•„ë˜) í”„ë¦¬ì…‹ ì˜ì—­: ë²„íŠ¼ìœ¼ë¡œ ë²„í¼ë§Œ ê°±ì‹ 
                    with box_preset:
                        st.markdown("#### ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ(ì„ íƒ ì‹œ ìœ„ ê³„ì • ëª©ë¡ì— **ì ìš© ë²„íŠ¼**ìœ¼ë¡œ ë°˜ì˜)")
                        chosen_cycles = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹",
                            options=list(cyc.CYCLE_KO.values()),
                            key="ts_cycles_ko",
                        )
                        if st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="ts_apply_preset"):
                            names_from_cycles = expand_cycles_to_account_names(
                                upload_id=upload_id,
                                cycles_ko=st.session_state.ts_cycles_ko,
                                master_df=master_df,
                            )
                            merged = list(dict.fromkeys([
                                *st.session_state.ts_accounts_names,
                                *names_from_cycles,
                            ]))
                            st.session_state.ts_acc_buffer = merged
                            st.session_state.ts_acc_needs_update = True
                            st.rerun()

                    # (ìœ„) ê³„ì • ì˜ì—­: ë©€í‹°ì…€ë ‰íŠ¸ ê·¸ë¦¬ê¸°(ê°’ ì£¼ì…ì€ ìƒë‹¨ í›…ì´ ë‹´ë‹¹)
                    with box_accounts:
                        all_account_names = (
                            master_df["ê³„ì •ëª…"].dropna().astype(str).sort_values().unique().tolist()
                        )
                        picked_names = st.multiselect(
                            "ëŒ€ìƒ ê³„ì •(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
                            options=all_account_names,
                            key="ts_accounts_names",
                            help="ì„ íƒí•œ ê³„ì •ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ í…Œì´ë¸”/ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
                        )

                    # ë¯¸ë˜ ì˜ˆì¸¡ ê°œì›” ìˆ˜ ìŠ¬ë¼ì´ë” ì¶”ê°€
                    forecast_horizon = st.slider(
                        "ë¯¸ë˜ ì˜ˆì¸¡ ê°œì›” ìˆ˜(ì‹œê°í™”ìš©)", min_value=0, max_value=12, value=0, step=1,
                        help="í‘œë³¸ N<6ì´ë©´ ìë™ìœ¼ë¡œ 0ìœ¼ë¡œ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤."
                        )

                    if not picked_names:
                        st.info("ì‹œê³„ì—´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ì„ íƒí•œ ê³„ì •/ê¸°ê°„ì— ë°ì´í„° ì—†ìŒ)")
                        st.stop()

                    # 2) ê³„ì •ëª… â†’ ê³„ì •ì½”ë“œ
                    name_to_code = (
                        master_df.dropna(subset=["ê³„ì •ëª…","ê³„ì •ì½”ë“œ"]).astype({"ê³„ì •ëª…":"string","ê³„ì •ì½”ë“œ":"string"})
                                 .drop_duplicates(subset=["ê³„ì •ëª…"]).set_index("ê³„ì •ëª…")["ê³„ì •ì½”ë“œ"].to_dict()
                    )
                    want_codes = [name_to_code.get(n) for n in picked_names if n in name_to_code]

                    # 3) ì •ì‹ ì‹œê³„ì—´ íŒŒì´í”„ë¼ì¸: ledger â†’ ì›”ë³„ì§‘ê³„(flow) â†’ balance(opening+ëˆ„ì ) â†’ ì˜ˆì¸¡/ì§„ë‹¨/ê·¸ë¦¼
                    lf_use = st.session_state.get('lf_hist')
                    st.caption("â“˜ ì‹œê³„ì—´ ë¶„ì„ì€ ì¢Œì¸¡ ìŠ¤ì½”í”„ ì„¤ì •ê³¼ ë¬´ê´€í•˜ê²Œ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    if lf_use is None:
                        st.info("ì›ì¥ì„ ë¨¼ì € ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
                        st.stop()

                    # (1) ë¶„ì„ ëŒ€ìƒ ìŠ¬ë¼ì´ìŠ¤
                    ldf = lf_use.df.copy()
                    ldf = ldf[ldf['ê³„ì •ì½”ë“œ'].astype(str).isin([str(x) for x in want_codes])]

                    # (2) í•„ìˆ˜ íŒŒìƒ: ë°œìƒì•¡/ìˆœì•¡ ë³´ì¥
                    from analysis.anomaly import compute_amount_columns
                    ldf = compute_amount_columns(ldf)

                    # (3) ë‚ ì§œ/ê¸ˆì•¡ ì»¬ëŸ¼ í”½ì—…(ì—†ìœ¼ë©´ ì•ˆì „ ì¢…ë£Œ)
                    from analysis.timeseries import DATE_CANDIDATES, AMT_CANDIDATES
                    date_col = next((c for c in DATE_CANDIDATES if c in ldf.columns), None)
                    amount_col = next((c for c in AMT_CANDIDATES if c in ldf.columns), None)
                    if not date_col or not amount_col:
                        st.error(
                            "í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
                            f"- ë‚ ì§œ í›„ë³´: {DATE_CANDIDATES}\n- ê¸ˆì•¡ í›„ë³´: {AMT_CANDIDATES}\n\n"
                            f"í˜„ì¬ ì»¬ëŸ¼: {list(ldf.columns)}"
                        )
                        st.stop()

                    # (4) opening(ì „ê¸°ì´ˆ) ë§µ êµ¬ì„± â€” BS Balanceìš©: Master 'ì „ì „ê¸°ë§ì”ì•¡' ìš°ì„  ì‚¬ìš©
                    opening_map = {}
                    if "ê³„ì •ì½”ë“œ" in master_df.columns:
                        # ì–´ë–¤ ì»¬ëŸ¼ì„ openingìœ¼ë¡œ ì“¸ì§€ ê²°ì •: ì „ì „ê¸°ë§ â†’ ì „ê¸°ë§ â†’ ì—†ìœ¼ë©´ 0
                        src_col = None
                        if "ì „ì „ê¸°ë§ì”ì•¡" in master_df.columns:
                            src_col = "ì „ì „ê¸°ë§ì”ì•¡"
                        elif "ì „ê¸°ë§ì”ì•¡" in master_df.columns:
                            src_col = "ì „ê¸°ë§ì”ì•¡"

                        if src_col:
                            _m = master_df[["ê³„ì •ì½”ë“œ", src_col]].dropna(subset=["ê³„ì •ì½”ë“œ"]).copy()
                            _m["ê³„ì •ì½”ë“œ"] = _m["ê³„ì •ì½”ë“œ"].astype(str)
                            _m[src_col] = pd.to_numeric(_m[src_col], errors="coerce").fillna(0.0)
                            # ë‚´ë¶€ í‚¤ëŠ” í†µì¼í•´ì„œ ì‚¬ìš©
                            opening_map = (
                                _m.rename(columns={src_col: "opening_bs"})
                                  .groupby("ê³„ì •ì½”ë“œ")["opening_bs"].first()
                                  .to_dict()
                            )
                        else:
                            opening_map = {}  # ëª¨ë“  ê³„ì • 0.0ìœ¼ë¡œ ì²˜ë¦¬(ì•„ë˜ì—ì„œ ê¸°ë³¸ê°’ ì ìš©)

                    # (5) BS/PL í”Œë˜ê·¸
                    is_bs_map = {}
                    if "BS/PL" in master_df.columns and "ê³„ì •ì½”ë“œ" in master_df.columns:
                        is_bs_map = (
                            master_df.dropna(subset=["ê³„ì •ì½”ë“œ","BS/PL"])
                            .astype({"ê³„ì •ì½”ë“œ":"string","BS/PL":"string"})
                            .drop_duplicates(subset=["ê³„ì •ì½”ë“œ"])
                            .assign(is_bs=lambda d: d["BS/PL"].str.upper().eq("BS"))
                            .set_index("ê³„ì •ì½”ë“œ")["is_bs"].to_dict()
                        )

                    # (5.5) ì°¨ë³€/ëŒ€ë³€ í”Œë˜ê·¸(ëŒ€ë³€ ê³„ì •ì€ ë¶€í˜¸ ë°˜ì „)
                    is_credit_map = {}
                    if "ì°¨ë³€/ëŒ€ë³€" in master_df.columns and "ê³„ì •ì½”ë“œ" in master_df.columns:
                        is_credit_map = (
                            master_df.dropna(subset=["ê³„ì •ì½”ë“œ","ì°¨ë³€/ëŒ€ë³€"])
                                     .assign(ê³„ì •ì½”ë“œ=lambda d: d["ê³„ì •ì½”ë“œ"].astype(str),
                                             credit=lambda d: d["ì°¨ë³€/ëŒ€ë³€"].astype(str).str.contains("ëŒ€ë³€"))
                                     .drop_duplicates(subset=["ê³„ì •ì½”ë“œ"])
                                     .set_index("ê³„ì •ì½”ë“œ")["credit"].to_dict()
                        )

                    # (6) ëª¨ë¸ ì„ íƒ(ë ˆì§€ìŠ¤íŠ¸ë¦¬)
                    st.caption("ëª¨í˜•: EMA(ê³ ì •). ë³µì¡ ëŸ¬ë„ˆëŠ” ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    backend = "ema"

                    PM = float(st.session_state.get("pm_value", PM_DEFAULT))
                    # (7) ê³„ì •ë³„ ì‹¤í–‰: ê²°ê³¼ ìˆ˜ì§‘ìš© ë²„í¼
                    gathered_flow = []
                    gathered_balance = []
                    results_per_account = {}

                    for code in want_codes:
                        sub = ldf[ldf["ê³„ì •ì½”ë“œ"].astype(str) == str(code)].copy()
                        if sub.empty:
                            continue
                        acc_name = (master_df[master_df["ê³„ì •ì½”ë“œ"].astype(str)==str(code)]["ê³„ì •ëª…"].dropna().astype(str).head(1).tolist() or [str(code)])[0]
                        is_bs = bool(is_bs_map.get(str(code), False))

                        PM = float(st.session_state.get("pm_value", PM_DEFAULT))
                        # âœ¨ ëŒ€ë³€ê³„ì •ì´ë©´ ë¶€í˜¸ ë°˜ì „
                        sign = -1.0 if bool(is_credit_map.get(str(code), False)) else 1.0
                        # ëª¨ë¸ ì…ë ¥ ì»¬ëŸ¼ì„ ìˆ˜ì¹˜í™” + ë°˜ì „
                        try:
                            sub[amount_col] = pd.to_numeric(sub[amount_col], errors="coerce").fillna(0.0) * float(sign)
                        except Exception:
                            sub[amount_col] = pd.to_numeric(sub.get(amount_col, 0.0), errors="coerce").fillna(0.0) * float(sign)
                        # BS ì”ì•¡ìš© openingë„ ë™ì¼ ê¸°ì¤€ìœ¼ë¡œ ë°˜ì „
                        opening = 0.0
                        if isinstance(opening_map, dict):
                            opening = float(opening_map.get(str(code), 0.0)) * float(sign)

                        out = run_timeseries_minimal(
                            sub,
                            account_name=acc_name,
                            date_col=date_col,
                            amount_col=amount_col,
                            is_bs=bool(is_bs),
                            opening=opening,
                            pm_value=PM
                        )

                        # (ìˆ˜ì§‘) í†µí•© ìš”ì•½í‘œ(1í–‰) + ê·¸ë˜í”„(ë‹¤í¬ì¸íŠ¸) ë¶„ë¦¬
                        if not out.empty:
                            tmp = out.copy()
                            tmp.insert(0, "ê³„ì •", acc_name)

                            # ê·¸ë˜í”„/ì§„ë‹¨ìš©(ì „ êµ¬ê°„)
                            results_per_account[acc_name] = tmp

                            # === ìš”ì•½í–‰(ë§ˆì§€ë§‰ 1í–‰) + í†µê³„ì—´ ì¶”ê°€ ===
                            for ms in ("flow", "balance"):
                                dfm = tmp[tmp["measure"].eq(ms)]
                                if dfm.empty:
                                    continue
                                stats = compute_series_stats(dfm)
                                last_row = dfm.tail(1).copy()
                                last_row["MAE"]  = stats["MAE"]
                                last_row["MAPE"] = stats["MAPE"]
                                last_row["RMSE"] = stats["RMSE"]
                                last_row["N"]    = stats["N"]

                                if ms == "flow":
                                    gathered_flow.append(last_row)
                                else:
                                    gathered_balance.append(last_row)

                    # === ê³µìš©: í‘œ ë†’ì´ ìë™ ê³„ì‚° ===
                    def _auto_table_height(df: pd.DataFrame, *, min_rows=3, max_rows=10, row_px=32, header_px=40, padding_px=16) -> int:
                        n = 0 if df is None else int(len(df))
                        n = max(min_rows, min(max_rows, n))
                        return header_px + n * row_px + padding_px

                    # === NEW: í†µí•© í…Œì´ë¸”(ê·¸ë˜í”„ë³´ë‹¤ ìœ„ì— í•œ ë²ˆë§Œ) ===
                    def _render_table(blocks, title):
                        if not blocks:
                            return
                        tbl = pd.concat(blocks, ignore_index=True)

                        show_cols = ["ê³„ì •","date","actual","predicted","error","z","risk","model",
                                     "MAE","MAPE","RMSE","N"]  # â† í†µê³„ ì—´ ì¶”ê°€
                        for c in show_cols:
                            if c not in tbl.columns:
                                tbl[c] = np.nan

                        # ì‚¬ìš©ì ì¹œí™” ë¼ë²¨/ì •ë ¬
                        tbl = (tbl.rename(columns={
                            "date":"ì¼ì","actual":"ì‹¤ì¸¡","predicted":"ì˜ˆì¸¡",
                            "error":"ì”ì°¨","risk":"ìœ„í—˜ë„","model":"ëª¨ë¸(MoR)"
                        })
                            .sort_values(["ê³„ì •","ì¼ì"])
                        )

                        st.subheader(title)
                        fmt = {}
                        for c in ["ì‹¤ì¸¡","ì˜ˆì¸¡","ì”ì°¨","MAE","RMSE"]:
                            if c in tbl.columns: fmt[c] = "{:,.0f}"
                        if "MAPE" in tbl.columns: fmt["MAPE"] = "{:.2f}%"
                        if "z" in tbl.columns: fmt["z"] = "{:.2f}"
                        if "ìœ„í—˜ë„" in tbl.columns: fmt["ìœ„í—˜ë„"] = "{:.2f}"

                        # í‘œ ë†’ì´: í–‰ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìë™(ìµœëŒ€ 420px)
                        rows = max(1, len(tbl))
                        height = min(420, 42 + rows * 28)

                        st.dataframe(tbl[["ê³„ì •","ì¼ì","ì‹¤ì¸¡","ì˜ˆì¸¡","ì”ì°¨","z","ìœ„í—˜ë„","ëª¨ë¸(MoR)","MAE","MAPE","RMSE","N"]]
                                     .style.format(fmt),
                                     use_container_width=True, height=height)

                        st.download_button(
                            "CSV ë‹¤ìš´ë¡œë“œ", data=tbl.to_csv(index=False).encode("utf-8-sig"),
                            file_name=f"timeseries_summary_{'flow' if 'Flow' in title else 'balance'}.csv",
                            mime="text/csv"
                        )

                    def _auto_height(df: pd.DataFrame, max_rows: int = 12) -> int:
                        rows = int(min(len(df), max_rows))
                        base_row = 34  # ì²´ê°ê°’
                        header = 38
                        pad = 8
                        return header + rows * base_row + pad

                    def _render_table_combined(flow_blocks, balance_blocks, title="ì„ íƒê³„ì • ìš”ì•½ (Flow+Balance)"):
                        import pandas as pd
                        import numpy as np
                        blocks = []

                        def _prep(df, label):
                            if df is None or len(df) == 0:
                                return None
                            x = pd.concat(df, ignore_index=True)
                            
                            # ë¨¼ì € renameì„ í•´ì„œ ì»¬ëŸ¼ëª…ì„ í†µì¼
                            rename_map = {
                                "account": "ê³„ì •", "date": "ì¼ì",
                                "actual": "ì‹¤ì¸¡", "predicted": "ì˜ˆì¸¡",
                                "error": "ì”ì°¨", "risk": "ìœ„í—˜ë„",
                                "model": "ëª¨ë¸(MoR)"
                            }
                            for k, v in rename_map.items():
                                if k in x.columns and v not in x.columns:  # ì¤‘ë³µ ë°©ì§€
                                    x.rename(columns={k: v}, inplace=True)
                            
                            # ê·¸ ë‹¤ìŒ 'ê¸°ì¤€' ì»¬ëŸ¼ ì¶”ê°€
                            x.insert(0, "ê¸°ì¤€", label)
                            return x

                        f = _prep(flow_blocks, "ë°œìƒì•¡(Flow)")
                        b = _prep(balance_blocks, "ì”ì•¡(Balance)")
                        if f is not None: blocks.append(f)
                        if b is not None: blocks.append(b)
                        if not blocks:
                                return

                        tbl = pd.concat(blocks, ignore_index=True)

                        # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±° (í˜¹ì‹œ ìˆë‹¤ë©´)
                        tbl = tbl.loc[:, ~tbl.columns.duplicated()]

                        # âœ… z ë¼ë²¨ ë³€ê²½(í‘œì—ì„œë§Œ)
                        col_map = {
                            "date":"ì¼ì","actual":"ì‹¤ì¸¡","predicted":"ì˜ˆì¸¡","error":"ì”ì°¨",
                            "z":"z(ì‹œê³„ì—´)","risk":"ìœ„í—˜ë„","model":"ëª¨ë¸(MoR)"
                        }
                        for k, v in col_map.items():
                            if k in tbl.columns:
                                tbl.rename(columns={k: v}, inplace=True)

                        want_cols = ["ê¸°ì¤€", "ê³„ì •", "ì¼ì", "ì‹¤ì¸¡", "ì˜ˆì¸¡", "ì”ì°¨", "z(ì‹œê³„ì—´)", "ìœ„í—˜ë„", "ëª¨ë¸(MoR)"]
                        show_cols = [c for c in want_cols if c in tbl.columns]
                        tbl = tbl[show_cols].copy()

                        # í¬ë§·
                        fmt = {"ì‹¤ì¸¡":"{:,.0f}","ì˜ˆì¸¡":"{:,.0f}","ì”ì°¨":"{:,.0f}","ìœ„í—˜ë„":"{:.2f}","z(ì‹œê³„ì—´)":"{:.2f}"}

                        st.subheader(title)

                        # ì¸ë±ìŠ¤ ìˆ¨ê¹€ + í†µì¼ëœ ë†’ì´
                        tbl = tbl.reset_index(drop=True)
                        try:
                            st.dataframe(
                                tbl.style.format(fmt),
                                use_container_width=True,
                                hide_index=True,
                                height=_auto_height(tbl)
                            )
                        except TypeError:
                            st.dataframe(
                                tbl.style.format(fmt),
                                use_container_width=True,
                                height=_auto_height(tbl)
                            )

                        # CSV
                            st.download_button(
                            "CSV ë‹¤ìš´ë¡œë“œ",
                            data=tbl.to_csv(index=False).encode("utf-8-sig"),
                            file_name="timeseries_summary_all.csv",
                                mime="text/csv"
                            )

                    def _render_outlier_alert(results_per_account: dict, *, topn: int = 10, z_thr: float = 2.0):
                        """
                        results_per_account: {ê³„ì •ëª… -> DataFrame}, DataFrameì€ ìµœì†Œ ì»¬ëŸ¼
                          ['date','actual','predicted','error','z','risk','model','measure'] ê°€ì •
                          measure âˆˆ {'flow','balance'}
                        """
                        import pandas as pd
                        import numpy as np
                        rows = []
                        for acc_name, df in (results_per_account or {}).items():
                            if df is None or df.empty or "z" not in df.columns:
                                continue
                            dfx = df.copy()
                            # ê³„ì •ëª… ë³´ê°• (í˜¹ì‹œ ëˆ„ë½ ëŒ€ë¹„)
                            if "ê³„ì •" not in dfx.columns:
                                dfx["ê³„ì •"] = acc_name
                            # ê¸°ì¤€ ë¼ë²¨(ë°œìƒì•¡/ì”ì•¡)
                            dfx["ê¸°ì¤€"] = dfx.get("measure", "").map(
                                {"flow": "ë°œìƒì•¡(Flow)", "balance": "ì”ì•¡(Balance)"}
                            ).fillna("ë°œìƒì•¡(Flow)")
                            # |z| í•„í„°
                            dfx = dfx[dfx["z"].abs() >= float(z_thr)]
                            if not dfx.empty:
                                rows.append(dfx)

                        # í—¤ë” + í…Œì´ë¸”
                        import streamlit as st
                        st.subheader(f"ì´ìƒì›” ì•Œë¦¼ (ìƒìœ„ {topn}ê±´, ê¸°ì¤€ |z| â‰¥ {z_thr:.1f})")

                        if not rows:
                            st.info(f"ì´ìƒì›” ì—†ìŒ(ê¸°ì¤€ |z| â‰¥ {z_thr:.1f})")
                            return

                        out = pd.concat(rows, ignore_index=True)

                        # ì •ë ¬: |z| ë‚´ë¦¼ì°¨ìˆœ â†’ |ì”ì°¨| ë³´ì¡°
                        out = out.sort_values(
                            by=["z", "error"],
                            key=lambda s: s.abs() if s.name in ("z", "error") else s,
                            ascending=[False, False]
                        ).head(int(topn))

                        # í‘œì‹œ ì»¬ëŸ¼/í•œê¸€ëª… + z ë¼ë²¨ ë³€ê²½
                        rename = {"date": "ì¼ì", "actual": "ì‹¤ì¸¡", "predicted": "ì˜ˆì¸¡",
                                  "error": "ì”ì°¨", "z": "z(ì‹œê³„ì—´)", "risk": "ìœ„í—˜ë„", "model": "ëª¨ë¸"}
                        for k, v in rename.items():
                            if k in out.columns:
                                out.rename(columns={k: v}, inplace=True)

                        show_cols = [c for c in ["ê³„ì •", "ì¼ì", "ì‹¤ì¸¡", "ì˜ˆì¸¡", "ì”ì°¨", "z(ì‹œê³„ì—´)", "ìœ„í—˜ë„", "ëª¨ë¸", "ê¸°ì¤€"] if c in out.columns]
                        out = out[show_cols]

                        fmt = {"ì‹¤ì¸¡":"{:,.0f}","ì˜ˆì¸¡":"{:,.0f}","ì”ì°¨":"{:,.0f}","ìœ„í—˜ë„":"{:.2f}","z(ì‹œê³„ì—´)":"{:.2f}"}

                        try:
                            st.dataframe(
                                out.style.format(fmt),
                                use_container_width=True,
                                hide_index=True,
                                height=_auto_height(out)
                            )
                        except TypeError:
                            # Streamlit êµ¬ë²„ì „ í˜¸í™˜
                            st.dataframe(
                                out.reset_index(drop=True).style.format(fmt),
                                use_container_width=True,
                                height=_auto_height(out)
                            )

                    # === NEW: ì„ íƒê³„ì • í†µê³„ ë° ì´ìƒì›” ë¦¬ìŠ¤íŠ¸ ë Œë”ë§ í•¨ìˆ˜ ì •ì˜ ===
                    def _safe_div(a, b):
                        try:
                            b = np.where(np.abs(b) < 1e-9, 1.0, b)
                            return a / b
                        except Exception:
                            return np.nan

                    _render_table_combined(gathered_flow, gathered_balance, title="ì„ íƒê³„ì • ìš”ì•½ (Flow+Balance)")

                    # í†µí•© ì´ìƒì›” ì•Œë¦¼ (|z| â‰¥ 2.0 ê³ ì •)
                    _render_outlier_alert(results_per_account, topn=10, z_thr=2.0)

                    # ============ ğŸ” ì‹œê³„ì—´ íŒŒì´í”„ë¼ì¸ ì§„ë‹¨(í˜„í™©íŒ) ============ #
                    with st.expander("ğŸ” ì‹œê³„ì—´ íŒŒì´í”„ë¼ì¸ ì§„ë‹¨(í˜„í™©íŒ)", expanded=False):
                        st.caption("ê° ë‹¨ê³„ë³„ë¡œ í¬ì¸íŠ¸ ìˆ˜/íƒ€ì…/ì •ê·œí™” ìƒíƒœë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤. ê·¸ë˜í”„ê°€ ì•ˆ ëœ¨ë©´ ì–´ë””ì„œ ëŠê²¼ëŠ”ì§€ ì—¬ê¸°ì„œ í™•ì¸í•˜ì„¸ìš”.")
                        # 0) ì›ë³¸ ìŠ¬ë¼ì´ìŠ¤ ìš”ì•½
                        st.markdown("**0) ì…ë ¥(ì›ì¥ ìŠ¬ë¼ì´ìŠ¤) ìš”ì•½**")
                        try:
                            st.write({
                                "ì„ íƒê³„ì • ìˆ˜": len(want_codes),
                                "ì›ì¥ í–‰ìˆ˜(ì„ íƒê³„ì •)": int(len(ldf)),
                                "date_col": date_col,
                                "amount_col": amount_col,
                                "date_dtype": str(ldf[date_col].dtype),
                                "amount_dtype": str(ldf[amount_col].dtype),
                                "NaT(ë‚ ì§œ)": int(pd.to_datetime(ldf[date_col], errors="coerce").isna().sum()),
                                "NaN(ê¸ˆì•¡)": int(pd.to_numeric(ldf[amount_col], errors="coerce").isna().sum()),
                                "ê¸°ê°„": f"{pd.to_datetime(ldf[date_col], errors='coerce').min()} ~ {pd.to_datetime(ldf[date_col], errors='coerce').max()}",
                            })
                            st.dataframe(
                                ldf[[date_col, "ê³„ì •ì½”ë“œ", "ê³„ì •ëª…", amount_col]].head(5),
                                use_container_width=True,
                                height=_auto_table_height(ldf.head(5))
                            )
                        except Exception as _e:
                            st.warning(f"ì…ë ¥ ìš”ì•½ ì‹¤íŒ¨: {_e}")

                        # 1) ê³„ì •Ã—ì›” ì§‘ê³„ í™•ì¸
                        st.markdown("**1) ì›”ë³„ ì§‘ê³„ ìƒíƒœ**")
                        try:
                            _tmp = ldf[[date_col, amount_col]].copy()
                            _tmp = _tmp.rename(columns={date_col: 'íšŒê³„ì¼ì', amount_col: 'ê±°ë˜ê¸ˆì•¡'})
                            _grp = aggregate_monthly(_tmp, date_col='íšŒê³„ì¼ì', amount_col='ê±°ë˜ê¸ˆì•¡').rename(columns={"amount":"flow"})
                            _grp["date"] = pd.to_datetime(_grp["date"], errors="coerce")
                            _norm_ok = int((_grp["date"].dt.hour.eq(0) & _grp["date"].dt.minute.eq(0)).sum())
                            st.write({
                                "ì§‘ê³„ í¬ì¸íŠ¸ ìˆ˜": int(len(_grp)),
                                "ì›”ë§ 00:00:00 ë¹„ìœ¨": f"{_norm_ok}/{len(_grp)}",
                                "ì˜ˆ: ì²« 3í–‰": None
                            })
                            st.dataframe(
                                _grp.head(3),
                                use_container_width=True,
                                height=_auto_table_height(_grp.head(3))
                            )
                            # (ë³´ë„ˆìŠ¤) ê²½ê³„ì„  ì˜ˆìƒ ê°œìˆ˜
                            try:
                                rng = pd.date_range(pd.to_datetime(ldf[date_col]).min(), pd.to_datetime(ldf[date_col]).max(), freq="M")
                                q_ends = [m for m in rng if m.month in (3,6,9,12)]
                                y_ends = [m for m in rng if m.month == 12]
                                st.write({"ê²½ê³„ì„ (ë¶„ê¸°ë§) ì˜ˆìƒ ê°œìˆ˜": len(q_ends), "ê²½ê³„ì„ (ì—°ë§) ì˜ˆìƒ ê°œìˆ˜": len(y_ends)})
                            except Exception as _ee:
                                st.info(f"ê²½ê³„ì„  ê°œìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {_ee}")
                        except Exception as _e:
                            st.warning(f"ì›”ë³„ ì§‘ê³„ ìƒíƒœ ê³„ì‚° ì‹¤íŒ¨: {_e}")

                        # 2) ëŸ¬ë„ˆ ê²°ê³¼ ìš”ì•½
                        st.markdown("**2) ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ìƒíƒœ(run_timeseries_minimal Â· EMA)**")
                        try:
                            if not (gathered_flow or gathered_balance):
                                st.warning("ëŸ¬ë„ˆ ì¶œë ¥(gathered_*)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìƒë‹¨ ì…ë ¥/ì§‘ê³„ ë‹¨ê³„ í™•ì¸ í•„ìš”.")
                            else:
                                parts = []
                                if gathered_flow: parts += gathered_flow
                                if gathered_balance: parts += gathered_balance
                                parts = [p for p in parts if isinstance(p, pd.DataFrame) and not p.empty]
                                if not parts:
                                    st.warning("ëŸ¬ë„ˆ ì¶œë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.(ìœ íš¨í•œ DataFrame ì—†ìŒ)")
                                else:
                                    _all = pd.concat(parts, ignore_index=True)
                                    st.write({
                                        "ê³„ì •Ã—ê¸°ì¤€(measure) ê°œìˆ˜": int(_all[["ê³„ì •","measure"]].drop_duplicates().shape[0]) if set(["ê³„ì •","measure"]).issubset(_all.columns) else 0,
                                        "actual ì¡´ì¬": bool("actual" in _all.columns),
                                        "predicted ì¡´ì¬": bool("predicted" in _all.columns),
                                        "flow í¬ì¸íŠ¸": int(_all[_all.get("measure","flow").eq("flow")].shape[0]) if "measure" in _all.columns else int(_all.shape[0]),
                                        "balance í¬ì¸íŠ¸": int(_all[_all.get("measure","flow").eq("balance")].shape[0]) if "measure" in _all.columns else 0,
                                    })
                                    st.dataframe(
                                        _all.head(5),
                                        use_container_width=True,
                                        height=_auto_table_height(_all.head(5))
                                    )
                        except Exception as _e:
                            st.warning(f"ëŸ¬ë„ˆ ì¶œë ¥ ìš”ì•½ ì‹¤íŒ¨: {type(_e).__name__}: {_e}")

                        # === Opening ì†ŒìŠ¤/ì ìš© ì—¬ë¶€ í‘œì‹œ ===
                        st.markdown("**Opening ì†ŒìŠ¤ ë° ì ìš© í˜„í™©**")
                        try:
                            src_used = ("ì „ì „ê¸°ë§ì”ì•¡" if "ì „ì „ê¸°ë§ì”ì•¡" in master_df.columns else
                                        "ì „ê¸°ë§ì”ì•¡" if "ì „ê¸°ë§ì”ì•¡" in master_df.columns else "N/A")
                            st.write({"opening_source_for_BS_balance": src_used})
                            # í‘œë³¸ 1~2ê°œ ê³„ì •ì— ëŒ€í•´ opening ì ìš©ê°’ ë¯¸ë¦¬ë³´ê¸°
                            _peek = []
                            for k, v in list(opening_map.items())[:2]:
                                _peek.append({"ê³„ì •ì½”ë“œ": k, "opening_raw": v})
                            if _peek:
                                st.dataframe(pd.DataFrame(_peek), use_container_width=True, height=120)
                        except Exception:
                            pass

                        # ë¶€í˜¸ ë³´ì • ê°€ë“œ í‘œì‹œ
                        st.markdown("**ë¶€í˜¸ ë³´ì • ê°€ë“œ**")
                        try:
                            pipeline_norm = any(c in ldf.columns for c in ["ë°œìƒì•¡_norm", "amount_norm", "__sign", "sign"])
                            plot_sign_flip = False  # í”Œë¡¯ ë ˆë²¨ ë°˜ì „ì€ í•˜ì§€ ì•ŠìŒ
                            st.write({
                                "pipeline_norm": bool(pipeline_norm),
                                "plot_sign_flip": False,   # í”Œë¡¯ ë ˆë²¨ ë°˜ì „ ê¸ˆì§€
                                "opening_sign_applied_in_pipeline": True,
                                "guard_ok": bool(pipeline_norm and not plot_sign_flip)
                            })
                            if pipeline_norm and plot_sign_flip:
                                st.warning("ê²½ê³ : íŒŒì´í”„ë¼ì¸ê³¼ í”Œë¡¯ì—ì„œ ëª¨ë‘ ë¶€í˜¸ë¥¼ ë§Œì§€ë©´ ì´ì¤‘ ë°˜ì „ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
                        except Exception as _e:
                            st.info(f"ë¶€í˜¸ ë³´ì • ê°€ë“œ ì ê²€ ì‹¤íŒ¨: {_e}")

                        # 3) ê·¸ë¦¼ ì…ë ¥ ì „ ì ê²€(ê³„ì •ë³„)
                        st.markdown("**3) ê·¸ë¦¼ ì…ë ¥ ì‚¬ì „ ì ê²€(create_timeseries_figure ì§ì „)**")
                        try:
                            for acc_name, df_all in results_per_account.items():
                                for ms in (["flow","balance"] if df_all["measure"].eq("balance").any() else ["flow"]):
                                    dfx = df_all[df_all["measure"].eq(ms)]
                                    st.write(f"- {acc_name} / {ms}: N={len(dfx)} Â· ì»¬ëŸ¼={list(dfx.columns)} Â· ë‚ ì§œë²”ìœ„={pd.to_datetime(dfx['date']).min()}~{pd.to_datetime(dfx['date']).max()}")
                                    st.dataframe(
                                        dfx[["date","actual","predicted"]].head(3),
                                        use_container_width=True,
                                        height=_auto_table_height(dfx.head(3))
                                    )
                        except Exception as _e:
                            st.warning(f"ê·¸ë¦¼ ì…ë ¥ ì ê²€ ì‹¤íŒ¨: {_e}")

                    # === ì—°/ë¶„ê¸° ê²½ê³„ì„  ê¸°ë³¸ í‘œì‹œ ===
                    show_dividers = True

                    # === ê³„ì •Ã—ê¸°ì¤€ í†µê³„ ìš”ì•½ ===
                    def _safe_stats_block(df_in: pd.DataFrame) -> dict:
                        s = {}
                        try:
                            a = pd.to_numeric(df_in.get("actual"), errors="coerce")
                            p = pd.to_numeric(df_in.get("predicted"), errors="coerce")
                            e = a - p
                            s["N"] = int(len(df_in))
                            s["MAE"] = float(np.nanmean(np.abs(e)))
                            denom = a.replace(0, np.nan)
                            s["MAPE(%)"] = float(np.nanmean(np.abs(e / denom)) * 100.0)
                            s["RMSE"] = float(np.sqrt(np.nanmean((e**2))))
                            z = pd.to_numeric(df_in.get("z"), errors="coerce")
                            s["|z|_max"] = float(np.nanmax(np.abs(z))) if z is not None else np.nan
                            s["last_z"] = float(z.iloc[-1]) if (z is not None and len(z) > 0) else np.nan
                            s["last_err"] = float(e.iloc[-1]) if len(e) > 0 else np.nan
                            s["model"] = str(df_in.get("model").iloc[-1]) if "model" in df_in.columns and len(df_in) > 0 else ""
                        except Exception:
                            pass
                        return s

                    stats_rows = []
                    for acc_name, df_all in results_per_account.items():
                        for ms in (["flow","balance"] if df_all["measure"].eq("balance").any() else ["flow"]):
                            d = df_all[df_all["measure"].eq(ms)]
                            if d.empty:
                                continue
                            row = {"ê³„ì •": acc_name, "ê¸°ì¤€": ("ë°œìƒì•¡(Flow)" if ms=="flow" else "ì”ì•¡(Balance)")}
                            row.update(_safe_stats_block(d))
                            stats_rows.append(row)

                    if stats_rows:
                        stats_df = pd.DataFrame(stats_rows)[
                            ["ê³„ì •","ê¸°ì¤€","N","MAE","MAPE(%)","RMSE","|z|_max","last_z","last_err","model"]
                        ]
                        fmt = {"MAE":"{:,.0f}","MAPE(%)":"{:.2f}","RMSE":"{:,.0f}","|z|_max":"{:.2f}","last_z":"{:.2f}","last_err":"{:,.0f}"}
                        st.subheader("ê³„ì •ë³„ í†µê³„ ìš”ì•½")
                        st.dataframe(
                            stats_df.style.format(fmt),
                            use_container_width=True,
                            height=_auto_table_height(stats_df)
                        )

                    # === ê·¸ë˜í”„ ë Œë”(ì•„ë˜): ê³„ì •ë³„ë¡œ í‘œì‹œ ===
                    for acc_name, df_all in results_per_account.items():
                        for measure in (["flow","balance"] if (df_all["measure"].eq("balance").any()) else ["flow"]):
                            dfm = df_all[df_all["measure"]==measure].rename(columns={"account":"ê³„ì •"})
                            title = f"{acc_name} â€” {'ë°œìƒì•¡(Flow)' if measure=='flow' else 'ì”ì•¡(Balance)'}"

                            # í‘œë³¸ìˆ˜ ê²Œì´íŠ¸: N<6ì´ë©´ ë¯¸ë˜ ìŒì˜ ë¹„í™œì„±í™”
                            stats = compute_series_stats(dfm)
                            _hz = int(forecast_horizon or 0)
                            if stats["N"] < 6:
                                _hz = 0

                            fig, stats_d = create_timeseries_figure(
                                dfm, measure=measure, title=title,
                                pm_value=PM, show_dividers=True   # â† ë¶„ê¸°/ì—° ê²½ê³„ì„  ì¼œê¸°
                            )

                            # ë¯¸ë˜ êµ¬ê°„ ìŒì˜(ì‹œê°í™” ì „ìš©)
                            if _hz > 0 and not dfm.empty:
                                last_date = pd.to_datetime(dfm["date"]).max()
                                fig = add_future_shading(fig, last_date, horizon_months=_hz)

                            st.subheader(title)
                            if fig is not None:
                                st.plotly_chart(fig, use_container_width=True)

                            # MoR ë¡œê·¸ í‘œì‹œ
                            log = (results_per_account.get(acc_name, pd.DataFrame())).attrs.get("mor_log", {})
                            if stats_d or log:
                                st.caption(
                                    f"ëª¨í˜•:{dfm['model'].iloc[-1] if not dfm.empty else '-'} Â· "
                                    f"ì„ ì •ê·¼ê±°:{log.get('metric','-')} "
                                    f"(MAPE={log.get('mape_best',''):g}%, MAE={log.get('mae_best',''):,.0f}) Â· "
                                    f"í‘œë³¸ì›”:{log.get('n_months','-')}"
                                    + ("" if _hz == forecast_horizon else " Â· (í‘œë³¸ ë¶€ì¡±ìœ¼ë¡œ ë¯¸ë˜ìŒì˜ ë¹„í™œì„±)")
                                )
                # âš ï¸ ê¸°ì¡´ tab5(ìœ„í—˜í‰ê°€) ë¸”ë¡ ì „ì²´ ì‚­ì œë¨
                
                with tab_corr:
                    st.header("ìƒê´€ê´€ê³„")
                    if not uploaded_file:
                        st.info("ğŸ“ ì›ì¥ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìƒê´€ê´€ê³„ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                    else:
                        upload_id = getattr(uploaded_file, 'name', '_default')
                        tb1, tb2, tb3 = st.tabs(["ê¸°ë³¸", "ê³ ê¸‰", "í¬ì»¤ìŠ¤(1ê³„ì •)"])
                        with tb1: _render_corr_basic_tab(upload_id=upload_id)
                        with tb2: _render_corr_advanced_tab(upload_id=upload_id)
                        with tb3: _render_corr_focus_tab(upload_id=upload_id)
                with tab_report:
                    st.header("ğŸ§  ë¶„ì„ ì¢…í•© ëŒ€ì‹œë³´ë“œ")
                    
                    # === ì•± ì´ˆê¸°í™” ì‹œ ë¹ˆ í™”ë©´ ë°©ì§€ ===
                    if not uploaded_file:
                        st.info("ğŸ“ ì›ì¥ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì¢…í•© ëŒ€ì‹œë³´ë“œê°€ í‘œì‹œë©ë‹ˆë‹¤.")
                    elif not st.session_state.get('modules', {}):
                        st.info("ğŸ“Š ë‹¤ë¥¸ ë¶„ì„ íƒ­(ì‹œê³„ì—´, ì´ìƒ íŒ¨í„´ ë“±)ì„ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ê°€ ì—¬ê¸°ì— ì§‘ê³„ë©ë‹ˆë‹¤.")
                        st.caption("ğŸ’¡ **ì‹œê³„ì—´ ì˜ˆì¸¡** íƒ­ì—ì„œ ê³„ì •ì„ ì„ íƒí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
                    else:
                        # --- Preview: modules session quick view ---
                        modules_list_preview = list(st.session_state.get('modules', {}).values())
                        with st.expander("ğŸ” ëª¨ë“ˆë³„ ìš”ì•½/ì¦ê±° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                            if not modules_list_preview:
                                st.info("ëª¨ë“ˆ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ê° ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                            else:
                                for mr in modules_list_preview:
                                    try:
                                        st.subheader(f"â€¢ {getattr(mr, 'name', 'module')}")
                                        if getattr(mr, 'summary', None):
                                            st.json(mr.summary)
                                        evs = list(getattr(mr, 'evidences', []))
                                        if evs:
                                            st.write("Evidence ìƒ˜í”Œ (ìƒìœ„ 3)")
                                            for ev in evs[:3]:
                                                try:
                                                    st.write(f"- reason={ev.reason} | risk={float(ev.risk_score):.2f} | amount={float(ev.financial_impact):,.0f}")
                                                except Exception:
                                                    st.write("- (í‘œì‹œ ì‹¤íŒ¨)")
                                        if getattr(mr, 'tables', None):
                                            try: st.caption(f"tables: {list(mr.tables.keys())}")
                                            except Exception: pass
                                        if getattr(mr, 'figures', None):
                                            try: st.caption(f"figures: {list(mr.figures.keys())}")
                                            except Exception: pass
                                    except Exception:
                                        st.caption("(ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨)")
                    # LLM í‚¤ ë¯¸ê°€ìš©ì´ì–´ë„ ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„± ê°€ëŠ¥
                    LLM_OK = False
                    try:
                        from services.llm import openai_available
                        LLM_OK = bool(openai_available())
                    except Exception:
                        LLM_OK = False
                    if not LLM_OK:
                        st.info("ğŸ”Œ OpenAI Key ì—†ìŒ: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ë¯¸ì‚¬ìš©)")
                    rendered_report = False

                    # === ëª¨ë¸/í† í°/ì»¨í…ìŠ¤íŠ¸ ì˜µì…˜ UI ===
                    colm1, colm2, colm3 = st.columns([1,1,1])
                    with colm1:
                        llm_model_choice = st.selectbox(
                            "LLM ëª¨ë¸", options=["gpt-5", "gpt-4o"], index=1,
                            help="gpt-5 ë¯¸ê°€ìš© ì‹œ ìë™ìœ¼ë¡œ gpt-4oë¡œ ëŒ€ì²´í•˜ì„¸ìš”(ì½”ë“œì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬)."
                        )
                    with colm2:
                        desired_tokens = st.number_input(
                            "ë³´ê³ ì„œ ìµœëŒ€ ì¶œë ¥ í† í°", min_value=512, max_value=32000, value=16000, step=512,
                            help="ì‹¤ì œ ì „ì†¡ê°’ì€ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ì™€ ì…ë ¥ í† í°ì„ ê³ ë ¤í•´ ì•ˆì „ í´ë¨í”„ë©ë‹ˆë‹¤."
                        )
                    with colm3:
                        ctx_topk = st.number_input("ì»¨í…ìŠ¤íŠ¸ Evidence Top-K(ëª¨ë“ˆë³„)", min_value=5, max_value=100, value=20, step=5)
                        st.caption("ìš”ì•½/ë„í‘œëŠ” ìµœì†Œí™”í•˜ê³  ì¦ê±°ëŠ” ìƒìœ„ Top-Kë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        st.session_state['ctx_topk'] = int(ctx_topk)

                    # ì„ íƒí•œ ëª¨ë¸/í† í°ì„ ì„¸ì…˜ì— ì €ì¥í•˜ì—¬ í•˜ë‹¨ í˜¸ì¶œë¶€ì—ì„œ ì‹¤ì œ ì‚¬ìš©
                    st.session_state['llm_model'] = llm_model_choice
                    st.session_state['llm_max_tokens'] = int(desired_tokens)

                    # --- ì…ë ¥ ì˜ì—­ ---
                    mdf = st.session_state.master_df
                    ldf = st.session_state.ledger_df

                    # â‘  ê³„ì • ì„ íƒ(í•„ìˆ˜) â€” ìë™ ì¶”ì²œ ì œê±°
                    acct_names_all = sorted(mdf['ê³„ì •ëª…'].dropna().unique().tolist())
                    pick_accounts = st.multiselect(
                        "ë³´ê³ ì„œ ëŒ€ìƒ ê³„ì •(ë“¤)ì„ ì„ íƒí•˜ì„¸ìš”. (ìµœì†Œ 1ê°œ)",
                        options=acct_names_all,
                        default=[],
                        key="report_accounts_pick"
                    )
                    cycles_map_now = cyc.get_effective_cycles(upload_id)
                    if cycles_map_now:
                        picked_cycles_report = st.multiselect(
                            "ì‚¬ì´í´ í”„ë¦¬ì…‹ ì„ íƒ", list(cyc.CYCLE_KO.values()),
                            default=[], key="report_cycles_pick"
                        )
                        st.button("â• í”„ë¦¬ì…‹ ì ìš©", key="btn_apply_cycles_report", on_click=_apply_cycles_to_picker,
                                  kwargs=dict(upload_id=upload_id,
                                              cycles_state_key="report_cycles_pick",
                                              accounts_state_key="report_accounts_pick",
                                              master_df=st.session_state.master_df))
                    # â‘¡ ì˜µì…˜ ì œê±°: í•­ìƒ ìˆ˜í–‰ í”Œë˜ê·¸
                    opt_knn_evidence = True
                    opt_patterns = True
                    opt_patterns_py = True

                    # â‘¢ ì‚¬ìš©ì ë©”ëª¨(ì„ íƒ)
                    manual_ctx = st.text_area(
                        "ë³´ê³ ì„œì— ì¶”ê°€í•  ë©”ëª¨/ì£¼ì˜ì‚¬í•­(ì„ íƒ)",
                        placeholder="ì˜ˆ: 5~7ì›” ëŒ€í˜• ìº í˜ì¸ ì§‘í–‰ ì˜í–¥, 3ë¶„ê¸°ë¶€í„° ë‹¨ê°€ ì¸ìƒ ì˜ˆì • ë“±"
                    )

                    # â‘£ ì„ íƒ ê³„ì •ì½”ë“œ ë§¤í•‘
                    pick_codes = (
                        mdf[mdf['ê³„ì •ëª…'].isin(st.session_state['report_accounts_pick'])]['ê³„ì •ì½”ë“œ']
                        .astype(str).tolist()
                    )

                    colA, colB, colC = st.columns([1,1,1])
                    with colA: st.write("ì„ íƒ ê³„ì •ì½”ë“œ:", ", ".join(pick_codes) if pick_codes else "-")
                    with colB: st.write("ê¸°ì¤€ ì—°ë„(CY):", int(ldf['ì—°ë„'].max()))
                    with colC: st.write("ë³´ê³ ì„œ ê¸°ì¤€:", "Current Year GL")

                    # ë²„íŠ¼ì€ ê³„ì • ë¯¸ì„ íƒ ì‹œ ë¹„í™œì„±í™”
                    btn = st.button("ğŸ“ ë³´ê³ ì„œ ìƒì„±", type="primary", disabled=(len(pick_codes) == 0))
                    if len(pick_codes) == 0:
                        st.info("ê³„ì • 1ê°œ ì´ìƒ ì„ íƒ ì‹œ ë²„íŠ¼ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

                    if btn:
                        import time
                        from analysis.anomaly import compute_amount_columns
                        from analysis.embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
                        from analysis.report import build_report_context, run_final_analysis, build_methodology_note, run_offline_fallback_report
                        from services.llm import LLMClient
                        from analysis.anomaly import ensure_zscore

                        t0 = time.perf_counter()
                        with st.status("ë³´ê³ ì„œ ì¤€ë¹„ ì¤‘...", expanded=True) as s:
                            # Step 1) ë°ì´í„° ìŠ¬ë¼ì´ì‹±
                            s.write("â‘  ìŠ¤ì½”í”„ ì ìš© ë° ë°ì´í„° ìŠ¬ë¼ì´ì‹±(CY/PY)â€¦")
                            cur_year = ldf['ì—°ë„'].max()
                            df_cy = ldf[(ldf['period_tag'] == 'CY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            df_py = ldf[(ldf['period_tag'] == 'PY') & (ldf['ê³„ì •ì½”ë“œ'].astype(str).isin(pick_codes))].copy()
                            s.write(f"    â”” CY {len(df_cy):,}ê±´ / PY {len(df_py):,}ê±´")

                            # Step 2) í•„ìˆ˜ íŒŒìƒ(ë°œìƒì•¡/ìˆœì•¡)
                            s.write("â‘¡ ê¸ˆì•¡ íŒŒìƒ ì»¬ëŸ¼ ìƒì„±(ë°œìƒì•¡/ìˆœì•¡)â€¦")
                            df_cy = compute_amount_columns(df_cy)
                            df_py = compute_amount_columns(df_py)

                            # Step 3) (ì„ íƒ) íŒ¨í„´ìš”ì•½: ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ (LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œì—ë§Œ)
                            cl_ok = False
                            if LLM_OK and opt_patterns and not df_cy.empty:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰(ì„ íƒ)â€¦")
                                # ì…ë ¥ í…ìŠ¤íŠ¸ í’ë¶€í™” + ì„ë² ë”© + HDBSCAN (ìµœëŒ€ N ì œí•œìœ¼ë¡œ ì•ˆì „ê°€ë“œ)
                                df_cy_small = df_cy.copy()
                                max_rows = 8000
                                if len(df_cy_small) > max_rows:
                                    df_cy_small = df_cy_small.sample(max_rows, random_state=42)
                                    s.write(f"    â”” ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                df_cy_small = ensure_rich_embedding_text(df_cy_small)
                                try:
                                    llm_service = LLMClient(model=st.session_state.get('llm_model', 'gpt-4o'))
                                    emb_client = llm_service.client  # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°ì²´
                                    naming_function = llm_service.name_cluster
                                    # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•´ LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë„¤ì´ë°ì„ í•„ìˆ˜ë¡œ ìš”êµ¬
                                    df_clu, ok = perform_embedding_and_clustering(
                                        df_cy_small, emb_client,
                                        name_with_llm=True, must_name_with_llm=True,
                                        use_large=bool(st.session_state.get("use_large_embedding", False)),
                                        rescue_tau=float(st.session_state.get("rescue_tau", HDBSCAN_RESCUE_TAU)),
                                        embed_texts_fn=get_or_embed_texts,
                                        naming_fn=naming_function,
                                    )
                                    if ok:
                                        # ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì´ë¦„ì„ LLMìœ¼ë¡œ í†µí•©
                                        df_clu, name_map = unify_cluster_names_with_llm(
                                            df_clu,
                                            sim_threshold=0.90,
                                            emb_model=st.session_state.get('embedding_model', None) or EMB_MODEL_SMALL,
                                            embed_texts_fn=get_or_embed_texts,
                                            confirm_pair_fn=make_synonym_confirm_fn(emb_client, st.session_state.get('llm_model', 'gpt-4o')),
                                        )
                                        # ì¶”ê°€ LLM ë¼ë²¨ í†µí•©(JSON ë§¤í•‘ ë°©ì‹) â€” CYì˜ cluster_groupì€ ìœ ì§€
                                        try:
                                            raw_map = unify_cluster_labels_llm(df_clu['cluster_name'].dropna().unique().tolist(), emb_client)
                                            if raw_map:
                                                df_clu['cluster_name'] = df_clu['cluster_name'].map(lambda x: raw_map.get(str(x), x))
                                                # â— cluster_groupëŠ” unify_cluster_names_with_llm()ì´ ì •í•œ canonicalì„ ìœ ì§€
                                        except Exception:
                                            pass
                                        # ê°„ë‹¨ ìš”ì•½(ìƒìœ„ 5ê°œ)
                                        topc = (df_clu.groupby('cluster_group')['ë°œìƒì•¡']
                                                .agg(['count','sum']).sort_values('sum', ascending=False).head(5))
                                        s.write("    â”” í´ëŸ¬ìŠ¤í„° ìƒìœ„ 5ê°œ ìš”ì•½:")
                                        st.dataframe(
                                            topc.rename(columns={'count':'ê±´ìˆ˜','sum':'ë°œìƒì•¡í•©ê³„'})
                                                .style.format({'ë°œìƒì•¡í•©ê³„':'{:,.0f}'}),
                                            use_container_width=True
                                        )
                                        # í’ˆì§ˆ ì§€í‘œ(ë…¸ì´ì¦ˆìœ¨Â·í´ëŸ¬ìŠ¤í„° ìˆ˜ ë“±) ê¸°ë¡
                                        try:
                                            n = int(len(df_clu))
                                            noise_rate = float((df_clu['cluster_id'] == -1).mean()) if n else 0.0
                                            n_clusters = int(df_clu.loc[df_clu['cluster_id'] != -1, 'cluster_id'].nunique())
                                            if n_clusters > 0:
                                                avg_size = float(df_clu[df_clu['cluster_id'] != -1].groupby('cluster_id').size().mean())
                                            else:
                                                avg_size = 0.0
                                            rescue_rate = float(df_clu.get('rescued', False).mean()) if 'rescued' in df_clu.columns else 0.0
                                            model_used = df_clu.attrs.get('embedding_model', 'unknown')
                                            umap_on = bool(df_clu.attrs.get('umap_used', False))
                                            s.write(
                                                f"    â”” Quality: N={n:,}, noise={noise_rate*100:.1f}%, "
                                                f"clusters={n_clusters}, avg_size={avg_size:.1f}, rescued={rescue_rate*100:.1f}%"
                                            )
                                            s.write(
                                                f"    â”” Model/UMAP: {model_used} | UMAP={'on' if umap_on else 'off'} | Ï„={float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)):.2f}"
                                            )
                                            # ëŒ€ì‹œë³´ë“œ ì¹´ë“œìš© í’ˆì§ˆ ì§€í‘œ ì €ì¥
                                            st.session_state['cluster_quality'] = {
                                                "N": n,
                                                "noise_rate": noise_rate,
                                                "n_clusters": n_clusters,
                                                "avg_size": avg_size,
                                                "rescued_rate": rescue_rate,
                                                "model": model_used,
                                                "umap": umap_on,
                                                "tau": float(st.session_state.get('rescue_tau', HDBSCAN_RESCUE_TAU)),
                                            }
                                        except Exception:
                                            pass
                                        # ë³´ê³ ì„œ ì»¨í…ìŠ¤íŠ¸ì— ë°˜ì˜: group/label ë™ì‹œ ë¶€ì°©
                                        df_cy = df_cy.merge(
                                            df_clu[['row_id','cluster_id','cluster_name','cluster_group']],
                                            on='row_id', how='left'
                                        )
                                        # í•„ìš” ì‹œ vectorë„ í•¨ê»˜ ë³‘í•© ê°€ëŠ¥:
                                        # df_cy = df_cy.merge(df_clu[['row_id','vector']], on='row_id', how='left')
                                        # (í˜„ì¬ëŠ” perform_embedding_only ë‹¨ê³„ì—ì„œ CY/PY dfì— vectorê°€ ì§ì ‘ ë¶€ì—¬ë¨)
                                        # --- PY clustering and alignment (optional) ---
                                        if opt_patterns_py and not df_py.empty:
                                            try:
                                                from analysis.embedding import cluster_year, align_yearly_clusters, unify_cluster_labels_llm
                                                # sampling guard similar to CY
                                                df_py_small = df_py.copy()
                                                max_rows = 8000
                                                if len(df_py_small) > max_rows:
                                                    df_py_small = df_py_small.sample(max_rows, random_state=42)
                                                    s.write(f"    â”” PY ë°ì´í„°ê°€ ë§ì•„ {max_rows:,}ê±´ìœ¼ë¡œ ìƒ˜í”Œë§")
                                                df_py_clu = cluster_year(
                                                    df_py_small, emb_client, embed_texts_fn=get_or_embed_texts
                                                )
                                                # ê°€ëŠ¥í•œ ê²½ìš° row_id ê¸°ì¤€ìœ¼ë¡œ PY ê²°ê³¼ ì»¬ëŸ¼ì„ df_pyì— ë³‘í•©
                                                if not df_py_clu.empty and 'row_id' in df_py.columns:
                                                    df_py = df_py.merge(df_py_clu, on='row_id', how='left', suffixes=("", "_pyclu"))
                                                # ì •ë ¬: PY í´ëŸ¬ìŠ¤í„°ë¥¼ CY í´ëŸ¬ìŠ¤í„°ì— ë§¤í•‘
                                                if 'cluster_id' in df_py_clu.columns:
                                                    mapping = align_yearly_clusters(df_clu, df_py_clu, sim_threshold=0.70)
                                                    # cluster_id â†’ (aligned_cy_cluster, aligned_sim)
                                                    cy_id_to_name = df_clu.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()
                                                    def _get_pair(cid):
                                                        try:
                                                            if pd.isna(cid):
                                                                return (np.nan, np.nan)
                                                            cid_int = int(cid)
                                                            return mapping.get(cid_int, (np.nan, np.nan))
                                                        except Exception:
                                                            return (np.nan, np.nan)
                                                    if 'cluster_id' in df_py.columns:
                                                        pairs = df_py['cluster_id'].map(_get_pair)
                                                        df_py[['aligned_cy_cluster', 'aligned_sim']] = pd.DataFrame(pairs.tolist(), index=df_py.index)
                                                        # ì´ë¦„ì€ CYì˜ ì´ë¦„ìœ¼ë¡œ ì •ë ¬(ê°€ëŠ¥í•œ ê²½ìš°)
                                                        df_py['cluster_name'] = df_py['aligned_cy_cluster'].map(cy_id_to_name).fillna(df_py.get('cluster_name'))
                                                # ìµœì¢… ë¼ë²¨ ì •í•©: ì „ì²´ ì´ë¦„ ì§‘í•© ê¸°ì¤€ìœ¼ë¡œ í†µí•©; CYì˜ cluster_groupì€ ìœ ì§€, PYëŠ” canonicalë¡œ ì •ë ¬
                                                try:
                                                    all_names = pd.Series([], dtype=object)
                                                    if 'cluster_name' in df_cy.columns:
                                                        all_names = pd.concat([all_names, df_cy['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    if 'cluster_name' in df_py.columns:
                                                        all_names = pd.concat([all_names, df_py['cluster_name'].dropna().astype(str)], ignore_index=True)
                                                    all_names = all_names.dropna().unique().tolist()
                                                    canon = unify_cluster_labels_llm(all_names, emb_client)
                                                    if canon:
                                                        if 'cluster_name' in df_cy.columns:
                                                            df_cy['cluster_name'] = df_cy['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_name' in df_py.columns:
                                                            df_py['cluster_name'] = df_py['cluster_name'].map(lambda x: canon.get(str(x), x))
                                                        if 'cluster_group' in df_py.columns:
                                                            df_py['cluster_group'] = df_py['cluster_name']
                                                except Exception:
                                                    pass
                                            except Exception as e:
                                                s.write(f"    â”” PY í´ëŸ¬ìŠ¤í„°ë§/ì •ë ¬ ìŠ¤í‚µ: {e}")
                                        # ì»¨í…ìŠ¤íŠ¸ì— ë³„ë„ ë…¸íŠ¸ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                                        cl_ok = True
                                    else:
                                        s.write("    â”” LLM í´ëŸ¬ìŠ¤í„° ì´ë¦„ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ â†’ ë³´ê³ ì„œ ìƒì„± ìš”ê±´ ë¯¸ì¶©ì¡±")
                                except Exception as e:
                                    s.write(f"    â”” ì„ë² ë”©/í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
                            else:
                                s.write("â‘¢ ì„ë² ë”©Â·í´ëŸ¬ìŠ¤í„°ë§: LLM ë¯¸ê°€ìš© ë˜ëŠ” ì˜µì…˜ ë¹„í™œì„± â†’ ìŠ¤í‚µ")

                            # Step 3-1) (ì˜µì…˜ A) ê·¼ê±° ì¸ìš©(KNN)ìš© ì„ë² ë”©ë§Œ ìˆ˜í–‰ (LLM ê°€ëŠ¥ ì‹œ)
                            if LLM_OK and opt_knn_evidence:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš©ìš© ì„ë² ë”©(CY/PY)â€¦")
                                from analysis.embedding import perform_embedding_only, ensure_rich_embedding_text
                                emb_client2 = LLMClient().client
                                df_cy = ensure_rich_embedding_text(df_cy)
                                df_py = ensure_rich_embedding_text(df_py)
                                df_cy = perform_embedding_only(
                                    df_cy, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                                df_py = perform_embedding_only(
                                    df_py, client=emb_client2,
                                    use_large=bool(st.session_state.get("use_large_embedding", False)),
                                    embed_texts_fn=get_or_embed_texts,
                                )
                            elif not LLM_OK:
                                s.write("â‘¢-1 ê·¼ê±° ì¸ìš© ì„ë² ë”©: LLM ë¯¸ê°€ìš© â†’ ìŠ¤í‚µ")

                            # Step 3-2) Z-Score: ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ í•¨
                            s.write("â‘¢-2 Z-Score ê³„ì‚°/ê²€ì¦â€¦")
                            df_cy, z_ok = ensure_zscore(df_cy, pick_codes)
                            df_py, _    = ensure_zscore(df_py, pick_codes)  # ì „ê¸°ì—ë„ Z-Score ê³„ì‚°(ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©)
                            if not z_ok:
                                s.write("    â”” Z-Score ë¯¸ê³„ì‚° ë˜ëŠ” ì „ë¶€ ê²°ì¸¡")

                            # âœ… ê²Œì´íŠ¸ ì™„í™”: Z-Scoreë§Œ í™•ë³´ë˜ë©´ ë³´ê³ ì„œ ì§„í–‰.
                            #    (í´ëŸ¬ìŠ¤í„° ì‹¤íŒ¨ ì‹œ ê´€ë ¨ ì„¹ì…˜ì€ ìë™ ì¶•ì•½/ìƒëµ)
                            if not z_ok:
                                st.error("ë³´ê³ ì„œ ìƒì„± ì¤‘ë‹¨: Z-Score ì—†ìŒ.")
                                s.update(label="ë³´ê³ ì„œ ìš”ê±´ ë¯¸ì¶©ì¡±", state="error")
                                st.stop()
                            if not cl_ok:
                                s.write("    â”” í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì—†ìŒ â†’ ë¦¬í¬íŠ¸ì—ì„œ í´ëŸ¬ìŠ¤í„° ì„¹ì…˜ì€ ìƒëµ/ì¶•ì•½ë©ë‹ˆë‹¤.")

                            # Step 4) ì»¨í…ìŠ¤íŠ¸ ìƒì„±(ì „ ëª¨ë“ˆ í¬í•¨) + ë°©ë²•ë¡  ë…¸íŠ¸
                            s.write("â‘£ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±(ì „ ëª¨ë“ˆ)â€¦")
                            from analysis.report_adapter import wrap_dfs_as_module_result
                            from analysis.report import generate_rag_context_from_modules
                            from analysis.integrity import run_integrity_module
                            from analysis.timeseries import run_timeseries_module

                            # (1) ì„¸ì…˜ ì´ˆê¸°í™” ë° ê³µí†µ ê°’ ì¤€ë¹„
                            st.session_state['modules'] = {}
                            lf_use = _lf_by_scope()
                            pm_use = float(st.session_state.get('pm_value', PM_DEFAULT))

                            # (2) ì£¼ìš” ëª¨ë“ˆ ì‹¤í–‰ ë° ìˆ˜ì§‘
                            if lf_use is not None:
                                # ì´ìƒì¹˜
                                try:
                                    amod = run_anomaly_module(lf_use, target_accounts=pick_codes or None,
                                                              topn=int(st.session_state.get('ctx_topk', 20)), pm_value=pm_use)
                                    _push_module(amod)
                                except Exception as _e:
                                    st.warning(f"anomaly ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ì¶”ì„¸(ì„ íƒ ê³„ì • í•„ìš”)
                                try:
                                    if pick_codes:
                                        _push_module(run_trend_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"trend ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ê±°ë˜ì²˜
                                try:
                                    if pick_codes:
                                        _push_module(run_vendor_module(lf_use, account_codes=pick_codes,
                                                                       min_amount=0.0, include_others=True))
                                except Exception as _e:
                                    st.warning(f"vendor ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ìƒê´€(2ê°œ ì´ìƒì¼ ë•Œë§Œ)
                                try:
                                    if len(pick_codes) >= 2:
                                        _push_module(run_correlation_module(
                                            lf_use, accounts=pick_codes,
                                            corr_threshold=float(CORR_THRESHOLD_DEFAULT),
                                            cycles_map=cyc.get_effective_cycles(upload_id),
                                            emit_evidences=True,
                                        ))
                                except Exception as _e:
                                    st.warning(f"correlation ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # ì •í•©ì„±(ModuleResult) â€” ì„ íƒ ê³„ì • í•„í„° ì ìš©
                                try:
                                    _push_module(run_integrity_module(lf_use, accounts=pick_codes))
                                except Exception as _e:
                                    st.warning(f"integrity ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")
                                # NEW: ì‹œê³„ì—´ í¬í•¨(ì§‘ê³„â†’DTO ë˜í•‘)
                                try:
                                    if not df_cy.empty:
                                        ts = pd.concat([df_cy, df_py], ignore_index=True)
                                        ts["date"] = month_end_00(ts["íšŒê³„ì¼ì"])  # ì›”ë§ 00:00:00 ì •ê·œí™”
                                        ts["account"] = ts["ê³„ì •ì½”ë“œ"].astype(str)
                                        ts["amount"] = ts.get("ë°œìƒì•¡", 0.0).astype(float)
                                        ts_in = ts.groupby(["account","date"], as_index=False)["amount"].sum()
                                        df_ts = run_timeseries_module(ts_in, account_col="account", date_col="date", amount_col="amount",
                                                                      pm_value=pm_use, output="flow", make_balance=False)
                                        summ_ts = {
                                            "n_series": int(df_ts["account"].nunique()) if not df_ts.empty else 0,
                                            "n_points": int(len(df_ts)),
                                            "max_abs_z": float(df_ts["z"].abs().max()) if ("z" in df_ts.columns and not df_ts.empty) else 0.0,
                                        }
                                        _push_module(ModuleResult(name="timeseries", summary=summ_ts,
                                                                  tables={"ts": df_ts}, figures={}, evidences=[], warnings=([] if not df_ts.empty else ["insufficient_points"])))
                                except Exception as _e:
                                    st.warning(f"timeseries ëª¨ë“ˆ ì‹¤íŒ¨: {_e}")

                            # (3) ë ˆê±°ì‹œ DFë„ ì–´ëŒ‘í„°ë¡œ í•¨ê»˜ í¬í•¨(ê²½ëŸ‰ ì»¨í…ìŠ¤íŠ¸ìš©)
                            mr_ctx = wrap_dfs_as_module_result(df_cy, df_py, name="report_ctx")
                            modules_list = list(st.session_state.get('modules', {}).values()) + [mr_ctx]
                            # (4) ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„±(Top-K ì ìš©) â€” ì‹ ê·œ ê²½ë¡œë§Œ ì‚¬ìš© + ë©”ëª¨ ì£¼ì…
                            ctx = generate_rag_context_from_modules(
                                modules_list,
                                pm_value=pm_use,
                                topk=int(st.session_state.get('ctx_topk', 20)),
                                manual_note=(manual_ctx or "")
                            )

                            # (ìƒë‹¨ ê³µí†µ ë¯¸ë¦¬ë³´ê¸°ë¡œ ëŒ€ì²´)
                            note = build_methodology_note(report_accounts=pick_codes)

                            # Step 5) LLM í˜¸ì¶œ ì „ ì ê²€(ê¸¸ì´/í† í°)
                            s.write("â‘¤ LLM í”„ë¡¬í”„íŠ¸ ì ê²€â€¦")
                            prompt_len = len(ctx) + len(note)
                            s.write(f"    â”” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {prompt_len:,} chars")
                            try:
                                import tiktoken
                                enc = tiktoken.get_encoding("cl100k_base")
                                est_tokens = len(enc.encode(ctx)) + len(enc.encode(note))
                                s.write(f"    â”” ì˜ˆìƒ í† í° ìˆ˜: ~{est_tokens:,} tokens")
                            except Exception:
                                s.write("    â”” tiktoken ë¯¸ì„¤ì¹˜: í† í° ì¶”ì • ìƒëµ")

                            # Step 6) ë³´ê³ ì„œ ìƒì„±: LLM ê°€ëŠ¥í•˜ë©´ ì‹œë„, ì‹¤íŒ¨/ë¶ˆê°€ ì‹œ ì˜¤í”„ë¼ì¸ í´ë°±
                            final_report = None
                            if LLM_OK:
                                s.write("â‘¥ LLM ìš”ì•½ ìƒì„± í˜¸ì¶œâ€¦")
                                try:
                                    t_llm0 = time.perf_counter()
                                    llm_client = LLMClient(model=st.session_state.get('llm_model'))
                                    final_report = run_final_analysis(
                                        context=ctx + "\n" + note,
                                        account_codes=pick_codes,
                                        model=st.session_state.get('llm_model'),
                                        max_tokens=int(st.session_state.get('llm_max_tokens', 16000)),
                                        generate_fn=llm_client.generate,
                                    )
                                    s.write(f"    â”” LLM ì™„ë£Œ (ê²½ê³¼ {time.perf_counter()-t_llm0:.1f}s)")
                                except Exception as e:
                                    s.write(f"    â”” LLM ì‹¤íŒ¨: {e} â†’ ì˜¤í”„ë¼ì¸ í´ë°±ìœ¼ë¡œ ì „í™˜")

                            if final_report is None:
                                s.write("â‘¥-í´ë°±: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±â€¦")
                                final_report = run_offline_fallback_report(
                                    current_df=df_cy,
                                    previous_df=df_py,
                                    account_codes=pick_codes,
                                    pm_value=float(st.session_state.get('pm_value', PM_DEFAULT))
                                )

                            s.update(label="ë³´ê³ ì„œ ì¤€ë¹„ ì™„ë£Œ", state="complete")

                            # ê²°ê³¼ ì¶œë ¥ ë° ì„¸ì…˜ ë³´ì¡´
                            st.session_state['last_report'] = final_report
                            st.session_state['last_context'] = ctx + "\n" + note
                            st.session_state['last_dfcy'] = df_cy
                            st.session_state['last_dfpy'] = df_py

                            st.success("ë³´ê³ ì„œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                            st.markdown(final_report)

                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])

                        # ZIP ë‹¨ì¼ ë‹¤ìš´ë¡œë“œ + RAW ë¯¸ë¦¬ë³´ê¸°
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()

                        raw_df = _build_raw_evidence(st.session_state['last_dfcy'])
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")

                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_current"  # ê³ ìœ  í‚¤(í˜„ì¬ ê²°ê³¼)
                        )

                        st.caption(f"â± ì´ ì†Œìš”: {time.perf_counter()-t0:.1f}s")
                        rendered_report = True

                    # === ìºì‹œëœ ì´ì „ ê²°ê³¼ ë Œë”(ë²„íŠ¼ ë¯¸í´ë¦­ ì‹œì—ë§Œ) ===
                    if st.session_state.get('last_report') and not btn:
                        st.success("ë³´ê³ ì„œê°€ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        st.markdown("### ğŸ“„ AI ìš”ì•½ ë³´ê³ ì„œ")
                        st.markdown(st.session_state['last_report'])
                        with st.expander("ğŸ” ê·¼ê±° ì»¨í…ìŠ¤íŠ¸(LLM ì…ë ¥)", expanded=False):
                            st.text(st.session_state['last_context'])
                        # RAW ë¯¸ë¦¬ë³´ê¸° + ZIP ë²„íŠ¼ ì¬ì¶œë ¥
                        import io, zipfile
                        def _build_raw_evidence(df_cy_in):
                            keep = [c for c in ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','ìˆœì•¡','Z-Score','cluster_group','cluster_name'] if c in df_cy_in.columns]
                            return df_cy_in[keep].copy() if keep else pd.DataFrame()
                        def _make_zip_blob(report_txt: str, context_txt: str, raw_df: pd.DataFrame) -> bytes:
                            mem = io.BytesIO()
                            with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
                                z.writestr('report.txt', report_txt)
                                z.writestr('context.txt', context_txt)
                                z.writestr('evidence_raw.csv', raw_df.to_csv(index=False, encoding='utf-8-sig'))
                            mem.seek(0)
                            return mem.getvalue()
                        raw_df = _build_raw_evidence(st.session_state.get('last_dfcy', pd.DataFrame()))
                        st.markdown("#### ğŸ“‘ ê·¼ê±°: ì„ íƒ ê³„ì • ì›ì¥(RAW) + í´ëŸ¬ìŠ¤í„°")
                        if not raw_df.empty:
                            st.dataframe(
                                raw_df.head(100).style.format({'ë°œìƒì•¡':'{:,.0f}','ìˆœì•¡':'{:,.0f}','Z-Score':'{:.2f}'}),
                                use_container_width=True, height=350
                            )
                        else:
                            st.info("í‘œì‹œí•  RAWê°€ ì—†ìŠµë‹ˆë‹¤.")
                        zip_bytes = _make_zip_blob(
                            report_txt=st.session_state['last_report'],
                            context_txt=st.session_state['last_context'],
                            raw_df=raw_df
                        )
                        st.download_button(
                            "ğŸ“¥ ë³´ê³ ì„œ+ê·¼ê±° ë‹¤ìš´ë¡œë“œ(ZIP)",
                            data=zip_bytes,
                            file_name="ai_report_with_evidence.zip",
                            mime="application/zip",
                            key="zip_dl_cached"  # ê³ ìœ  í‚¤(ìºì‹œ ê²°ê³¼)
                        )
                        # (ê°€ëŠ¥ ì‹œ) í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ì¹´ë“œ í‘œì‹œ
                        cq = st.session_state.get("cluster_quality")
                        if cq:
                            st.markdown("---")
                            st.subheader("í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ìš”ì•½")
                            c1, c2, c3, c4 = st.columns(4)
                            c1.metric("Noise rate", f"{cq['noise_rate']*100:.1f}%")
                            c2.metric("#Clusters", f"{cq['n_clusters']}")
                            c3.metric("Avg size", f"{cq['avg_size']:.1f}")
                            c4.metric("Rescued", f"{cq['rescued_rate']*100:.1f}%")
                            st.caption(f"Model: {cq['model']} | UMAP: {'on' if cq['umap'] else 'off'} | Ï„={cq['tau']:.2f} | N={cq['N']:,}")
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            if st.button("ë§¤í•‘ ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸°"):
                st.session_state.mapping_confirmed = False
                st.rerun()
else:
    st.info("â¬…ï¸ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")





==============================
ğŸ“„ FILE: config.py
==============================

LLM_MODEL = "gpt-4o"
LLM_TEMPERATURE = 0.2
LLM_JSON_MODE = True
PM_DEFAULT = 500_000_000  # Project-wide Performance Materiality (KRW)

EMBED_BATCH_SIZE = 256
EMBED_CACHE_DIR = ".cache/embeddings"

# í›ˆë‹˜ ê²°ì • ë°˜ì˜ âœ…
SHAP_TOP_N_PER_ACCOUNT_DEFAULT = 25   # ì‚¬ìš©ì UIì—ì„œ 20~30 ë²”ìœ„ ì„ íƒ ê°€ëŠ¥
CYCLE_RECOMMENDER = "llm_only"        # LLM 100% ìë™ ì¶”ì²œ
PM_DEFAULT = PM_DEFAULT              # (kept above; single source of truth)

# --- Correlation defaults ---
# New canonical names
CORR_DEFAULT_METHOD = "pearson"
CORR_THRESHOLD_DEFAULT = 0.70
CORR_MIN_ACTIVE_MONTHS_DEFAULT = 6
CORR_MAX_LAG_DEFAULT = 6
CORR_ROLLING_WINDOW_DEFAULT = 6

# Backward-compatible aliases (kept for existing imports)
CORR_METHOD_DEFAULT = CORR_DEFAULT_METHOD
CORR_MIN_ACTIVE_MONTHS = CORR_MIN_ACTIVE_MONTHS_DEFAULT
CORR_ROLLWIN_DEFAULT = CORR_ROLLING_WINDOW_DEFAULT

# ---- NEW: Embedding & Clustering defaults ----
# Embedding model switch (Small by default; Large improves semantics at higher cost)
EMB_MODEL_SMALL = "text-embedding-3-small"
EMB_MODEL_LARGE = "text-embedding-3-large"
EMB_USE_LARGE_DEFAULT = False           # UI/auto-upscale can override per run

# UMAP threshold (apply UMAP â†’ HDBSCAN only when N is large)
UMAP_APPLY_THRESHOLD = 8000             # set 0/None to disable
UMAP_N_COMPONENTS = 20
UMAP_N_NEIGHBORS = 30
UMAP_MIN_DIST = 0.0

# HDBSCAN noise-rescue cosine threshold
HDBSCAN_RESCUE_TAU = 0.75               # 0.72~0.78 usually works well

# Adaptive clustering knobs (computed per run, not static)
# min_cluster_size = max(8, int(sqrt(N))); min_samples = max(2, int(0.5 * min_cluster_size))

# ===== NEW: Materiality & Risk Weights =====
# === í†µí•© ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜(ê³ ì •ê°’; v2.0-RC ë™ê²°) ===
# ì ìˆ˜ = 0.5*A(|Z|ì •ê·œí™”) + 0.4*F(PM ëŒ€ë¹„ ë¹„ìœ¨ capped 1) + 0.1*K(PM ì´ˆê³¼=1)
RISK_WEIGHT_A = 0.5
RISK_WEIGHT_F = 0.4
RISK_WEIGHT_K = 0.1

# --- NEW: Z-Score â†’ sigmoid ìŠ¤ì¼€ì¼ ì¡°ì • (ë¡œë“œë§µ í˜¸í™˜)
# anomaly_score = sigmoid(|Z| / Z_SIGMOID_DIVISOR)
# ë¡œë“œë§µ ê¶Œê³ : 3.0 (ê³¼ë„ í¬í™” ì™„í™”)
Z_SIGMOID_DIVISOR = 3.0
Z_SIGMOID_SCALE = Z_SIGMOID_DIVISOR  # í•˜ìœ„í˜¸í™˜

# --- í‘œì¤€ íšŒê³„ ì‚¬ì´í´ (STANDARD_ACCOUNTING_CYCLES) ---
# í‚¤: ì‚¬ì´í´ ì‹ë³„ì, ê°’: í•´ë‹¹ ì‚¬ì´í´ì— ë§¤í•‘ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ê³„ì •ëª… í‚¤ì›Œë“œ(ë¶€ë¶„ì¼ì¹˜)
# *í•œêµ­ì–´/ì˜ë¬¸ í˜¼ìš©. í•„ìš” ì‹œ í”„ë¡œì íŠ¸ ë„ë©”ì¸ì— ë§ì¶° ë³´ê°•í•˜ì„¸ìš”.
STANDARD_ACCOUNTING_CYCLES = {
    "Cash": ["í˜„ê¸ˆ", "ì˜ˆê¸ˆ", "ë‹¨ê¸°ê¸ˆìœµ", "Cash", "Bank"],
    "Revenue": ["ë§¤ì¶œ", "íŒë§¤ìˆ˜ìµ", "Sales", "Revenue"],
    "Receivables": ["ë§¤ì¶œì±„ê¶Œ", "ì™¸ìƒë§¤ì¶œê¸ˆ", "ë¯¸ìˆ˜ê¸ˆ", "Receivable", "A/R"],
    "Inventory": ["ì¬ê³ ", "ìƒí’ˆ", "ì œí’ˆ", "ì›ì¬ë£Œ", "ì¬ê³µí’ˆ", "Inventory"],
    "Payables": ["ë§¤ì…ì±„ë¬´", "ì™¸ìƒë§¤ì…ê¸ˆ", "ë¯¸ì§€ê¸‰ê¸ˆ", "Payable", "A/P"],
    "Expenses": ["ë³µë¦¬í›„ìƒë¹„", "ê¸‰ì—¬", "ì„ì°¨ë£Œ", "ì ‘ëŒ€ë¹„", "ê°ê°€ìƒê°ë¹„", "ë¹„ìš©", "Expense"],
    "FixedAssets": ["ìœ í˜•ìì‚°", "ê°ê°€ìƒê°ëˆ„ê³„", "ê¸°ê³„ì¥ì¹˜", "ê±´ë¬¼", "ë¹„í’ˆ", "PPE", "Fixed Asset"],
    "Equity": ["ìë³¸ê¸ˆ", "ì´ìµì‰ì—¬ê¸ˆ", "ìë³¸ì‰ì—¬ê¸ˆ", "Equity", "Capital"],
}

# --- NEW: Anomaly (Semantic & Isolation Forest) defaults ---
IFOREST_ENABLED_DEFAULT = True
IFOREST_N_ESTIMATORS = 256
IFOREST_MAX_SAMPLES = "auto"
IFOREST_CONTAM_DEFAULT = 0.03
IFOREST_RANDOM_STATE = 42

# Semantic outlier thresholds
SEMANTIC_Z_THRESHOLD = 2.5
SEMANTIC_MIN_RECORDS = 12
ANOMALY_IFOREST_SCORE_THRESHOLD = 0.70

# --- Provisional rule naming (ë„ë©”ì¸ í•©ì˜ ì „) ---
PROVISIONAL_RULE_VERSION = "v1.0"
PROVISIONAL_RULE_NAME = f"ì ì • ê¸°ì¤€({PROVISIONAL_RULE_VERSION})"

def provisional_risk_formula_str() -> str:
    """UI/ë¦¬í¬íŠ¸ ì•ˆë‚´ë¬¸ì— ì“°ì¼ ê°€ì¤‘ì¹˜ ìš”ì•½ ë¬¸ìì—´ì„ ë™ì ìœ¼ë¡œ ìƒì„±"""
    a = int(RISK_WEIGHT_A * 100)
    f = int(RISK_WEIGHT_F * 100)
    k = int(RISK_WEIGHT_K * 100)
    return f"í†µê³„ì  ì´ìƒ({a}%) + ì¬ë¬´ì  ì˜í–¥({f}%) + KIT ì—¬ë¶€({k}%)"

# ë¦¬í¬íŠ¸(ìµœì¢…ë³¸) í¬í•¨ ì¡°ê±´ ë…¸ë¸Œ (ê¸°ë³¸: í¬í•¨ ì•ˆ í•¨)
INCLUDE_RISK_MATRIX_SUMMARY_IN_FINAL = False
# â€˜ìƒìœ„ Nâ€™ ê²°ê³¼ê°€ ì´ ê°’ ë¯¸ë§Œì´ë©´ ìµœì¢…ë³¸ì— ìƒëµ (ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ì—” ìœ ì§€)
RISK_MATRIX_SECTION_MIN_ITEMS = 3

# --- TimeSeries forecast knobs ---
FORECAST_MIN_POINTS = 8         # Prophet/ARIMA ì‚¬ìš© ê¶Œì¥ ìµœì†Œ ê¸¸ì´(ê¶Œê³ ì¹˜)
ARIMA_DEFAULT_ORDER = (1,1,1)

# --- User overrides for STANDARD_ACCOUNTING_CYCLES ---
CYCLES_USER_OVERRIDES_PATH = ".cache/cycles_overrides.json"



==============================
ğŸ“„ FILE: analysis/aggregation.py
==============================

import pandas as pd


def month_end_00(date_series: pd.Series) -> pd.Series:
    """
    ì›”ë§ì„ 'í•´ë‹¹ì›” ë§ì¼ 00:00:00'ë¡œ ì •ê·œí™” (ë°˜ì˜¬ë¦¼/ì˜¬ë¦¼ ì—†ì´ ê³ ì •)
    ì˜ˆ: 2025-06-30 00:00:00
    """
    ds = pd.to_datetime(date_series, errors="coerce")
    return ds.dt.to_period("M").dt.to_timestamp("M")  # ì›”ë§ 00:00:00


def aggregate_monthly(df: pd.DataFrame, date_col: str, amount_col: str) -> pd.DataFrame:
    """
    ì›”ë³„ ë°œìƒì•¡ í•©ê³„ë¥¼ ë°˜í™˜.
    - ë‚ ì§œ ì»¬ëŸ¼ì€ ë°˜ë“œì‹œ month_end_00ë¡œ ì •ê·œí™”
    """
    work = df[[date_col, amount_col]].copy()
    work[date_col] = month_end_00(work[date_col])
    out = (
        work.groupby(date_col, dropna=True, as_index=False)[amount_col]
        .sum()
        .sort_values(date_col)
    )
    out = out.rename(columns={date_col: "date", amount_col: "amount"})
    return out





==============================
ğŸ“„ FILE: analysis/anomaly.py
==============================

from __future__ import annotations
import numpy as np, math
import pandas as pd
from typing import List, Optional, Dict, Any, Tuple
from utils.helpers import find_column_by_keyword
from analysis.embedding import ensure_rich_embedding_text, perform_embedding_only  # â† services ì£¼ì…ì‹ ì„ë² ë”© ì‚¬ìš©
from config import (
    IFOREST_ENABLED_DEFAULT, IFOREST_N_ESTIMATORS, IFOREST_MAX_SAMPLES,
    IFOREST_CONTAM_DEFAULT, IFOREST_RANDOM_STATE,
    SEMANTIC_Z_THRESHOLD, SEMANTIC_MIN_RECORDS, ANOMALY_IFOREST_SCORE_THRESHOLD
)


def compute_amount_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ë°œìƒì•¡(ì ˆëŒ€ ê·œëª¨) / ìˆœì•¡(ì°¨-ëŒ€) ê³„ì‚°."""
    dcol = find_column_by_keyword(df.columns, 'ì°¨ë³€')
    ccol = find_column_by_keyword(df.columns, 'ëŒ€ë³€')
    df = df.copy()
    if not dcol or not ccol:
        df['ë°œìƒì•¡'] = 0.0; df['ìˆœì•¡'] = 0.0; df['ê±°ë˜ê¸ˆì•¡'] = 0.0
        return df
    d = pd.to_numeric(df[dcol], errors='coerce').fillna(0.0)
    c = pd.to_numeric(df[ccol], errors='coerce').fillna(0.0)
    row_amt = np.where((d > 0) & (c == 0), d,
              np.where((c > 0) & (d == 0), c,
              np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
    df['ë°œìƒì•¡'] = row_amt
    df['ìˆœì•¡']  = d - c
    df['ê±°ë˜ê¸ˆì•¡'] = df['ìˆœì•¡']
    return df


def calculate_grouped_stats_and_zscore(df: pd.DataFrame, target_accounts: List[str], data_type: str = "ë‹¹ê¸°") -> pd.DataFrame:
    """ì„ íƒ ê³„ì • ê·¸ë£¹ì˜ ë°œìƒì•¡ ë¶„í¬ ê¸°ì¤€ Z-Score ì‚°ì¶œ."""
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    df = compute_amount_columns(df.copy())
    if not acct_col:
        df['Z-Score'] = 0.0
        return df
    is_target = df[acct_col].astype(str).isin([str(x) for x in target_accounts])
    tgt = df.loc[is_target, 'ë°œìƒì•¡'].astype(float)
    df['Z-Score'] = 0.0
    if tgt.empty:
        return df
    mu = float(tgt.mean()); std = float(tgt.std(ddof=1))
    if std and std > 0:
        df.loc[is_target, 'Z-Score'] = (df.loc[is_target, 'ë°œìƒì•¡'] - mu) / std
    else:
        med = float(tgt.median()); mad = float((np.abs(tgt - med)).median())
        df.loc[is_target, 'Z-Score'] = 0.0 if mad == 0 else 0.6745 * (df.loc[is_target, 'ë°œìƒì•¡'] - med) / mad
    return df

# ---------------------- NEW: Semantic features -------------------------
def _cosine(a: np.ndarray, b: np.ndarray) -> float:
    da = float(np.linalg.norm(a)); db = float(np.linalg.norm(b))
    if da == 0.0 or db == 0.0: return 0.0
    return float(np.dot(a, b) / (da * db))

def _zseries(x: pd.Series) -> pd.Series:
    x = x.astype(float)
    mu, sd = float(x.mean()), float(x.std(ddof=1))
    if sd and sd > 0: return (x - mu) / sd
    # MAD fallback
    med = float(x.median()); mad = float((x.sub(med).abs()).median())
    return pd.Series(0.0, index=x.index) if mad == 0 else 0.6745 * (x - med) / mad

def _maybe_subcluster_vectors(X: np.ndarray) -> np.ndarray:
    """Return labels for vectors (auto-k KMeans via KDMeans shim)."""
    try:
        from analysis.kdmeans_shim import HDBSCAN
        model = HDBSCAN(n_clusters=None, random_state=42)
        return model.fit_predict(X).astype(int)
    except Exception:
        return np.zeros(len(X), dtype=int)

def _add_semantic_features(
    df: pd.DataFrame,
    *,
    acct_col: str,
    embed_client: Any,
    embed_texts_fn,              # injected (e.g., services.cache.get_or_embed_texts)
    use_large: Optional[bool] = None,
    subcluster: bool = False
) -> pd.DataFrame:
    """ì„ë² ë”© ë²¡í„°, ê³„ì •/í´ëŸ¬ìŠ¤í„° ì„¼íŠ¸ë¡œì´ë“œ, semantic_z(ì½”ì‚¬ì¸ ê±°ë¦¬ z) ìƒì„±."""
    if df is None or df.empty: return df
    base = ensure_rich_embedding_text(df.copy())  # desc+vendor+ì›”+ê·œëª¨+ì„±ê²© ì¡°í•© í…ìŠ¤íŠ¸ ìƒì„±
    base = perform_embedding_only(
        base, client=embed_client, text_col="embedding_text",
        use_large=use_large, embed_texts_fn=embed_texts_fn
    )
    if 'vector' not in base.columns or base['vector'].isna().any():
        return base
    # ë²¡í„° í–‰ë ¬
    V = np.vstack(base['vector'].values).astype(float)
    # ì„ íƒ: ê³„ì • ë‚´ ì„œë¸Œí´ëŸ¬ìŠ¤í„°
    if subcluster:
        labels = pd.Series(index=base.index, dtype=int)
        for code, sub in base.groupby(base[acct_col].astype(str)):
            idx = sub.index
            Xi = np.vstack(sub['vector'].values)
            if len(Xi) < max(SEMANTIC_MIN_RECORDS, 4):
                labels.loc[idx] = 0
            else:
                labels.loc[idx] = _maybe_subcluster_vectors(Xi)
        base['cluster_id'] = labels.astype(int)
    else:
        base['cluster_id'] = 0
    # ê·¸ë£¹(ê³„ì •Ã—í´ëŸ¬ìŠ¤í„°) ì„¼íŠ¸ë¡œì´ë“œ & ì½”ì‚¬ì¸ ê±°ë¦¬
    dists = []
    for (acct, cid), sub in base.groupby([base[acct_col].astype(str), 'cluster_id']):
        vecs = np.vstack(sub['vector'].values)
        c = vecs.mean(axis=0)
        # 1 - cosine sim â†’ semantic distance
        dd = [1.0 - _cosine(v, c) for v in vecs]
        dists.append(pd.Series(dd, index=sub.index))
    base['semantic_dist'] = pd.concat(dists).sort_index()
    # z-í‘œì¤€í™”(ê³„ì •Ã—í´ëŸ¬ìŠ¤í„°ë³„)
    base['semantic_z'] = (
        base.groupby([base[acct_col].astype(str), 'cluster_id'])['semantic_dist']
            .transform(_zseries)
            .astype(float)
    )
    return base

# ---------------------- NEW: Isolation Forest --------------------------
def _fit_iforest_and_score(F: pd.DataFrame, *, contamination: float) -> np.ndarray:
    """Return anomaly scores in [0,1]."""
    try:
        from sklearn.ensemble import IsolationForest
    except Exception:
        return np.zeros(len(F), dtype=float)
    # NaN ë°©ì–´ ë° ìŠ¤ì¼€ì¼ë§ ê°„ë‹¨ ì ìš©
    X = F.fillna(0.0).astype(float).values
    iso = IsolationForest(
        n_estimators=int(IFOREST_N_ESTIMATORS),
        max_samples=IFOREST_MAX_SAMPLES,
        contamination=float(contamination),
        random_state=int(IFOREST_RANDOM_STATE),
        n_jobs=-1
    ).fit(X)
    raw = -iso.score_samples(X)              # ë” í´ìˆ˜ë¡ ì´ìƒ
    lo, hi = float(np.min(raw)), float(np.max(raw))
    s = (raw - lo) / (hi - lo + 1e-12)      # [0,1]
    return s

# --- NEW: ensure_zscore ---
def ensure_zscore(df: pd.DataFrame, account_codes: List[str]):
    """
    Recompute Z-Score for the given account subset and return (df, ok).
    ok=True only if Z-Score column exists and has at least one non-null value.
    """
    df2 = calculate_grouped_stats_and_zscore(df.copy(), target_accounts=[str(x) for x in account_codes] if account_codes else [])
    z = df2.get('Z-Score')
    ok = (z is not None) and (z.notna().any())
    return df2, bool(ok)




# === (ADD) v0.18: ModuleResult ëŸ¬ë„ˆ ===
from analysis.contracts import ModuleResult, EvidenceDetail
from config import PM_DEFAULT, RISK_WEIGHT_A, RISK_WEIGHT_F, RISK_WEIGHT_K, Z_SIGMOID_SCALE, Z_SIGMOID_DIVISOR
import plotly.express as px
import numpy as np
import pandas as pd


def _z_bins_025_sigma(series: pd.Series):
    """0.25Ïƒ ê°„ê²© bin (Â±3Ïƒ í…Œì¼ í¬í•¨)."""
    # ê²½ê³„ì— +3.0 í¬í•¨(+inf í…Œì¼) â†’ ì´ bin ìˆ˜ = 24(ì½”ì–´) + 2(í…Œì¼) = 26
    edges = [-np.inf] + [round(x, 2) for x in np.arange(-3.0, 3.0 + 0.25, 0.25)] + [np.inf]
    core_lefts = [x for x in np.arange(-3.0, 3.0, 0.25)]  # 24ê°œ
    labels_mid = [f"{a:.2f}~{a+0.25:.2f}Ïƒ" for a in core_lefts]
    labels = ["â‰¤-3Ïƒ"] + labels_mid + ["â‰¥3Ïƒ"]               # 26ê°œ
    cats = pd.cut(
        series.astype(float),
        bins=edges,
        labels=labels,
        right=False,
        include_lowest=True,
    )
    # ë¹ˆ êµ¬ê°„ë„ 0ìœ¼ë¡œ ì±„ì›Œ ìˆœì„œ ìœ ì§€
    counts = cats.value_counts(sort=False).reindex(labels, fill_value=0)
    out = pd.DataFrame({"êµ¬ê°„": labels, "ê±´ìˆ˜": counts.values})
    order = labels
    return out, order


def _sigmoid(x: float) -> float:
    import math
    return 1.0 / (1.0 + math.exp(-x))


def _risk_from(z_abs: float, amount: float, pm: float):
    """ë¦¬ìŠ¤í¬ ì ìˆ˜ êµ¬ì„±ìš”ì†Œ ê³„ì‚°.
    - a: ì‹œê·¸ëª¨ì´ë“œ ì •ê·œí™”ëœ ì´íƒˆ ê°•ë„(|Z|/scale). scaleì€ ì„¤ì •ê°’.
    - f: PM ëŒ€ë¹„ ê¸ˆì•¡ë¹„ìœ¨(0~1ë¡œ ìº¡). PMì´ 0/ìŒìˆ˜ë©´ 0ìœ¼ë¡œ ê°•ì œ.
    - k: Key Item í”Œë˜ê·¸(PM ì´ˆê³¼ì‹œ 1). PMì´ 0/ìŒìˆ˜ë©´ 0ìœ¼ë¡œ ê°•ì œ.
    """
    # ìš°ì„ ìˆœìœ„: Z_SIGMOID_DIVISOR(ì‹ ê·œ ë…¸ë¸Œ) > Z_SIGMOID_SCALE(êµ¬ëª…). ê¸°ë³¸ 1.0
    div = None
    try:
        div = float(Z_SIGMOID_DIVISOR)
    except Exception:
        div = None
    if not div or div <= 0:
        try:
            div = float(Z_SIGMOID_SCALE)
        except Exception:
            div = 1.0
    if not div or div <= 0:
        div = 1.0
    a = _sigmoid(float(abs(z_abs)) / float(div))      # anomaly_score
    # PM ê°€ë“œ: pm<=0ì´ë©´ f=0, k=0
    if pm is None or float(pm) <= 0:
        f = 0.0
        k = 0.0
    else:
        f = min(1.0, abs(float(amount)) / float(pm))  # PM ratio (capped at 1)
        k = 1.0 if abs(float(amount)) >= float(pm) else 0.0
    score = RISK_WEIGHT_A * a + RISK_WEIGHT_F * f + RISK_WEIGHT_K * k
    return a, f, k, score


def _assertions_for_row(z_val: float) -> List[str]:
    # ê¸°ë³¸ ê·œì¹™: AëŠ” í•­ìƒ í¬í•¨. ìŒì˜ í° ì´íƒˆ(C), ì–‘ì˜ í° ì´íƒˆ(E)ì„ ë³´ê°•.
    out = {"A"}
    try:
        if float(z_val) <= -2.0:
            out.add("C")
        if float(z_val) >=  2.0:
            out.add("E")
    except Exception:
        pass
    return sorted(out)


def run_anomaly_module(
    lf,
    target_accounts=None,
    topn=20,
    pm_value: Optional[float] = None,
    *,
    # --- NEW: injection knobs (analysis ë ˆì´ì–´ëŠ” servicesì— ì§ì ‘ ì˜ì¡´ ê¸ˆì§€) ---
    embed_client: Any = None,
    embed_texts_fn=None,
    use_large_embedding: Optional[bool] = None,
    semantic_enabled: bool = True,
    subcluster_enabled: bool = False,
    iforest_enabled: Optional[bool] = None,
    iforest_contamination: Optional[float] = None,
):
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    if not acct_col:
        return ModuleResult("anomaly", {}, {}, {}, [], ["ê³„ì •ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."])

    # ëŒ€ìƒ ê³„ì • ì„œë¸Œì…‹
    if target_accounts:
        codes = [str(x) for x in target_accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    # Z-Score ê³„ì‚°
    df = calculate_grouped_stats_and_zscore(df, target_accounts=df[acct_col].astype(str).unique().tolist())
    if 'íšŒê³„ì¼ì' in df.columns:
        df['ì—°ì›”'] = df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)

    # ì´ìƒì¹˜ í”Œë˜ê·¸ (Â±3Ïƒ)
    df['is_outlier'] = df['Z-Score'].abs() >= 3

    # === (NEW) ì˜ë¯¸í”¼ì²˜/IForest ìƒì„± ===
    if semantic_enabled and (embed_client is not None) and (embed_texts_fn is not None):
        try:
            df = _add_semantic_features(
                df, acct_col=acct_col, embed_client=embed_client,
                embed_texts_fn=embed_texts_fn, use_large=use_large_embedding,
                subcluster=subcluster_enabled
            )
        except Exception:
            # ì˜ë¯¸í”¼ì²˜ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ Z-Score íë¦„ì€ ìœ ì§€
            pass
    # Isolation Forest (ì˜ë¯¸í”¼ì²˜ê°€ ìˆë“  ì—†ë“  ìˆ˜ì¹˜íŠ¹ì§•ë§Œìœ¼ë¡œë„ ë™ì‘)
    if iforest_enabled is None:
        iforest_enabled = bool(IFOREST_ENABLED_DEFAULT)
    if iforest_enabled:
        try:
            feats: Dict[str, Any] = {}
            feats['amt']      = pd.to_numeric(df.get('ë°œìƒì•¡', 0.0), errors='coerce').abs()
            feats['amt_log']  = np.log1p(feats['amt'])
            feats['z_abs']    = df.get('Z-Score', 0.0).abs()
            feats['sem_abs']  = df.get('semantic_z', 0.0).abs() if 'semantic_z' in df.columns else 0.0
            if 'ì—°ì›”' in df.columns:
                # ê°„ë‹¨ ì›” ì¸ë±ìŠ¤(ëª¨ë¸ì˜ ì‹œí€€ìŠ¤ surrogate)
                feats['month_idx'] = pd.Categorical(df['ì—°ì›”']).codes.astype(float)
            F = pd.DataFrame(feats, index=df.index)
            contam = float(iforest_contamination) if iforest_contamination is not None else float(IFOREST_CONTAM_DEFAULT)
            df['iforest_score'] = _fit_iforest_and_score(F, contamination=contam)
        except Exception:
            pass

    # ì´ìƒì¹˜ í›„ë³´ í…Œì´ë¸” (ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒìœ„)
    out_cols = [c for c in ['row_id','íšŒê³„ì¼ì','ì—°ì›”','ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','Z-Score'] if c in df.columns]
    # (NEW) í…Œì´ë¸”ì— ì‹ í˜¸ ì»¬ëŸ¼ ë…¸ì¶œ
    for extra in ['semantic_z','iforest_score','cluster_id']:
        if extra in df.columns and extra not in out_cols:
            out_cols.append(extra)
    cand = (df.assign(absz=df['Z-Score'].abs())
              .sort_values('absz', ascending=False)
              .drop(columns=['absz'])
              .head(int(topn)))
    table = cand[out_cols + (['is_outlier'] if 'is_outlier' in cand.columns and 'is_outlier' not in out_cols else [])] if out_cols else cand

    # === EvidenceDetail ìƒì„± (KIT + |Z| ê¸°ì¤€) ===
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    ev_rows: List[EvidenceDetail] = []
    # ì¦ê±° ì±„ì§‘ ëŒ€ìƒ: (1) PM ì´ˆê³¼ or (2) |Z|>=2.5 or (3) ìƒìœ„ topn
    #               + (4) semantic_z ê³¼ëŒ€ or (5) iforest_score ê³¼ëŒ€
    mask_key = df['ë°œìƒì•¡'].abs() >= pm if 'ë°œìƒì•¡' in df.columns else pd.Series(False, index=df.index)
    mask_z   = df['Z-Score'].abs() >= 2.5 if 'Z-Score' in df.columns else pd.Series(False, index=df.index)
    mask_sem = df['semantic_z'].abs() >= float(SEMANTIC_Z_THRESHOLD) if 'semantic_z' in df.columns else pd.Series(False, index=df.index)
    thr_ifo  = float(ANOMALY_IFOREST_SCORE_THRESHOLD)
    mask_ifo = df['iforest_score'] >= thr_ifo if 'iforest_score' in df.columns else pd.Series(False, index=df.index)
    idx_sel  = set(df.index[mask_key | mask_z | mask_sem | mask_ifo].tolist()) | set(table.index.tolist())
    sub = df.loc[sorted(idx_sel)].copy() if len(idx_sel)>0 else df.head(0).copy()
    for _, r in sub.iterrows():
        z  = float(r.get('Z-Score', 0.0)) if pd.notna(r.get('Z-Score', np.nan)) else 0.0
        za = abs(z)
        amt = float(r.get('ë°œìƒì•¡', 0.0))
        a, f, k, score = _risk_from(za, amt, pm)   # (ê¸°ì¡´) í†µí•© ìœ„í—˜ ì ìˆ˜ëŠ” PM/|Z| ê¸°ë°˜ ìœ ì§€
        # (NEW) anomaly_scoreì— ì˜ë¯¸/IForest ì‹ í˜¸ë¥¼ ë°˜ì˜í•´ íƒìƒ‰ ìš°ì„ ìˆœìœ„ ê°œì„ 
        semz = float(abs(r.get('semantic_z', 0.0))) if pd.notna(r.get('semantic_z', np.nan)) else 0.0
        ifo  = float(r.get('iforest_score', 0.0)) if pd.notna(r.get('iforest_score', np.nan)) else 0.0
        try:
            div = float(Z_SIGMOID_DIVISOR) if float(Z_SIGMOID_DIVISOR) > 0 else 3.0
        except Exception:
            div = 3.0
        sem_a = 1.0 / (1.0 + math.exp(-(semz/div))) if semz > 0 else 0.0
        anomaly_score = float(max(a, sem_a, ifo))
        ev_rows.append(EvidenceDetail(
            row_id=str(r.get('row_id','')),
            reason="; ".join(filter(None, [
                f"amt_z={z:+.2f}",
                (f"sem_z={r.get('semantic_z'):+.2f}" if 'semantic_z' in r and pd.notna(r['semantic_z']) else ""),
                (f"iforest={ifo:.2f}" if 'iforest_score' in r and pd.notna(r['iforest_score']) else "")
            ])),
            anomaly_score=anomaly_score,
            financial_impact=abs(amt),
            risk_score=float(score),
            is_key_item=bool(abs(amt) >= pm),
            impacted_assertions=_assertions_for_row(z),
            links={
                "account_code": str(r.get('ê³„ì •ì½”ë“œ','')),
                "account_name": str(r.get('ê³„ì •ëª…','')),
                "vendor":      str(r.get('ê±°ë˜ì²˜','')),
                "narration":   str(r.get('ì ìš”','')),
                "cluster_name": str(r.get('cluster_name','')) if 'cluster_name' in r.index else "",
                "cluster_group": str(r.get('cluster_group','')) if 'cluster_group' in r.index else "",
                "month":       str(r.get('ì—°ì›”','')) if 'ì—°ì›”' in r.index else "",
                "period_tag": str(r.get('period_tag','')),
            }
        ))

    # step-Ïƒ bin ë¶„í¬ ë§‰ëŒ€
    figures = {}
    try:
        dist_df, order = _z_bins_025_sigma(df['Z-Score'])
        total_n = int(len(df))
        outlier_rate = float((df['Z-Score'].abs() >= 3).mean() * 100) if total_n else 0.0
        title = f"Z-Score ë¶„í¬ (0.25Ïƒ bin, Â±3Ïƒ ì§‘ê³„) â€” N={total_n:,}, outlierâ‰ˆ{outlier_rate:.1f}%"
        fig = px.bar(dist_df, x='êµ¬ê°„', y='ê±´ìˆ˜', title=title)
        fig.update_yaxes(separatethousands=True)
        fig.update_layout(bargap=0.10)
        figures = {"zscore_hist": fig}
    except Exception:
        pass

    summary = {
        "n_rows": int(len(df)),
        "n_candidates": int(len(table)),
        "accounts": sorted(df[acct_col].astype(str).unique().tolist()),
        "period_tag_coverage": dict(df.get('period_tag', pd.Series(dtype=str)).value_counts()) if 'period_tag' in df.columns else {}
    }
    # Evidence ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸”(ì„ íƒ)
    try:
        import pandas as _pd
        ev_tbl = _pd.DataFrame([{
            "row_id": e.row_id,
            "ê³„ì •ì½”ë“œ": e.links.get("account_code",""),
            "ê³„ì •ëª…":   e.links.get("account_name",""),
            "risk_score": e.risk_score,
            "is_key_item": e.is_key_item,
            "impacted": ",".join(e.impacted_assertions),
            "reason": e.reason,
        } for e in ev_rows]).sort_values("risk_score", ascending=False).head(100)
    except Exception:
        ev_tbl = None

    return ModuleResult(
        name="anomaly",
        summary=summary,
        tables={"anomaly_top": table, **({"evidence_preview": ev_tbl} if ev_tbl is not None else {})},
        figures=figures,
        evidences=ev_rows,
        warnings=[]
    )


==============================
ğŸ“„ FILE: analysis/assertion_risk.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from analysis.contracts import ModuleResult, EvidenceDetail, ASSERTIONS


HEATMAP_BS_RISK = "max"  # or "balance_only" / "weighted"


def _agg_bs_risk(rows: pd.DataFrame) -> float:
    """BS ì…€ ìœ„í—˜ë„ ì§‘ê³„ ê·œì¹™(ê¸°ë³¸ max).
    - EvidenceDetail.measureê°€ ì œê³µë˜ëŠ” ê²½ìš°ì—ë§Œ ì ìš© ê°€ëŠ¥.
    - 'weighted'ëŠ” balance 0.6, flow 0.4 ê°€ì¤‘.
    """
    if rows is None or rows.empty:
        return 0.0
    if HEATMAP_BS_RISK == "balance_only":
        r = rows.loc[rows.get("measure", pd.Series()).eq("balance"), "risk_score"]
        return float(r.max() if not r.empty else rows["risk_score"].max())
    if HEATMAP_BS_RISK == "weighted":
        w = rows.get("measure", pd.Series(index=rows.index)).map({"balance": 0.6, "flow": 0.4}).fillna(0.5)
        try:
            return float(np.average(rows["risk_score"].astype(float), weights=w))
        except Exception:
            return float(rows["risk_score"].max())
    return float(rows["risk_score"].max())


def build_matrix(modules: List[ModuleResult]):
    """
    ëª¨ë“ˆ EvidenceDetail â†’ (ê³„ì • Ã— ì£¼ì¥) ìµœëŒ€ risk_score ë§¤íŠ¸ë¦­ìŠ¤ + ë“œë¦´ë‹¤ìš´ ë§µ
    ë°˜í™˜: (matrix_df[account_name x ASSERTIONS], evidence_map[(acct, asrt)] -> [row_id...])
    """
    bucket_rows: Dict[Tuple[str,str], List[Dict]] = {}
    emap: Dict[Tuple[str,str], List[str]] = {}
    accts: set[str] = set()

    for mod in modules:
        for ev in (mod.evidences or []):
            acct = ev.links.get("account_name") or ev.links.get("account_code") or "UNMAPPED"
            accts.add(acct)
            for a in (ev.impacted_assertions or []):
                key = (acct, a)
                bucket_rows.setdefault(key, []).append({
                    "risk_score": float(ev.risk_score),
                    "measure": getattr(ev, "measure", None)
                })
                emap.setdefault(key, []).append(str(ev.row_id))

    idx = sorted(accts)
    mat = pd.DataFrame(index=idx, columns=ASSERTIONS, data=0.0)
    for (acct, asrt), rows in bucket_rows.items():
        df = pd.DataFrame(rows)
        mat.loc[acct, asrt] = _agg_bs_risk(df) if not df.empty else 0.0
    return mat.fillna(0.0), emap





==============================
ğŸ“„ FILE: analysis/contracts.py
==============================

from dataclasses import dataclass, field
import pandas as pd
from typing import Dict, List, Any, Optional, Literal
# --- New: Measure íƒ€ì… íŒíŠ¸("flow" ë˜ëŠ” "balance") ---
Measure = Literal["flow", "balance"]


@dataclass(frozen=True)
class LedgerFrame:
    df: pd.DataFrame
    meta: Dict[str, Any]  # ì˜ˆ: {"company": "...", "file_name": "...", "uploaded_at": ...}

# CEAVOP assertions
ASSERTIONS = ["C","E","A","V","O","P"]

@dataclass(frozen=True)
class EvidenceDetail:
    row_id: str
    reason: str                  # e.g., "|Z|=3.1 (CY group mean-based)"
    anomaly_score: float         # 0~1 normalized
    financial_impact: float      # KRW absolute amount
    risk_score: float            # integrated score
    is_key_item: bool            # PM exceed flag
    # --- NEW: measurement basis and sign rule ---
    measure: Measure = "flow"     # "flow"(ì›”ë³„ ë°œìƒì•¡, Î”ì”ì•¡/ìˆœì•¡) ë˜ëŠ” "balance"
    sign_rule: str = "assets/expensesâ†‘=+, liabilities/equityâ†‘=-"
    # --- NEW: ì‹œê³„ì—´ ì˜ˆì¸¡ ë©”íƒ€ (ì˜µì…”ë„) ---
    model: Optional[str] = None           # ì‚¬ìš©ëœ ëª¨ë¸ëª… (ì˜ˆ: EMA/MA/ARIMA/Prophet)
    window_policy: Optional[str] = None   # ì˜ˆ: "PY+CY"
    data_span: Optional[str] = None       # ì˜ˆ: "YYYY-MM ~ YYYY-MM"
    train_months: Optional[int] = None    # í•™ìŠµ ì›” ìˆ˜
    horizon: Optional[int] = None         # ì˜ˆì¸¡ ìˆ˜í‰(ì›”)
    basis_note: Optional[str] = None      # ì˜ˆ: "BSëŠ” ì”ì•¡Â·ë°œìƒì•¡ ë³‘ë ¬ ê³„ì‚°"
    extra: Optional[Dict[str, Any]] = field(default_factory=dict)
    impacted_assertions: List[str] = field(default_factory=list)  # e.g., ["A","C"]
    links: Dict[str, Any] = field(default_factory=dict)           # e.g., {"account_code": "...", "account_name": "..."}

@dataclass(frozen=True)
class ModuleResult:
    name: str
    summary: Dict[str, Any]             # LLM ì…ë ¥ìš© í•µì‹¬ ìˆ˜ì¹˜/ì§€í‘œ
    tables: Dict[str, pd.DataFrame]
    figures: Dict[str, Any]             # plotly Figure
    evidences: List[EvidenceDetail]     # structured evidences
    warnings: List[str]


# ê³µê°œ API ëª…ì‹œ(ìŠ¤í‚¤ë§ˆ ê³ ì •ì— ë„ì›€)
__all__ = [
    "LedgerFrame", "EvidenceDetail", "ModuleResult", "ASSERTIONS"
]




==============================
ğŸ“„ FILE: analysis/correlation.py
==============================

from __future__ import annotations
import pandas as pd
import numpy as np
import plotly.express as px
from typing import List, Dict, Any, Tuple, Optional, Mapping, Sequence
import re
from analysis.contracts import LedgerFrame, ModuleResult, EvidenceDetail
from utils.helpers import find_column_by_keyword
from config import (
    CORR_DEFAULT_METHOD, CORR_THRESHOLD_DEFAULT, CORR_MIN_ACTIVE_MONTHS_DEFAULT
)


def _monthly_pivot(df: pd.DataFrame, acct_col: str) -> pd.DataFrame:
    """ê³„ì •ì½”ë“œÃ—ì—°ì›” í”¼ë²—(ê±°ë˜ê¸ˆì•¡ í•©ê³„). PL/BS ëª¨ë‘ ì›” íë¦„ ê¸°ì¤€."""
    if 'íšŒê³„ì¼ì' not in df.columns:
        raise ValueError("íšŒê³„ì¼ì í•„ìš”")
    g = (df.assign(ì—°ì›”=df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str))
           .groupby([acct_col, 'ì—°ì›”'])['ê±°ë˜ê¸ˆì•¡'].sum()
           .unstack('ì—°ì›”', fill_value=0.0)
           .sort_index())
    return g
 
def _filter_accounts_for_corr(piv: pd.DataFrame, min_active_months: int = 6) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    - Drop accounts with zero variance across months (std == 0) OR
      with insufficient active months (abs(value)>0 in fewer than min_active_months months).
    - Return filtered pivot and an exclusions dataframe with reasons.
    """
    if piv.empty:
        return piv, pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ì‚¬ìœ ','í™œë™ì›”ìˆ˜','í‘œì¤€í¸ì°¨'])
    std = piv.std(axis=1)
    active = (piv.abs() > 0).sum(axis=1)
    reason = []
    idx = piv.index.astype(str)
    keep = (std > 0) & (active >= int(min_active_months))
    for code, s, a, k in zip(idx, std, active, keep):
        if k:
            continue
        r = []
        if s == 0:
            r.append("ë³€ë™ì—†ìŒ(í‘œì¤€í¸ì°¨ 0)")
        if a < int(min_active_months):
            r.append(f"í™œë™ ì›” ë¶€ì¡±(<{int(min_active_months)})")
        reason.append((code, " & ".join(r) if r else "ì œì™¸", int(a), float(s)))
    excluded = pd.DataFrame(reason, columns=['ê³„ì •ì½”ë“œ','ì‚¬ìœ ','í™œë™ì›”ìˆ˜','í‘œì¤€í¸ì°¨'])
    return piv.loc[keep], excluded


def _infer_cycle(account_name: str, cycles_map: Mapping[str, Sequence[str]]) -> Optional[str]:
    """
    STANDARD_ACCOUNTING_CYCLES ê¸°ë°˜ì˜ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤í•‘.
    ê°€ì¥ ë¨¼ì € ë§¤ì¹­ë˜ëŠ” ì‚¬ì´í´ì„ ë°˜í™˜(ìš°ì„ ìˆœìœ„: dict ì •ì˜ ìˆœì„œ).
    """
    name = str(account_name or "").lower()
    for cycle, keywords in cycles_map.items():
        for kw in keywords:
            if kw and re.search(re.escape(str(kw).lower()), name):
                return cycle
    return None


def map_accounts_to_cycles(accounts: List[str], *, cycles_map: Mapping[str, Sequence[str]]) -> Dict[str, Optional[str]]:
    """ë°°ì¹˜ ë§¤í•‘: ê³„ì •ëª… ë¦¬ìŠ¤íŠ¸ â†’ {ê³„ì •ëª…: ì‚¬ì´í´(or None)}.
    cycles_mapì€ ìƒìœ„ ë ˆì´ì–´(app/services)ì—ì„œ ì£¼ì…í•©ë‹ˆë‹¤.
    """
    return {acc: _infer_cycle(acc, cycles_map) for acc in accounts}


def _normalize_cycles_map(df: pd.DataFrame, cycles_map):
    """
    cycles_map ì…ë ¥ ìœ ì—°í™”:
    - {ê³„ì •ì½”ë“œ -> ì‚¬ì´í´ì½”ë“œ} í˜•íƒœë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - {ì‚¬ì´í´ì½”ë“œ -> [í‚¤ì›Œë“œ]} í˜•íƒœë©´ ê³„ì •ëª… ê¸°ë°˜ìœ¼ë¡œ ì¶”ì • ë§¤í•‘ ìƒì„±
    - Noneì´ë©´ ë¹ˆ dict
    """
    if not cycles_map:
        return {}
    # code->cycle í˜•íƒœ íŒë³„
    # ê°’ì´ ë¬¸ìì—´ì´ë©´ ì‚¬ì´í´ ì½”ë“œë¼ê³  ê°€ì •
    if isinstance(next(iter(cycles_map.values())), str):
        return {str(k): str(v) for k, v in cycles_map.items()}
    # cycle->keywords í˜•íƒœë©´ ê³„ì •ëª…ìœ¼ë¡œ ìœ ì¶”
    try:
        name_map = (df.drop_duplicates("ê³„ì •ì½”ë“œ")
                      .assign(ê³„ì •ì½”ë“œ=lambda d: d["ê³„ì •ì½”ë“œ"].astype(str))
                      .set_index("ê³„ì •ì½”ë“œ")["ê³„ì •ëª…"].astype(str).to_dict())
    except Exception:
        name_map = {}
    out = {}
    for code, nm in name_map.items():
        cyc = _infer_cycle(nm, cycles_map)
        if cyc: out[code] = cyc
    return out


def friendly_correlation_explainer() -> str:
    return (
        "### í•´ì„ ê°€ì´ë“œ(ìš”ì•½)\n"
        "- **ìƒê´€ â‰  ì¸ê³¼**: í•¨ê»˜ ì›€ì§ì¸ë‹¤ê³  ì›ì¸/ê²°ê³¼ëŠ” ì•„ë‹™ë‹ˆë‹¤.\n"
        "- **í‘œë³¸ ê¸¸ì´**ì™€ **í™œë™ì›” ìˆ˜**ê°€ ì§§ìœ¼ë©´ ìˆ˜ì¹˜ê°€ í”ë“¤ë¦½ë‹ˆë‹¤.\n"
        "- **ìŒ(-)ì˜ ìƒê´€**ì€ í•œìª½ì´ ì˜¤ë¥´ë©´ ë‹¤ë¥¸ ìª½ì´ ë‚´ë¦¬ëŠ” ë™í–‰ì…ë‹ˆë‹¤.\n"
        "- ê³ ê¸‰ íƒ­ì˜ **ì‹œì°¨** ê²°ê³¼ê°€ í¬ë©´, â€˜ì„ í›„â€™ ê´€ê³„ ë‹¨ì„œê°€ ë  ìˆ˜ ìˆìœ¼ë‚˜ ì¸ê³¼ ì…ì¦ì€ ì•„ë‹™ë‹ˆë‹¤.\n"
        "- **ë¡¤ë§ ì•ˆì •ì„±**ì´ ë‚®ìœ¼ë©´(ë³€ë™ì„±â†‘) ì¼ì‹œì  ìƒê´€ì¼ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\n"
    )


def suggest_anchor_accounts(lf: LedgerFrame, *, cycles_codes: list[str] | None = None,
                            corr_threshold: float = CORR_THRESHOLD_DEFAULT, topn: int = 5) -> pd.DataFrame:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    if not acct_col or df.empty:
        return pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ê·œëª¨í•©ê³„','í‘œì¤€í¸ì°¨','degree','score'])
    piv = _monthly_pivot(df, acct_col)
    if piv.empty:
        return pd.DataFrame()
    if cycles_codes:
        idx_keep = piv.index.astype(str).isin([str(x) for x in cycles_codes])
        piv = piv.loc[idx_keep]
    if piv.shape[0] < 1:
        return pd.DataFrame()
    abs_piv = piv.abs()
    size = abs_piv.sum(axis=1)
    vol  = abs_piv.std(axis=1)
    corr = piv.T.corr('pearson').fillna(0.0)
    deg  = (corr.abs() >= float(corr_threshold)).sum(axis=1) - 1
    nz = lambda s: (s - s.min()) / (s.max() - s.min() + 1e-12)
    score = 0.4*nz(size) + 0.4*nz(vol) + 0.2*nz(deg)
    try:
        name_map = (
            df.drop_duplicates('ê³„ì •ì½”ë“œ')
              .assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str))
              .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict()
        )
    except Exception:
        name_map = {}
    out = (
        pd.DataFrame({
            'ê³„ì •ì½”ë“œ': piv.index.astype(str),
            'ê³„ì •ëª…':  piv.index.astype(str).map(name_map),
            'ê·œëª¨í•©ê³„': size.values, 'í‘œì¤€í¸ì°¨': vol.values,
            'degree': deg.reindex(piv.index).values, 'score': score.values
        })
        .sort_values('score', ascending=False)
        .head(int(topn))
    )
    return out


def run_correlation_module(
    lf: LedgerFrame,
    accounts: List[str] | None = None,
    *,
    method: str = CORR_DEFAULT_METHOD,
    corr_threshold: float = CORR_THRESHOLD_DEFAULT,
    min_active_months: int = CORR_MIN_ACTIVE_MONTHS_DEFAULT,
    cycles_map: Mapping[str, Sequence[str]] | Mapping[str, str] | None = None,
    within_same_cycle: bool | None = None,
    emit_evidences: bool = False,
) -> ModuleResult:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    if not acct_col:
        return ModuleResult("correlation", {}, {}, {}, [], ["ê³„ì •ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."])

    # ëŒ€ìƒ ê³„ì • í•„í„°
    if accounts:
        codes = [str(a) for a in accounts]
        df = df[df[acct_col].astype(str).isin(codes)].copy()

    if df.empty:
        return ModuleResult("correlation", {}, {}, {}, [], ["ì„ íƒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."])

    piv = _monthly_pivot(df, acct_col)
    piv_f, excluded = _filter_accounts_for_corr(piv, min_active_months=min_active_months)
    if piv_f.shape[0] < 2:
        warn = "ìƒê´€ì„ ê³„ì‚°í•  ê³„ì •ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤."
        if not excluded.empty:
            warn += f" (ì œì™¸ëœ ê³„ì • {len(excluded)}ê°œ: ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)"
        return ModuleResult("correlation", {}, {"excluded_accounts": excluded}, {}, [], [warn])

    corr = piv_f.T.corr(method=method)  # ê³„ì •Ã—ê³„ì •
    # ê³„ì •ì½”ë“œ â†’ ê³„ì •ëª… ë§¤í•‘
    try:
        name_map = (
            df.drop_duplicates('ê³„ì •ì½”ë“œ')
              .assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str))
              .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…']
              .astype(str).to_dict()
        )
    except Exception:
        name_map = {}
    xn = [name_map.get(str(c), str(c)) for c in corr.columns]
    yn = [name_map.get(str(r), str(r)) for r in corr.index]
    fig = px.imshow(
        corr,
        text_auto=False,
        title="ê³„ì • ê°„ ì›”ë³„ ìƒê´€ íˆíŠ¸ë§µ",
        labels=dict(x="ê³„ì •", y="ê³„ì •", color="ìƒê´€ê³„ìˆ˜"),
        aspect='auto',
        x=xn,
        y=yn,
    )
    fig.update_traces(hovertemplate="ê³„ì •: %{y} Ã— %{x}<br>ìƒê´€ê³„ìˆ˜: %{z:.3f}<extra></extra>")
    fig.update_coloraxes(cmin=-1, cmax=1)
    fig.update_xaxes(type='category')
    fig.update_yaxes(type='category')

    # ì„ê³„ ìƒê´€ìŒ í…Œì´ë¸” (idempotent-safe)
    def build_strong_pairs(corr_matrix: pd.DataFrame, code_to_name: dict, threshold: float = 0.7) -> pd.DataFrame:
        import numpy as _np
        import pandas as _pd
        mask = _np.triu(_np.ones_like(corr_matrix, dtype=bool), k=1)
        cm = corr_matrix.copy().mask(mask)
        rows = []
        abs_vals = cm.abs().values
        idx_i, idx_j = _np.where(abs_vals >= threshold)
        for i, j in zip(idx_i, idx_j):
            rows.append({
                "ê³„ì •ì½”ë“œ_A": corr_matrix.index[i],
                "ê³„ì •ì½”ë“œ_B": corr_matrix.columns[j],
                "ìƒê´€ê³„ìˆ˜": float(cm.values[i, j]),
            })
        pairs_df = _pd.DataFrame(rows)
        if pairs_df.empty:
            return pairs_df
        pairs_df = pairs_df.assign(
            ê³„ì •ëª…_A=pairs_df["ê³„ì •ì½”ë“œ_A"].map(code_to_name),
            ê³„ì •ëª…_B=pairs_df["ê³„ì •ì½”ë“œ_B"].map(code_to_name),
        )
        base_cols = ["ê³„ì •ëª…_A", "ê³„ì •ì½”ë“œ_A", "ê³„ì •ëª…_B", "ê³„ì •ì½”ë“œ_B", "ìƒê´€ê³„ìˆ˜"]
        pairs_df = pairs_df[base_cols]
        pairs_df = pairs_df.reindex(
            pairs_df["ìƒê´€ê³„ìˆ˜"].abs().sort_values(ascending=False).index
        )
        return pairs_df

    pairs_df = build_strong_pairs(corr, name_map, threshold=float(corr_threshold))

    # === Evidence ìƒì„±: |r|â‰¥thr ìŒì„ êµ¬ì¡°í™” (risk_score = |r|, financial_impact = min(ë‘ ê³„ì •ì˜ ì›”ë³„ ì ˆëŒ€í•©))
    evidences = []
    try:
        # ê³„ì •ë³„ ê·œëª¨(ì ˆëŒ€ íë¦„) í•©ê³„
        abs_sum = piv_f.abs().sum(axis=1).astype(float)  # index: ê³„ì •ì½”ë“œ
        cyc_map_norm = _normalize_cycles_map(df, cycles_map)
        for _, row in pairs_df.iterrows():
            code_a = str(row["ê³„ì •ì½”ë“œ_A"]); code_b = str(row["ê³„ì •ì½”ë“œ_B"])
            same_cyc = None
            if cyc_map_norm:
                same_cyc = str(cyc_map_norm.get(code_a,"")) == str(cyc_map_norm.get(code_b,""))
            if within_same_cycle is True and same_cyc is not True:
                continue  # ë™ì¼ ì‚¬ì´í´ë§Œ ë‚¨ê¹€
            r = float(row["ìƒê´€ê³„ìˆ˜"])
            fin = float(min(abs_sum.get(code_a, 0.0), abs_sum.get(code_b, 0.0)))
            evidences.append(EvidenceDetail(
                row_id=f"{code_a}|{code_b}",
                reason=f"corr={r:+.2f}" + (f" Â· same_cycle={bool(same_cyc)}" if same_cyc is not None else ""),
                anomaly_score=abs(r),           # ì •ê·œí™”(0~1)
                financial_impact=fin,           # ì ì¬ ê³µë™ë³€ë™ ê·œëª¨ì˜ ë³´ìˆ˜ì  ê·¼ì‚¬
                risk_score=abs(r),              # rì˜ í¬ê¸°ê°€ í•´ì„ ë³µì¡ë„/ì¶”ì  í•„ìš”ë„ë¥¼ ëŒ€ë³€
                is_key_item=False,
                impacted_assertions=[],         # Assertions ë¹„í™œì„±(í›ˆë‹˜ ë°©ì¹¨)
                links={
                    "account_code_a": code_a, "account_code_b": code_b,
                    "account_name_a": row.get("ê³„ì •ëª…_A",""), "account_name_b": row.get("ê³„ì •ëª…_B",""),
                    "corr": r, "same_cycle": bool(same_cyc) if same_cyc is not None else None
                }
            ))
    except Exception:
        evidences = []

    # ì‚¬ì´í´ ë§¤í•‘ ìš”ì•½(ê³„ì •ëª… í•„ìš”í•˜ë¯€ë¡œ ë³„ë„ í‘œì—ì„œëŠ” ê³„ì •ëª… ë§¤í•‘ í•„ìš” ì‹œ upstreamì—ì„œ ì²˜ë¦¬)
    summary = {
        "n_accounts": int(corr.shape[0]),
        "n_pairs_over_threshold": int(len(pairs_df)),
        "corr_threshold": float(corr_threshold),
        "method": str(method)
    }
    return ModuleResult(
        name="correlation",
        summary=summary,
        tables={"strong_pairs": pairs_df, "corr_matrix": corr, "excluded_accounts": excluded},
        figures={"heatmap": fig},
        evidences=evidences,
        warnings=([f"ì œì™¸ëœ ê³„ì • {len(excluded)}ê°œ(ë³€ë™ì—†ìŒ/í™œë™ì›” ë¶€ì¡±)."] if not excluded.empty else [])
    )


# --- NEW: Focus ëª¨ë“ˆ (ë‹¨ì¼ ê³„ì • vs ë‚˜ë¨¸ì§€) ---
def run_correlation_focus_module(
    lf: LedgerFrame,
    focus_account: str,                 # ê³„ì •ì½”ë“œ ë˜ëŠ” ê³„ì •ëª…
    *,
    cycles_map: Mapping[str, Sequence[str]] | Mapping[str, str] | None = None,
    method: str = CORR_DEFAULT_METHOD,
    min_active_months: int = CORR_MIN_ACTIVE_MONTHS_DEFAULT,
    within_same_cycle: bool = True,
    corr_threshold: float = CORR_THRESHOLD_DEFAULT,
) -> ModuleResult:
    df = lf.df.copy()
    acct_col = find_column_by_keyword(df.columns, 'ê³„ì •ì½”ë“œ')
    if not acct_col:
        return ModuleResult("correlation_focus", {}, {}, {}, [], ["ê³„ì •ì½”ë“œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."])
    # ì½”ë“œ/ì´ë¦„ ë°©ì–´
    fc = str(focus_account)
    mask = (df[acct_col].astype(str) == fc) | (df.get("ê³„ì •ëª…","").astype(str) == fc)
    if not mask.any():
        return ModuleResult("correlation_focus", {}, {}, {}, [], ["ì„ íƒí•œ ê³„ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."])
    piv = _monthly_pivot(df, acct_col)
    piv_f, excluded = _filter_accounts_for_corr(piv, min_active_months=min_active_months)
    # focus ì¡´ì¬ ë³´ì¥
    # ì´ë¦„â†’ì½”ë“œ ë§¤í•‘
    name_to_code = (
        df.drop_duplicates("ê³„ì •ëª…")
          .assign(ê³„ì •ì½”ë“œ=lambda d: d["ê³„ì •ì½”ë“œ"].astype(str))
          .set_index("ê³„ì •ëª…")["ê³„ì •ì½”ë“œ"].astype(str).to_dict()
    )
    code = fc if fc in piv_f.index else name_to_code.get(fc)
    if code not in piv_f.index:
        return ModuleResult("correlation_focus", {}, {"excluded_accounts": excluded}, {}, [], ["í¬ì»¤ìŠ¤ ê³„ì •ì— ìœ íš¨í•œ ì›”ë³„ ë³€ë™ì´ ì—†ìŠµë‹ˆë‹¤."])
    # within_same_cycle í•„í„°
    cyc_map_norm = _normalize_cycles_map(df, cycles_map)
    if within_same_cycle and cyc_map_norm:
        my_cycle = str(cyc_map_norm.get(str(code), ""))
        keep = [ix for ix in piv_f.index if str(cyc_map_norm.get(str(ix), "")) == my_cycle]
        piv_f = piv_f.loc[keep] if len(keep) >= 2 else piv_f
    if piv_f.shape[0] < 2:
        return ModuleResult("correlation_focus", {}, {"excluded_accounts": excluded}, {}, [], ["ìƒê´€ì„ ê³„ì‚°í•  íƒ€ ê³„ì •ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."])
    rvec = piv_f.T.corr(method=method)[str(code)].drop(labels=[str(code)], errors="ignore").sort_values(key=lambda s: s.abs(), ascending=False)
    # ì½”ë“œâ†’ì´ë¦„ ë§µ
    name_map = (df.drop_duplicates('ê³„ì •ì½”ë“œ').assign(ê³„ì •ì½”ë“œ=lambda d: d['ê³„ì •ì½”ë“œ'].astype(str))
                   .set_index('ê³„ì •ì½”ë“œ')['ê³„ì •ëª…'].astype(str).to_dict())
    tbl = pd.DataFrame({
        "ìƒëŒ€ê³„ì •ì½”ë“œ": rvec.index.astype(str),
        "ìƒëŒ€ê³„ì •ëª…": [name_map.get(c, c) for c in rvec.index.astype(str)],
        "ìƒê´€ê³„ìˆ˜": rvec.values
    })
    # ì‹œê°í™”(ë°” ì°¨íŠ¸)
    fig = px.bar(tbl.head(30), x="ìƒëŒ€ê³„ì •ëª…", y="ìƒê´€ê³„ìˆ˜", title=f"í¬ì»¤ìŠ¤: {name_map.get(str(code), str(code))} vs íƒ€ ê³„ì •")
    fig.update_yaxes(range=[-1,1])
    # evidence (ì„ê³„ ì´ìƒ Top-N)
    evid = []
    abs_sum = piv_f.abs().sum(axis=1).astype(float)
    for _, r in tbl.iterrows():
        v = float(r["ìƒê´€ê³„ìˆ˜"])
        if abs(v) < float(corr_threshold): break
        c2 = str(r["ìƒëŒ€ê³„ì •ì½”ë“œ"])
        fin = float(min(abs_sum.get(str(code),0.0), abs_sum.get(c2,0.0)))
        same_cyc = None
        if cyc_map_norm:
            same_cyc = str(cyc_map_norm.get(str(code),"")) == str(cyc_map_norm.get(c2,""))
            if within_same_cycle and not same_cyc: 
                continue
        evid.append(EvidenceDetail(
            row_id=f"{code}|{c2}",
            reason=f"focus_corr={v:+.2f}" + (f" Â· same_cycle={bool(same_cyc)}" if same_cyc is not None else ""),
            anomaly_score=abs(v),
            financial_impact=fin,
            risk_score=abs(v),
            is_key_item=False,
            impacted_assertions=[],
            links={"focus_code": str(code), "other_code": c2, "focus_name": name_map.get(str(code), str(code)), "other_name": r["ìƒëŒ€ê³„ì •ëª…"], "corr": v}
        ))
    summ = {"focus_code": str(code), "n_candidates": int(len(tbl))}
    return ModuleResult("correlation_focus", summ, {"focus_corr": tbl, "excluded_accounts": excluded}, {"bar": fig}, evid, [])



==============================
ğŸ“„ FILE: analysis/corr_advanced.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import List, Dict
from analysis.contracts import ModuleResult, EvidenceDetail, LedgerFrame
from config import CORR_THRESHOLD_DEFAULT, CORR_MAX_LAG_DEFAULT, CORR_ROLLWIN_DEFAULT
import plotly.express as px


def _pivot_monthly_flow(lf: LedgerFrame, accounts: List[str]) -> pd.DataFrame:
    """
    ì…ë ¥ accounts ê°€ 'ê³„ì •ì½”ë“œ' ë˜ëŠ” 'ê³„ì •ëª…' ì–´ëŠ ìª½ì´ë“  ë™ì‘í•˜ë„ë¡ ë°©ì–´.
    (UIì—ì„œ ê³„ì •ëª…ì„ ì „ë‹¬í–ˆì„ ë•Œ ë¹ˆ í”¼ë²—ì´ ë˜ë˜ ë¬¸ì œ ìˆ˜ì •)
    """
    df = lf.df.copy()
    accs = {str(a) for a in (accounts or [])}
    if not accs:
        return pd.DataFrame()
    code_mask = df["ê³„ì •ì½”ë“œ"].astype(str).isin(accs)
    name_mask = df["ê³„ì •ëª…"].astype(str).isin(accs) if "ê³„ì •ëª…" in df.columns else False
    df = df[code_mask | name_mask].copy()
    if df.empty:
        return pd.DataFrame()
    df["ì›”"] = pd.to_datetime(df["íšŒê³„ì¼ì"], errors="coerce").dt.to_period("M").astype(str)
    # ì›” ê¸°ì¤€ ë°œìƒì•¡(ì ˆëŒ€ê°’) í•©ê³„ í”¼ë²—
    pivot = df.pivot_table(index="ì›”", columns="ê³„ì •ëª…", values="ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’", aggfunc="sum").fillna(0.0)
    return pivot.sort_index()


def _corr_with_lag(a: pd.Series, b: pd.Series, lag: int) -> float:
    if lag > 0:
        return a.iloc[lag:].corr(b.iloc[:-lag])
    elif lag < 0:
        return a.iloc[:lag].corr(b.iloc[-lag:])
    else:
        return a.corr(b)


def _best_lag_pair(pivot: pd.DataFrame, max_lag: int) -> List[Dict[str, object]]:
    cols = list(pivot.columns)
    out: List[Dict[str, object]] = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            s1, s2 = pivot[cols[i]], pivot[cols[j]]
            best_lag, best_val = 0, np.nan
            for lag in range(-max_lag, max_lag + 1):
                v = _corr_with_lag(s1, s2, lag)
                if not np.isnan(v) and (np.isnan(best_val) or abs(v) > abs(best_val)):
                    best_lag, best_val = lag, v
            if not np.isnan(best_val):
                out.append({"ê³„ì •A": cols[i], "ê³„ì •B": cols[j], "ìµœì ì‹œì°¨": best_lag, "ìƒê´€ê³„ìˆ˜": best_val})
    out.sort(key=lambda x: abs(x["ìƒê´€ê³„ìˆ˜"]), reverse=True)
    return out


def _rolling_stability(pivot: pd.DataFrame, window: int = 6) -> List[Dict[str, object]]:
    cols = list(pivot.columns)
    out: List[Dict[str, object]] = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            r = pivot[cols[i]].rolling(window).corr(pivot[cols[j]])
            if len(r.dropna()) == 0:
                continue
            vol = float(r.std(skipna=True))
            mean = float(r.mean(skipna=True))
            out.append({"ê³„ì •A": cols[i], "ê³„ì •B": cols[j], "ë¡¤ë§í‰ê· ": mean, "ë¡¤ë§ë³€ë™ì„±": vol})
    out.sort(key=lambda x: x["ë¡¤ë§ë³€ë™ì„±"])  # ë‚®ì€ ë³€ë™ì„± ìš°ì„ 
    return out


def run_corr_advanced(
    lf: LedgerFrame,
    accounts: List[str],
    *,
    method: str = "pearson",
    corr_threshold: float = CORR_THRESHOLD_DEFAULT,
    max_lag: int = CORR_MAX_LAG_DEFAULT,
    rolling_window: int = CORR_ROLLWIN_DEFAULT,
    cycles_map=None,
    within_same_cycle: bool=False,
) -> ModuleResult:
    name = "corr_advanced"
    if lf is None or getattr(lf, "df", None) is None:
        return ModuleResult(name=name, summary={}, tables={}, figures={}, evidences=[], warnings=["LedgerFrame ì—†ìŒ"])
    if not accounts:
        return ModuleResult(name=name, summary={"n_accounts": 0}, tables={}, figures={}, evidences=[], warnings=["ì„ íƒ ê³„ì • ì—†ìŒ"])

    pivot = _pivot_monthly_flow(lf, accounts)
    if pivot.empty or len(pivot.columns) < 2:
        return ModuleResult(name=name, summary={"n_accounts": len(accounts)}, tables={}, figures={}, evidences=[], warnings=["ë°ì´í„° ë¶€ì¡±"])

    corr = pivot.corr(method=method).replace([np.inf, -np.inf], np.nan).fillna(0.0)

    # íˆíŠ¸ë§µ (ê³„ì •ëª…ìœ¼ë¡œ)
    fig_heat = px.imshow(
        corr,
        text_auto=False,
        color_continuous_scale="Blues",
        labels=dict(color="ìƒê´€ê³„ìˆ˜"),
        x=corr.columns,
        y=corr.index,
        title="ê³„ì • ê°„ ì›”ë³„ ìƒê´€ íˆíŠ¸ë§µ",
    )

    # ì„ê³„ì¹˜ ì´ìƒ ìŒ
    strong: List[Dict[str, object]] = []
    cols = list(corr.columns)
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            v = float(corr.iloc[i, j])
            if abs(v) >= float(corr_threshold):
                strong.append({"ê³„ì •A": cols[i], "ê³„ì •B": cols[j], "ìƒê´€ê³„ìˆ˜": v})
    # ë™ì¼ ì‚¬ì´í´ í•„í„°(ì„ íƒ)
    if within_same_cycle and cycles_map and "ê³„ì •ëª…" in lf.df.columns:
        # ì´ë¦„->ì½”ë“œ ì—­ë§¤í•‘
        nm2cd = (lf.df.drop_duplicates("ê³„ì •ëª…")
                    .assign(ê³„ì •ì½”ë“œ=lambda d: d["ê³„ì •ì½”ë“œ"].astype(str))
                    .set_index("ê³„ì •ëª…")["ê³„ì •ì½”ë“œ"].astype(str).to_dict())
        def _same(a,b):
            ca, cb = nm2cd.get(a), nm2cd.get(b)
            return (cycles_map.get(str(ca)) == cycles_map.get(str(cb))) if (ca and cb and isinstance(cycles_map, dict)) else True
        strong = [r for r in strong if _same(r["ê³„ì •A"], r["ê³„ì •B"])]
    strong_df = pd.DataFrame(strong)

    # ìµœì  ì‹œì°¨ ìƒê´€
    lag_pairs = pd.DataFrame(_best_lag_pair(pivot, int(max_lag)))

    # ë¡¤ë§ ì•ˆì •ì„±(ë‚®ì€ ë³€ë™ì„± ìš°ì„ )
    roll = pd.DataFrame(_rolling_stability(pivot, int(rolling_window)))

    # Evidence ìƒ˜í”Œ
    evid: List[EvidenceDetail] = []
    for row in strong[: min(10, len(strong))]:
        evid.append(EvidenceDetail(
            row_id=f"{row['ê³„ì •A']}|{row['ê³„ì •B']}",
            reason=f"corr={row['ìƒê´€ê³„ìˆ˜']:+.2f} (|r|â‰¥{corr_threshold})",
            risk_score=min(1.0, abs(float(row["ìƒê´€ê³„ìˆ˜"]))),
            financial_impact=0.0,
            is_key_item=False,
            impacted_assertions=[],
            links={"account_a": row["ê³„ì •A"], "account_b": row["ê³„ì •B"], "type": "corr_strong"},
        ))

    summary = {
        "n_accounts": int(len(accounts)),
        "n_pairs_over_threshold": int(len(strong)),
        "corr_threshold": float(corr_threshold),
        "max_lag": int(max_lag),
        "rolling_window": int(rolling_window),
    }

    tables = {
        "corr_matrix": corr,
        "strong_pairs": strong_df,
        "lagged_pairs": lag_pairs,
        "rolling_stability": roll,
    }
    figures = {"heatmap": fig_heat}

    return ModuleResult(name=name, summary=summary, tables=tables, figures=figures, evidences=evid, warnings=[])





==============================
ğŸ“„ FILE: analysis/embedding.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
import time
from typing import Dict, List, Optional, Tuple, Callable, Sequence, Any
# --- KDMeans ê¸°ë°˜ HDBSCAN ëŒ€ì²´ ì‚¬ìš©(ì˜ë¯¸ìƒ HDBSCANê³¼ ìœ ì‚¬ ë™ì‘) ---
from analysis.kdmeans_shim import HDBSCAN   # (ì£¼ì˜) ë‚´ë¶€ì ìœ¼ë¡œ KMeans ê¸°ë°˜ êµ¬í˜„
_HAS_HDBSCAN = True
# ---------------------------------------

from utils.helpers import find_column_by_keyword
from config import (
    EMB_MODEL_SMALL, EMB_MODEL_LARGE, EMB_USE_LARGE_DEFAULT,
    UMAP_APPLY_THRESHOLD, UMAP_N_COMPONENTS, UMAP_N_NEIGHBORS, UMAP_MIN_DIST,
    HDBSCAN_RESCUE_TAU,
)

# Embedding call defaults (can be overridden via pick_emb_model / params)
EMB_BATCH_SIZE = 128
EMB_TIMEOUT = 60
EMB_MAX_RETRY = 4
EMB_TRUNC_CHARS = 2000


def embed_texts_batched(
    texts: Sequence[str],
    *,
    embed_texts_fn: Callable[..., Any],
    client,
    model: str,
    batch_size: int = EMB_BATCH_SIZE,
    timeout: int = EMB_TIMEOUT,
    max_retry: int = EMB_MAX_RETRY,
    trunc_chars: int = EMB_TRUNC_CHARS,
) -> Dict[str, List[float]]:
    """ë°°ì¹˜ ì„ë² ë”© ìœ í‹¸. {ì›ë³¸ë¬¸ìì—´: ë²¡í„°} ë°˜í™˜.
    services ë ˆì´ì–´ì— ì§ì ‘ ì˜ì¡´í•˜ì§€ ì•Šê³ , í˜¸ì¶œìê°€ ì„ë² ë”© í•¨ìˆ˜(embed_texts_fn)ë¥¼ ì£¼ì…í•œë‹¤.
    """
    if not texts:
        return {}
    san: List[str] = []
    for t in texts:
        s = t if isinstance(t, str) else str(t)
        san.append(s[:trunc_chars] if trunc_chars and len(s) > trunc_chars else s)

    # í˜¸ì¶œìë¡œë¶€í„° ì£¼ì…ë°›ì€ í•¨ìˆ˜ ì‚¬ìš©(ì˜ˆ: services.cache.get_or_embed_texts)
    return embed_texts_fn(
        san, client=client, model=model, batch_size=batch_size, timeout=timeout, max_retry=max_retry
    )


def _clean_text_series(s: pd.Series) -> pd.Series:
    """Lightweight denoising: collapse long numbers, squeeze spaces, trim."""
    s = s.astype(str)
    s = s.str.replace(r"\d{8,}", "#NUM", regex=True)
    s = s.str.replace(r"\s+", " ", regex=True).str.strip()
    return s

def ensure_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """Ensure df['embedding_text'] exists (desc+vendor) and is cleaned."""
    if 'embedding_text' not in df.columns:
        desc = df['ì ìš”'].fillna('').astype(str) if 'ì ìš”' in df.columns else ''
        cp   = df['ê±°ë˜ì²˜'].fillna('').astype(str) if 'ê±°ë˜ì²˜' in df.columns else ''
        df['embedding_text'] = desc + " (ê±°ë˜ì²˜: " + cp + ")"
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def _amount_bucket(a: float) -> str:
    a = float(abs(a))
    if a < 1_000_000:   return "1ë°±ë§Œ ë¯¸ë§Œ"
    if a < 10_000_000:  return "1ì²œë§Œ ë¯¸ë§Œ"
    if a < 100_000_000: return "1ì–µì› ë¯¸ë§Œ"
    if a < 500_000_000: return "5ì–µì› ë¯¸ë§Œ"
    if a < 1_000_000_000:return "10ì–µì› ë¯¸ë§Œ"
    if a < 5_000_000_000:return "50ì–µì› ë¯¸ë§Œ"
    return "50ì–µì› ì´ìƒ"


def ensure_rich_embedding_text(df: pd.DataFrame) -> pd.DataFrame:
    """ì ìš”+ê±°ë˜ì²˜+ì›”+ê¸ˆì•¡êµ¬ê°„+ì°¨/ëŒ€ ì„±ê²©ì„ ì¡°í•©í•´ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±."""
    # ë°œìƒì•¡/ìˆœì•¡ì€ anomaly.compute_amount_columnsë¥¼ ì“°ë©´ ìˆœí™˜ importê°€ ìƒê¹€ â†’ ìµœì†Œ í•„ë“œë§Œ ê³„ì‚°
    def _compute_amount_cols(_df: pd.DataFrame) -> pd.DataFrame:
        dcol = find_column_by_keyword(_df.columns, 'ì°¨ë³€')
        ccol = find_column_by_keyword(_df.columns, 'ëŒ€ë³€')
        if not dcol or not ccol:
            _df['ë°œìƒì•¡'] = 0.0; _df['ìˆœì•¡'] = 0.0
            return _df
        d = pd.to_numeric(_df[dcol], errors='coerce').fillna(0.0)
        c = pd.to_numeric(_df[ccol], errors='coerce').fillna(0.0)
        row_amt = np.where((d > 0) & (c == 0), d,
                  np.where((c > 0) & (d == 0), c,
                  np.where((d == 0) & (c == 0), 0.0, np.abs(d - c))))
        _df['ë°œìƒì•¡'] = row_amt
        _df['ìˆœì•¡']  = d - c
        return _df

    df = _compute_amount_cols(df.copy())
    month = df['íšŒê³„ì¼ì'].dt.month.fillna(0).astype(int).astype(str).str.zfill(2) if 'íšŒê³„ì¼ì' in df.columns else "00"
    amtbin = df['ë°œìƒì•¡'].apply(_amount_bucket)
    sign   = np.where(df['ìˆœì•¡'] >= 0, "ì°¨ë³€ì„±", "ëŒ€ë³€ì„±")
    desc = df['ì ìš”'].fillna('').astype(str) if 'ì ìš”' in df.columns else ''
    cp   = df['ê±°ë˜ì²˜'].fillna('').astype(str) if 'ê±°ë˜ì²˜' in df.columns else ''
    df['embedding_text'] = desc + " | ê±°ë˜ì²˜:" + cp + " | ì›”:" + month + " | ê¸ˆì•¡êµ¬ê°„:" + amtbin + " | ì„±ê²©:" + sign
    df['embedding_text'] = _clean_text_series(df['embedding_text'])
    return df


def perform_embedding_only(
    df: pd.DataFrame,
    client,
    text_col: str = 'embedding_text',
    *,
    use_large: bool|None=None,
    embed_texts_fn: Callable[..., Any],
) -> pd.DataFrame:
    """df[text_col]ì„ ë°°ì¹˜ ì„ë² ë”©í•´ì„œ df['vector'] ì¶”ê°€"""
    if df.empty: return df
    if text_col not in df.columns:
        raise ValueError(f"ì„ë² ë”© í…ìŠ¤íŠ¸ ì»¬ëŸ¼ '{text_col}'ì´ ì—†ìŠµë‹ˆë‹¤.")
    uniq = df[text_col].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(
        uniq,
        embed_texts_fn=embed_texts_fn,
        client=client,
        model=model,
    )
    df = df.copy()
    df['vector'] = df[text_col].astype(str).map(mapping)
    # Guard embedding failures
    if df is None or df.empty:
        return df
    # ëˆ„ë½ ë³´ê°• ì‹œë„
    if df['vector'].isna().any():
        miss = df.loc[df['vector'].isna(), text_col].astype(str).unique().tolist()
        if miss:
            fb = embed_texts_batched(
                miss,
                embed_texts_fn=embed_texts_fn,
                client=client,
                model=model,
            )
            df.loc[df['vector'].isna(), 'vector'] = df.loc[df['vector'].isna(), text_col].astype(str).map(fb)
    return df


def _l2_normalize(X: np.ndarray) -> np.ndarray:
    return X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)

def _adaptive_hdbscan(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    n = int(X.shape[0])
    model = HDBSCAN(
        n_clusters=None,           # ìë™ k ì„ íƒ(ì‹¤ë£¨ì—£ ê¸°ë°˜)
        min_cluster_size=max(8, int(np.sqrt(max(2, n)))),  # ë„ˆë¬´ ë§ì€ êµ°ì§‘ ë°©ì§€
        max_k=None,                # í•„ìš”ì‹œ ìƒí•œ ì§€ì • ê°€ëŠ¥
        k_search="silhouette",     # íœ´ë¦¬ìŠ¤í‹± ëŒ€ì‹  ì‹¤ë£¨ì—£ ê¸°ë°˜
        sample_size=2000,
        random_state=42,
        n_init="auto",
    )
    model.fit(X)
    labels = model.labels_.astype(int)
    try:
        probs = model.probabilities_.astype(float)
    except Exception:
        probs = np.ones(shape=(n,), dtype=float)
    return labels, probs

def _optional_umap(X: np.ndarray, enabled: Optional[bool] = None) -> Tuple[np.ndarray, bool]:
    """Dimensionality reduction control.
    - enabled=True: always try UMAP; on failure return (X, False)
    - enabled=False: skip â†’ (X, False)
    - enabled=None: apply only if dataset size >= UMAP_APPLY_THRESHOLD
    Returns (X_or_reduced, used_flag).
    """
    if enabled is False:
        return X, False
    force = enabled is True
    try:
        thr = int(UMAP_APPLY_THRESHOLD) if UMAP_APPLY_THRESHOLD else None
    except Exception:
        thr = None
    if force or (thr and X.shape[0] >= thr):
        try:
            import umap
            reducer = umap.UMAP(
                n_components=int(UMAP_N_COMPONENTS),
                n_neighbors=int(UMAP_N_NEIGHBORS),
                min_dist=float(UMAP_MIN_DIST),
                random_state=42,
                metric="euclidean",
            )
            return reducer.fit_transform(X), True
        except Exception:
            return X, False
    return X, False

def _rescue_noise(df: pd.DataFrame, tau: float = HDBSCAN_RESCUE_TAU) -> pd.DataFrame:
    # KDMeansëŠ” ë…¸ì´ì¦ˆ(-1) ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ êµ¬ì¡°ì  ë¦¬ìŠ¤í ë¶ˆí•„ìš”
    return df

def pick_emb_model(use_large: bool|None=None) -> str:
    """Select embedding model (small/large)."""
    flag = EMB_USE_LARGE_DEFAULT if use_large is None else bool(use_large)
    return EMB_MODEL_LARGE if flag else EMB_MODEL_SMALL


def postprocess_cluster_names(df: pd.DataFrame) -> pd.DataFrame:
    """(ê°„ì†Œí™”) LLMì´ ì¤€ cluster_nameì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤. íƒœê·¸/ì ‘ë¯¸ì‚¬ ë¯¸ë¶€ì—¬."""
    return df


def perform_embedding_and_clustering(
    df: pd.DataFrame,
    client,
    *,
    name_with_llm: bool = True,
    must_name_with_llm: bool = False,
    naming_fn: Optional[Callable[[list[str], list[str]], Optional[str]]] = None,
    use_large: bool|None = None,
    rescue_tau: float = HDBSCAN_RESCUE_TAU,
    umap_enabled: bool|None = None,   # None => use config threshold
    embed_texts_fn: Callable[..., Any],
):
    """
    Embedding + (optional UMAP) + L2-normalized Euclidean HDBSCAN + noise rescue + (LLM naming).
    Returns: (df, ok)
    ok=False if: no vectors, or LLM naming required but missing/failed.
    """
    df = ensure_embedding_text(df.copy())
    uniq = df['embedding_text'].astype(str).unique().tolist()
    model = pick_emb_model(use_large=use_large)
    mapping = embed_texts_batched(
        uniq,
        embed_texts_fn=embed_texts_fn,
        client=client,
        model=model,
    )
    df['vector'] = df['embedding_text'].astype(str).map(mapping)
    # Guard: embedding may fail and return None vectors
    if df is None or df.empty:
        return None, False
    # keep only valid vectors
    mask = df['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)
    df = df.loc[mask].copy()
    if df.empty:
        return None, False

    X = np.vstack(df['vector'].values).astype(float)
    # Optional UMAP if dataset large (threshold controlled by config)
    X, umap_used = _optional_umap(X, enabled=umap_enabled)
    # L2 normalize and cluster with Euclidean (â‰ˆ cosine)
    Xn = _l2_normalize(X)
    labels, probs = _adaptive_hdbscan(Xn)
    df['cluster_id'] = labels
    df['cluster_prob'] = probs
    # telemetry attrs
    try:
        df.attrs['embedding_model'] = model
        df.attrs['umap_used'] = bool(umap_used)
        df.attrs['rescue_tau'] = float(rescue_tau) if rescue_tau is not None else None
    except Exception:
        pass

    # --- Cluster naming via injected LLM callback (with graceful fallback) ---
    labels_uniq = sorted(pd.Series(labels).unique())
    names = {}
    if name_with_llm and (naming_fn is not None):
        for cid in labels_uniq:
            if cid == -1:
                names[cid] = "í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)"
                continue
            sub = df[df['cluster_id'] == cid]
            descs = sub['ì ìš”'].dropna().astype(str).unique().tolist()[:5] if 'ì ìš”' in sub.columns else []
            vendors = sub['ê±°ë˜ì²˜'].dropna().astype(str).unique().tolist()[:5] if 'ê±°ë˜ì²˜' in sub.columns else []
            # ì½œë°± ì‚¬ìš©(services.cluster_namingì—ì„œ ìƒì„±)
            try:
                cand = naming_fn(descs, vendors)
            except Exception:
                cand = None
            # fallback rule-based name if LLM failed
            if not cand or cand == "ì´ë¦„ ìƒì„± ì‹¤íŒ¨":
                # heuristic: frequent vendor or keyword + amount tag
                amt_tag = "ê·œëª¨ ì¤‘ê°„"
                try:
                    abs_amt = sub.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs().median()
                    if float(abs_amt) >= 1e8: amt_tag = "1ì–µì› ì´ìƒ"
                    elif float(abs_amt) >= 1e7: amt_tag = "1ì²œë§Œ~1ì–µ"
                except Exception:
                    pass
                top_vendor = sub.get('ê±°ë˜ì²˜', pd.Series(dtype=str)).value_counts().index.tolist()
                vname = top_vendor[0] if top_vendor else "ì¼ë°˜"
                cand = f"{vname} ì¤‘ì‹¬({amt_tag})"
            names[cid] = cand
    else:
        for cid in labels_uniq:
            names[cid] = "í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)" if cid == -1 else "ì´ë¦„ ìƒì„± ì‹¤íŒ¨"

    df['cluster_name'] = df['cluster_id'].map(names)
    df = postprocess_cluster_names(df)

    # --- Noise rescue: reassign -1 to nearest centroid if cosine >= tau ---
    if rescue_tau and float(rescue_tau) > 0:
        df = _rescue_noise(df, tau=float(rescue_tau))

    # gate: if must_name_with_llm, all non-noise clusters must have valid names
    if must_name_with_llm:
        non_noise = df[df['cluster_id'] != -1]
        has_any = not non_noise.empty
        invalid = non_noise['cluster_name'].isna() | non_noise['cluster_name'].astype(str).str.contains("^ì´ë¦„ ìƒì„± ì‹¤íŒ¨|^í´ëŸ¬ìŠ¤í„°\\s", regex=True)
        if (not has_any) or bool(invalid.any()):
            return df, False

    # default reporting group equals the (validated) cluster_name; may be unified later
    df['cluster_group'] = df['cluster_name']
    return df, True



# --- NEW: LLM synonym grouping for cluster names ---
def _cosine_sim_matrix(vecs: list[list[float]]):
    import numpy as np
    V = np.asarray(vecs, dtype=float)
    if V.ndim != 2 or V.shape[0] == 0:
        return np.zeros((0, 0))
    Vn = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    return Vn @ Vn.T


def unify_cluster_names_with_llm(
    df: pd.DataFrame,
    sim_threshold: float = 0.90,
    emb_model: str = EMB_MODEL_SMALL,
    *,
    embed_texts_fn: Callable[..., Any],
    confirm_pair_fn: Optional[Callable[[str, str], bool]] = None,
):
    """
    Collapse clusters with effectively identical names.
    Strategy:
      1) Embed unique names (excluding noise), preselect candidate pairs via cosine >= sim_threshold.
      2) Ask LLM YES/NO if two names are synonyms for accounting transaction categories.
      3) Union-Find merge; choose canonical = most frequent name in df (fallback shortest).
    Returns: (df_with_cluster_group, mapping{name->canonical})
    """
    import numpy as np
    import itertools
    base = df.copy()
    if 'cluster_name' not in base.columns:
        base['cluster_group'] = base.get('cluster_name', None)
        return base, {}
    names = (
        base.loc[base['cluster_id'] != -1, 'cluster_name']
        .dropna().astype(str).unique().tolist()
    )
    if not names:
        base['cluster_group'] = base['cluster_name']
        return base, {}

    # Embedding prefilter
    name2vec = embed_texts_batched(
        names,
        embed_texts_fn=embed_texts_fn,
        client=None,
        model=emb_model,
    )
    ordered = [n for n in names if n in name2vec]
    vecs = [name2vec[n] for n in ordered]
    S = _cosine_sim_matrix(vecs)

    # Union-Find
    parent = {n: n for n in ordered}
    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x
    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    # Pair confirmation (LLM or ë‹¤ë¥¸ ì •ì±…) â€” ë°˜ë“œì‹œ ì£¼ì… ë°›ì€ confirm_pair_fnì„ ì‚¬ìš©
    for i, j in itertools.combinations(range(len(ordered)), 2):
        if S[i, j] < float(sim_threshold):
            continue
        a, b = ordered[i], ordered[j]
        if confirm_pair_fn is None:
            # ì½œë°±ì´ ì—†ìœ¼ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ merge ìƒëµ(ì•„í‚¤í…ì²˜ ì¤€ìˆ˜)
            continue
        try:
            if confirm_pair_fn(a, b):
                union(a, b)
        except Exception:
            continue

    # Build groups
    groups = {}
    for n in ordered:
        r = find(n)
        groups.setdefault(r, []).append(n)

    # Choose canonical per group
    freq = base['cluster_name'].value_counts().to_dict()
    mapping = {}
    for root, members in groups.items():
        cand = sorted(members, key=lambda x: (-freq.get(x, 0), len(x)))[0]
        for m in members:
            mapping[m] = cand

    base['cluster_group'] = base['cluster_name'].map(lambda x: mapping.get(x, x))
    return base, mapping


# --- NEW: Utilities for PYâ†’CY mapping and label unification ---
def _cosine(a, b):
    import numpy as np
    if a is None or b is None:
        return np.nan
    a = np.asarray(a)
    b = np.asarray(b)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    return float(np.dot(a, b) / denom) if denom else np.nan


def map_previous_to_current_clusters(df_cur: pd.DataFrame, df_prev: pd.DataFrame) -> pd.DataFrame:
    """
    ì „ê¸° ì „í‘œë¥¼ ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ì„¼íŠ¸ë¡œì´ë“œì— ìµœê·¼ì ‘ ë°°ì •í•˜ì—¬ (mapped_cluster_id/name, mapped_sim) ë¶€ì—¬.
    - ë…¸ì´ì¦ˆ(-1) ì„¼íŠ¸ë¡œì´ë“œëŠ” ì œì™¸
    - ë°˜í™˜: prev_df(with mapped_cluster_id, mapped_cluster_name, mapped_sim)
    """
    import numpy as np
    import pandas as pd
    need_cols = ['cluster_id', 'cluster_name', 'vector']
    if any(c not in df_cur.columns for c in need_cols) or 'vector' not in df_prev.columns:
        return df_prev.copy()
    cur = df_cur[df_cur['cluster_id'] != -1].copy()
    if cur.empty:
        return df_prev.copy()
    # ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°
    cents = (
        cur.groupby(['cluster_id', 'cluster_name'])['vector']
           .apply(lambda s: np.mean(np.vstack(list(s)), axis=0))
           .reset_index()
    )
    prev = df_prev.copy()

    def _pick(row: pd.Series) -> pd.Series:
        v = row.get('vector', None)
        if v is None:
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        sims = cents['vector'].apply(lambda c: _cosine(v, c))
        if len(sims) == 0 or sims.isna().all():
            return pd.Series({'mapped_cluster_id': np.nan, 'mapped_cluster_name': None, 'mapped_sim': np.nan})
        idx = int(sims.idxmax())
        return pd.Series({
            'mapped_cluster_id': int(cents.loc[idx, 'cluster_id']),
            'mapped_cluster_name': cents.loc[idx, 'cluster_name'],
            'mapped_sim': float(sims.max()) if not np.isnan(sims.max()) else np.nan,
        })

    prev[['mapped_cluster_id', 'mapped_cluster_name', 'mapped_sim']] = prev.apply(_pick, axis=1)
    return prev


def unify_cluster_labels_llm(*_args, **_kwargs) -> dict:
    """Deprecated in analysis layer. Use services.cluster_naming.unify_cluster_labels_llm instead."""
    return {}


# --- NEW: Yearly clustering helpers and alignment ---
def cluster_year(df: pd.DataFrame, client, *, embed_texts_fn: Callable[..., Any]) -> pd.DataFrame:
    """
    ë‹¹ê¸°/ì „ê¸° ë“± ì…ë ¥ dfì— ëŒ€í•´ í’ë¶€ ì„ë² ë”© í…ìŠ¤íŠ¸ë¥¼ ë³´ì¥í•˜ê³  HDBSCAN+LLM ë„¤ì´ë°ì„ ì‹¤í–‰.
    ë°˜í™˜: ['row_id','cluster_id','cluster_name','cluster_prob','vector']ê°€ í¬í•¨ëœ DataFrame(ë¶€ë¶„ì§‘í•© ê°€ëŠ¥).
    ì…ë ¥ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜.
    """
    if df is None or df.empty:
        return pd.DataFrame()
    from .embedding import ensure_rich_embedding_text, perform_embedding_and_clustering
    df_in = ensure_rich_embedding_text(df.copy())
    df_out, ok = perform_embedding_and_clustering(
        df_in,
        client,
        name_with_llm=True,
        must_name_with_llm=False,
        embed_texts_fn=embed_texts_fn,
    )
    if not ok or df_out is None:
        return pd.DataFrame()
    keep = [c for c in ['row_id','cluster_id','cluster_name','cluster_prob','vector'] if c in df_out.columns]
    return df_out[keep].copy()


def compute_centroids(df: pd.DataFrame) -> pd.DataFrame:
    """
    ['cluster_id','vector']ë¥¼ ê°–ëŠ” dfì—ì„œ í´ëŸ¬ìŠ¤í„°ë³„ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°(-1 ì œì™¸).
    'cluster_name'ì´ ìˆìœ¼ë©´ í•¨ê»˜ ìœ ì§€.
    ë°˜í™˜: columns=['cluster_id','cluster_name','vector']
    """
    import numpy as np
    import pandas as pd
    need = ['cluster_id','vector']
    if df is None or df.empty or any(c not in df.columns for c in need):
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    base = df[df['cluster_id'] != -1].copy()
    if base.empty:
        return pd.DataFrame(columns=['cluster_id','cluster_name','vector'])
    def _mean_stack(s):
        try:
            return np.mean(np.vstack(list(s)), axis=0)
        except Exception:
            return None
    cents = base.groupby('cluster_id')['vector'].apply(_mean_stack).reset_index()
    if 'cluster_name' in base.columns:
        name_map = base.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name']
        cents['cluster_name'] = cents['cluster_id'].map(name_map)
    else:
        cents['cluster_name'] = None
    # re-order columns
    cents = cents[['cluster_id','cluster_name','vector']]
    # drop rows with invalid vectors
    cents = cents[cents['vector'].apply(lambda v: isinstance(v, (list, tuple)) and len(v) > 0)]
    return cents.reset_index(drop=True)


def align_yearly_clusters(df_cy: pd.DataFrame, df_py: pd.DataFrame, sim_threshold: float = 0.70) -> dict:
    """
    CY/PY ì„¼íŠ¸ë¡œì´ë“œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬ ê¸°ë°˜ Hungarian ë§¤ì¹­(cost=1-sim).
    ë°˜í™˜: {py_cluster_id: (cy_cluster_id, sim)} (ì„ê³„ì¹˜ ë¯¸ë§Œì€ ê°’ None)
    """
    import numpy as np
    py_c = compute_centroids(df_py)
    cy_c = compute_centroids(df_cy)
    if py_c.empty or cy_c.empty:
        return {}
    # build similarity matrix
    py_vecs = list(py_c['vector'].values)
    cy_vecs = list(cy_c['vector'].values)
    S_py = _cosine_sim_matrix(py_vecs)
    S_cy = _cosine_sim_matrix(cy_vecs)
    # We need PY x CY sims; compute directly
    # Efficient: normalize and dot
    import numpy as np
    def _norm(V):
        V = np.asarray([np.asarray(v, dtype=float) for v in V], dtype=float)
        return V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)
    Npy = _norm(py_vecs)
    Ncy = _norm(cy_vecs)
    sim = Npy @ Ncy.T  # shape: [n_py, n_cy]
    # Hungarian matching on cost = 1 - sim
    try:
        from scipy.optimize import linear_sum_assignment
        cost = 1.0 - sim
        row_ind, col_ind = linear_sum_assignment(cost)
    except Exception:
        # Fallback: greedy matching by highest sim without replacement
        pairs = []
        used_py = set(); used_cy = set()
        # flatten and sort
        flat = [
            (i, j, float(sim[i, j]))
            for i in range(sim.shape[0])
            for j in range(sim.shape[1])
        ]
        flat.sort(key=lambda x: x[2], reverse=True)
        for i, j, s in flat:
            if i in used_py or j in used_cy:
                continue
            pairs.append((i, j))
            used_py.add(i); used_cy.add(j)
        row_ind = np.array([p[0] for p in pairs], dtype=int)
        col_ind = np.array([p[1] for p in pairs], dtype=int)
    mapping: dict[int, tuple[int, float] | None] = {}
    for k in range(len(row_ind)):
        i = int(row_ind[k]); j = int(col_ind[k])
        s = float(sim[i, j])
        py_id = int(py_c.loc[i, 'cluster_id'])
        cy_id = int(cy_c.loc[j, 'cluster_id'])
        if s >= float(sim_threshold):
            mapping[py_id] = (cy_id, s)
        else:
            mapping[py_id] = None
    # Ensure all PY clusters are present in mapping
    for py_id in py_c['cluster_id'].tolist():
        if py_id not in mapping:
            mapping[py_id] = None
    return mapping




==============================
ğŸ“„ FILE: analysis/evidence.py
==============================

from __future__ import annotations
import numpy as np
import pandas as pd
from typing import Optional


def build_knn_index(prev_df: pd.DataFrame):
    """prev_df['vector']ë¡œ KNN ìƒì„±."""
    if 'vector' not in prev_df.columns or prev_df['vector'].isna().any():
        raise ValueError("build_knn_index: prev_dfì— 'vector' í•„ìš”")
    from sklearn.neighbors import NearestNeighbors
    X = np.vstack(prev_df['vector'].values)
    knn = NearestNeighbors(metric='cosine', n_neighbors=min(10, len(X))).fit(X)
    return knn, X


def cluster_centroid_vector(cluster_df: pd.DataFrame):
    if 'vector' not in cluster_df.columns or cluster_df.empty:
        return None
    return np.mean(np.vstack(cluster_df['vector'].values), axis=0)


def retrieve_similar_from_previous(prev_df, prev_knn, prev_X, query_vec, topk=5, dedup_by_vendor=True, min_sim=0.7):
    if query_vec is None or prev_X is None or len(prev_X) == 0:
        return pd.DataFrame()
    dist, idx = prev_knn.kneighbors([query_vec], n_neighbors=min(max(10, topk*3), len(prev_X)))
    cands = prev_df.iloc[idx[0]].copy()
    cands['similarity'] = (1 - dist[0])
    cands = cands[cands['similarity'] >= min_sim]
    if dedup_by_vendor and 'ê±°ë˜ì²˜' in cands.columns:
        cands = cands.sort_values('similarity', ascending=False).drop_duplicates('ê±°ë˜ì²˜', keep='first')
    cands = cands.sort_values('similarity', ascending=False).head(topk)
    cols = ['íšŒê³„ì¼ì','ê³„ì •ì½”ë“œ','ê±°ë˜ì²˜','ì ìš”','ë°œìƒì•¡','similarity']
    for c in cols:
        if c not in cands.columns: cands[c] = np.nan
    return cands[cols]


def build_cluster_evidence_block(current_df: pd.DataFrame, previous_df: pd.DataFrame,
                                 topk: int = 3, restrict_same_months: bool = True, min_sim: float = 0.7,
                                 dedup_by_vendor: bool = True) -> str:
    if any(col not in current_df.columns for col in ['cluster_id','vector']):
        return "\n\n## ê·¼ê±° ì¸ìš©(ì „ê¸° ìœ ì‚¬ ê±°ë˜)\n- í˜„ì¬ ë°ì´í„°ì— í´ëŸ¬ìŠ¤í„°/ë²¡í„°ê°€ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## ê·¼ê±° ì¸ìš©(ì „ê¸° ìœ ì‚¬ ê±°ë˜)\n- ì „ê¸° ë°ì´í„° ì„ë² ë”©ì´ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    def _ok_vec(v):
        return v is not None and isinstance(v, (list, tuple, np.ndarray)) and len(v) > 0
    lines = ["\n\n## ê·¼ê±° ì¸ìš©(ì „ê¸° ìœ ì‚¬ ê±°ë˜)"]
    for cid in sorted(current_df['cluster_id'].unique()):
        cur_c = current_df[current_df['cluster_id'] == cid]
        if cur_c.empty: continue
        cname = cur_c['cluster_name'].iloc[0] if 'cluster_name' in cur_c.columns else str(cid)
        lines.append(f"[í´ëŸ¬ìŠ¤í„° #{cid} | {cname}]")
        prev_subset = previous_df.copy()
        if restrict_same_months and 'íšŒê³„ì¼ì' in cur_c.columns and cur_c['íšŒê³„ì¼ì'].notna().any():
            months = set(cur_c['íšŒê³„ì¼ì'].dt.month.dropna().unique().tolist())
            filtered = previous_df[previous_df['íšŒê³„ì¼ì'].dt.month.isin(months)]
            prev_subset = filtered if not filtered.empty else previous_df
        if 'vector' in prev_subset.columns:
            prev_subset = prev_subset[prev_subset['vector'].apply(_ok_vec)].copy()
        if prev_subset.empty:
            lines.append("    â”” ì „ê¸° ìœ ì‚¬ ë²¡í„° ì—†ìŒ"); continue
        try:
            knn, X = build_knn_index(prev_subset)
        except Exception as e:
            lines.append(f"    â”” ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}"); continue
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_subset, knn, X, qv, topk=topk, dedup_by_vendor=dedup_by_vendor, min_sim=min_sim)
        if ev.empty:
            lines.append("    â”” ìœ ì‚¬ ì „í‘œ: ì—†ìŒ")
        else:
            def _fmt_date(x): 
                try: return x.strftime('%Y-%m-%d') if pd.notna(x) else ""
                except: return ""
            def _fmt_money(x):
                try: return f"{int(x):,}ì›"
                except: return str(x)
            def _fmt_sim(s):
                try: return f"{float(s):.2f}"
                except: return "N/A"
            for rank, (_, r) in enumerate(ev.sort_values('similarity', ascending=False).iterrows(), 1):
                lines.append(f"    {rank}) {_fmt_date(r['íšŒê³„ì¼ì'])} | {str(r['ê±°ë˜ì²˜'])} | {_fmt_money(r['ë°œìƒì•¡'])} | sim {_fmt_sim(r['similarity'])}")
    return "\n".join(lines)



def build_transaction_evidence_block(current_df, previous_df, topn=10, per_tx_topk=3, min_sim=0.8):
    import numpy as np, pandas as pd
    def _ok_vec(v): return isinstance(v, (list, tuple, np.ndarray)) and len(v)>0
    if current_df.empty or 'vector' not in current_df.columns: 
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- í˜„ì¬ ë°ì´í„°ì— ë²¡í„°ê°€ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    if previous_df.empty or 'vector' not in previous_df.columns:
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- ì „ê¸° ë°ì´í„° ì„ë² ë”©ì´ ì—†ì–´ ê·¼ê±°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    cur = current_df.copy()
    if 'Z-Score' in cur.columns and cur['Z-Score'].notna().any():
        order_idx = cur['Z-Score'].abs().sort_values(ascending=False).index
    else:
        # Z-Score ë¯¸ì‹œí–‰ ì‹œ ë°œìƒì•¡ ìƒìœ„
        amt = cur.get('ë°œìƒì•¡', pd.Series(dtype=float))
        order_idx = amt.sort_values(ascending=False).index
    cur = cur.reindex(order_idx).head(int(topn))

    # ì „ê¸° ë²¡í„° ìœ íš¨ì„± í•„í„°
    prev = previous_df.copy()
    prev = prev[prev['vector'].apply(_ok_vec)]
    if prev.empty:
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- ì „ê¸° ë°ì´í„° ë²¡í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    from .evidence import build_knn_index, retrieve_similar_from_previous
    try:
        knn, X = build_knn_index(prev)
    except Exception:
        return "\n\n## ê±°ë˜ë³„ ê·¼ê±°\n- ì „ê¸° KNN ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨."

    lines = [f"\n\n## ê±°ë˜ë³„ ê·¼ê±° (ìƒìœ„ {len(cur)}ê±´)"]
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        if qv is None: continue
        # ë™ì›” ìš°ì„ 
        psub = prev
        if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']):
            m = r['íšŒê³„ì¼ì'].month
            cand = prev[prev['íšŒê³„ì¼ì'].dt.month == m]
            if not cand.empty: psub = cand
            knn, X = build_knn_index(psub)
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=int(per_tx_topk), dedup_by_vendor=True, min_sim=float(min_sim))
        dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
        amt = r.get('ë°œìƒì•¡', 0.0); z = r.get('Z-Score', np.nan)
        ztxt = f" | Z={z:+.2f}" if not pd.isna(z) else ""
        lines.append(f"[{i}] {dt} | ê±°ë˜ì²˜:{r.get('ê±°ë˜ì²˜','')} | ê¸ˆì•¡:{int(amt):,}ì›{ztxt}")
        if ev.empty:
            lines.append("    â”” ìœ ì‚¬ ì „í‘œ: ì—†ìŒ")
        else:
            lines.append(f"    â”” ì „ê¸° ìœ ì‚¬ Top {len(ev)}")
            for _, rr in ev.iterrows():
                d2 = rr['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(rr['íšŒê³„ì¼ì']) else ''
                lines.append(f"       â€¢ {d2} | {rr['ê±°ë˜ì²˜']} | {int(rr['ë°œìƒì•¡']):,}ì› | sim {rr['similarity']:.2f}")
    return "\n".join(lines)

# --- NEW: Structured evidence blocks for the redesigned context ---
def build_current_cluster_block(current_df: pd.DataFrame) -> str:
    """
    ## ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡
    - One bullet per cluster_group: total absolute amount, count, and ONE example voucher.
    """
    import pandas as pd
    if current_df.empty or 'cluster_group' not in current_df.columns:
        return "\n\n## ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì—†ìŒ)"
    lines = ["\n\n## ë‹¹ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡"]
    grp = current_df.copy()
    grp['abs_amt'] = grp.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs()
    for name, cdf in grp.groupby('cluster_group', dropna=False):
        tot = cdf['abs_amt'].sum()
        cnt = len(cdf)
        ex = cdf.sort_values('abs_amt', ascending=False).head(1).iloc[0]
        dt = ex['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in ex and pd.notna(ex['íšŒê³„ì¼ì']) else ''
        vend = ex.get('ê±°ë˜ì²˜', '')
        amt = int(ex.get('ë°œìƒì•¡', 0.0))
        lines.append(f"- [{name}] ê±´ìˆ˜ {cnt}ê±´, ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›")
        lines.append(f"  Â· ì˜ˆì‹œ: {dt} | {vend} | {amt:,.0f}ì›")
    return "\n".join(lines)

def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float = 0.70) -> str:
    """
    ## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡
    Project PY vouchers onto CY cluster centroids; report total abs amount, avg similarity, and ONE example.
    """
    import pandas as pd
    import numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous, cluster_centroid_vector
    if current_df.empty or previous_df.empty or 'vector' not in previous_df.columns or 'cluster_group' not in current_df.columns:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° ë°ì´í„°/ë²¡í„°/í´ëŸ¬ìŠ¤í„° ì •ë³´ ì—†ìŒ)"
    lines = ["\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡"]
    prev_ok = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)]
    if prev_ok.empty:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° ìœ íš¨ ë²¡í„° ì—†ìŒ)"
    try:
        knn, X = build_knn_index(prev_ok)
    except Exception:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° KNN ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨)"
    for name, cur_c in current_df.groupby('cluster_group', dropna=False):
        qv = cluster_centroid_vector(cur_c)
        ev = retrieve_similar_from_previous(prev_ok, knn, X, qv, topk=10, dedup_by_vendor=True, min_sim=float(min_sim))
        if ev.empty:
            lines.append(f"- [{name}] ìœ ì‚¬ ì „í‘œ ì—†ìŒ")
            continue
        ev['abs_amt'] = ev.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs()
        tot = ev['abs_amt'].sum()
        avg_sim = ev['similarity'].mean()
        ex = ev.sort_values('similarity', ascending=False).head(1).iloc[0]
        dt = ex['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(ex['íšŒê³„ì¼ì']) else ''
        lines.append(f"- [{name}] ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›, í‰ê·  ìœ ì‚¬ë„ {avg_sim:.2f}")
        lines.append(f"  Â· ì˜ˆì‹œ: {dt} | {ex['ê±°ë˜ì²˜']} | {int(ex['ë°œìƒì•¡']):,}ì› | sim {ex['similarity']:.2f}")
    return "\n".join(lines)

def build_zscore_top5_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## Z-score ê¸°ì¤€ TOP5 ì „í‘œ
    List top |Z| vouchers with one counterpart from PY (same-month preferred), no row-id.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous
    if current_df.empty or 'Z-Score' not in current_df.columns:
        return "\n\n## Z-score ê¸°ì¤€ TOP5 ì „í‘œ\n- (Z-Score ë¯¸ê³„ì‚°)"
    cur = current_df.copy()
    order = cur['Z-Score'].abs().sort_values(ascending=False).index
    cur = cur.reindex(order).head(int(topn))
    lines = [f"\n\n## Z-score ê¸°ì¤€ TOP5 ì „í‘œ"]
    if previous_df.empty or 'vector' not in previous_df.columns:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    # KNN on PY (same-month preferred)
    prev = previous_df[previous_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if prev.empty:
        for i, (_, r) in enumerate(cur.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)
    knn_all, X_all = build_knn_index(prev)
    for i, (_, r) in enumerate(cur.iterrows(), 1):
        qv = r.get('vector', None)
        dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
        head = f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}"
        if qv is None:
            lines.append(head)
            continue
        psub = prev
        if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']):
            m = r['íšŒê³„ì¼ì'].month
            cand = prev[prev['íšŒê³„ì¼ì'].dt.month == m]
            if not cand.empty:
                psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all
        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  Â· ì „ê¸° ëŒ€ì‘: ì—†ìŒ")
        else:
            rr = ev.iloc[0]
            d2 = rr['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(rr['íšŒê³„ì¼ì']) else ''
            lines.append(f"  Â· ì „ê¸° ëŒ€ì‘: {d2} | {rr['ê±°ë˜ì²˜']} | {int(rr['ë°œìƒì•¡']):,}ì› | sim {rr['similarity']:.2f}")
    return "\n".join(lines)


# --- NEW: ì „ê¸° ê¸°ì¤€ TOP5 ë¸”ë¡ ---
def build_zscore_top5_block_for_py(previous_df: pd.DataFrame, current_df: pd.DataFrame, topn: int = 5, min_sim: float = 0.70) -> str:
    """
    ## ì „ê¸° Z-score ê¸°ì¤€ TOP5 ì „í‘œ
    ì „ê¸° ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ |Z| ìƒìœ„ 5ê±´ì„ ë‚˜ì—´í•˜ê³ , ê°€ëŠ¥í•œ ê²½ìš° ë‹¹ê¸° ëŒ€ì‘ 1ê±´ì„ í•¨ê»˜ í‘œì‹œ.
    previous_dfì— Z-Scoreê°€ ìˆì–´ì•¼ í•œë‹¤.
    """
    import pandas as pd, numpy as np
    from .evidence import build_knn_index, retrieve_similar_from_previous

    if previous_df.empty or 'Z-Score' not in previous_df.columns:
        return "\n\n## ì „ê¸° Z-score ê¸°ì¤€ TOP5 ì „í‘œ\n- (ì „ê¸° Z-Score ë¯¸ê³„ì‚°)"

    prev = previous_df.copy()
    order = prev['Z-Score'].abs().sort_values(ascending=False).index
    prev = prev.reindex(order).head(int(topn))

    lines = [f"\n\n## ì „ê¸° Z-score ê¸°ì¤€ TOP5 ì „í‘œ"]

    if current_df.empty or 'vector' not in current_df.columns:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    cur_ok = current_df[current_df['vector'].apply(lambda v: isinstance(v, (list, tuple, np.ndarray)) and len(v)>0)].copy()
    if cur_ok.empty:
        for i, (_, r) in enumerate(prev.iterrows(), 1):
            dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
            lines.append(f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}")
        return "\n".join(lines)

    knn_all, X_all = build_knn_index(cur_ok)

    for i, (_, r) in enumerate(prev.iterrows(), 1):
        dt = r['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']) else ''
        head = f"- [{i}] {dt} | {r.get('ê±°ë˜ì²˜','')} | {int(r.get('ë°œìƒì•¡',0)):,.0f}ì› | Z={float(r.get('Z-Score',0)):+.2f}"

        qv = r.get('vector', None)
        if qv is None:
            lines.append(head); continue

        psub = cur_ok
        if 'íšŒê³„ì¼ì' in r and pd.notna(r['íšŒê³„ì¼ì']):
            m = r['íšŒê³„ì¼ì'].month
            cand = cur_ok[cur_ok['íšŒê³„ì¼ì'].dt.month == m]
            if not cand.empty: psub = cand
        try:
            knn, X = build_knn_index(psub)
        except Exception:
            knn, X = knn_all, X_all

        ev = retrieve_similar_from_previous(psub, knn, X, qv, topk=1, dedup_by_vendor=True, min_sim=float(min_sim))
        lines.append(head)
        if ev.empty:
            lines.append("  Â· ë‹¹ê¸° ëŒ€ì‘: ì—†ìŒ")
        else:
            rr = ev.iloc[0]
            d2 = rr['íšŒê³„ì¼ì'].strftime('%Y-%m-%d') if pd.notna(rr['íšŒê³„ì¼ì']) else ''
            lines.append(f"  Â· ë‹¹ê¸° ëŒ€ì‘: {d2} | {rr['ê±°ë˜ì²˜']} | {int(rr['ë°œìƒì•¡']):,}ì› | sim {rr['similarity']:.2f}")

    return "\n".join(lines)


==============================
ğŸ“„ FILE: analysis/integrity.py
==============================

import pandas as pd
from typing import List, Dict, Any, Optional
try:
    from analysis.contracts import ModuleResult, LedgerFrame
except Exception:
    ModuleResult = None  # íƒ€ì… íŒíŠ¸/ëŸ°íƒ€ì„ ê°€ë“œ
    LedgerFrame = None   # íƒ€ì… íŒíŠ¸/ëŸ°íƒ€ì„ ê°€ë“œ


def analyze_reconciliation(ledger_df: pd.DataFrame, master_df: pd.DataFrame):
    """Masterì™€ Ledger ë°ì´í„° ê°„ì˜ ì •í•©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.

    ë°˜í™˜ê°’:
    - overall_status: "Pass" | "Warning" | "Fail"
    - ê²°ê³¼ DataFrame
    """
    results, overall_status = [], "Pass"
    cy_ledger_df = ledger_df[ledger_df['ì—°ë„'] == ledger_df['ì—°ë„'].max()]
    for _, master_row in master_df.iterrows():
        account_code = master_row['ê³„ì •ì½”ë“œ']
        bspl = master_row.get('BS/PL', 'PL').upper()
        bop = master_row.get('ì „ê¸°ë§ì”ì•¡', 0)
        eop_master = master_row.get('ë‹¹ê¸°ë§ì”ì•¡', 0)

        net_change_gl = cy_ledger_df[cy_ledger_df['ê³„ì •ì½”ë“œ'] == account_code]['ê±°ë˜ê¸ˆì•¡'].sum()
        eop_gl = (bop + net_change_gl) if bspl == 'BS' else net_change_gl

        difference = eop_master - eop_gl
        diff_pct = abs(difference) / max(abs(eop_master), 1)
        status = "Fail" if diff_pct > 0.001 else "Warning" if abs(difference) > 0 else "Pass"
        if status == "Fail":
            overall_status = "Fail"
        elif status == "Warning" and overall_status == "Pass":
            overall_status = "Warning"

        results.append({
            'ê³„ì •ì½”ë“œ': account_code,
            'ê³„ì •ëª…': master_row.get('ê³„ì •ëª…', ''),
            'êµ¬ë¶„': bspl,
            'ê¸°ì´ˆì”ì•¡(Master)': bop,
            'ë‹¹ê¸°ì¦ê°ì•¡(Ledger)': net_change_gl,
            'ê³„ì‚°ëœ ê¸°ë§ì”ì•¡(GL)': eop_gl,
            'ê¸°ë§ì”ì•¡(Master)': eop_master,
            'ì°¨ì´': difference,
            'ìƒíƒœ': status
        })
    return overall_status, pd.DataFrame(results)


# NEW: í‘œì¤€ DTO(ModuleResult) ë°˜í™˜ ë˜í¼
def run_integrity_module(lf: LedgerFrame, accounts: Optional[List[str]] = None):
    """
    ê¸°ì¡´ analyze_reconciliation ê²°ê³¼ë¥¼ í‘œì¤€ ModuleResultë¡œ ê°ìŒ‰ë‹ˆë‹¤.
    - summary: ìƒíƒœ/ê³„ì • ìˆ˜/FailÂ·Warning ê±´ìˆ˜/ìµœëŒ€ ì°¨ì´
    - tables: {"reconciliation": ê²°ê³¼ DF}
    - evidences: (MVP ë‹¨ê³„) ë¹„ì›€
    """
    ledger_df = lf.df if hasattr(lf, 'df') else lf
    master_df = (lf.meta or {}).get('master_df') if hasattr(lf, 'meta') else None
    status, df = analyze_reconciliation(ledger_df, master_df)
    if accounts:
        try:
            accs = [str(a) for a in accounts]
            df = df[df['ê³„ì •ì½”ë“œ'].astype(str).isin(accs)].copy()
        except Exception:
            pass
    summary: Dict[str, Any] = {
        "status": str(status),
        "n_accounts": int(df["ê³„ì •ì½”ë“œ"].nunique()) if (df is not None and not df.empty and "ê³„ì •ì½”ë“œ" in df.columns) else 0,
        "n_fail": int((df["ìƒíƒœ"] == "Fail").sum()) if (df is not None and "ìƒíƒœ" in df.columns) else 0,
        "n_warn": int((df["ìƒíƒœ"] == "Warning").sum()) if (df is not None and "ìƒíƒœ" in df.columns) else 0,
        "max_abs_diff": float(df["ì°¨ì´"].abs().max()) if (df is not None and not df.empty and "ì°¨ì´" in df.columns) else 0.0,
    }
    warnings: List[str] = [] if status == "Pass" else [f"ì •í•©ì„± ìƒíƒœ: {status}"]
    if ModuleResult is None:
        # contracts ë¯¸ê°€ìš© í™˜ê²½ ì•ˆì „ê°€ë“œ
        from typing import NamedTuple
        class _MR(NamedTuple):
            name: str; summary: Dict[str, Any]; tables: Dict[str, pd.DataFrame]; figures: Dict; evidences: List; warnings: List[str]
        return _MR("integrity", summary, {"reconciliation": df}, {}, [], warnings)
    return ModuleResult(
        name="integrity",
        summary=summary,
        tables={"reconciliation": df},
        figures={},
        evidences=[],
        warnings=warnings,
    )



==============================
ğŸ“„ FILE: analysis/kdmeans_shim.py
==============================

from __future__ import annotations
from typing import Optional, Sequence
import numpy as np

try:
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score
except Exception as e:
    raise ImportError("scikit-learnì´ í•„ìš”í•©ë‹ˆë‹¤. `pip install scikit-learn`") from e


class HDBSCAN:
    """
    KDMeans: KMeansë¥¼ ì‚¬ìš©í•˜ë˜ HDBSCANì˜ ìµœì†Œ ì†ì„± ì¸í„°í˜ì´ìŠ¤ë¥¼ í‰ë‚´ëƒ„.
    - fit(X): labels_, probabilities_ ì„¤ì •
    - labels_: np.ndarray[int], [0..k-1]
    - probabilities_: np.ndarray[float], 0~1 (KDMeansì—ì„œëŠ” ì „ë¶€ 1.0ë¡œ ì„¤ì •)
    ë§¤ê°œë³€ìˆ˜:
      - n_clusters: ê³ ì • k (Noneì´ë©´ ìë™ ì„ íƒ)
      - min_cluster_size: k ìƒí•œì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ íŒíŠ¸(ë„ˆë¬´ ë§ì€ êµ°ì§‘ ë°©ì§€)
      - max_k: ìë™ ì„ íƒ ì‹œ k ìƒí•œ(ê¸°ë³¸: ë°ì´í„° í¬ê¸°ì™€ min_cluster_sizeë¡œ ìœ ë„)
      - k_search: "silhouette" | "heuristic"
      - sample_size: ìë™ ì„ íƒ ì‹œ ì‹¤ë£¨ì—£ ê³„ì‚°ì— ì‚¬ìš©í•  ìƒ˜í”Œ í¬ê¸°(ê¸°ë³¸ 2000)
      - random_state: ì¬í˜„ì„±
      - n_init: KMeans ì´ˆê¸°í™” íšŸìˆ˜(ë˜ëŠ” "auto")
    """
    def __init__(
        self,
        n_clusters: Optional[int] = None,
        min_cluster_size: int = 8,
        max_k: Optional[int] = None,
        k_search: str = "silhouette",
        sample_size: int = 2000,
        random_state: int = 42,
        n_init: str | int = "auto",
    ):
        self.n_clusters = n_clusters
        self.min_cluster_size = max(2, int(min_cluster_size))
        self.max_k = max_k
        self.k_search = k_search
        self.sample_size = int(sample_size)
        self.random_state = int(random_state)
        self.n_init = n_init

        # í•™ìŠµ í›„ ì†ì„±(HDBSCAN í˜¸í™˜)
        self.labels_: Optional[np.ndarray] = None
        self.probabilities_: Optional[np.ndarray] = None
        # ì¶”ê°€ í…”ë ˆë©”íŠ¸ë¦¬
        self.chosen_k_: Optional[int] = None
        self.silhouette_: Optional[float] = None

    # --- ë‚´ë¶€: k í›„ë³´ ì‚°ì • ---
    def _candidate_ks(self, n: int) -> Sequence[int]:
        if n < 2:
            return [1]
        base = max(2, int(np.sqrt(n)))
        # ìµœì†Œ í¬ê¸° ì œì•½ ê¸°ë°˜ ìƒí•œ
        max_by_min = max(2, n // self.min_cluster_size)
        # ì™¸ë¶€ ìƒí•œ ì ìš©
        if self.max_k is not None:
            max_by_min = min(max_by_min, int(self.max_k))
        # ì§€ë‚˜ì¹˜ê²Œ í° këŠ” ê³„ì‚° ë¹„ìš© ì´ìŠˆ â†’ ì‹¤ë¬´ì ìœ¼ë¡œ ìº¡
        hard_cap = 24 if n >= 1200 else 12
        k_hi = max(2, min(max_by_min, hard_cap))

        ks = {2, 3, 5, base - 1, base, base + 1, int(np.log2(n)) + 1, k_hi}
        ks = {int(k) for k in ks if 2 <= int(k) <= k_hi}
        return sorted(ks)

    # --- ë‚´ë¶€: ìƒ˜í”Œë§ ---
    def _sample(self, X: np.ndarray) -> np.ndarray:
        n = X.shape[0]
        if n <= self.sample_size:
            return X
        rng = np.random.default_rng(self.random_state)
        idx = rng.choice(n, size=self.sample_size, replace=False)
        return X[idx]

    # --- ë‚´ë¶€: k ìë™ ì„ íƒ (ì‹¤ë£¨ì—£) ---
    def _choose_k(self, X: np.ndarray) -> int:
        n = X.shape[0]
        if n < 2:
            return 1
        if self.n_clusters is not None:
            return max(1, int(self.n_clusters))

        # í›„ë³´ ëª©ë¡
        ks = self._candidate_ks(n)
        if len(ks) == 0:
            return max(2, int(np.sqrt(n)))

        if self.k_search != "silhouette":
            # íœ´ë¦¬ìŠ¤í‹±: âˆšnì— ê°€ì¥ ê°€ê¹Œìš´ ê°’
            base = max(2, int(np.sqrt(n)))
            return min(ks, key=lambda k: abs(k - base))

        Xs = self._sample(X)
        best_k, best_s = None, -1.0

        for k in ks:
            if k >= len(Xs):   # ìƒ˜í”Œë³´ë‹¤ í° k ë¶ˆê°€
                continue
            try:
                km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
                labels = km.fit_predict(Xs)
                # ëª¨ë“  ë¼ë²¨ì´ í•˜ë‚˜ë©´ ì‹¤ë£¨ì—£ ê³„ì‚° ë¶ˆê°€
                if len(set(labels)) < 2:
                    continue
                s = silhouette_score(Xs, labels, metric="euclidean")
                if s > best_s:
                    best_k, best_s = int(k), float(s)
            except Exception:
                continue

        if best_k is None:
            # í´ë°±: âˆšn ì¸ê·¼
            base = max(2, int(np.sqrt(n)))
            best_k = min(ks, key=lambda k: abs(k - base))
            best_s = float("nan")

        self.silhouette_ = best_s
        return int(best_k)

    # --- ê³µê°œ API ---
    def fit(self, X: np.ndarray):
        X = np.asarray(X, dtype=float)
        if X.ndim != 2 or X.shape[0] < 1:
            raise ValueError("X must be 2D array with at least 1 row")

        k = self._choose_k(X)
        self.chosen_k_ = k

        km = KMeans(n_clusters=int(k), n_init=self.n_init, random_state=self.random_state)
        labels = km.fit_predict(X)

        # HDBSCAN í˜¸í™˜ ì†ì„± ë¶€ì—¬
        self.labels_ = labels.astype(int)
        self.probabilities_ = np.ones(shape=(X.shape[0],), dtype=float)
        return self

    def fit_predict(self, X: np.ndarray) -> np.ndarray:
        return self.fit(X).labels_





==============================
ğŸ“„ FILE: analysis/report.py
==============================

from __future__ import annotations
import pandas as pd
from typing import List, Callable, Optional, Any
from .evidence import (
    build_current_cluster_block,
    # build_previous_projection_block,  # íŒŒì¼ í•˜ë‹¨ ë¡œì»¬ ì •ì˜ ì‚¬ìš©
    build_zscore_top5_block,
    build_zscore_top5_block_for_py,
)
from .timeseries import run_timeseries_module
from .embedding import map_previous_to_current_clusters
import numpy as np
from config import PM_DEFAULT
import re
import json
import time


def _fmt_money(x):
    try:
        return f"{float(x):,.0f}ì›"
    except Exception:
        return str(x)


# --- ë‹¨ìœ„ ê°•ì œ í›„ì²˜ë¦¬: ì–µ/ë§Œ â†’ ì› ë‹¨ìœ„ ---
_NUM = r'(?:\d{1,3}(?:,\d{3})*|\d+)'


def _to_int(s):
    return int(str(s).replace(',', ''))


def _replace_korean_units(m):
    # ì¼€ì´ìŠ¤: "3ì–µ 5,072ë§Œ ì›" / "54ì–µ 1,444ë§Œ ì›" / "2ì–µ ì›" / "370ë§Œ ì›"
    eok = m.group('eok')
    man = m.group('man')
    won = m.group('won')
    total = 0
    if eok:
        total += _to_int(eok) * 100_000_000
    if man:
        total += _to_int(man) * 10_000
    if won:
        total += _to_int(won)
    return f"{total:,.0f}ì›"


def _enforce_won_units(text: str) -> str:
    # 1) ì–µ/ë§Œ/ì› í˜¼í•©ì„ ì› ë‹¨ìœ„ë¡œ ì¹˜í™˜
    pat = re.compile(
        rf'(?:(?P<eok>{_NUM})\s*ì–µ)?\s*(?:(?P<man>{_NUM})\s*ë§Œ)?\s*(?:(?P<won>{_NUM})\s*ì›)?'
        r'(?!\s*ë‹¨ìœ„)', flags=re.IGNORECASE)

    def _smart_sub(s):
        out = []
        last = 0
        for m in pat.finditer(s):
            # ì˜ë¯¸ ì—†ëŠ” ë¹ˆ ë§¤ì¹­ ë°©ì§€: ì–µ/ë§Œì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ(ì´ë¯¸ ì› ë‹¨ìœ„ì¼ ê°€ëŠ¥ì„±)
            if not any(m.group(g) for g in ('eok', 'man')):
                continue
            out.append(s[last:m.start()])
            out.append(_replace_korean_units(m))
            last = m.end()
        out.append(s[last:])
        return ''.join(out)

    return _smart_sub(text)


def _boldify_bracket_headers(text: str) -> str:
    # [ìš”ì•½], [ì£¼ìš” ê±°ë˜], [ê²°ë¡ ], [ìš©ì–´ ì„¤ëª…] â†’ **[...]**\n
    text = re.sub(r'^\[(ìš”ì•½|ì£¼ìš” ê±°ë˜|ê²°ë¡ |ìš©ì–´ ì„¤ëª…)\]\s*', r'**[\1]**\n', text, flags=re.MULTILINE)
    return text
def _strip_control(s: str) -> str:
    # íƒ­/ê°œí–‰ ì œì™¸ ëª¨ë“  ì œì–´ë¬¸ì ì œê±° (0x00-0x1F, 0x7F)
    return re.sub(r"[\x00-\x08\x0b-\x1f\x7f]", "", s or "")


# --- ModuleResult ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸(ê²½ëŸ‰): ì •ë ¬Â·ê¸ˆì•¡Â·ë©”ëª¨ ì§€ì› + Top-K ì¼ê´€í™” ---
def build_report_context_from_modules(
    modules: List["ModuleResult"],
    pm_value: float,
    topk: int = 20,
    manual_note: str = ""
) -> str:
    """
    ì—¬ëŸ¬ ModuleResultì—ì„œ summary/ìƒìœ„ evidencesë¥¼ ë½‘ì•„ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±.
    - Evidence ì •ë ¬: risk_score â†’ financial_impact ë‚´ë¦¼ì°¨ìˆœ
    - ê¸ˆì•¡ í‘œê¸°: financial_impactë¥¼ ê¸ˆì•¡ìœ¼ë¡œ í‘œê¸°
    - ê°ì‚¬ ë©”ëª¨(manual_note) ì£¼ì…
    - Top-K ì¼ê´€ ì ìš©
    """
    lines: List[str] = []
    lines.append(f"[PM] {pm_value:,.0f} KRW")
    for m in modules or []:
        try:
            lines.append(f"\n## Module: {getattr(m, 'name', 'module')}")
            summ = getattr(m, "summary", None)
            if summ:
                lines.append(f"- summary: {summ}")

            # Evidence ì •ë ¬(ë‚´ë¦¼ì°¨ìˆœ): ìœ„í—˜ë„ â†’ ê¸ˆì•¡
            evs = list(getattr(m, "evidences", []))
            def _key(e):
                try:
                    return (
                        float(getattr(e, "risk_score", 0.0)),
                        float(getattr(e, "financial_impact", 0.0)),
                    )
                except Exception:
                    return (0.0, 0.0)
            evs = sorted(evs, key=_key, reverse=True)
            k = max(0, int(topk))

            if evs and k > 0:
                lines.append(f"- evidences(top{k}):")
                for e in evs[:k]:
                    try:
                        lnk = getattr(e, "links", {}) or {}
                        acct_nm = lnk.get("account_name", "")
                        acct_cd = lnk.get("account_code", "")
                        risk = float(getattr(e, "risk_score", 0.0))
                        kit  = bool(getattr(e, "is_key_item", False))
                        amt  = getattr(e, "financial_impact", None)
                        amt_txt = f" amount={amt:,.0f}" if isinstance(amt, (int, float)) else ""
                        measure = getattr(e, "measure", None)
                        model = getattr(e, "model", None)
                        tag = ""
                        if measure: tag += f"[{measure}]"
                        if model:   tag += f"[{model}]"
                        rsn  = str(getattr(e, "reason", ""))
                        desc = []
                        for k2 in ["vendor","narration","cluster_name","cluster_group","month"]:
                            v = lnk.get(k2)
                            if v: desc.append(f"{k2}={str(v)[:60]}")
                        desc_txt = (" " + "; ".join(desc)) if desc else ""
                        lines.append(
                            _strip_control(
                                f"  - {tag} {acct_nm}({acct_cd}) risk={risk:.2f} KIT={kit}{amt_txt} reason={rsn}{desc_txt}"
                            )
                        )
                    except Exception:
                        continue

            # í‘œ í¬ê¸° ìš”ì•½(ìœ ì§€)
            tbls = getattr(m, "tables", None)
            if tbls:
                for nm, df in (tbls or {}).items():
                    try:
                        lines.append(f"- table[{nm}]: rows={len(df)} cols={len(df.columns)}")
                    except Exception:
                        pass
        except Exception:
            continue

    if manual_note:
        lines.append("\n## Auditor Note\n" + manual_note.strip())

    return _strip_control("\n".join(lines)).strip()


# Backward-compatible alias with explicit name used in app layer
def generate_rag_context_from_modules(
    modules: List["ModuleResult"],
    pm_value: float,
    topk: int = 20,
    manual_note: str = ""
) -> str:
    return build_report_context_from_modules(modules, pm_value, topk=topk, manual_note=manual_note)


def _safe_load(s: str):
    """ì—„ê²©í•œ JSON ë¡œë”: ì½”ë“œ íœìŠ¤ ì œê±° í›„ strict json.loads.
    ì£¼ë³€ í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ í—ˆìš©í•˜ì§€ ì•ŠìŒ.
    """
    text = (s or "").strip()
    # ì‹œì‘ íœìŠ¤ ì œê±°
    text = re.sub(r"^\s*```(?:json|JSON)?\s*\n", "", text)
    # ë íœìŠ¤ ì œê±°
    text = re.sub(r"\n\s*```\s*$", "", text)
    text = text.strip()
    return json.loads(text)


def build_report_context(master_df: pd.DataFrame, current_df: pd.DataFrame, previous_df: pd.DataFrame,
                         account_codes: List[str], manual_context: str = "",
                         include_risk_summary: bool = False, pm_value: float | None = None) -> str:
    acc_info = master_df[master_df['ê³„ì •ì½”ë“œ'].astype(str).isin(account_codes)]
    acc_names = ", ".join(acc_info['ê³„ì •ëª…'].unique().tolist())
    master_summary = f"- ë¶„ì„ ëŒ€ìƒ ê³„ì • ê·¸ë£¹: {acc_names} ({', '.join(account_codes)})"
    if not acc_info.empty:
        acct_type = acc_info.iloc[0].get('BS/PL', 'PL')
        has_dates_cur = ('íšŒê³„ì¼ì' in current_df.columns) and current_df['íšŒê³„ì¼ì'].notna().any()
        has_dates_prev = ('íšŒê³„ì¼ì' in previous_df.columns) and previous_df['íšŒê³„ì¼ì'].notna().any()
        if str(acct_type).upper() == 'PL' and has_dates_cur:
            min_date = current_df['íšŒê³„ì¼ì'].min(); max_date = current_df['íšŒê³„ì¼ì'].max()
            if has_dates_prev:
                # ë˜í•‘ êµ¬ê°„(ì˜ˆ: 11~2ì›”) ì˜¤íŒ ë°©ì§€: ì—°-ì›” Periodë¡œ ë¹„êµ
                cur_months = current_df['íšŒê³„ì¼ì'].dt.to_period('M')
                prev_months = previous_df['íšŒê³„ì¼ì'].dt.to_period('M')
                mask = prev_months.isin(cur_months.unique())
                prev_f = previous_df.loc[mask]
            else:
                prev_f = previous_df.copy()
            # Net first (ìˆœì•¡: ì°¨-ëŒ€), absolute as reference (ê·œëª¨(ì ˆëŒ€ê°’))
            cur_net = current_df.get('ìˆœì•¡', pd.Series(dtype=float)).sum()
            prev_net = prev_f.get('ìˆœì•¡', pd.Series(dtype=float)).sum()
            cur_abs = current_df.get('ë°œìƒì•¡', pd.Series(dtype=float)).sum()
            prev_abs = prev_f.get('ë°œìƒì•¡', pd.Series(dtype=float)).sum()
            var = cur_net - prev_net
            var_pct = (var / prev_net * 100) if prev_net not in (0, 0.0) else float('inf')
            period = f"{min_date.strftime('%mì›”')}~{max_date.strftime('%mì›”')}"
            master_summary += (
                f"\n- ë‹¹ê¸° **ìˆœì•¡(ì°¨-ëŒ€)** í•©ê³„ ({period}): {cur_net:,.0f}ì›"
                f" | ì „ê¸° ë™ê¸°ê°„ ìˆœì•¡: {prev_net:,.0f}ì› | ìˆœì•¡ ì¦ê°: {var:,.0f}ì› ({var_pct:+.2f}%)"
                f"\n- (ì°¸ê³ ) **ê·œëª¨(ì ˆëŒ€ê°’)** ë°œìƒì•¡: ë‹¹ê¸° {cur_abs:,.0f}ì› | ì „ê¸° {prev_abs:,.0f}ì›"
                f" | ì°¨ì´: {cur_abs - prev_abs:,.0f}ì›"
            )
        else:
            cur_bal = acc_info.get('ë‹¹ê¸°ë§ì”ì•¡', pd.Series(dtype=float)).sum()
            prior_bal = acc_info.get('ì „ê¸°ë§ì”ì•¡', pd.Series(dtype=float)).sum()
            var = cur_bal - prior_bal
            var_pct = (var / prior_bal * 100) if prior_bal not in (0, 0.0) else float('inf')
            master_summary += f"\n- ë‹¹ê¸°ë§ ì”ì•¡(í•©ì‚°): {cur_bal:,.0f}ì› | ì „ê¸°ë§ ì”ì•¡(í•©ì‚°): {prior_bal:,.0f}ì› | ì¦ê°: {var:,.0f}ì› ({var_pct:+.2f}%)"

    manual_summary = f"\n\n## ì‚¬ìš©ì ì œê³µ ì¶”ê°€ ì •ë³´\n{manual_context}" if manual_context and not manual_context.isspace() else ""

    # --- New context layout ---
    sec_info = f"## ë¶„ì„ëŒ€ìƒ ê³„ì •ì •ë³´\n{master_summary}{manual_summary}"
    sec_cur = build_current_cluster_block(current_df)
    # Prior-year: ì „í‘œ ì „ì²´ë¥¼ CY ì„¼íŠ¸ë¡œì´ë“œì— ìµœê·¼ì ‘ ë§¤í•‘í•˜ì—¬ í•©ì‚°í•˜ëŠ” ì¦ê±° ë¸”ë¡ ì‚¬ìš©
    sec_prev = build_previous_projection_block(current_df, previous_df)
    sec_top5_cy = build_zscore_top5_block(current_df, previous_df, topn=5)
    sec_top5_py = build_zscore_top5_block_for_py(previous_df, current_df, topn=5)
    sec_ts = build_timeseries_summary_block(current_df)

    # --- ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½ ì œê±°(ê²½ì˜ì§„ì£¼ì¥/ë§¤íŠ¸ë¦­ìŠ¤ ë¹„í™œì„±í™”) ---
    # sec_risk = ""
    # if include_risk_summary:
    #     try:
    #         sec_risk = _build_risk_matrix_section(current_df, pm_value=pm_value)
    #     except Exception:
    #         sec_risk = ""

    parts = [sec_info, sec_cur, sec_prev, sec_ts, sec_top5_cy, sec_top5_py]
    # if sec_risk:
    #     parts.insert(2, sec_risk)  # ì •ë³´â†’(ìœ„í—˜)â†’í´ëŸ¬ìŠ¤í„° ìˆœ
    return "\n".join(parts)


def build_timeseries_summary_block(current_df: pd.DataFrame, topn: int = 5) -> str:
    """
    ## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½
    ê³„ì •ë³„ ì›”ë³„ í•©ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì§€ë§‰ í¬ì¸íŠ¸ì˜ ì˜ˆì¸¡ ëŒ€ë¹„ ì´íƒˆì„ ìš”ì•½.
    """
    if current_df is None or current_df.empty or 'íšŒê³„ì¼ì' not in current_df.columns:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ë°ì´í„° ì—†ìŒ)"
    df = current_df.copy()
    # ë‚ ì§œ/ê³„ì • ê°€ë“œ
    try:
        df['íšŒê³„ì¼ì'] = pd.to_datetime(df['íšŒê³„ì¼ì'], errors='coerce')
    except Exception:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜)"
    if 'ê³„ì •ëª…' not in df.columns:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ê³„ì •ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤)"
    df['ì—°ì›”'] = df['íšŒê³„ì¼ì'].dt.to_period('M')
    # ê¸ˆì•¡ ì»¬ëŸ¼ ìœ ì—° ì¸ì‹
    cand = ['ê±°ë˜ê¸ˆì•¡', 'ë°œìƒì•¡', 'ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’', 'amount', 'ê¸ˆì•¡']
    val_col = next((c for c in cand if c in df.columns), None)
    if not val_col:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ê¸ˆì•¡ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤)"

    m = (df.groupby(['ê³„ì •ëª…','ì—°ì›”'], as_index=False)[val_col].sum())
    m['account'] = m['ê³„ì •ëª…']
    # FlowëŠ” ì›” â€˜ì§‘ê³„â€™ ê°œë…ì´ë¯€ë¡œ ë‚´ë¶€ ì•µì»¤ëŠ” ì›”ì´ˆ(start)ë¡œ í†µì¼
    m['date'] = m['ì—°ì›”'].dt.to_timestamp(how='start')
    m['amount'] = m[val_col]

    rows = run_timeseries_module(m[['account','date','amount']])
    if rows is None or rows.empty:
        return "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½\n- (ìœ ì˜ë¯¸í•œ ì´íƒˆ ì—†ìŒ)"

    rows = rows.sort_values('risk', ascending=False).head(int(topn))
    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime('%Y-%m-%d') if _pd.notna(x) else ""
        except:
            return ""
    lines = [
        "\n\n## ì˜ˆì¸¡ ì´íƒˆ ìš”ì•½",
        "â€» ê¸°ë³¸ì€ 'ì›”ë³„ ë°œìƒì•¡(Î”ì”ì•¡/flow)'. BS ê³„ì •ì€ **balance** ê¸°ì¤€ë„ ë‚´ë¶€ í‰ê°€í•˜ë©°, ì•„ë˜ í‘œê¸°ëŠ” MoRê³¼ zÂ·riskë¥¼ í•¨ê»˜ ë³´ì—¬ì¤ë‹ˆë‹¤."
    ]
    for _, r in rows.iterrows():
        _m = str(r.get('measure','flow'))
        _when = "ì›”í•©ê³„" if _m == "flow" else "ì›”ë§"
        lines.append(
            f"- [{_fmt_dt(r['date'])}Â·{_when}] {r['account']} ({_m}, MoR={r.get('model','-')})"
            f" | ì‹¤ì œ {r['actual']:,.0f}ì› vs ì˜ˆì¸¡ {r['predicted']:,.0f}ì›"
            f" â†’ {'ìƒíšŒ' if float(r['error'])>0 else 'í•˜íšŒ'} | z={float(r['z']):+.2f} | risk={float(r['risk']):.2f}"
        )
    return "\n".join(lines)


def _build_risk_matrix_section(*_args, **_kwargs) -> str:
    # [Removed] CEAVOP/ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ ì„¹ì…˜ì€ 2025-08-30 ê¸°ì¤€ ë¹„í™œì„±í™”.
    # ì¬ë„ì… ì „ê¹Œì§€ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ì—¬ í˜¸ì¶œë¶€ë¥¼ ê¹¨ì§€ ì•ŠìŒ.
    return ""


def build_methodology_note(report_accounts=None) -> str:
    lines = [
        "\n\n## ë¶„ì„ ê¸°ì¤€(ì•Œë¦¼)",
        "- ì´ë²ˆ ë¶„ì„ì€ UIì—ì„œ ì„ íƒëœ ê³„ì • ê¸°ì¤€ìœ¼ë¡œ ì‚°ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "- ìš”ì•½ ìˆ˜ì¹˜: **ìˆœì•¡(ì°¨-ëŒ€)** ê¸°ì¤€. (ë°œìƒì•¡=ê·œëª¨(ì ˆëŒ€ê°’)ì€ ì°¸ê³ ìš©)",
        "- Z-Score: ì„ íƒ ê³„ì •ë“¤ì˜ **ë°œìƒì•¡(ì ˆëŒ€ê°’)** ë¶„í¬ ê¸°ì¤€.",
        "- ìœ ì‚¬ë„/ê·¼ê±°: **ì ìš”+ê±°ë˜ì²˜** ì„ë² ë”© í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ì „ê¸° ë™ì›” ìš°ì„ ).",
        "- 'í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)'ëŠ” ì˜ë¯¸ê°€ ì¶©ë¶„íˆ ëª¨ì´ì§€ ì•Šì•„ ìë™ìœ¼ë¡œ ë¬¶ì´ì§€ ì•Šì€ ì‚°ë°œì  ê±°ë˜ ë¬¶ìŒì…ë‹ˆë‹¤.",
    ]
    return "\n".join(lines)


def _format_from_json(obj: dict) -> str:
    """
    ë‹¨ìˆœ ìŠ¤í‚¤ë§ˆ(JSON) â†’ ìµœì¢… ë§ˆí¬ë‹¤ìš´.
    - key_transactions: LLMì´ ì‘ì„±í•œ ì „ì²´ ì„¹ì…˜ ë§ˆí¬ë‹¤ìš´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - glossary: í•„ìˆ˜ í•­ëª© ë³´ê°•(ì—†ì„ ê²½ìš° ê¸°ë³¸ ì •ì˜ ì¶”ê°€)
    """
    summary = (obj.get("summary") or "").strip()
    kt_val = obj.get("key_transactions")
    # ê³¼ê±° í˜¸í™˜(ì˜¤ë¸Œì íŠ¸ê°€ ì˜¤ë©´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì‹œë„)
    if isinstance(kt_val, dict):
        parts = []
        for k, v in kt_val.items():
            if isinstance(v, str):
                parts.append(v.strip())
        key_tx_md = "\n\n".join(p for p in parts if p)
    else:
        key_tx_md = (kt_val or "").strip()

    conclusion = (obj.get("conclusion") or "").strip()

    md = (
        f"**[ìš”ì•½]**\n{summary}\n\n"
        f"**[ì£¼ìš” ê±°ë˜]**\n{key_tx_md}\n\n"
        f"**[ê²°ë¡ ]**\n{conclusion}"
    )
    return md


# --- ì „ê¸° ì „ì²´ ë§¤í•‘ í•©ì‚° ë°©ì‹ìœ¼ë¡œ êµì²´: ì´ì „ ì „í‘œë¥¼ CY í´ëŸ¬ìŠ¤í„°ì— ìµœê·¼ì ‘ ë§¤í•‘ í›„ í•©ì‚° ---
def build_previous_projection_block(current_df: pd.DataFrame, previous_df: pd.DataFrame, min_sim: float | None = None) -> str:
    """
    Project all PY vouchers onto CY cluster centroids and aggregate absolute amounts by the CY cluster_group.
    - No similarity computation is shown or used for filtering.
    - Output contains only total absolute amount and ONE example voucher (no sim).
    """
    import pandas as pd
    if current_df is None or previous_df is None or current_df.empty or previous_df.empty:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ì „ê¸° ë°ì´í„° ì—†ìŒ)"
    need_cur = {'cluster_id','cluster_name','vector'}
    if not need_cur.issubset(current_df.columns) or 'vector' not in previous_df.columns:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (í´ëŸ¬ìŠ¤í„°/ë²¡í„° ì •ë³´ ë¶€ì¡±)"

    prev_m = map_previous_to_current_clusters(current_df, previous_df)
    if prev_m is None or prev_m.empty or 'mapped_cluster_id' not in prev_m.columns:
        return "\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡\n- (ë§¤í•‘ ì‹¤íŒ¨)"

    if 'cluster_group' in current_df.columns:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_group'].to_dict()
    else:
        id2group = current_df.drop_duplicates('cluster_id').set_index('cluster_id')['cluster_name'].to_dict()

    prev_m = prev_m.copy()
    prev_m['mapped_group'] = prev_m['mapped_cluster_id'].map(id2group)
    prev_m['abs_amt'] = prev_m.get('ë°œìƒì•¡', pd.Series(dtype=float)).abs()

    agg = (
        prev_m.groupby('mapped_group', dropna=False)
              .agg(ê·œëª¨=('abs_amt','sum'))
              .reset_index()
              .sort_values('ê·œëª¨', ascending=False)
    )

    lines = ["\n\n## ì „ê¸° í´ëŸ¬ìŠ¤í„° ë° ê¸ˆì•¡"]
    for _, row in agg.iterrows():
        g = row['mapped_group'] if pd.notna(row['mapped_group']) else '(ë¯¸ë§¤í•‘)'
        tot = row['ê·œëª¨']
        sub = prev_m[prev_m['mapped_group'] == row['mapped_group']]
        if not sub.empty:
            ex = sub.sort_values('abs_amt', ascending=False).head(1).iloc[0]
            raw_dt = ex.get('íšŒê³„ì¼ì', None)
            if pd.notna(raw_dt):
                try:
                    _dt = pd.to_datetime(raw_dt, errors='coerce')
                    dt = _dt.strftime('%Y-%m-%d') if pd.notna(_dt) else ''
                except Exception:
                    dt = ''
            else:
                dt = ''
            lines.append(f"- [{g}] ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›")
            lines.append(f"  Â· ì˜ˆì‹œ: {dt} | {ex.get('ê±°ë˜ì²˜','')} | {int(ex.get('ë°œìƒì•¡',0)):,.0f}ì›")
        else:
            lines.append(f"- [{g}] ê·œëª¨(ì ˆëŒ€ê°’) {tot:,.0f}ì›")
    return "\n".join(lines)


def run_final_analysis(
    context: str,
    account_codes: list[str],
    *,
    model: str | None = None,
    max_tokens: int | None = 16000,
    generate_fn: Optional[Callable[..., str]] = None,
) -> str:
    system = (
        "You are a CPA. Do all hidden reasoning internally and output ONLY the JSON object in the EXACT schema below. "
        "Language: Korean (ko-KR) for every natural-language value.\n"
        "Schema: {"
        '"summary": str,'
        '"key_transactions": str,'
        '"conclusion": str,'
        '"glossary": [str]'
        "}\n"
        "Authoring rules:\n"
        "â€¢ Monetary values MUST be formatted in KRW like '1,234ì›' (never ì–µ/ë§Œì›).\n"
        "â€¢ [ìš”ì•½]ì€ ê³„ì •êµ° ìˆ˜ì¤€ì˜ ë³€ë™ê³¼ ê·œëª¨ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…ë£Œíˆ.\n"
        "â€¢ [ì£¼ìš” ê±°ë˜]ëŠ” ì „ì²´ ì„œìˆ ì„ ë„¤ê°€ ì„¤ê³„í•˜ë˜, ì»¨í…ìŠ¤íŠ¸(CY/PY í´ëŸ¬ìŠ¤í„°, ë§¤í•‘, Z-score ìƒìœ„ í•­ëª©)ë¥¼ ê·¼ê±°ë¡œ êµ¬ì„± ë¹„ì¤‘/ì´ìƒì¹˜/ì „ê¸° ëŒ€ì‘ê´€ê³„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë¼. í•„ìš”í•˜ë©´ ë¶ˆë¦¿Â·ì†Œì œëª©ì„ ì„ì˜ë¡œ ì‚¬ìš©í•´ ê°€ë…ì„±ì„ ë†’ì—¬ë¼.\n"
        "â€¢ [ê²°ë¡ ]ì€ ì›ì¸Â·ë¦¬ìŠ¤í¬Â·í†µì œÂ·ì•¡ì…˜ì•„ì´í…œ ì¤‘ì‹¬ìœ¼ë¡œ ì‹¤ë¬´ì  ì œì•ˆ ìœ„ì£¼ë¡œ ì‘ì„±í•œë‹¤.\n"
        "â€¢ [ìš©ì–´ ì„¤ëª…]ì—ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ ë‘ í•­ëª©ì´ í¬í•¨ë˜ë„ë¡ í•œë‹¤:   1) 'í´ëŸ¬ìŠ¤í„° ë…¸ì´ì¦ˆ(-1)' ì •ì˜, 2) 'Z-Score'ê°€ â€˜í‰ê· ì—ì„œ ëª‡ í‘œì¤€í¸ì°¨â€™ì¸ì§€ì˜ ì§ê´€ì  ì˜ë¯¸.\n"
        "Compliance: Output MUST be the JSON object itself, with no markdown/code-fences or extra text."
    )
    user = (
        f"Target accounts: {', '.join(account_codes)}\n"
        f"{context}\n"
        "Return ONLY the JSON per schema via function call."
    )

    tool_schema = {
        "type": "function",
        "function": {
            "name": "emit_report",
            "description": "Return the report strictly in the fixed JSON schema.",
            "parameters": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "key_transactions": {"type": "string"},
                    "conclusion": {"type": "string"},
                    "glossary": {"type": "array","items":{"type":"string"}}
                },
                "required": ["summary","key_transactions","conclusion","glossary"]
            }
        }
    }

    max_retries = 2
    last_err = None

    for attempt in range(max_retries + 1):
        try:
            if generate_fn is None:
                raise RuntimeError("generate_fn not provided (LLM dependency must be injected)")
            raw = generate_fn(
                system=system, user=user, model=model,
                max_tokens=max_tokens, tools=[tool_schema], force_json=False
            )
            obj = _safe_load(raw)
            text = _format_from_json(obj)
            return _enforce_won_units(text)
        except Exception as e:
            last_err = e
            if attempt < max_retries:
                time.sleep(1.0)
            continue

    raise ValueError(f"LLM failed to produce valid JSON report after retries. Details: {last_err}")


# --- NEW: LLM ë¯¸ì‚¬ìš©/ì‹¤íŒ¨ ì‹œ í´ë°± ë¦¬í¬íŠ¸ (ìˆœìˆ˜ ë¡œì»¬ ê³„ì‚°) ---
def run_offline_fallback_report(current_df: pd.DataFrame,
                                previous_df: pd.DataFrame,
                                account_codes: list[str],
                                pm_value: float | None = None) -> str:
    """
    ì™¸ë¶€ LLMì„ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê°„ë‹¨ ë³´ê³ ì„œë¥¼ ìƒì„±í•œë‹¤.
    - ìš”ì•½: CY/PY ìˆœì•¡/ê·œëª¨ ë¹„êµ
    - ì£¼ìš” ê±°ë˜: |Z| Top 5 (ê°€ëŠ¥í•˜ë©´ Z ê¸°ì¤€, ì—†ìœ¼ë©´ ë°œìƒì•¡ ìƒìœ„)
    - ê²°ë¡ : KIT(â‰¥PM) ê±´ìˆ˜/ë¹„ì¤‘ ë° ë¦¬ìŠ¤í¬ ì£¼ì˜
    - ìš©ì–´: Z-Score, Key Item ê¸°ë³¸ ì •ì˜
    """
    pm = float(pm_value) if pm_value is not None else float(PM_DEFAULT)
    cur = current_df.copy()
    prev = previous_df.copy()

    def _safe_sum(df, col): 
        return float(df.get(col, pd.Series(dtype=float)).sum()) if not df.empty else 0.0

    cur_net  = _safe_sum(cur, "ìˆœì•¡")
    prev_net = _safe_sum(prev, "ìˆœì•¡")
    cur_abs  = _safe_sum(cur, "ë°œìƒì•¡")
    prev_abs = _safe_sum(prev, "ë°œìƒì•¡")

    var_net = cur_net - prev_net
    var_abs = cur_abs - prev_abs
    var_pct = (var_net / prev_net * 100.0) if prev_net not in (0, 0.0) else float("inf")

    # Top 5: Z-Score ìš°ì„ , ì—†ìœ¼ë©´ ë°œìƒì•¡ ìƒìœ„
    top_df = cur.copy()
    if "Z-Score" in top_df.columns and top_df["Z-Score"].notna().any():
        top_df = top_df.reindex(top_df["Z-Score"].abs().sort_values(ascending=False).index)
    else:
        top_df = top_df.reindex(top_df.get("ë°œìƒì•¡", pd.Series(dtype=float)).abs().sort_values(ascending=False).index)
    top_df = top_df.head(5)

    # KIT ì§‘ê³„(ì ˆëŒ€ë°œìƒì•¡ ê¸°ì¤€)
    kit_mask = top_df.get("ë°œìƒì•¡", pd.Series(dtype=float)).abs() >= pm if not top_df.empty else pd.Series([], dtype=bool)
    kit_cnt  = int(kit_mask.sum()) if not top_df.empty else 0

    def _fmt_dt(x):
        try:
            import pandas as _pd
            return x.strftime("%Y-%m-%d") if _pd.notna(x) else ""
        except Exception:
            return ""

    # Compose sections (ê°„ë‹¨ Markdown)
    summary = (
        f"ì„ íƒ ê³„ì •({', '.join(account_codes)}) ê¸°ì¤€ìœ¼ë¡œ ë‹¹ê¸° **ìˆœì•¡** {cur_net:,.0f}ì›,"
        f" ì „ê¸° {prev_net:,.0f}ì› â†’ ì¦ê° {var_net:,.0f}ì› ({var_pct:+.2f}%).\n"
        f"(ì°¸ê³ ) **ê·œëª¨(ë°œìƒì•¡ ì ˆëŒ€ê°’)** ë‹¹ê¸° {cur_abs:,.0f}ì›, ì „ê¸° {prev_abs:,.0f}ì› â†’ ì°¨ì´ {var_abs:,.0f}ì›."
    )

    kt_lines = []
    if not top_df.empty:
        for i, (_, r) in enumerate(top_df.iterrows(), 1):
            dt = _fmt_dt(r.get("íšŒê³„ì¼ì"))
            vend = str(r.get("ê±°ë˜ì²˜", "") or "")
            amt = float(r.get("ë°œìƒì•¡", 0.0))
            z   = r.get("Z-Score", np.nan)
            ztxt = f" | Z={float(z):+.2f}" if not pd.isna(z) else ""
            kt_lines.append(f"- [{i}] {dt} | {vend} | {amt:,.0f}ì›{ztxt}")
    key_tx = "\n".join(kt_lines) if kt_lines else "- ìƒìœ„ í•­ëª©ì„ ì‚°ì¶œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    conclusion = (
        f"PM {pm:,.0f}ì› ê¸°ì¤€ **Key Item(KIT)** í›„ë³´ëŠ” ìƒìœ„ ë¦¬ìŠ¤íŠ¸ ì¤‘ {kit_cnt}ê±´ì…ë‹ˆë‹¤. "
        "Z-Scoreê°€ í° í•­ëª©ì€ ì ìš”Â·ê±°ë˜ì²˜ ë“± ê·¼ê±° í™•ì¸ê³¼ ì›ì¸ íŒŒì•…ì´ í•„ìš”í•©ë‹ˆë‹¤. "
        "ì£¼ìš” ë³€ë™ì€ ì›”ë³„ ì¶”ì´/ìƒê´€ ë¶„ì„ê³¼ í•¨ê»˜ êµì°¨ê²€í† í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    )
    return (
        f"**[ìš”ì•½]**\n{summary}\n\n"
        f"**[ì£¼ìš” ê±°ë˜]**\n{key_tx}\n\n"
        f"**[ê²°ë¡ ]**\n{conclusion}"
    )




==============================
ğŸ“„ FILE: analysis/report_adapter.py
==============================

from __future__ import annotations
from typing import Any, Dict
import pandas as pd
# Assuming contracts are available in the environment
try:
    from analysis.contracts import ModuleResult
except ImportError:
    # Define a simple fallback if contracts are missing
    from typing import NamedTuple, List, Dict
    class ModuleResult(NamedTuple):
        name: str; summary: Dict; tables: Dict; figures: Dict; evidences: List; warnings: List

def wrap_dfs_as_module_result(df_cy: pd.DataFrame, df_py: pd.DataFrame, name: str = "report_ctx") -> ModuleResult:
    """Wraps legacy df_cy/df_py into a standard ModuleResult for unified processing."""
    df_cy = (df_cy.copy() if df_cy is not None else pd.DataFrame())
    df_py = (df_py.copy() if df_py is not None else pd.DataFrame())
    summary: Dict[str, Any] = {
        "rows_cy": int(len(df_cy)),
        "rows_py": int(len(df_py)),
        "accounts_cy": int(df_cy["ê³„ì •ì½”ë“œ"].nunique()) if "ê³„ì •ì½”ë“œ" in df_cy.columns else 0,
        "accounts_py": int(df_py["ê³„ì •ì½”ë“œ"].nunique()) if "ê³„ì •ì½”ë“œ" in df_py.columns else 0,
    }
    return ModuleResult(
        name=name,
        summary=summary,
        tables={"current": df_cy, "previous": df_py},
        figures={},
        evidences=[],   # Adapter only wraps DFs, does not generate new evidences
        warnings=[]
    )



==============================
ğŸ“„ FILE: analysis/summarization.py
==============================




==============================
ğŸ“„ FILE: analysis/timeseries.py
==============================

# timeseries.py
# v3 â€” Compact TS module with PY+CY window, MoR(EMA/MA/ARIMA/Prophet), dual-basis(flow/balance)
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple, Callable, Mapping
import math
import numpy as np
STANDARD_COLS = ["date","account","measure","model","actual","predicted","error","z","risk"]

def _ensure_ts_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì‹œê³„ì—´ ê²°ê³¼ DFë¥¼ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ì •ê·œí™”í•œë‹¤.
    ëˆ„ë½ëœ ì»¬ëŸ¼ì€ NaNìœ¼ë¡œ ì¶”ê°€í•˜ê³ , dateëŠ” datetimeìœ¼ë¡œ ê°•ì œ.
    """
    if df is None or len(df) == 0:
        return pd.DataFrame(columns=STANDARD_COLS)
    out = df.copy()
    if "date" in out.columns:
        out["date"] = pd.to_datetime(out["date"], errors="coerce")
    for c in STANDARD_COLS:
        if c not in out.columns:
            out[c] = np.nan
    # ë¶ˆí•„ìš” ì»¬ëŸ¼ì€ ë³´ì¡´í•˜ë˜, ê¸°ì¤€ ì»¬ëŸ¼ ìš°ì„  ë°˜í™˜
    extra = [c for c in out.columns if c not in STANDARD_COLS]
    return out[STANDARD_COLS + extra]
import pandas as pd
import plotly.graph_objects as go
from pandas.tseries.offsets import MonthEnd
def to_month_end_index(idx) -> pd.DatetimeIndex:
    """Convert a datetime-like or period-like index to month-end DatetimeIndex.
    NEW: Always normalize to month-end 00:00:00 (floor to day) for stable axes.
    """
    pidx = pd.PeriodIndex(idx, freq="M")
    _end = pidx.to_timestamp(how="end").floor("D")
    return pd.DatetimeIndex(_end)

# Attempt to import visualization and helper utilities (assuming they exist in the project structure)
try:
    from utils.viz import add_period_guides, add_materiality_threshold
except ImportError:
    # Fallbacks if utils.viz is not found
    def add_period_guides(fig, dates):
        return fig
    def add_materiality_threshold(fig, threshold):
        return fig
try:
    from utils.helpers import model_reason_text
except ImportError:
    # Fallback if utils.helpers is not found
    def model_reason_text(model_name, diagnostics):
        return f"Model {model_name} was selected based on cross-validation metrics."

# Optional contracts import for DTO outputs
try:
    from analysis.contracts import LedgerFrame, ModuleResult, EvidenceDetail
except Exception:  # pragma: no cover - keep loose coupling for tests
    LedgerFrame = None  # type: ignore
    ModuleResult = None  # type: ignore
    EvidenceDetail = None  # type: ignore

# -------- Optional config / anomaly imports with safe fallbacks --------
try:
    from config import PM_DEFAULT as _PM_DEFAULT
except Exception:
    # ì‹¤ìš´ì˜ ê¸°ë³¸ê°’: 5ì–µ (ì‚¬ìš©ì ì…ë ¥ ë¯¸ì œê³µ ì‹œ ìµœí›„ì˜ ì•ˆì „ê°’)
    _PM_DEFAULT = 500_000_000
try:
    from config import FORECAST_MIN_POINTS as _FORECAST_MIN_POINTS
except Exception:
    _FORECAST_MIN_POINTS = 8
try:
    from config import ARIMA_DEFAULT_ORDER as _ARIMA_DEFAULT_ORDER
except Exception:
    _ARIMA_DEFAULT_ORDER = (1, 1, 1)

def _risk_from_fallback(z_abs: float, amount: float, pm: float) -> float:
    # simple logistic mapping as a fallback
    return float(1.0 / (1.0 + math.exp(-abs(z_abs))))

try:
    from analysis.anomaly import _risk_from as _RISK_EXTERNAL  # type: ignore
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        try:
            r = _RISK_EXTERNAL(z_abs, amount=amount, pm=float(pm))
            return float(r[-1] if isinstance(r, (list, tuple)) else r)
        except Exception:
            return _risk_from_fallback(z_abs, amount, pm)
except Exception:
    def _risk_score(z_abs: float, amount: float, pm: float) -> float:
        return _risk_from_fallback(z_abs, amount, pm)

# ----------------------------- Utilities ------------------------------
DATE_CANDIDATES = ['íšŒê³„ì¼ì','ì „í‘œì¼ì','ê±°ë˜ì¼ì','ì¼ì','date','Date']
AMT_CANDIDATES  = ['ê±°ë˜ê¸ˆì•¡','ë°œìƒì•¡','ê¸ˆì•¡','ê¸ˆì•¡(ì›)','ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’','ë°œìƒì•¡_ì ˆëŒ€ê°’','ìˆœì•¡','ìˆœì•¡(ì›)']

def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    for c in candidates:
        if c in df.columns:
            return c
    return None
def _to_month_period_index(dates: pd.Series) -> pd.PeriodIndex:
    return pd.to_datetime(dates).dt.to_period("M")

def _to_month_end(ts: pd.Series) -> pd.Series:
    """Normalize datetimes to month-end and floor to seconds for clean display."""
    ts = pd.to_datetime(ts, errors="coerce")
    return (ts + MonthEnd(0)).dt.floor("S")

def _monthly_flow_and_balance(
    df: pd.DataFrame,
    date_col: str,
    amount_col: str,
    opening: float = 0.0,
) -> Tuple[pd.Series, pd.Series]:
    """ì›”ë³„ ë°œìƒì•¡ í•©ê³„(flow)ì™€ ê¸°ì´ˆ+ëˆ„ì ë°œìƒì•¡(balance) ë°˜í™˜.
    ë°˜í™˜ SeriesëŠ” month-end DatetimeIndexë¥¼ ê°€ì§€ë©° index.name="date"ë¡œ ì„¤ì •ëœë‹¤.
    """
    if df is None or df.empty:
        idx = pd.DatetimeIndex([], name="date")
        return pd.Series(dtype=float, index=idx), pd.Series(dtype=float, index=idx)
    p = pd.to_datetime(df[date_col], errors="coerce").dt.to_period("M")
    amt = pd.to_numeric(df[amount_col], errors="coerce").fillna(0.0)
    flow = amt.groupby(p).sum().astype(float)
    balance = (flow.cumsum() + float(opening)).astype(float)
    month_end = flow.index.to_timestamp(how="end").floor("D")
    flow_s = pd.Series(flow.values, index=month_end)
    bal_s = pd.Series(balance.values, index=month_end)
    flow_s.index.name = bal_s.index.name = "date"
    return flow_s, bal_s

def _longest_contiguous_month_run(periods: pd.PeriodIndex) -> pd.PeriodIndex:
    if len(periods) <= 1: return periods
    p = pd.PeriodIndex(np.unique(np.asarray(periods)), freq="M")
    best_s = best_e = cur_s = 0
    for i in range(1, len(p)):
        if (p[i] - p[i-1]).n != 1:
            if i-1 - cur_s > best_e - best_s:
                best_s, best_e = cur_s, i-1
            cur_s = i
    if len(p)-1 - cur_s > best_e - best_s:
        best_s, best_e = cur_s, len(p)-1
    return p[best_s:best_e+1]

def _smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    denom = np.maximum(1e-12, np.abs(yt) + np.abs(yp))
    return float(np.mean(200.0 * np.abs(yt - yp) / denom))

def _std_last(x: np.ndarray, w: int = 6) -> float:
    if len(x) < 2: return 0.0
    s = np.std(x[-min(w, len(x)):], ddof=1) if len(x) > 1 else 0.0
    return float(s if math.isfinite(s) else 0.0)

def z_and_risk(residuals: np.ndarray, pm: float = _PM_DEFAULT) -> Tuple[np.ndarray, np.ndarray]:
    """ì”ì°¨ ì‹œí€€ìŠ¤ì— ëŒ€í•´ í‘œì¤€í™” zì™€ ìœ„í—˜ë„ ë°°ì—´ì„ ë°˜í™˜.
    í…ŒìŠ¤íŠ¸ í˜¸í™˜ì„ ìœ„í•´ ê°„ë‹¨í•œ ì •ê·œí™”ì™€ |z|â†’risk ë§¤í•‘ì„ ì‚¬ìš©.
    """
    r = np.asarray(residuals, dtype=float)
    if r.size <= 1:
        z = np.zeros_like(r)
    else:
        sd = float(np.std(r, ddof=1))
        z = (r / sd) if sd > 0 else np.zeros_like(r)
    risk_vals = np.array([_risk_score(abs(float(zi)), amount=1.0, pm=float(pm)) for zi in z], dtype=float)
    return z, risk_vals

def _has_seasonality(y: pd.Series) -> bool:
    y = pd.Series(y, dtype=float)
    if len(y) < 12: return False
    ac = np.abs(np.fft.rfft((y - y.mean()).values))
    core = ac[2:] if len(ac) > 2 else ac
    return bool(core.size and (core.max() / (core.mean() + 1e-9) > 5.0))

# --------------------------- Model backends ---------------------------
def _model_registry() -> Dict[str, bool]:
    ok_arima = ok_prophet = False
    try:
        import statsmodels.api as _  # noqa
        ok_arima = True
    except Exception:
        pass
    try:
        from prophet import Prophet as _  # noqa
        ok_prophet = True
    except Exception:
        pass
    return {"ema": True, "ma": True, "arima": ok_arima, "prophet": ok_prophet}

def model_registry() -> Dict[str, bool]:
    """ê³µê°œ API: ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—”ë“œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë°˜í™˜."""
    return _model_registry()

# EMA
def _fit_ema(y: pd.Series, alpha: float = 0.3) -> Dict[str, Any]:
    return {"alpha": float(alpha), "y": y}

def _pred_ema(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]
    alpha = float(m["alpha"])
    pred = y.ewm(alpha=alpha, adjust=False).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# MA
def _fit_ma(y: pd.Series, window: int = 6) -> Dict[str, Any]:
    return {"window": int(window), "y": y}

def _pred_ma(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    y: pd.Series = m["y"]; w = int(m["window"])
    pred = y.rolling(w, min_periods=1).mean().shift(1).fillna(y.iloc[:1].values[0]).values
    return pred if steps is None else np.repeat(pred[-1], int(steps))

# ARIMA
def _fit_arima(y: pd.Series, order: Tuple[int,int,int] = _ARIMA_DEFAULT_ORDER):
    import statsmodels.api as sm
    return sm.tsa.ARIMA(y, order=tuple(order)).fit()

def _pred_arima(m, steps: Optional[int] = None) -> np.ndarray:
    if steps is None:
        fv = pd.Series(m.fittedvalues).shift(1).fillna(method="bfill")
        return fv.values
    return np.asarray(m.forecast(steps=int(steps)))

# Prophet
def _fit_prophet(y: pd.Series):
    from prophet import Prophet
    df = pd.DataFrame({"ds": y.index.to_timestamp(), "y": y.values})
    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    m.fit(df)
    return {"m": m, "idx": y.index}

def _pred_prophet(m: Dict[str, Any], steps: Optional[int] = None) -> np.ndarray:
    model = m["m"]; idx: pd.PeriodIndex = m["idx"]
    if steps is None:
        fit = model.predict(pd.DataFrame({"ds": idx.to_timestamp()}))["yhat"].values
        return np.roll(fit, 1)  # 1-step ahead approx.
    last = idx[-1].to_timestamp()
    future = pd.date_range(last + pd.offsets.MonthBegin(1), periods=int(steps), freq="MS")
    return model.predict(pd.DataFrame({"ds": future}))["yhat"].values

# -------------------- Model selection (MoR) via rolling CV -------------
def _rolling_origin_cv(
    y: pd.Series,
    fit_fn, pred_fn,
    k: int = 3, min_train: int = 6
) -> float:
    y = y.dropna(); n = len(y)
    if n < max(min_train + k, 8):
        try:
            m = fit_fn(y); yhat = pred_fn(m)
        except Exception:
            return 999.0
        yhat = np.asarray(yhat)[:n] if yhat is not None else np.repeat(y.iloc[:1].values, n)
        return _smape(y.values, yhat)
    step = max((n - min_train) // (k + 1), 1)
    scores = []
    for i in range(min_train, n, step):
        tr = y.iloc[:i]; te = y.iloc[i:i+step]
        if te.empty: break
        try:
            m = fit_fn(tr); yh = pred_fn(m, steps=len(te))
            scores.append(_smape(te.values, np.asarray(yh)[:len(te)]))
        except Exception:
            scores.append(999.0)
    return float(np.mean(scores)) if scores else 999.0

def _choose_model(y: pd.Series, measure: str) -> Tuple[str, np.ndarray]:
    reg = _model_registry()
    cands: List[Tuple[str, np.ndarray, Any, Any]] = []
    # always EMA/MA
    m_ema = _fit_ema(y); yhat_ema = _pred_ema(m_ema); cands.append(("EMA", yhat_ema, _fit_ema, _pred_ema))
    m_ma  = _fit_ma(y);  yhat_ma  = _pred_ma(m_ma);   cands.append(("MA",  yhat_ma,  _fit_ma,  _pred_ma))
    # ARIMA
    if reg["arima"]:
        try:
            m = _fit_arima(y); yhat = _pred_arima(m); cands.append(("ARIMA", yhat, _fit_arima, _pred_arima))
        except Exception:
            pass
    # Prophet: only for flow, enough data & seasonal
    if measure == "flow" and reg["prophet"] and len(y) >= 12 and _has_seasonality(y):
        try:
            m = _fit_prophet(y); yhat = _pred_prophet(m); cands.append(("Prophet", yhat, _fit_prophet, _pred_prophet))
        except Exception:
            pass
    # pick by CV
    scores = [(nm, _rolling_origin_cv(y, fit, pred)) for (nm, _, fit, pred) in cands]
    best = min(scores, key=lambda x: x[1])[0] if scores else "EMA"
    # return best in-sample prediction
    if best == "EMA": return "EMA", yhat_ema
    if best == "MA":  return "MA",  yhat_ma
    if best == "ARIMA":
        try: return "ARIMA", _pred_arima(_fit_arima(y))
        except Exception: return "EMA", yhat_ema
    if best == "Prophet":
        try: return "Prophet", _pred_prophet(_fit_prophet(y))
        except Exception: return "EMA", yhat_ema
    return "EMA", yhat_ema

# --------------------------- Core predictors --------------------------
def _prepare_monthly(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=[date_col]).copy()
    # ì›”ë§ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •
    _dates = pd.to_datetime(df[date_col], errors="coerce")
    p = _to_month_period_index(_dates)
    df2 = df.copy()
    df2["_p"] = p
    df2 = df2.dropna(subset=["_p"]).sort_values("_p")
    run = _longest_contiguous_month_run(df2["_p"])
    return df2[df2["_p"].isin(run)].reset_index(drop=True)

def _one_track_lastrow(
    monthly: pd.DataFrame,
    value_col: str,
    measure: str,
    pm_value: float
) -> Optional[Dict[str, Any]]:
    df = _prepare_monthly(monthly, "date")
    if df.empty or value_col not in df.columns: return None
    y = pd.Series(df[value_col].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2: return None
    model, yhat = _choose_model(y, measure=measure)
    resid = y.values - yhat
    error_last = float(resid[-1])
    sigma = _std_last(resid, w=6)
    z = float(error_last / sigma) if sigma > 0 else 0.0
    risk = _risk_score(abs(z), amount=float(y.iloc[-1]), pm=float(pm_value))
    return {
        "date": y.index[-1].to_timestamp(how='end'),
        "measure": measure,
        "actual": float(y.iloc[-1]),
        "predicted": float(yhat[-1]),
        "error": float(error_last),
        "z": float(z),
        "z_label": "resid_z",
        "risk": float(risk),
        "model": model,
    }

# ------------------------------- API ----------------------------------
def run_timeseries_for_account(
    monthly: pd.DataFrame,
    account: str,
    is_bs: bool,
    flow_col: str = "flow",
    balance_col: Optional[str] = None,
    allow_prophet: bool = True,   # kept for backward compatibility (no-op switch)
    pm_value: float = _PM_DEFAULT,
    **kwargs: Any,                # absorb legacy args safely
) -> pd.DataFrame:
    """
    ë‹¨ì¼ ê³„ì •ì˜ ì›”ë³„ ë°ì´í„°ì—ì„œ ë§ˆì§€ë§‰ í¬ì¸íŠ¸ë¥¼ í‰ê°€.
    - BS ê³„ì •: flow/balance 2í–‰(í•´ë‹¹ ì‹œ ì¡´ì¬) ë°˜í™˜
    - PL ê³„ì •: flow 1í–‰ ë°˜í™˜
    ë°˜í™˜ ì»¬ëŸ¼: ["date","account","measure","actual","predicted","error","z","risk","model"]
    """
    rows: List[Dict[str, Any]] = []
    # flow
    if flow_col in monthly.columns:
        r = _one_track_lastrow(monthly.rename(columns={flow_col: "val"}), "val", "flow", pm_value)
        if r: rows.append(r)
    # balance
    if is_bs:
        if balance_col and (balance_col in monthly.columns):
            r = _one_track_lastrow(monthly.rename(columns={balance_col: "val"}), "val", "balance", pm_value)
            if r: rows.append(r)
        else:
            if flow_col in monthly.columns:
                tmp = monthly[["date", flow_col]].copy()
                tmp["val"] = tmp[flow_col].astype(float).cumsum()
                r = _one_track_lastrow(tmp[["date","val"]], "val", "balance", pm_value)
                if r: rows.append(r)
    out = pd.DataFrame(rows)
    if not out.empty:
        out["account"] = account
        out = out[["date","account","measure","actual","predicted","error","z","risk","model"]]
        out = out.sort_values(["account","measure","date"]).reset_index(drop=True)
    else:
        out = pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"])
    return _ensure_ts_schema(out)

def run_timeseries_module(
    df: pd.DataFrame,
    *,
    account_col: str = "account",
    date_col: str = "date",
    amount_col: str = "amount",
    targets: Optional[List[str]] = None,
    pm_value: float = _PM_DEFAULT,
    make_balance: bool = False,  # ê¸°ë³¸ê°’ Falseë¡œ ë³€ê²½: í•„ìš” ì‹œ balance êµ¬ì„±
    output: str = "all",        # "all" | "flow" | "balance"
    evidence_adapter: Optional[Callable[[Dict[str, Any]], Any]] = None,
    **kwargs: Any,
) -> pd.DataFrame:
    """
    ì§‘ê³„í˜•: ê³„ì •ë³„ ì›”í•©ê³„(amount)ë§Œ ì£¼ì–´ì§„ ê²½ìš°.
    ê¸°ë³¸: flowë§Œ ê³„ì‚°. make_balance=Trueì¼ ë•Œ balance(ëˆ„ì í•©)ë„ í•¨ê»˜ ê³„ì‚°.
    outputìœ¼ë¡œ ìµœì¢… ë°˜í™˜ í•„í„°ë§ ê°€ëŠ¥("flow"/"balance").
    """
    if df is None or df.empty:
        return _ensure_ts_schema(pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"]))
    work = df[[account_col, date_col, amount_col]].copy()
    work.columns = ["account","date","amount"]
    work = work.sort_values(["account","date"])
    if targets:
        try:
            tgt = set(map(str, targets))
            work["account"] = work["account"].astype(str)
            work = work[work["account"].isin(tgt)]
        except Exception:
            pass
    # ìµœì†Œ í¬ì¸íŠ¸ ê°€ë“œ: ê³„ì •ë³„ date ìœ ë‹ˆí¬ê°€ ë¶€ì¡±í•˜ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
    MIN_POINTS = 6
    if work["date"].nunique() < MIN_POINTS:
        return _ensure_ts_schema(pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"]))
    all_rows: List[pd.DataFrame] = []
    for acc, g in work.groupby("account", dropna=False):
        mon = g[["date","amount"]].rename(columns={"amount":"flow"}).copy()
        if make_balance:
            mon["balance"] = mon["flow"].astype(float).cumsum()
        out = run_timeseries_for_account(mon, str(acc), is_bs=make_balance, flow_col="flow",
                                         balance_col=("balance" if make_balance else None),
                                         pm_value=float(pm_value))
        if not out.empty:
            # CEAVOP ì œì•ˆ(ê°„ë‹¨ ê·œì¹™): error>0 â†’ E(ì¡´ì¬), error<=0 â†’ C(ì™„ì „ì„±)
            try:
                out["assertion"] = out["error"].map(lambda e: "E" if float(e) > 0 else "C")
            except Exception:
                out["assertion"] = "E"
        if output in ("flow", "balance") and not out.empty:
            out = out[out["measure"] == output]
        all_rows.append(out)
    result = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=["account","date","measure","actual","predicted","error","z","risk","model"])
    if evidence_adapter is not None and not result.empty:
        rows = []
        for r in result.to_dict(orient="records"):
            d = dict(r)
            if "amount" not in d:
                d["amount"] = float(d.get("actual", 0.0))
            if "z_abs" not in d:
                try:
                    d["z_abs"] = abs(float(d.get("z", 0.0)))
                except Exception:
                    d["z_abs"] = 0.0
            if "assertion" not in d:
                try:
                    d["assertion"] = "E" if float(d.get("error", 0.0)) > 0 else "C"
                except Exception:
                    d["assertion"] = "E"
            rows.append(evidence_adapter(d))
        return rows  # type: ignore[return-value]
    return _ensure_ts_schema(result)

def run_timeseries_module_with_flag(
    df: pd.DataFrame,
    account_col: str,
    date_col: str,
    amount_col: str,
    account_name: str,
    is_bs: bool,
    backend: str = "ema",
    *,
    opening_map: Mapping[str, float] | None = None,
    return_mode: str = "insample",
) -> pd.DataFrame:
    """
    ê¸°ì¡´ ë‹¨ì¼ ì¶œë ¥ì—ì„œ í™•ì¥: BS ê³„ì •ì€ balance/flow dual ë¡œì§ ì ìš©.
    balance = opening(ì „ê¸°ë§ì”ì•¡ ë“±) + flow.cumsum()
    """
    if df is None or df.empty:
        return _ensure_ts_schema(pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"]))

    # ì›”ë³„ flow/balance ìƒì„± (ì›”ë§ 00:00:00 ë³´ì¥)
    acct = str(df[account_col].iloc[0]) if (account_col in df.columns and not df.empty) else account_name
    opening = 0.0
    if opening_map:
        opening = float(opening_map.get(acct, opening_map.get(account_name, 0.0)))
    flow_s, bal_s = _monthly_flow_and_balance(df, date_col, amount_col, opening=opening)

    def _run(track: str, s: pd.Series) -> pd.DataFrame:
        base = pd.DataFrame({"date": s.index, track: s.values}).sort_values("date")
        if return_mode == "lastrow":
            ins = base.tail(1).rename(columns={track: "actual"})
            ins["predicted"] = np.nan; ins["error"] = np.nan; ins["z"] = np.nan; ins["risk"] = np.nan
            ins["model"] = backend.upper()
        else:
            ins = insample_predict_df(base, value_col=track, measure=track, pm_value=_PM_DEFAULT)
        ins["account"] = account_name
        ins["measure"] = track
        ins["date"] = to_month_end_index(ins["date"])  # ì•ˆì „ ë³´ì •
        return ins[["date","account","measure","actual","predicted","error","z","risk","model"]]

    res = [_run("flow", flow_s)]
    if is_bs:
        res.append(_run("balance", bal_s))
    final_df = pd.concat(res, ignore_index=True).sort_values(["measure","date"]) if res else pd.DataFrame(columns=["date","account","measure","actual","predicted","error","z","risk","model"])
    return _ensure_ts_schema(final_df)

# ----------------------- Helper: in-sample prediction -------------------
def insample_predict_df(
    monthly: pd.DataFrame,
    value_col: str,
    measure: str,
    pm_value: float = _PM_DEFAULT,
) -> pd.DataFrame:
    """
    ì›”ë³„ ë°ì´í„°(monthly: ['date', value_col])ì— ëŒ€í•´ MoRì´ ê³ ë¥¸ ëª¨ë¸ì˜ in-sample ì˜ˆì¸¡ì„ ì„ ë°˜í™˜.
    ë°˜í™˜ ì»¬ëŸ¼: date, actual, predicted, model, train_months, data_span, sigma_win, measure, value_col
    """
    df = _prepare_monthly(monthly, "date")
    if df.empty or value_col not in df.columns:
        return pd.DataFrame(columns=["date","actual","predicted","model","train_months","data_span","sigma_win","measure","value_col"])
    y = pd.Series(df[value_col].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2:
        return pd.DataFrame(columns=["date","actual","predicted","model","train_months","data_span","sigma_win","measure","value_col"])
    model, yhat = _choose_model(y, measure=measure)
    span = f"{y.index[0].strftime('%Y-%m')} ~ {y.index[-1].strftime('%Y-%m')}"
    # ì›”ë§ ê³ ì • + ì´ˆ ë‹¨ìœ„ë¡œ ë‚´ë¦¼
    idx = to_month_end_index(y.index)
    out = pd.DataFrame({
        "date": idx,
        "actual": y.values,
        "predicted": yhat,
        "model": model,
    })
    out["train_months"] = len(y)
    out["data_span"] = span
    out["sigma_win"] = 6  # z ê³„ì‚°ì— ì“°ëŠ” ìµœê·¼ ë¶„ì‚° ìœˆë„ìš°
    out["measure"] = str(measure)
    out["value_col"] = str(value_col)
    return out


# ------------------- Validation Builder (tidy helper) -------------------
def build_trend_validation_data(
    monthly: pd.DataFrame,
    *,
    flow_col: str = "flow",
    balance_col: Optional[str] = None,
    is_bs: bool = False,
    pm_value: float = _PM_DEFAULT,
) -> pd.DataFrame:
    """
    íŠ¸ë Œë“œ ê²€ì¦ìš© tidy ë°ì´í„° ìƒì„±:
      columns â†’ ['date','actual','predicted','measure','model']
    - PL: flowë§Œ
    - BS: flow + balance(ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ flow ëˆ„ì í•©ìœ¼ë¡œ ìƒì„±)
    """
    rows: List[pd.DataFrame] = []

    # flow
    if flow_col in monthly.columns:
        df_flow = insample_predict_df(
            monthly[["date", flow_col]].rename(columns={flow_col: "val"}),
            value_col="val",
            measure="flow",
            pm_value=pm_value,
        )
        if not df_flow.empty:
            rows.append(df_flow[["date","actual","predicted","measure","model"]])

    # balance (BSë§Œ) â€” balanceëŠ” 'ì›”ë§' ì‹œì  ê¸°ì¤€
    if is_bs:
        if balance_col and (balance_col in monthly.columns):
            base = monthly[["date", balance_col]].rename(columns={balance_col: "val"})
        else:
            # ëˆ„ì í•©ìœ¼ë¡œ balance ê°€ìƒ ìƒì„±(ì›”ë§ ì‹œì ìœ¼ë¡œ í‘œì‹œë¨)
            if flow_col not in monthly.columns:
                base = None
            else:
                tmp = monthly[["date", flow_col]].copy()
                tmp["val"] = tmp[flow_col].astype(float).cumsum()
                base = tmp[["date","val"]]
        if base is not None:
            df_bal = insample_predict_df(base, value_col="val", measure="balance", pm_value=pm_value)
            if not df_bal.empty:
                rows.append(df_bal[["date","actual","predicted","measure","model"]])

    if not rows:
        return pd.DataFrame(columns=["date","actual","predicted","measure","model"])
    return pd.concat(rows, ignore_index=True)

# ------------------- Validation: reconcile with trend -------------------
def reconcile_with_trend(
    ts_flow: pd.Series,
    ts_bal: pd.Series,
    trend_flow: pd.Series,
    trend_bal: pd.Series,
    tol: int = 1,
) -> pd.DataFrame:
    """
    timeseries ì…ë ¥(flow/balance)ê³¼ trend ì‚°ì¶œì¹˜(flow/balance)ë¥¼ ì›”ë³„ ëŒ€ì¡°.
    tol ì ˆëŒ€ì°¨(ì›) ì´ˆê³¼ì¸ í–‰ë§Œ ë°˜í™˜.
    """
    import pandas as _pd
    idx = sorted(set(_pd.to_datetime(getattr(ts_flow, 'index', [])).tolist()) |
                 set(_pd.to_datetime(getattr(trend_flow, 'index', [])).tolist()))
    rows = []
    for d in idx:
        af = int(_pd.Series(ts_flow).get(d, 0))
        tf = int(_pd.Series(trend_flow).get(d, 0))
        ab = int(_pd.Series(ts_bal).get(d, 0))
        tb = int(_pd.Series(trend_bal).get(d, 0))
        rows.append({
            "month": d,
            "flow(ts)": af, "flow(trend)": tf, "Î”flow": af - tf,
            "bal(ts)": ab,  "bal(trend)": tb,  "Î”bal": ab - tb,
        })
    df_chk = pd.DataFrame(rows).set_index("month")
    return df_chk[(df_chk["Î”flow"].abs() > tol) | (df_chk["Î”bal"].abs() > tol)]


# ------------------- Optional: lightweight validation summary -----------
def validation_summary(
    monthly: pd.DataFrame,
    *,
    date_col: str = "date",
    value_col: str = "amount",
    pm_value: float = _PM_DEFAULT,
    last_k: int = 6,
) -> Dict[str, Any]:
    """ë§‰ëŒ€ ëŒ€ì¡° ëŒ€ì‹  ì“°ëŠ” ê²½ëŸ‰ ìˆ«ì ì§„ë‹¨ ì¹´ë“œ."""
    df = monthly[[date_col, value_col]].rename(columns={date_col: "date", value_col: "val"}).copy()
    df = _prepare_monthly(df, "date")
    if df.empty or "val" not in df.columns:
        return {"n_points": 0}
    y = pd.Series(df["val"].astype(float).values, index=pd.PeriodIndex(df["_p"], freq="M"))
    if len(y) < 2:
        return {"n_points": int(len(y))}
    scores = [("EMA", _rolling_origin_cv(y, _fit_ema, _pred_ema)),
              ("MA",  _rolling_origin_cv(y, _fit_ma,  _pred_ma))]
    reg = _model_registry()
    if reg["arima"]:
        try: scores.append(("ARIMA", _rolling_origin_cv(y, _fit_arima, _pred_arima)))
        except Exception: pass
    if reg["prophet"] and len(y) >= 12 and _has_seasonality(y):
        try: scores.append(("Prophet", _rolling_origin_cv(y, _fit_prophet, _pred_prophet)))
        except Exception: pass
    best_cv_name, best_cv_smape = min(scores, key=lambda x: x[1])
    model_name, yhat = _choose_model(y, measure="flow")
    resid = y.values - yhat
    k = min(last_k, len(y))
    smape_k = _smape(y.values[-k:], yhat[-k:])
    last_err = float(resid[-1])
    sigma = _std_last(resid, w=6)
    z = float(last_err / sigma) if sigma > 0 else 0.0
    pm_ratio = (abs(float(y[-1])) / float(pm_value)) if pm_value else 0.0
    return {
        "n_points": int(len(y)),
        "mor": model_name,
        "cv_smape": float(best_cv_smape),
        "smape_last_k": float(smape_k),
        "last_month": str(y.index[-1].to_timestamp(how='start').date()),
        "last_actual": float(y[-1]),
        "last_pred": float(yhat[-1]),
        "last_error": last_err,
        "last_z": z,
        "pm_ratio": pm_ratio,
    }


# ========================================================================
# ============= NEW: Functions moved from app.py for Refactoring =========
# ========================================================================

# ----------------------- TS Diagnostics (moved from app.py) -------------

def _adf_stationary(y_vals: np.ndarray) -> Tuple[bool, float]:
    """ADF test for stationarity."""
    try:
        from statsmodels.tsa.stattools import adfuller
        y_clean = np.asarray(y_vals, dtype=float)
        # ADF requires finite values and sufficient length
        if not np.all(np.isfinite(y_clean)) or len(y_clean) < 3:  # ADF needs at least 3 points for default settings
            return (False, np.nan)

        p = float(adfuller(y_clean)[1])
        return (p < 0.05, p)  # True=ì •ìƒì„± í™•ë³´
    except Exception:
        # ê°„ë‹¨ í´ë°± (ì›ë³¸ ë¡œì§ ìœ ì§€)
        y = np.asarray(y_vals, dtype=float)
        if len(y) < 6:
            return (False, np.nan)

        std_orig = np.nanstd(y)
        std_diff = np.nanstd(np.diff(y))

        if not math.isfinite(std_orig) or not math.isfinite(std_diff):
            return (False, np.nan)

        # Avoid division by zero if original std is 0
        if std_orig == 0:
            return (std_diff == 0, np.nan)

        return (std_diff < 0.9 * std_orig, np.nan)

def _has_seasonality_safe(y_vals: np.ndarray) -> bool:
    """Safe wrapper for seasonality check."""
    try:
        return bool(_has_seasonality(pd.Series(y_vals)))
    except Exception:
        return False

# ----------------------- TS Visualization (moved from app.py) -----------

def create_timeseries_figure(
    df_hist: pd.DataFrame,
    measure: str,
    title: str,
    pm_value: float,
    show_dividers: bool = False
) -> Tuple[Optional[go.Figure], Optional[Dict[str, Any]]]:
    """
    Creates a timeseries figure (actual vs predicted) and returns the figure and stats.
    (Refactored from app.py's _make_ts_fig_with_stats, removing Streamlit dependencies)
    """
    vcol = 'flow' if measure == 'flow' else 'balance'
    work = df_hist.copy()
    if "date" not in work.columns:
        return None, {"error": "Column 'date' not found in data."}
    work["date"] = pd.to_datetime(work["date"], errors="coerce")
    work = work.dropna(subset=["date"]).sort_values("date")

    # 1) ëª¨ë¸ë§ëœ ì…ë ¥(actual/predicted)ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if {"actual","predicted"}.issubset(work.columns):
        cols = ["date","actual","predicted"] + (["model"] if "model" in work.columns else [])
        ins = work[cols].copy()
    else:
        # 2) ê°’ ì»¬ëŸ¼ì„ ì¡ì•„ in-sample ì˜ˆì¸¡ì„  ìƒì„±
        if vcol in work.columns:
            base = work[["date", vcol]].rename(columns={vcol: "val"})
        elif "actual" in work.columns:
            base = work[["date","actual"]].rename(columns={"actual":"val"})
        elif "value" in work.columns:
            base = work[["date","value"]].rename(columns={"value":"val"})
        else:
            return None, {"error": f"Neither '{vcol}' nor 'actual'/'value' column found."}
        base = base.dropna(subset=["val"])  # ì•ˆì „ ê°€ë“œ
        ins = insample_predict_df(
            base.rename(columns={"val": vcol}),
            value_col=vcol,
            measure=measure,
            pm_value=float(pm_value)
        )

    if ins.empty or len(ins) < 2:
        reason = "points<2" if (not ins.empty) else "empty"
        return None, {"error": f"Insufficient data for plot ({reason}).",
                      "diagnostics": {"n_months": int(len(ins))}}

    # --- 1. Figure Creation ---
    fig = go.Figure()
    # Using styles consistent with the original app.py
    fig.add_trace(go.Scatter(x=ins['date'], y=ins['actual'], mode='lines', name='actual'))
    fig.add_trace(go.Scatter(x=ins['date'], y=ins['predicted'], mode='lines', name='predicted', line=dict(dash='dot')))

    fig.update_layout(
        title=title,
        xaxis_title='month',
        yaxis_title='ì›'
    )
    fig.update_yaxes(tickformat=",.0f", separatethousands=True, ticksuffix="")

    # Add PM threshold
    try:
        fig = add_materiality_threshold(fig, float(pm_value))
    except Exception:
        pass  # Proceed without PM line if utility fails

    # Add time dividers (Year/Quarter) â€“ ê¸°ë³¸ OFF
    if show_dividers:
        try:
            fig = add_period_guides(fig, ins['date'])
        except Exception:
            pass  # Proceed without dividers if utility fails

    # --- 2. Statistics Calculation (Logic preserved from app.py) ---
    stats_output: Dict[str, Any] = {}
    y_vals = np.asarray(ins['actual'].values, dtype=float)
    n_months = int(np.isfinite(y_vals).sum())

    # Diagnostics (Seasonality, Stationarity)
    seas = _has_seasonality_safe(y_vals)
    stat_ok, pval = _adf_stationary(y_vals)

    stats_output["diagnostics"] = {
        "seasonality": seas,
        "stationary": stat_ok,
        "p_value": pval,
        "n_months": n_months,
        "is_short": n_months < 12,
    }

    # Model Metrics (MAE, MAPE, AIC/BIC)
    # Sticking to the original definitions used in app.py
    mae = float(np.mean(np.abs(ins['actual'] - ins['predicted'])))
    # Handle division by zero safely within the mean calculation
    actuals = ins['actual']
    predicted = ins['predicted']
    mape = float(np.mean(np.where(actuals != 0, np.abs((actuals - predicted) / actuals) * 100, 0)))

    aic = bic = np.nan
    best_model_name = str(ins['model'].iloc[-1]) if ('model' in ins.columns and not ins.empty) else "EMA"

    if best_model_name.upper() == "ARIMA":
        try:
            # Refit ARIMA to get AIC/BIC (as done in the original app.py)
            _y = ins['actual'].reset_index(drop=True)
            _ar = _fit_arima(_y)
            aic = float(getattr(_ar, "aic", np.nan))
            bic = float(getattr(_ar, "bic", np.nan))
        except Exception:
            pass

    # Trend analysis and Seasonality strength (logic moved exactly from app.py)
    recent_trend = False
    # Using y_vals (already defined as float array)
    if n_months >= 6:
        x = np.arange(len(y_vals))
        # Original logic was potentially unsafe with NaNs, but preserving it as requested
        try:
            slope = np.polyfit(x, y_vals, 1)[0]
            recent_trend = abs(slope) > 0.3 * (y_vals.std() + 1e-9)
        except Exception:
            pass  # Handle potential issues during polyfit

    try:
        # Calculate seasonality strength (logic moved exactly from app.py)
        ac = np.abs(np.fft.rfft((y_vals - y_vals.mean())))
        core = ac[2:] if ac.size > 2 else ac
        seas_strength_raw = float(core.max() / (core.mean() + 1e-9)) if core.size else 0.0
        seas_strength = max(0.0, min((seas_strength_raw - 1.0) / 4.0, 1.0))
    except Exception:
        seas_strength = 0.0

    # Prepare diagnostics dictionary for model_reason_text
    diagnostics_for_reasoning = {
        "n_points": n_months,
        "seasonality_strength": seas_strength,
        "stationary": bool(stat_ok),
        "recent_trend": bool(recent_trend),
        "cv_mape_rank": 1,  # Assuming this is the best model (Rank 1)
        "mae": mae, "mape": mape, "aic": aic, "bic": bic,
    }

    # Get model reasoning text
    reasoning_text = model_reason_text(best_model_name, diagnostics_for_reasoning)

    # Metadata
    train_months = int(ins['train_months'].iloc[-1]) if 'train_months' in ins.columns else int(len(ins))
    if 'data_span' in ins.columns and not ins['data_span'].empty:
        span_txt = str(ins['data_span'].iloc[-1])
    else:
        try:
            dmin = ins['date'].min(); dmax = ins['date'].max()
            span_txt = f"{dmin:%Y-%m} ~ {dmax:%Y-%m}"
        except Exception:
            span_txt = "-"
    sigma_window = int(ins['sigma_win'].iloc[-1]) if 'sigma_win' in ins.columns else 6

    stats_output["metrics"] = {"mae": mae, "mape": mape, "aic": aic, "bic": bic}
    stats_output["metadata"] = {
        "model": best_model_name, "train_months": train_months,
        "data_span": span_txt, "sigma_window": sigma_window,
        "reasoning": reasoning_text,
    }

    # Detailed stats for the "expander" view in UI
    stats_output["details"] = {
        "ëª¨ë¸": best_model_name, "í•™ìŠµê¸°ê°„(ì›”)": train_months, "ë°ì´í„° êµ¬ê°„": span_txt,
        "Ïƒ ìœˆë„ìš°(ìµœê·¼)": sigma_window, "CV(K)": 3,
    }

    return fig, stats_output


# --------------------------- Lightweight series API ---------------------------
def build_series(ledger, accounts: List[str]) -> Tuple[pd.DataFrame, str]:
    """
    ì›ì¥(ledger.df)ì—ì„œ ë‚ ì§œ/ê¸ˆì•¡ ì»¬ëŸ¼ì„ ìë™ íƒìƒ‰í•˜ì—¬ ì›”ë³„ ì‹œê³„ì—´ì„ ìƒì„±.
    - ì •ìƒ(ledger ëª¨ë“œ): ì›ì¥ì—ì„œ ì›”ë³„ í•©ê³„ë¥¼ ì‚°ì¶œí•˜ì—¬ tidy ë°˜í™˜
    - í´ë°±(master ëª¨ë“œ): meta.master_dfì˜ ì”ì•¡ 3í¬ì¸íŠ¸(ì „ì „ê¸°ë§/ì „ê¸°ë§/ë‹¹ê¸°ë§)ë¥¼ ë°˜í™˜

    ë°˜í™˜: (df, mode)
      df columns â†’ ['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','month','value']
      mode â†’ 'ledger' | 'master'
    """
    df = getattr(ledger, 'df', None)
    if df is None or df.empty:
        return pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','month','value']), 'master'

    # ê³„ì • í•„í„° ì¤€ë¹„(ë¬¸ìì—´ í†µì¼)
    accounts = list(map(str, accounts or []))
    work = df.copy()
    if 'ê³„ì •ì½”ë“œ' in work.columns:
        try:
            work['ê³„ì •ì½”ë“œ'] = work['ê³„ì •ì½”ë“œ'].astype(str)
        except Exception:
            pass

    date_col = _pick_col(work, DATE_CANDIDATES)
    amt_col  = _pick_col(work, AMT_CANDIDATES)

    if date_col and amt_col and ('ê³„ì •ì½”ë“œ' in work.columns) and ('ê³„ì •ëª…' in work.columns):
        # ë‚ ì§œ/ê¸ˆì•¡ ì•ˆì „ ì •ê·œí™”
        work[date_col] = pd.to_datetime(work[date_col], errors='coerce')
        work[amt_col] = pd.to_numeric(work[amt_col], errors='coerce')
        work = work.dropna(subset=[date_col, amt_col, 'ê³„ì •ì½”ë“œ', 'ê³„ì •ëª…'])

        # ëŒ€ìƒ ê³„ì • í•„í„° â†’ ì›”ë§ ê³ ì •
        tmp = work[work['ê³„ì •ì½”ë“œ'].astype(str).isin(accounts)].copy()
        if tmp.empty:
            return pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','month','value']), 'ledger'
        tmp['_month'] = tmp[date_col].dt.to_period('M').dt.to_timestamp(how='end').floor('D')
        mon = (tmp.groupby(['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','_month'], as_index=False)[amt_col].sum())
        mon = mon.rename(columns={'_month': 'month', amt_col: 'value'}).sort_values(['ê³„ì •ì½”ë“œ','month'])
        return mon, 'ledger'

    # --- í´ë°±: master ì”ì•¡ 3í¬ì¸íŠ¸ ---
    m = getattr(ledger, 'meta', {}).get('master_df') if hasattr(ledger, 'meta') else None
    need = {'ì „ì „ê¸°ë§ì”ì•¡','ì „ê¸°ë§ì”ì•¡','ë‹¹ê¸°ë§ì”ì•¡'}
    if m is None or not need.issubset(set(m.columns)):
        # ì™„ì „ í´ë°± ì‹¤íŒ¨ ì‹œ ë¹ˆ í”„ë ˆì„ ë°˜í™˜(í˜¸ì¶œì¸¡ì—ì„œ ë©”ì‹œì§€ ì²˜ë¦¬)
        return pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','month','value']), 'master'
    try:
        cy = int(getattr(ledger, 'meta', {}).get('CY', pd.Timestamp.today().year))
    except Exception:
        cy = pd.Timestamp.today().year
    dates = [pd.Timestamp(cy-2, 12, 31), pd.Timestamp(cy-1, 12, 31), pd.Timestamp(cy, 12, 31)]
    m2 = m.copy()
    try:
        m2['ê³„ì •ì½”ë“œ'] = m2['ê³„ì •ì½”ë“œ'].astype(str)
    except Exception:
        pass
    mm = m2[m2['ê³„ì •ì½”ë“œ'].isin(accounts)][['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','ì „ì „ê¸°ë§ì”ì•¡','ì „ê¸°ë§ì”ì•¡','ë‹¹ê¸°ë§ì”ì•¡']]
    rows: List[pd.DataFrame] = []
    for _, r in mm.iterrows():
        vals = [r.get('ì „ì „ê¸°ë§ì”ì•¡', 0), r.get('ì „ê¸°ë§ì”ì•¡', 0), r.get('ë‹¹ê¸°ë§ì”ì•¡', 0)]
        rows.append(pd.DataFrame({
            'ê³„ì •ì½”ë“œ': r['ê³„ì •ì½”ë“œ'],
            'ê³„ì •ëª…' : r['ê³„ì •ëª…'],
            'month' : dates,
            'value' : vals
        }))
    g = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['ê³„ì •ì½”ë“œ','ê³„ì •ëª…','month','value'])
    return g, 'master'



==============================
ğŸ“„ FILE: analysis/trend.py
==============================

import pandas as pd
import plotly.express as px
from typing import List, Dict, Any, Optional
from analysis.contracts import LedgerFrame, ModuleResult
from utils.viz import add_materiality_threshold
from utils.helpers import is_credit_account


def create_monthly_trend_figure(ledger_df: pd.DataFrame, master_df: pd.DataFrame, account_code: str, account_name: str):
    """BS/PL, ì°¨/ëŒ€ë³€ ì„±ê²©ì„ ë°˜ì˜í•˜ì—¬ ì›”ë³„ ì¶”ì´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    mrow = master_df[master_df['ê³„ì •ì½”ë“œ'] == account_code]
    if mrow.empty:
        return None  # ì•ˆì „ ê°€ë“œ
    master_row = mrow.iloc[0]
    bspl = str(master_row.get('BS/PL', 'PL') or 'PL').upper()
    dc = master_row.get('ì°¨ë³€/ëŒ€ë³€', None)
    # ëŒ€ë³€ ì„±ê²©ì´ë©´ ê·¸ë˜í”„ ë¶€í˜¸ë¥¼ ë’¤ì§‘ì–´ ì‹œê°í™”
    sign = -1.0 if is_credit_account(bspl, dc) else 1.0

    if 'ì—°ë„' not in ledger_df.columns or ledger_df['ì—°ë„'].isna().all():
        return None
    current_year = int(ledger_df['ì—°ë„'].max())
    df_filtered = ledger_df[(ledger_df['ê³„ì •ì½”ë“œ'] == account_code) & (ledger_df['ì—°ë„'].isin([current_year, current_year - 1]))]
    months = list(range(1, 13))
    plot_df_list = []

    if bspl == 'BS':
        def _f(x):
            try:
                v = float(x)
                return 0.0 if pd.isna(v) else v
            except Exception:
                return 0.0
        bop_cy = _f(master_row.get('ì „ê¸°ë§ì”ì•¡', 0))
        bop_py = _f(master_row.get('ì „ì „ê¸°ë§ì”ì•¡', 0))
        for year, bop, year_label in [(current_year, bop_cy, 'CY'), (current_year - 1, bop_py, 'PY')]:
            monthly_flow = df_filtered[df_filtered['ì—°ë„'] == year].groupby('ì›”')['ê±°ë˜ê¸ˆì•¡'].sum() if 'ê±°ë˜ê¸ˆì•¡' in df_filtered.columns else pd.Series(dtype=float)
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(monthly_flow)
            monthly_balance = bop + monthly_series.cumsum()
            plot_df_list.append(pd.DataFrame({'ì›”': months, 'ê¸ˆì•¡': monthly_balance.values * sign, 'êµ¬ë¶„': year_label}))
        title_suffix = "ì›”ë³„ ì”ì•¡ ì¶”ì´ (BS Â· ì›”ë§)"
    else:
        # PL: ê¸ˆì•¡ ì»¬ëŸ¼ ìœ ì—° ì¸ì‹
        cand = ['ê±°ë˜ê¸ˆì•¡', 'ë°œìƒì•¡', 'ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’', 'amount', 'ê¸ˆì•¡']
        amt_col = next((c for c in cand if c in df_filtered.columns), None)
        if amt_col is None:
            return None
        monthly_sum = df_filtered.groupby(['ì—°ë„', 'ì›”'])[amt_col].sum().reset_index()
        for year, year_label in [(current_year, 'CY'), (current_year - 1, 'PY')]:
            year_data = monthly_sum[monthly_sum['ì—°ë„'] == year]
            monthly_series = pd.Series(index=months, data=0.0)
            monthly_series.update(year_data.set_index('ì›”')[amt_col])
            plot_df_list.append(pd.DataFrame({'ì›”': months, 'ê¸ˆì•¡': monthly_series.values * sign, 'êµ¬ë¶„': year_label}))
        title_suffix = "ì›”ë³„ ë°œìƒì•¡ ì¶”ì´ (PL Â· ì›”í•©ê³„)"

    if not plot_df_list:
        return None

    plot_df = pd.concat(plot_df_list)
    fig = px.bar(
        plot_df,
        x='ì›”', y='ê¸ˆì•¡', color='êµ¬ë¶„', barmode='group',
        title=f"'{account_name}' ({account_code}) {title_suffix}",
        labels={'ì›”': 'ì›”', 'ê¸ˆì•¡': 'ê¸ˆì•¡', 'êµ¬ë¶„': 'ì—°ë„'},
        color_discrete_map={'PY': '#a9a9a9', 'CY': '#1f77b4'}
    )
    fig.update_xaxes(dtick=1)
    # ğŸ”¢ ì¶•/íˆ´íŒ í¬ë§·: ì²œë‹¨ìœ„ ì‰¼í‘œ, SI ë‹¨ìœ„ ì œê±°
    fig.update_yaxes(separatethousands=True, tickformat=',.0f', showexponent='none', exponentformat='none')
    _note = "ì‹œì =ì›”ë§" if bspl == 'BS' else "ì§‘ê³„=ì›”í•©ê³„"
    fig.update_traces(hovertemplate=f'ì›”=%{{x}}<br>ê¸ˆì•¡=%{{y:,.0f}} ì›<br>{_note}<br>êµ¬ë¶„=%{{fullData.name}}<extra></extra>')
    fig.update_layout(xaxis_title="ì›”", yaxis_title="ê¸ˆì•¡(ì›)")
    return fig


# (ì œê±°ë¨) ìë™ ì¶”ì²œ ë¡œì§: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì„ íƒí•œ ê³„ì •ë§Œ ì‚¬ìš©


def run_trend_module(lf: LedgerFrame, accounts: Optional[List[str]] = None) -> ModuleResult:
    """ì›”ë³„ ì¶”ì´ ëª¨ë“ˆ: ì„ íƒ ê³„ì • í•„í„° + ì›”ë³„ ë°œìƒì•¡ tables ì œê³µ."""
    df = lf.df.copy()
    master_df = lf.meta.get("master_df")
    if master_df is None:
        return ModuleResult(
            name="trend",
            summary={},
            tables={},
            figures={},
            evidences=[],
            warnings=["Master DFê°€ ì—†ìŠµë‹ˆë‹¤."]
        )

    # ê³„ì • í•„í„° ê°•ì œ
    acc_codes = [str(a) for a in (accounts or [])]
    if acc_codes:
        df = df[df['ê³„ì •ì½”ë“œ'].astype(str).isin(acc_codes)]

    # ì›”ë³„ ë°œìƒì•¡ ì§‘ê³„ í…Œì´ë¸”
    try:
        if 'ë°œìƒì•¡' not in df.columns:
            # ìœ ì—° ì¸ì‹: ê±°ë˜ê¸ˆì•¡/ê¸ˆì•¡ ë“±ì—ì„œ ëŒ€ì²´ ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
            cand = ['ê±°ë˜ê¸ˆì•¡', 'ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’', 'amount', 'ê¸ˆì•¡']
            alt = next((c for c in cand if c in df.columns), None)
            if alt is not None:
                _df = df.rename(columns={alt: 'ë°œìƒì•¡'})
            else:
                _df = df.copy()
        else:
            _df = df.copy()
        _df['ì›”'] = _df['íšŒê³„ì¼ì'].dt.to_period('M').astype(str)
        flow_tbl = (
            _df.groupby(['ê³„ì •ì½”ë“œ', 'ì›”'])['ë°œìƒì•¡'].sum().reset_index().rename(columns={'ë°œìƒì•¡': 'ì›”ë³„ë°œìƒì•¡'})
        )
    except Exception:
        flow_tbl = df.head(0).copy()

    figures: Dict[str, Any] = {}
    warns: List[str] = []
    pm_value = (lf.meta or {}).get("pm_value")
    for code in sorted(flow_tbl['ê³„ì •ì½”ë“œ'].astype(str).unique().tolist() if not flow_tbl.empty else acc_codes):
        m = master_df[master_df['ê³„ì •ì½”ë“œ'].astype(str) == code]
        if m.empty:
            warns.append(f"ê³„ì •ì½”ë“œ {code}ê°€ Masterì— ì—†ìŠµë‹ˆë‹¤.")
            continue
        name = m.iloc[0].get('ê³„ì •ëª…', str(code))
        fig = create_monthly_trend_figure(df, master_df, code, name)
        if fig:
            if pm_value:
                add_materiality_threshold(fig, pm_value=pm_value)
            figures[f"{code}:{name}"] = fig
        else:
            warns.append(f"{name}({code}) ê·¸ë¦¼ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±).")

    summary = {
        "picked_accounts": acc_codes or sorted(flow_tbl['ê³„ì •ì½”ë“œ'].astype(str).unique().tolist() if not flow_tbl.empty else []),
        "n_figures": len(figures),
        "period_tag_coverage": dict(df['period_tag'].value_counts()) if 'period_tag' in df.columns else {},
    }

    return ModuleResult(
        name="trend",
        summary=summary,
        tables={"monthly_flow": flow_tbl},
        figures=figures,
        evidences=[],
        warnings=warns
    )




==============================
ğŸ“„ FILE: analysis/ts_v2.py
==============================

# analysis/ts_v2.py
from __future__ import annotations
import numpy as np
import pandas as pd
from .aggregation import aggregate_monthly, month_end_00


def run_timeseries_minimal(df, *, account_name, date_col, amount_col, is_bs, opening=0.0, pm_value=0.0):
    x = df[[date_col, amount_col]].copy()
    x.rename(columns={date_col: "date", amount_col: "amount"}, inplace=True)
    x["date"] = month_end_00(pd.to_datetime(x["date"], errors="coerce"))
    x["amount"] = pd.to_numeric(x["amount"], errors="coerce").fillna(0.0)
    x = x.dropna(subset=["date"])

    g = (x.groupby("date", as_index=False)["amount"].sum()
           .sort_values("date").reset_index(drop=True))

    y = g["amount"].astype(float)
    
    # === ë¯¸ë˜êµ¬ê°„ ê²Œì´íŠ¸(í•™ìŠµ N<6 â†’ horizon=0) ===
    n = int(len(y))
    min_pts = 6
    
    # === MoR(ìë™ ì„ íƒ) ë¡œê·¸ ë…¸ì¶œ(EMA vs MA ë¹„êµÂ·ì„ ì • ê·¼ê±°) ===
    candidates = []
    
    # EMA í›„ë³´ë“¤
    for alpha in [0.3, 0.5]:
        span_ema = int(2/alpha - 1)
        pred_ema = y.ewm(span=span_ema, adjust=False).mean().shift(1)
        err_ema = y - pred_ema
        mae_ema = err_ema.abs().mean()
        mape_ema = (err_ema.abs() / y.abs().replace(0, np.nan)).mean() * 100
        candidates.append({
            'name': f'EMA(Î±={alpha})',
            'pred': pred_ema,
            'mae': mae_ema,
            'mape': mape_ema if not np.isnan(mape_ema) else 999.0
        })
    
    # MA í›„ë³´ë“¤
    for w in [3, 6]:
        if len(y) >= w:
            pred_ma = y.rolling(window=w).mean().shift(1)
            err_ma = y - pred_ma
            mae_ma = err_ma.abs().mean()
            mape_ma = (err_ma.abs() / y.abs().replace(0, np.nan)).mean() * 100
            candidates.append({
                'name': f'MA({w})',
                'pred': pred_ma,
                'mae': mae_ma,
                'mape': mape_ma if not np.isnan(mape_ma) else 999.0
            })
    
    # ìµœì  ì„ íƒ: MAPE ìµœì†Œ (tie -> ì‘ì€ MAE)
    best = min(candidates, key=lambda x: (x['mape'], x['mae']))
    winner_name = best['name']
    best_mape = best['mape']
    best_mae = best['mae']
    
    # MoR ë¡œê·¸ ìƒì„±
    mor_log = {
        "winner": winner_name,
        "metric": "MAPE",
        "mape_best": float(best_mape),
        "mae_best": float(best_mae),
        "n_months": n,
        "used": winner_name
    }
    
    # ì„ íƒëœ ì˜ˆì¸¡ê°’ ì‚¬ìš©
    pred_flow = best['pred'] if n >= min_pts else y  # í¬ì¸íŠ¸ ë¶€ì¡±ì‹œ ì™„ì „ in-sample
    
    err  = y - pred_flow
    sig  = err.rolling(window=min(6, max(2, len(err))), min_periods=2).std(ddof=0)
    z    = err / sig.replace(0, np.nan)

    kit      = (y.abs() > float(pm_value)).astype(float)
    pm_ratio = np.minimum(1.0, err.abs() / float(pm_value) if pm_value else 0.0)
    risk     = np.minimum(1.0, 0.5 * (z.abs() / 3.0) + 0.3 * pm_ratio + 0.2 * kit)

    flow = g.assign(
        account=str(account_name),
        measure="flow",
        actual=y.values,
        predicted=pred_flow.values,
        error=err.values,
        z=z.values,
        risk=risk.values,
        model=winner_name,  # í‘œ/íˆ´íŒì— ë…¸ì¶œ
    )

    if is_bs:
        # === Balance ì•ˆì „ê°€ë“œ: opening + ëˆ„ì ë§Œ ì‚¬ìš© ===
        # opening ì—†ìœ¼ë©´ 0.0 (ì•ˆì „ê°€ë“œ)
        opening_safe = 0.0 if opening is None or pd.isna(opening) else float(opening)
        
        # flow_actual: ì›”ë³„ í•©ê³„(ë¶€í˜¸ë³´ì • í¬í•¨) ì‹œë¦¬ì¦ˆ
        flow_actual = flow["actual"].astype(float)
        flow_pred = flow["predicted"].astype(float)
        
        # BalanceëŠ” opening + cumsum(flow)ë§Œ ì‚¬ìš©
        balance_actual = opening_safe + flow_actual.cumsum()
        balance_pred = opening_safe + flow_pred.cumsum()
        
        bal = flow.copy()
        bal["measure"] = "balance"
        bal["actual"] = balance_actual.values
        bal["predicted"] = balance_pred.values
        bal["error"] = bal["actual"] - bal["predicted"]
        
        # balance error/z/riskëŠ” flowì™€ ë™ì¼ ë¡œì§ìœ¼ë¡œ íŒŒìƒ(í‘œì¤€í™” ì°½ ë™ì¼ k=6)
        bal_sig = bal["error"].rolling(window=min(6, max(2, len(bal))), min_periods=2).std(ddof=0)
        bal["z"] = bal["error"] / bal_sig.replace(0, np.nan)
        bal["risk"] = np.minimum(1.0, 0.5 * (bal["z"].abs() / 3.0) + 0.2 * (bal["actual"].abs() > float(pm_value)).astype(float))
        
        out = pd.concat([flow, bal], ignore_index=True)
    else:
        out = flow

    # === ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ê³„ì•½(Contract) ê³ ì • + ë‚ ì§œ ì •ê·œí™” ===
    # í•„ìˆ˜ 9ì»¬ëŸ¼ ë³´ì¥
    need_cols = ["date","account","measure","actual","predicted","error","z","risk","model"]
    for c in need_cols:
        if c not in out.columns:
            out[c] = np.nan

    # ë‚ ì§œ ì •ê·œí™” (ì›”ë§ 00:00:00)
    out["date"] = month_end_00(pd.to_datetime(out["date"], errors="coerce"))

    # ìˆ«ìí˜• ê°•ì œ(í‘œ í¬ë§·/ë‹¤ìš´ë¡œë“œ ì•ˆì •)
    num_cols = ["actual","predicted","error","z","risk"]
    out[num_cols] = out[num_cols].apply(pd.to_numeric, errors="coerce")
    
    # ë¬¸ìí˜• ê°•ì œ
    out["model"]   = out["model"].astype(str)
    out["measure"] = out["measure"].astype(str)
    out["account"] = out["account"].astype(str)

    # ì»¬ëŸ¼ ìˆœì„œ ê°•ì œ
    out = out[need_cols]
    
    # MoR ë¡œê·¸ë¥¼ attrsì— ì €ì¥
    out.attrs["mor_log"] = mor_log
    
    return out.reset_index(drop=True)


# --- BEGIN: TS Utilities (stats, anomaly table, future shading) ---

import numpy as np
import pandas as pd

def compute_series_stats(dfm: pd.DataFrame) -> dict:
    """
    dfm: ë‹¨ì¼ ê³„ì •Ã—ë‹¨ì¼ measure(=flow/balance) êµ¬ê°„ì˜ in-sample ê²°ê³¼ í”„ë ˆì„
         (í•„ìˆ˜ ì»¬ëŸ¼: ['actual','predicted'])
    ë°˜í™˜: {'MAE':..., 'MAPE':..., 'RMSE':..., 'N':...}
    """
    if dfm is None or dfm.empty:
        return {'MAE': np.nan, 'MAPE': np.nan, 'RMSE': np.nan, 'N': 0}
    s = dfm[['actual', 'predicted']].apply(pd.to_numeric, errors='coerce').dropna()
    n = int(len(s))
    if n == 0:
        return {'MAE': np.nan, 'MAPE': np.nan, 'RMSE': np.nan, 'N': 0}
    err = (s['actual'] - s['predicted']).to_numpy()
    mae = float(np.mean(np.abs(err)))
    rmse = float(np.sqrt(np.mean(err ** 2)))
    # 0 ë‚˜ëˆ—ì…ˆ ë°©ì§€: |actual|<epsëŠ” ë¬´ì‹œí•˜ì—¬ MAPEë¥¼ NaNìœ¼ë¡œ ì²˜ë¦¬ â†’ ì „ì²´ëŠ” nanmean
    denom = s['actual'].to_numpy()
    denom = np.where(np.abs(denom) < 1e-9, np.nan, np.abs(denom))
    mape = float(np.nanmean(np.abs(err) / denom) * 100.0)
    return {'MAE': mae, 'MAPE': mape, 'RMSE': rmse, 'N': n}

def _ensure_z_with_rolling(dfm: pd.DataFrame, k: int = 6) -> pd.DataFrame:
    """
    zê°€ ì—†ê±°ë‚˜ ì „ë¶€ ê²°ì¸¡ì´ë©´ ìµœê·¼ kê°œì›” ë¡¤ë§-í‘œì¤€í¸ì°¨(+expanding ë³´ì •)ë¡œ zë¥¼ ê³„ì‚°.
    z = error / std_k  (ì´ˆê¸° êµ¬ê°„ì€ expanding stdë¡œ ë³´ê°•)
    """
    out = dfm.copy()
    if 'z' in out.columns and out['z'].notna().any():
        return out
    out['actual'] = pd.to_numeric(out.get('actual'), errors='coerce')
    out['predicted'] = pd.to_numeric(out.get('predicted'), errors='coerce')
    if 'error' not in out.columns:
        out['error'] = out['actual'] - out['predicted']
    out = out.sort_values('date')
    k = int(max(3, k))
    std_k = out['error'].rolling(window=k, min_periods=3).std()
    std_exp = out['error'].expanding(min_periods=3).std()
    std = std_k.fillna(std_exp).replace(0, np.nan)
    out['z'] = out['error'] / std
    return out

def build_anomaly_table(df_all: pd.DataFrame, *, topn: int = 10, k: int = 6, pm_value: float | None = None) -> pd.DataFrame:
    """
    ë‹¨ì¼ ê³„ì •ì˜ ì „ì²´ measure(flow/balance)ë¥¼ ì…ë ¥ë°›ì•„ Top-|z| ì›” ìƒìœ„ Ní–‰ì„ ë°˜í™˜.
    ë°˜í™˜ ì»¬ëŸ¼: ['ì¼ì','ì‹¤ì¸¡','ì˜ˆì¸¡','ì”ì°¨','z','PMëŒ€ë¹„','ìœ„í—˜ë„','ëª¨ë¸','ê¸°ì¤€','|z|']
    """
    frames = []
    for ms in ('flow', 'balance'):
        d = df_all[df_all['measure'].eq(ms)]
        if d.empty:
            continue
        d = _ensure_z_with_rolling(d, k=k)
        d['PMëŒ€ë¹„'] = (
            np.minimum(1.0, np.abs(d['error']) / float(pm_value))
            if pm_value and pm_value > 0 else np.nan
        )
        d['ê¸°ì¤€'] = np.where(d['measure'].eq('flow'), 'Flow', 'Balance')
        keep = ['date','actual','predicted','error','z','PMëŒ€ë¹„','risk','model','ê¸°ì¤€']
        for c in keep:
            if c not in d.columns:
                d[c] = np.nan
        frames.append(d[keep].copy())

    if not frames:
        cols = ['ì¼ì','ì‹¤ì¸¡','ì˜ˆì¸¡','ì”ì°¨','z','PMëŒ€ë¹„','ìœ„í—˜ë„','ëª¨ë¸','ê¸°ì¤€','|z|']
        return pd.DataFrame(columns=cols)

    out = pd.concat(frames, ignore_index=True)
    out['|z|'] = np.abs(out['z'])
    out = out.sort_values(['|z|','date'], ascending=[False, False]).head(int(topn)).copy()
    out = out.rename(columns={
        'date':'ì¼ì','actual':'ì‹¤ì¸¡','predicted':'ì˜ˆì¸¡','error':'ì”ì°¨','risk':'ìœ„í—˜ë„','model':'ëª¨ë¸'
    })
    return out

def add_future_shading(fig, last_date, horizon_months: int = 0):
    """
    ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê° ê°•ì¡°ìš© ìŒì˜. horizon_months<=0ì´ë©´ ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ.
    """
    if fig is None or not horizon_months or horizon_months <= 0:
        return fig
    last = pd.to_datetime(last_date)
    x0 = last + pd.offsets.MonthBegin(1)      # ë‹¤ìŒ ë‹¬ ì‹œì‘
    x1 = last + pd.offsets.MonthEnd(horizon_months)
    try:
        fig.add_vrect(
            x0=x0, x1=x1,
            fillcolor="LightGrey", opacity=0.15, line_width=0,
            layer="below"  # â† ë°ì´í„°/ê²½ê³„ì„  ì•„ë˜ë¡œ ê¹”ê¸°
        )
    except Exception:
        pass
    return fig

# --- END: TS Utilities ---


==============================
ğŸ“„ FILE: analysis/vendor.py
==============================

from typing import List, Dict, Any
import pandas as pd
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from itertools import product
from analysis.contracts import LedgerFrame, ModuleResult
from utils.viz import add_materiality_threshold, add_pm_badge


# ê³µí†µ: ê¸ˆì•¡ ì»¬ëŸ¼ í›„ë³´ì—ì„œ í•˜ë‚˜ë¥¼ ê³ ë¥´ê³ , í•„ìš” ì‹œ ì ˆëŒ€ê°’ìœ¼ë¡œ ë³€í™˜
_AMOUNT_CANDIDATES = ["ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’", "ê±°ë˜ê¸ˆì•¡", "ë°œìƒì•¡", "amount", "ê¸ˆì•¡"]

def _pick_amount_column(df: pd.DataFrame) -> str | None:
    for c in _AMOUNT_CANDIDATES:
        if c in df.columns:
            return c
    return None


def create_pareto_figure(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True, pm_value: float | None = None):
    """ê±°ë˜ì²˜ë³„ ê±°ë˜ê¸ˆì•¡ íŒŒë ˆí†  ì°¨íŠ¸.
    - min_amount ì´ìƒì¸ ê±°ë˜ì²˜ë§Œ ê°œë³„ í‘œê¸°
    - ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ í•©ì‚°(ì˜µì…˜)
    - 1ì°¨ Yì¶•ì— PM ì ì„ /ë¼ë²¨ í‘œì‹œ
    """
    if ledger_df is None or ledger_df.empty or "ì—°ë„" not in ledger_df.columns:
        return None

    cy_df = ledger_df[ledger_df["ì—°ë„"] == ledger_df["ì—°ë„"].max()]
    if "ê±°ë˜ì²˜" not in cy_df.columns or cy_df["ê±°ë˜ì²˜"].nunique() < 1:
        return None

    amt_col = _pick_amount_column(cy_df)
    if amt_col is None:
        return None

    # ì ˆëŒ€ê°’ ê¸°ì¤€ í•©ê³„ (í‘œì‹œì˜ ì¼ê´€ì„±ì„ ìœ„í•´)
    series_raw = cy_df.groupby("ê±°ë˜ì²˜")[amt_col].sum()
    vendor_sum = series_raw.abs()

    if vendor_sum.empty:
        return None

    # ì„ê³„ì¹˜ í•„í„°
    above = vendor_sum[vendor_sum >= float(min_amount)].sort_values(ascending=False)
    etc_sum = float(vendor_sum.sum() - above.sum())

    series = above
    if include_others and etc_sum > 0:
        series = pd.concat([above, pd.Series({"ê¸°íƒ€": etc_sum})])

    cum_ratio = series.cumsum() / series.sum() * 100.0

    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(go.Bar(x=series.index, y=series.values, name="ê±°ë˜ ê¸ˆì•¡"), secondary_y=False)
    fig.add_trace(go.Scatter(x=series.index, y=cum_ratio.values, name="ëˆ„ì  ë¹„ìœ¨(%)", mode="lines+markers"), secondary_y=True)
    fig.update_layout(title="ê±°ë˜ì²˜ ì§‘ì¤‘ë„ ë¶„ì„ (Pareto)", yaxis_title="ê¸ˆì•¡", yaxis2_title="ëˆ„ì  ë¹„ìœ¨(%)")

    # ğŸ”¢ ì¶•/íˆ´íŒ í¬ë§·
    fig.update_yaxes(separatethousands=True, tickformat=",.0f", showexponent="none", exponentformat="none", secondary_y=False)
    fig.update_yaxes(tickformat=".1f", ticksuffix="%", secondary_y=True)
    fig.update_traces(hovertemplate="%{x}<br>%{y:,.0f} ì›<extra></extra>", selector=dict(type="bar"))
    fig.update_traces(hovertemplate="%{x}<br>ëˆ„ì  ë¹„ìœ¨=%{y:.1f}%<extra></extra>", selector=dict(type="scatter"))

    # PM ì ì„ /ë¼ë²¨ (1ì°¨ yì¶•)
    if pm_value and float(pm_value) > 0:
        add_materiality_threshold(fig, pm_value=float(pm_value))

    return fig


def create_vendor_heatmap(ledger_df: pd.DataFrame, min_amount: float = 0, include_others: bool = True, pm_value: float | None = None):
    """ê±°ë˜ì²˜ë³„ ì›”ë³„ í™œë™ íˆíŠ¸ë§µ.
    - min_amount ì´ìƒì¸ ê±°ë˜ì²˜ë§Œ ê°œë³„ í‘œê¸°
    - ë‚˜ë¨¸ì§€ëŠ” 'ê¸°íƒ€'ë¡œ ì›”ë³„ í•©ì‚°(ì˜µì…˜)
    - ìš°ì¸¡ ìƒë‹¨ì— PM ë°°ì§€ í‘œì‹œ
    """
    if ledger_df is None or ledger_df.empty or "ê±°ë˜ì²˜" not in ledger_df.columns:
        return None

    df = ledger_df.copy()
    if "íšŒê³„ì¼ì" not in df.columns:
        return None

    amt_col = _pick_amount_column(df)
    if amt_col is None:
        return None

    df["ì—°ì›”"] = pd.to_datetime(df["íšŒê³„ì¼ì"], errors="coerce").dt.to_period("M").astype(str)
    pivot = df.pivot_table(index="ê±°ë˜ì²˜", columns="ì—°ì›”", values=amt_col, aggfunc="sum").fillna(0).abs()
    if pivot.empty:
        return None

    totals = pivot.sum(axis=1)
    above_idx = totals >= float(min_amount)
    pivot_above = pivot.loc[above_idx].copy()
    pivot_above["_tot_"] = pivot_above.sum(axis=1)
    pivot_above = pivot_above.sort_values("_tot_", ascending=False).drop(columns=["_tot_"])

    pivot_final = pivot_above
    if include_others:
        below = pivot.loc[~above_idx]
        if not below.empty:
            etc_row = pd.DataFrame([below.sum(axis=0)], index=["ê¸°íƒ€"])
            pivot_final = pd.concat([pivot_above, etc_row], axis=0)

    fig = px.imshow(pivot_final, title="ê±°ë˜ì²˜ ì›”ë³„ í™œë™ íˆíŠ¸ë§µ", labels=dict(x="ì—°ì›”", y="ê±°ë˜ì²˜", color="ê±°ë˜ê¸ˆì•¡"))

    # ğŸ”¢ ì»¬ëŸ¬ë°”/íˆ´íŒ í¬ë§·
    fig.update_coloraxes(colorbar=dict(tickformat=",.0f"))
    fig.update_traces(hovertemplate="ì—°ì›”=%{x}<br>ê±°ë˜ì²˜=%{y}<br>ê±°ë˜ê¸ˆì•¡=%{z:,.0f} ì›<extra></extra>")

    # PM ë°°ì§€
    if pm_value and float(pm_value) > 0:
        add_pm_badge(fig, pm_value=float(pm_value))

    return fig


def create_vendor_detail_figure(ledger_df: pd.DataFrame, vendor_name: str, all_months: List[str]):
    """íŠ¹ì • ê±°ë˜ì²˜ì˜ ì›”ë³„ ê±°ë˜ì•¡ì„ ê³„ì •ë³„ ëˆ„ì  ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ì „ì²´ ê¸°ê°„ Xì¶• ë³´ì¥)"""
    vendor_df = ledger_df[ledger_df["ê±°ë˜ì²˜"] == vendor_name].copy()
    amt_col = _pick_amount_column(vendor_df)
    if amt_col is None:
        return None

    vendor_df["ì—°ì›”"] = pd.to_datetime(vendor_df["íšŒê³„ì¼ì"], errors="coerce").dt.to_period("M").astype(str)
    summary = vendor_df.groupby(["ì—°ì›”", "ê³„ì •ëª…"], as_index=False)[amt_col].sum()
    summary[amt_col] = summary[amt_col].abs()

    unique_accounts = vendor_df["ê³„ì •ëª…"].unique()
    axis_labels = {"ì—°ì›”": "ê±°ë˜ì›”", amt_col: "ê±°ë˜ê¸ˆì•¡"}

    if len(unique_accounts) == 0:
        empty_df = pd.DataFrame({"ì—°ì›”": all_months, "ê³„ì •ëª…": [None] * len(all_months), amt_col: [0] * len(all_months)})
        fig = px.bar(empty_df, x="ì—°ì›”", y=amt_col, labels=axis_labels)
    else:
        template_df = pd.DataFrame(list(product(all_months, unique_accounts)), columns=["ì—°ì›”", "ê³„ì •ëª…"])
        merged_summary = pd.merge(template_df, summary, on=["ì—°ì›”", "ê³„ì •ëª…"], how="left").fillna(0)
        fig = px.bar(
            merged_summary,
            x="ì—°ì›”", y=amt_col, color="ê³„ì •ëª…",
            category_orders={"ì—°ì›”": all_months},
            labels=axis_labels,
        )

    fig.update_layout(
        barmode="stack",
        title=f"'{vendor_name}' ê±°ë˜ì²˜ ì›”ë³„/ê³„ì •ë³„ ìƒì„¸ ë‚´ì—­"
    )
    # ğŸ”¢ ì¶•/íˆ´íŒ í¬ë§·: ì²œë‹¨ìœ„ ì‰¼í‘œ, SI ì œê±°
    fig.update_yaxes(separatethousands=True, tickformat=",.0f", showexponent="none", exponentformat="none")
    fig.update_traces(hovertemplate="ì—°ì›”=%{x}<br>ê¸ˆì•¡=%{y:,.0f} ì›<br>ê³„ì •ëª…=%{fullData.name}<extra></extra>")
    return fig


def run_vendor_module(lf: LedgerFrame, account_codes: List[str] | None = None,
                      min_amount: float = 0, include_others: bool = True) -> ModuleResult:
    """ê±°ë˜ì²˜ ëª¨ë“ˆ: ì„ íƒ ê³„ì • í•„í„° + ìµœì†Œ ê±°ë˜ê¸ˆì•¡ í•„í„°('ê¸°íƒ€' í•©ì‚°) + PM ë¼ë²¨/ë°°ì§€."""
    df = lf.df
    use_df = df.copy()
    if account_codes:
        acs = [str(a) for a in account_codes]
        use_df = use_df[use_df["ê³„ì •ì½”ë“œ"].astype(str).isin(acs)]

    figures: Dict[str, Any] = {}
    warnings: List[str] = []
    pm_value = (lf.meta or {}).get("pm_value")

    pareto = create_pareto_figure(use_df, min_amount=min_amount, include_others=include_others, pm_value=pm_value)
    heatmap = create_vendor_heatmap(use_df, min_amount=min_amount, include_others=include_others, pm_value=pm_value)

    if pareto: figures["pareto"] = pareto
    else: warnings.append("Pareto ê·¸ë˜í”„ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±).")
    if heatmap: figures["heatmap"] = heatmap
    else: warnings.append("íˆíŠ¸ë§µ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±).")

    # ìš”ì•½ ì •ë³´
    if "ì—°ë„" in use_df.columns:
        cy = use_df[use_df["ì—°ë„"] == use_df["ì—°ë„"].max()]
    else:
        cy = use_df.iloc[:0]

    amt_col = _pick_amount_column(cy) if not cy.empty else None
    vendor_sum = cy.groupby("ê±°ë˜ì²˜")[amt_col].sum().abs() if (amt_col and "ê±°ë˜ì²˜" in cy.columns and not cy.empty) else pd.Series(dtype=float)
    n_above = int((vendor_sum >= float(min_amount)).sum()) if not vendor_sum.empty else 0
    n_below = int((vendor_sum < float(min_amount)).sum()) if not vendor_sum.empty else 0

    summary = {
        "filtered_accounts": [str(a) for a in account_codes] if account_codes else [],
        "min_amount": float(min_amount),
        "include_others": bool(include_others),
        "n_above_threshold": n_above,
        "n_below_threshold": n_below,
        "n_figures": len(figures),
        "period_tag_coverage": dict(use_df["period_tag"].value_counts()) if "period_tag" in use_df.columns else {},
    }
    return ModuleResult(
        name="vendor",
        summary=summary,
        tables={},
        figures=figures,
        evidences=[],
        warnings=warnings
    )



==============================
ğŸ“„ FILE: analysis/__init__.py
==============================






==============================
ğŸ“„ FILE: infra/env_loader.py
==============================

from __future__ import annotations
import os, sys
from typing import Optional


def _read_kv_file(path: str) -> dict:
    # ë‹¨ìˆœ .env íŒŒì„œ (python-dotenv ì—†ì´ë„ ì‘ë™)
    data = {}
    if not os.path.exists(path):
        return data
    with open(path, "r", encoding="utf-8") as f:
        for raw in f:
            line = raw.strip()
            if not line or line.startswith("#"):
                continue
            if "=" not in line:
                continue
            k, v = line.split("=", 1)
            k = k.strip()
            v = v.strip().strip("\"'")  # ì–‘ìª½ ë”°ì˜´í‘œ ì œê±°
            data[k] = v
    return data


def _maybe_import_dotenv():
    try:
        from dotenv import load_dotenv  # type: ignore
        return load_dotenv
    except Exception:
        return None


# ë™ì˜ì–´ í‚¤ -> í‘œì¤€ í‚¤ ì •ê·œí™”
_OPENAI_ALIASES = [
    "OPENAI_API_KEY", "OPENAI_KEY", "OPENAI_TOKEN", "OPENAIAPIKEY", "OPENAI_APIKEY",
    # Azure/OpenAI ë³€í˜•ë“¤ (ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œë„ í—ˆìš©)
    "AZURE_OPENAI_API_KEY",
]


def _normalize_env(d: dict) -> None:
    # í‘œì¤€ í‚¤ê°€ ì—†ê³ , ë™ì˜ì–´ê°€ ìˆìœ¼ë©´ ëŒì–´ì™€ì„œ OPENAI_API_KEY ì„¸íŒ…
    if not d.get("OPENAI_API_KEY"):
        for k in _OPENAI_ALIASES:
            if k in d and d[k]:
                d["OPENAI_API_KEY"] = d[k]
                break


def ensure_api_keys_loaded() -> bool:
    # 1) python-dotenvê°€ ìˆìœ¼ë©´ ë¨¼ì € ì‹œë„
    load_dotenv = _maybe_import_dotenv()
    if load_dotenv:
        # ë‘ íŒŒì¼ì„ ëª¨ë‘ ì‹œë„(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì ìš©)
        for p in (".env", "API_KEY.env"):
            try:
                load_dotenv(dotenv_path=p, override=False)
            except Exception:
                pass

    # 2) ìˆ˜ë™ íŒŒì‹± (dotenvê°€ ì—†ê±°ë‚˜, ëª» ì½ì€ ê²½ìš° ëŒ€ë¹„)
    merged = {}
    for p in (".env", "API_KEY.env"):
        try:
            merged.update(_read_kv_file(p))
        except Exception:
            pass

    # 3) ë™ì˜ì–´ ì •ê·œí™” â†’ OPENAI_API_KEY
    _normalize_env(merged)

    # 4) í™˜ê²½ë³€ìˆ˜ì— ë°˜ì˜(ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì„¸íŒ…)
    for k, v in merged.items():
        if k not in os.environ and v:
            os.environ[k] = v

    ok = bool(os.environ.get("OPENAI_API_KEY") or os.environ.get("AZURE_OPENAI_API_KEY"))
    # ê°€ì‹œì  í”Œë˜ê·¸
    os.environ["LLM_AVAILABLE"] = "1" if ok else "0"
    return ok


def is_llm_ready() -> bool:
    return os.environ.get("LLM_AVAILABLE", "0") == "1"


def log_llm_status(logger=None):
    # í•œêµ­ì–´ ìƒíƒœ ë¡œê·¸
    if is_llm_ready():
        msg = "ğŸ”Œ OpenAI Key ê°ì§€: ì˜¨ë¼ì¸ LLM ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ì‚¬ìš©)"
    else:
        msg = "ğŸ”Œ OpenAI Key ì—†ìŒ: ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ëª¨ë“œë¡œ ìƒì„±í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°/ìš”ì•½ LLM ë¯¸ì‚¬ìš©)"
    if logger:
        try:
            logger.info(msg)
            return
        except Exception:
            pass
    print(msg, file=sys.stderr)


# ì•± ë¶€íŒ… ì‹œ ì‚¬ìš©í•  ì§„ì…ì  (importë§Œìœ¼ë¡œ ë¶€íŒ…ì´ˆê¸°í™”í•˜ê³  ì‹¶ì„ ë•Œ)
def boot():
    ensure_api_keys_loaded()
    log_llm_status()





==============================
ğŸ“„ FILE: services/cache.py
==============================

# services/cache.py  (ì „ì²´ êµì²´ë³¸)
# - ì„ë² ë”© ìºì‹œ(SQLite)
# - LLM ë§¤í•‘ ìºì‹œ(ìŠ¹ì¸/ë²„ì „)
# - ìºì‹œ ì •ë³´ í—¬í¼

from __future__ import annotations
import os, sqlite3, json, hashlib, threading, time
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from config import EMBED_CACHE_DIR

# ëª¨ë¸ë³„ DB íŒŒì¼ ì ‘ê·¼ ì‹œ ë ˆì´ìŠ¤ë¥¼ ë§‰ê¸° ìœ„í•œ ë½ ë§µ
_LOCKS: Dict[str, threading.Lock] = {}

def _model_dir(model: str) -> Path:
    # ëª¨ë¸ëª…ì„ ì•ˆì „í•œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜
    safe = model.replace("/", "_").replace(":", "_")
    d = Path(EMBED_CACHE_DIR) / safe
    d.mkdir(parents=True, exist_ok=True)
    return d

def _db_path(model: str) -> str:
    return str(_model_dir(model) / "embeddings.sqlite3")

def _sha1(s: str) -> str:
    """
    FIPS ëª¨ë“œì—ì„œë„ ë™ì‘í•˜ë„ë¡ ì•ˆì „ ê°€ë“œ:
    1) hashlib.sha1(..., usedforsecurity=False) ì‹œë„
    2) íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›/ì°¨ë‹¨ ì‹œ hashlib.sha1(b) ì‹œë„
    3) ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ hashlib.new("sha1") ì‹œë„
    4) ìµœì¢… í´ë°±: blake2s(20ë°”ì´íŠ¸) â€” ìºì‹œ í‚¤ ìš©ë„ë¼ 160-bit ê¸¸ì´ë§Œ ìœ ì§€ë˜ë©´ ì¶©ë¶„
    """
    b = s.encode("utf-8")
    try:
        return hashlib.sha1(b, usedforsecurity=False).hexdigest()  # type: ignore[call-arg]
    except TypeError:
        try:
            return hashlib.sha1(b).hexdigest()
        except Exception:
            pass
    except Exception:
        pass
    try:
        h = hashlib.new("sha1", b)
        return h.hexdigest()
    except Exception:
        # final fallback: 160-bit digest for key-length parity
        return hashlib.blake2s(b, digest_size=20).hexdigest()

def _get_lock(model: str) -> threading.Lock:
    if model not in _LOCKS:
        _LOCKS[model] = threading.Lock()
    return _LOCKS[model]

def _ensure_db(conn: sqlite3.Connection):
    # ê¸°ë³¸ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë³´ì¥
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS emb (
            k TEXT PRIMARY KEY,
            text TEXT,
            vec TEXT
        )
    """)
    conn.commit()

def _open(model: str) -> sqlite3.Connection:
    p = _db_path(model)
    conn = sqlite3.connect(p, timeout=30)
    _ensure_db(conn)
    return conn

def cache_get_many(model: str, texts: List[str]) -> Dict[str, List[float]]:
    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìºì‹œ ì¡°íšŒ
    if not texts:
        return {}
    keys = [(t, _sha1(f"{model}|{t}")) for t in texts]
    with _get_lock(model):
        conn = _open(model)
        try:
            cur = conn.cursor()
            qmarks = ",".join("?" for _ in keys)
            cur.execute(f"SELECT k, vec FROM emb WHERE k IN ({qmarks})", [k for _, k in keys])
            rows = {k: json.loads(vec) for (k, vec) in cur.fetchall()}
        finally:
            conn.close()
    out: Dict[str, List[float]] = {}
    for t, k in keys:
        if k in rows:
            out[t] = rows[k]
    return out

def cache_put_many(model: str, pairs: List[Tuple[str, List[float]]]) -> None:
    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸-ë²¡í„° ìŒì„ ìºì‹œì— ì €ì¥
    if not pairs:
        return
    records = [(_sha1(f"{model}|{t}"), t, json.dumps(vec)) for (t, vec) in pairs]
    with _get_lock(model):
        conn = _open(model)
        try:
            conn.executemany("INSERT OR REPLACE INTO emb(k,text,vec) VALUES (?,?,?)", records)
            conn.commit()
        finally:
            conn.close()

def get_or_embed_texts(
    texts: List[str],
    client,
    *,
    model: str,
    batch_size: int,
    timeout: int,
    max_retry: int,
) -> Dict[str, List[float]]:
    # í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ìºì‹œë¥¼ ìš°ì„  ì¡°íšŒí•˜ê³ , ëˆ„ë½ë¶„ë§Œ ì„ë² ë”© API í˜¸ì¶œ
    texts = list(dict.fromkeys([str(t) for t in texts]))
    cached = cache_get_many(model, texts)
    missing = [t for t in texts if t not in cached]
    if not missing:
        return cached
    out = dict(cached)
    for s in range(0, len(missing), batch_size):
        sub = missing[s:s+batch_size]
        last_err = None
        for attempt in range(max_retry):
            try:
                try:
                    resp = client.embeddings.create(model=model, input=sub, timeout=timeout)
                except TypeError:
                    # ì¼ë¶€ í´ë¼ì´ì–¸íŠ¸ëŠ” timeout íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                    resp = client.embeddings.create(model=model, input=sub)
                vecs = [d.embedding for d in resp.data]
                pairs = list(zip(sub, vecs))
                cache_put_many(model, pairs)
                out.update({sub[i]: vecs[i] for i in range(len(sub))})
                last_err = None
                break
            except Exception as e:
                last_err = e
        if last_err:
            # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ, ë§ˆì§€ë§‰ ì˜ˆì™¸ë¥¼ ì „íŒŒ
            raise last_err
    return out

# ===== LLM ë§¤í•‘ ìºì‹œ (ìŠ¹ì¸/ë²„ì „ ê³ ì •) =====
DEFAULT_CACHE_PATH = os.path.join(".cache", "llm_mappings.json")

class LLMMappingCache:
    # ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ìŠ¹ì¸/ë²„ì „ ê´€ë¦¬
    def __init__(self, path: str = DEFAULT_CACHE_PATH):
        self.path = path
        self._lock = threading.Lock()
        self._state: Dict[str, Any] = {"approved": {}, "proposed": {}, "versions": {}}
        self._load()

    def _load(self) -> None:
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        if os.path.exists(self.path):
            try:
                with open(self.path, "r", encoding="utf-8") as f:
                    self._state = json.load(f)
            except Exception:
                # íŒŒì† íŒŒì¼ì€ ì¡°ìš©íˆ ë¬´ì‹œ
                pass

    def _save(self) -> None:
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self._state, f, ensure_ascii=False, indent=2)

    def get_approved(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["approved"].get(key)
            return dict(item) if item else None

    def get_proposed(self, key: str) -> Optional[Dict[str, Any]]:
        with self._lock:
            item = self._state["proposed"].get(key)
            return dict(item) if item else None

    def propose(self, key: str, value: Any, *, model: str, meta: Optional[Dict[str, Any]] = None) -> None:
        with self._lock:
            self._state["proposed"][key] = {
                "value": value,
                "model": model,
                "meta": meta or {},
                "ts": time.time(),
            }
            self._save()

    def approve(self, key: str, *, value_override: Any | None = None, note: str = "") -> Dict[str, Any]:
        # ì´ˆì•ˆì„ ìŠ¹ì¸í•˜ì—¬ ë²„ì „ì„ ì˜¬ë¦¬ê³  í™•ì •
        with self._lock:
            src = self._state["proposed"].get(key) or self._state["approved"].get(key)
            if not src:
                raise KeyError(f"No proposed/approved entry for key={key!r}")
            prev_ver = int(self._state["versions"].get(key, 0))
            new_ver = prev_ver + 1
            final_value = src["value"] if value_override is None else value_override
            rec = {
                "value": final_value,
                "version": f"v{new_ver}",
                "note": note,
                "approved_ts": time.time(),
                "model": src.get("model"),
                "meta": src.get("meta", {}),
            }
            self._state["approved"][key] = rec
            self._state["versions"][key] = new_ver
            if key in self._state["proposed"]:
                del self._state["proposed"][key]
            self._save()
            return dict(rec)

# ===== ì„ë² ë”© ìºì‹œ ì •ë³´ í—¬í¼ (app.py ì‚¬ì´ë“œë°” ë“±ì—ì„œ ì‚¬ìš©) =====
def get_cache_info(model: str) -> Dict[str, Any]:
    p = _db_path(model)
    info = {"model": model, "path": p, "exists": os.path.exists(p)}
    if not os.path.exists(p):
        return info
    try:
        conn = sqlite3.connect(p, timeout=10)
        _ensure_db(conn)
        try:
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) FROM emb")
            nrows = int(cur.fetchone()[0])
            size = os.path.getsize(p)
            info.update({"rows": nrows, "size_bytes": size})
        finally:
            conn.close()
    except Exception as e:
        info["error"] = str(e)
    return info



==============================
ğŸ“„ FILE: services/cluster_naming.py
==============================

from __future__ import annotations
from typing import Optional, Callable, List, Dict

# --- Factories: analysis ê³„ì¸µì— ë„˜ê²¨ì¤„ ì½œë°± ìƒì„± ---

def make_llm_name_fn(client, model: str = "gpt-4o-mini") -> Callable[[List[str], List[str]], Optional[str]]:
    """(services ì „ìš©) í´ëŸ¬ìŠ¤í„° ì´ë¦„ì„ ìƒì„±í•˜ëŠ” ì½œë°±ì„ ë§Œë“¤ì–´ ë°˜í™˜."""
    def _name_fn(samples_desc: List[str], samples_vendor: List[str]) -> Optional[str]:
        import textwrap
        prompt = textwrap.dedent(f"""
        ë„ˆëŠ” íšŒê³„ ê°ì‚¬ ë³´ì¡° AIë‹¤. ì•„ë˜ ê±°ë˜ ìƒ˜í”Œì„ ë³´ê³  ì´ ê·¸ë£¹ì˜ ì„±ê²©ì„ ê°€ì¥ ì˜ ë“œëŸ¬ë‚´ëŠ”
        í•œêµ­ì–´ **í´ëŸ¬ìŠ¤í„° ì´ë¦„ 1ê°œë§Œ**, 10ì ë‚´ì™¸ë¡œ ì œì‹œí•´ë¼.
        ìˆ«ì/ê¸°í˜¸/ë”°ì˜´í‘œ/ì ‘ë‘ì‚¬ ê¸ˆì§€. ì˜ˆ: ì§ì› ê¸‰ì—¬, í†µì‹ ë¹„, ì„ì› ë³µì§€.

        [ê±°ë˜ ì ìš” ìƒ˜í”Œ]
        - {'\n- '.join(samples_desc) if samples_desc else '(ìƒ˜í”Œ ë¶€ì¡±)'}

        [ì£¼ìš” ê±°ë˜ì²˜ ìƒ˜í”Œ]
        - {'\n- '.join(samples_vendor) if samples_vendor else '(ìƒ˜í”Œ ë¶€ì¡±)'}

        ì •ë‹µ:
        """).strip()
        try:
            resp = client.chat.completions.create(
                model=model, messages=[{"role":"user","content":prompt}], temperature=0
            )
            cand = (resp.choices[0].message.content or "").strip().splitlines()[0].strip(" \"'[]()")
            if not cand:
                return None
            bad = {"í´ëŸ¬ìŠ¤í„°","unknown","ì´ë¦„ì—†ìŒ","ë¯¸ì •","ê¸°íƒ€"}
            if cand.lower() in bad or cand.startswith("í´ëŸ¬ìŠ¤í„°"):
                return None
            return cand[:20]
        except Exception:
            return None
    return _name_fn


def make_synonym_confirm_fn(client, model: str = "gpt-4o-mini") -> Callable[[str, str], bool]:
    """(services ì „ìš©) ë‘ ì´ë¦„ì´ ì‚¬ì‹¤ìƒ ê°™ì€ ì˜ë¯¸ì¸ì§€ YES/NOë¡œ í™•ì¸í•˜ëŠ” ì½œë°±."""
    def _confirm(a: str, b: str) -> bool:
        try:
            q = (
                "ë„ˆëŠ” íšŒê³„ ê°ì‚¬ ë³´ì¡° AIë‹¤. ë‹¤ìŒ ë‘ í‘œí˜„ì´ 'íšŒê³„ ê±°ë˜ ì¹´í…Œê³ ë¦¬' ì´ë¦„ìœ¼ë¡œì„œ "
                "ì‚¬ì‹¤ìƒ ê°™ì€ ì˜ë¯¸ì¸ì§€ YES/NOë¡œë§Œ ë‹µí•˜ë¼.\n"
                f"A: {a}\nB: {b}\nì •ë‹µ:"
            )
            resp = client.chat.completions.create(
                model=model, messages=[{"role": "user", "content": q}], temperature=0
            )
            ans = (resp.choices[0].message.content or "").strip().split()[0].upper()
            return ans.startswith("Y")
        except Exception:
            return False
    return _confirm


def unify_cluster_labels_llm(names: List[str], client, model: str = "gpt-4o-mini") -> Dict[str, str]:
    """
    ìœ ì‚¬ ì˜ë¯¸ì˜ í•œê¸€ í´ëŸ¬ìŠ¤í„°ëª…ì„ LLMìœ¼ë¡œ ë¬¶ì–´ canonical name ë§¤í•‘ì„ ë¦¬í„´.
    ì…ë ¥: ["ê²½ë¹„ ê´€ë¦¬","ê²½ë¹„ ì²˜ë¦¬","ê´€ë¦¬ ê²½ë¹„", ...]
    ì¶œë ¥: {"ê²½ë¹„ ì²˜ë¦¬":"ê²½ë¹„ ê´€ë¦¬", ...}
    """
    uniq = sorted([n for n in set([str(x) for x in names]) if n and n.lower() != 'nan'])
    if not uniq:
        return {}
    prompt = (
        "ë‹¤ìŒ í•œêµ­ì–´ í´ëŸ¬ìŠ¤í„° ì´ë¦„ë“¤ì„ ì˜ë¯¸ê°€ ê°™ì€ ê²ƒë¼ë¦¬ ë¬¶ì–´ í•˜ë‚˜ì˜ ëŒ€í‘œëª…ìœ¼ë¡œ í†µí•©í•˜ì„¸ìš”.\n"
        "ê·œì¹™: 1) ê°€ì¥ ì¼ë°˜ì /ì§§ì€ í‘œí˜„ì„ ëŒ€í‘œëª…ìœ¼ë¡œ, 2) JSON ê°ì²´ë¡œë§Œ ì‘ë‹µ, 3) í˜•ì‹: {ì›ë˜ëª…:ëŒ€í‘œëª…, ...}.\n"
        f"ëª©ë¡: {uniq}"
    )
    try:
        resp = client.chat.completions.create(
            model=model, messages=[{"role": "user", "content": prompt}], temperature=0
        )
        import json
        txt = (resp.choices[0].message.content or "").strip()
        mapping = json.loads(txt)
        if isinstance(mapping, dict):
            return mapping
    except Exception:
        pass
    return {n: n for n in uniq}





==============================
ğŸ“„ FILE: services/cycles_store.py
==============================

# services/cycles_store.py
from __future__ import annotations
from typing import Dict, Iterable, List
import os, json

# -------------------------------------------------
# 0) ì‚¬ì´í´ ì½”ë“œ/ë¼ë²¨
# -------------------------------------------------
ALLOWED_CYCLES: List[str] = [
    "CLOSE", "REV", "PUR", "HR",
    "TREASURY_INVEST", "TREASURY_FINANCE",
    "TAX", "PPE", "INTANG", "LEASE", "OTHER",
]

CYCLE_KO: Dict[str, str] = {
    "CLOSE":            "ê²°ì‚°",
    "REV":              "ë§¤ì¶œ",
    "PUR":              "ë§¤ì…Â·ë¹„ìš©",
    "HR":               "ì¸ì‚¬",
    "TREASURY_INVEST":  "ìê¸ˆìš´ìš©",
    "TREASURY_FINANCE": "ìê¸ˆì¡°ë‹¬",
    "TAX":              "ì„¸ë¬´",
    "PPE":              "ìœ í˜•ìì‚°",
    "INTANG":           "ë¬´í˜•ìì‚°",
    "LEASE":            "ë¦¬ìŠ¤",
    "OTHER":            "ê¸°íƒ€",
}
KO_TO_CODE = {v: k for k, v in CYCLE_KO.items()}

def code_to_ko(code: str) -> str:
    return CYCLE_KO.get(str(code).upper(), "ê¸°íƒ€")

def ko_to_code(label: str) -> str:
    return KO_TO_CODE.get(str(label), "OTHER")

# -------------------------------------------------
# 1) ì—…ë¡œë“œë³„ ê³„ì •â†’ì‚¬ì´í´ â€˜ë§¤í•‘â€™ ì €ì¥/ì¡°íšŒ (ì„¸ì…˜ ë©”ëª¨ë¦¬)
# -------------------------------------------------
_MEM: Dict[str, Dict[str, str]] = {}   # {upload_id: {account_code: cycle_code}}

def set_cycles_map(upload_id: str, mapping: Dict[str, str]) -> None:
    """ì—…ë¡œë“œ ì‹ë³„ì ë³„ ê³„ì •â†’ì‚¬ì´í´ ë§¤í•‘ ì €ì¥."""
    if not upload_id:
        upload_id = "_default"
    cleaned: Dict[str, str] = {}
    for k, v in (mapping or {}).items():
        lab = (str(v).strip().upper() if v is not None else "OTHER")
        cleaned[str(k)] = lab if lab in ALLOWED_CYCLES else "OTHER"
    _MEM[upload_id] = cleaned

def get_cycles_map(upload_id: str) -> Dict[str, str]:
    """í•´ë‹¹ ì—…ë¡œë“œì˜ ë§¤í•‘ ì¡°íšŒ(ì—†ìœ¼ë©´ ë¹ˆ dict)."""
    if not upload_id:
        upload_id = "_default"
    return dict(_MEM.get(upload_id, {}))

def get_effective_cycles(upload_id: str | None = None) -> Dict[str, str]:
    """
    UI í¸ì˜ìš©: í˜„ì¬ ì—…ë¡œë“œì˜ 'ê³„ì •â†’ì‚¬ì´í´ ë§¤í•‘'ë§Œ ë°˜í™˜.
    (âš ï¸ í”„ë¦¬ì…‹ê³¼ ë‹¤ë¦„. ì¸ì í•„ìš”)
    """
    if upload_id and upload_id in _MEM:
        return dict(_MEM[upload_id])
    return {}

def accounts_for_cycles(mapping: Dict[str, str], cycles: Iterable[str]) -> List[str]:
    want = set(map(str, (cycles or [])))
    return [code for code, cyc in (mapping or {}).items() if str(cyc) in want]

def accounts_for_cycles_ko(mapping: Dict[str, str], cycles_ko: Iterable[str]) -> List[str]:
    codes = [ko_to_code(x) for x in (cycles_ko or [])]
    return accounts_for_cycles(mapping, codes)

# -------------------------------------------------
# 2) â€˜í”„ë¦¬ì…‹â€™(ì‚¬ì´í´ ì¹´í…Œê³ ë¦¬ ì •ì˜) â€” í‘œì¤€ + ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œ
# -------------------------------------------------
try:
    from config import STANDARD_ACCOUNTING_CYCLES, CYCLES_USER_OVERRIDES_PATH
except Exception:
    STANDARD_ACCOUNTING_CYCLES, CYCLES_USER_OVERRIDES_PATH = {}, ""

def _load_overrides() -> Dict[str, List[str]]:
    p = CYCLES_USER_OVERRIDES_PATH
    if not p or not os.path.exists(p):
        return {}
    try:
        with open(p, "r", encoding="utf-8") as f:
            data = json.load(f)
        return {str(k): [str(x) for x in v] for k, v in data.items() if isinstance(v, list)}
    except Exception:
        return {}

def get_cycles_preset() -> Dict[str, List[str]]:
    """
    'ì‚¬ì´í´ ì½”ë“œ â†’ [í‚¤ì›Œë“œ/ìƒ˜í”Œê³„ì • ë“±]' í”„ë¦¬ì…‹ ì •ì˜ ë°˜í™˜.
    (âš ï¸ ë§¤í•‘ ì•„ë‹˜. ì¸ì ì—†ìŒ)
    """
    base = {str(k): list(v) for k, v in (STANDARD_ACCOUNTING_CYCLES or {}).items()}
    ov = _load_overrides()
    for k, v in ov.items():
        base[str(k)] = list(v)
    return base

# -------------------------------------------------
# 3) ë£° ê¸°ë°˜ ì´ˆì•ˆ + (ì„ íƒ) LLM ë³´ì • â†’ ë§¤í•‘ ë¹Œë“œ
# -------------------------------------------------
def rule_based_guess(code_to_name: Dict[str, str]) -> Dict[str, str]:
    KW = {
        "CLOSE":  ["ê²°ì‚°","ì¡°ì •","ëŒ€ì†","ì¶©ë‹¹ê¸ˆ","í‰ê°€ì†ì‹¤","í‰ê°€ì´ìµ","ì™¸í™”í™˜ì‚°"],
        "REV":    ["ë§¤ì¶œ","ìš©ì—­ìˆ˜ìµ","ìˆ˜ìµ","ì™¸ìƒë§¤ì¶œ","ë§¤ì¶œì±„ê¶Œ","Sales","Revenue"],
        "PUR":    ["ë§¤ì…","êµ¬ë§¤","ì›ì¬ë£Œ","ìš©ì—­ë¹„","ì§€ê¸‰ìˆ˜ìˆ˜ë£Œ","ìš´ë°˜ë¹„","ê´‘ê³ ","ì ‘ëŒ€","ì„ì°¨","ìˆ˜ìˆ˜ë£Œ","ìˆ˜ë„","í†µì‹ "],
        "HR":     ["ê¸‰ì—¬","ì„ê¸ˆ","ìƒì—¬","í‡´ì§","ë³µë¦¬í›„ìƒ","ì—°ê¸ˆ","ì‹ëŒ€","ê²½ì¡°","ì˜ë£Œ"],
        "TREASURY_INVEST": ["ì˜ˆê¸ˆ","CMA","ê¸ˆìœµìƒí’ˆ","ìœ ê°€ì¦ê¶Œ","íˆ¬ì","íŒŒìƒ","ì´ììˆ˜ìµ","ë°°ë‹¹"],
        "TREASURY_FINANCE":["ì°¨ì…ê¸ˆ","ëŒ€ì¶œ","ì‚¬ì±„","ì–´ìŒ","ì´ìë¹„ìš©","ê¸ˆìœµë¹„ìš©"],
        "TAX":    ["ë¶€ê°€ì„¸","ë²•ì¸ì„¸","ì›ì²œ","ì§€ë°©ì„¸","ê°€ì‚°ì„¸","ì„¸ê¸ˆê³¼ê³µê³¼"],
        "PPE":    ["ìœ í˜•ìì‚°","ê±´ì„¤ì¤‘","ê¸°ê³„","ë¹„í’ˆ","ì°¨ëŸ‰","ê±´ë¬¼","í† ì§€","ê°ê°€ìƒê°","ì²˜ë¶„ì†ìµ"],
        "INTANG": ["ë¬´í˜•ìì‚°","ê°œë°œë¹„","ì†Œí”„íŠ¸ì›¨ì–´","ìƒí‘œê¶Œ","ì˜ì—…ê¶Œ","ìƒê°"],
        "LEASE":  ["ë¦¬ìŠ¤","ì‚¬ìš©ê¶Œìì‚°","ë¦¬ìŠ¤ë¶€ì±„","ë¦¬ìŠ¤ë£Œ"],
    }
    out: Dict[str, str] = {}
    for code, nm in (code_to_name or {}).items():
        name = str(nm or "")
        label = "OTHER"
        for cyc, kws in KW.items():
            if any(kw in name for kw in kws):
                label = cyc
                break
        out[str(code)] = label
    return out

def merge_cycles(base: Dict[str, str], override: Dict[str, str]) -> Dict[str, str]:
    out = dict(base)
    for k, v in (override or {}).items():
        if v:
            out[str(k)] = str(v)
    return out

def build_cycles_preset(upload_id: str, code_to_name: Dict[str, str], use_llm: bool = False) -> Dict[str, str]:
    """
    ê³„ì •â†’ì‚¬ì´í´ ë§¤í•‘ ì´ˆì•ˆ ìƒì„± í›„ ì €ì¥. (ë£°ë² ì´ìŠ¤ + ì„ íƒì  LLM ë³´ì •)
    """
    base = rule_based_guess(code_to_name)
    if use_llm:
        try:
            from services.llm import suggest_cycles_for_accounts
            llm_map = suggest_cycles_for_accounts(code_to_name)
            base = merge_cycles(base, llm_map)
        except Exception:
            pass
    set_cycles_map(upload_id, base)
    return base

__all__ = [
    "ALLOWED_CYCLES","CYCLE_KO","code_to_ko","ko_to_code",
    "set_cycles_map","get_cycles_map","get_effective_cycles",
    "get_cycles_preset","accounts_for_cycles","accounts_for_cycles_ko",
    "rule_based_guess","merge_cycles","build_cycles_preset",
]



==============================
ğŸ“„ FILE: services/external.py
==============================




==============================
ğŸ“„ FILE: services/llm.py
==============================

"""
# services/llm.py
# - í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ì´ì–¸íŠ¸: í‚¤ê°€ ìˆìœ¼ë©´ OpenAI ì˜¨ë¼ì¸ ëª¨ë“œ, ì—†ìœ¼ë©´ ì˜¤í”„ë¼ì¸ ìŠ¤í…
"""

import os
from functools import lru_cache


def openai_available() -> bool:
    try:
        # ë¶€íŒ… ì‹œ env_loaderê°€ LLM_AVAILABLE í”Œë˜ê·¸ë¥¼ ì…‹ì—…
        from infra.env_loader import ensure_api_keys_loaded, is_llm_ready
        ensure_api_keys_loaded()
        return bool(is_llm_ready())
    except Exception:
        # ì§ì ‘ í™˜ê²½ë³€ìˆ˜ í™•ì¸ (í´ë°±)
        return bool(os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY"))


class _DummyEmbeddings:
    def create(self, *args, **kwargs):
        raise RuntimeError("Embeddings API not available in offline stub.")


class _DummyChat:
    class _Resp:
        class Choice:
            class Msg:
                content = ""
            message = Msg()
        choices = [Choice()]

    def completions(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")

    def completions_create(self, *args, **kwargs):
        raise RuntimeError("Chat completions not available in offline stub.")


class _DummyClient:
    embeddings = _DummyEmbeddings()
    chat = _DummyChat()


@lru_cache(maxsize=1)
def _openai_client():
    try:
        from openai import OpenAI  # type: ignore
    except Exception as e:
        return None
    # í‚¤ëŠ” env_loaderê°€ ì´ë¯¸ ì£¼ì…í•¨
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_KEY")
    try:
        return OpenAI(api_key=api_key) if api_key else OpenAI()
    except Exception:
        return None


class LLMClient:
    def __init__(self, model: str | None = None, temperature: float | None = None, json_mode: bool | None = None):
        self._online = openai_available() and (_openai_client() is not None)
        self.client = _openai_client() if self._online else _DummyClient()
        # í˜¸ì¶œ í¸ì˜ íŒŒë¼ë¯¸í„° ì €ì¥(online generateì—ì„œ ì‚¬ìš©)
        self.model = model or os.getenv("LLM_MODEL", "gpt-4o")
        try:
            self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.2" if temperature is None else str(temperature)))
        except Exception:
            self.temperature = 0.2
        self.json_mode = True if (os.getenv("LLM_JSON_MODE", "true").lower() in ("1","true","yes")) else False

    def generate(self, system: str, user: str, tools=None, *, model: str | None = None, max_tokens: int | None = None, force_json: bool | None = None) -> str:
        if not self._online or self.client is None:
            raise RuntimeError("LLM not available: no API key or client. Run in offline mode.")
        try:
            use_model = model or self.model
            kwargs = dict(
                model=use_model,
                temperature=float(self.temperature),
                messages=[{"role":"system","content":system},{"role":"user","content":user}],
            )
            if max_tokens is not None:
                kwargs["max_tokens"] = int(max_tokens)
            if tools:
                kwargs["tools"] = tools
                try:
                    first_tool = tools[0]["function"]["name"]
                    if first_tool:
                        kwargs["tool_choice"] = {"type": "function", "function": {"name": first_tool}}
                except Exception:
                    pass
            else:
                use_force_json = self.json_mode if force_json is None else bool(force_json)
                if use_force_json:
                    kwargs["response_format"] = {"type": "json_object"}

            try:
                resp = self.client.chat.completions.create(**kwargs, timeout=60)
            except TypeError:
                resp = self.client.chat.completions.create(**kwargs)
            msg = resp.choices[0].message
            try:
                tool_calls = getattr(msg, "tool_calls", None)
                if tool_calls:
                    call = tool_calls[0]
                    args = getattr(call.function, "arguments", None)
                    return (args or "").strip()
            except Exception:
                pass
            return (msg.content or "").strip()
        except Exception as e:
            raise

    def name_cluster(self, samples_desc: list[str], samples_vendor: list[str]) -> str | None:
        """ê±°ë˜ ìƒ˜í”Œì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        import textwrap
        if not self._online or self.client is None:
            return None
        prompt = textwrap.dedent(f"""
        ë„ˆëŠ” íšŒê³„ ê°ì‚¬ ë³´ì¡° AIë‹¤. ì•„ë˜ ê±°ë˜ ìƒ˜í”Œì„ ë³´ê³  ì´ ê·¸ë£¹ì˜ ì„±ê²©ì„ ê°€ì¥ ì˜ ë“œëŸ¬ë‚´ëŠ”
        í•œêµ­ì–´ **í´ëŸ¬ìŠ¤í„° ì´ë¦„ 1ê°œë§Œ**, 10ì ë‚´ì™¸ë¡œ ì œì‹œí•´ë¼.
        ìˆ«ì/ê¸°í˜¸/ë”°ì˜´í‘œ/ì ‘ë‘ì‚¬ ê¸ˆì§€. ì˜ˆ: ì§ì› ê¸‰ì—¬, í†µì‹ ë¹„, ì„ì› ë³µì§€.

        [ê±°ë˜ ì ìš” ìƒ˜í”Œ]
        - {'\n- '.join(samples_desc) if samples_desc else '(ìƒ˜í”Œ ë¶€ì¡±)'}

        [ì£¼ìš” ê±°ë˜ì²˜ ìƒ˜í”Œ]
        - {'\n- '.join(samples_vendor) if samples_vendor else '(ìƒ˜í”Œ ë¶€ì¡±)'}

        ì •ë‹µ:
        """).strip()
        try:
            resp = self.client.chat.completions.create(
                model=self.model, messages=[{"role":"user","content":prompt}], temperature=0
            )
            cand = (resp.choices[0].message.content or "").strip().splitlines()[0].strip(" \"'[]()")
            if not cand:
                return None
            bad = {"í´ëŸ¬ìŠ¤í„°","unknown","ì´ë¦„ì—†ìŒ","ë¯¸ì •","ê¸°íƒ€"}
            if cand.lower() in bad or cand.startswith("í´ëŸ¬ìŠ¤í„°"):
                return None
            return cand[:20]
        except Exception:
            return None


import json, re
from typing import Dict

LLM_ENABLED = True

CYCLE_DEFS = {
    "CLOSE":"ê²°ì‚°Â·ì¡°ì •, ì¶©ë‹¹ê¸ˆ/í‰ê°€ì†ìµ/í™˜ì‚° ë“± ê¸°ë§ì¡°ì •",
    "REV":"ë§¤ì¶œÂ·ìˆ˜ìµ ê´€ë ¨",
    "PUR":"ë§¤ì…Â·êµ¬ë§¤Â·ë¹„ìš©(ê´‘ê³ , ìˆ˜ìˆ˜ë£Œ, ì„ì°¨ë£Œ ë“± í¬í•¨)",
    "HR":"ê¸‰ì—¬Â·ìƒì—¬Â·í‡´ì§Â·ë³µë¦¬í›„ìƒ",
    "TREASURY_INVEST":"ì˜ˆê¸ˆÂ·ê¸ˆìœµìƒí’ˆÂ·íˆ¬ìÂ·íŒŒìƒÂ·ì´ì/ë°°ë‹¹ìˆ˜ìµ",
    "TREASURY_FINANCE":"ì°¨ì…Â·ì‚¬ì±„Â·ì–´ìŒÂ·ì´ìë¹„ìš©",
    "TAX":"ë¶€ê°€ì„¸Â·ë²•ì¸ì„¸Â·ì›ì²œ ë“± ì„¸ë¬´",
    "PPE":"ìœ í˜•ìì‚°Â·ê°ê°€ìƒê°Â·ê±´ì„¤ì¤‘",
    "INTANG":"ë¬´í˜•ìì‚°Â·ìƒê°",
    "LEASE":"ë¦¬ìŠ¤Â·ì‚¬ìš©ê¶Œìì‚°Â·ë¦¬ìŠ¤ë¶€ì±„",
    "OTHER":"ê¸°íƒ€(ì •ë§ ë¶„ë¥˜ê°€ ë¶ˆê°€í•  ë•Œë§Œ)",
}

def _normalize_cycle_label(label: str) -> str:
    t = str(label).strip().upper()
    aliases = {
        "ê²°ì‚°": "CLOSE", "ë§¤ì¶œ": "REV", "ë§¤ì…": "PUR", "ë§¤ì…Â·ë¹„ìš©":"PUR", "ë¹„ìš©":"PUR",
        "ì¸ì‚¬":"HR", "ê¸‰ì—¬":"HR", "ë³µë¦¬í›„ìƒ":"HR",
        "ìê¸ˆìš´ìš©":"TREASURY_INVEST", "íˆ¬ì":"TREASURY_INVEST", "íŒŒìƒ":"TREASURY_INVEST",
        "ìê¸ˆì¡°ë‹¬":"TREASURY_FINANCE", "ì°¨ì…":"TREASURY_FINANCE", "ì‚¬ì±„":"TREASURY_FINANCE",
        "ì„¸ë¬´":"TAX", "ìœ í˜•ìì‚°":"PPE", "ë¬´í˜•ìì‚°":"INTANG", "ë¦¬ìŠ¤":"LEASE",
        "ê¸°íƒ€":"OTHER"
    }
    return aliases.get(t, t if t in CYCLE_DEFS else "OTHER")

def suggest_cycles_for_accounts(account_names: Dict[str, str]) -> Dict[str, str]:
    if not LLM_ENABLED or not account_names:
        return {}
    try:
        client = LLMClient()
        if not client._online or client.client is None:
            return {}
        def short_defs():
            items = [f"- {k}: {v}" for k, v in CYCLE_DEFS.items() if k != "OTHER"]
            items.append("- OTHER: ìœ„ ì–´ë–¤ ë²”ì£¼ì—ë„ ëª…í™•íˆ ì†í•˜ì§€ ì•Šì„ ë•Œë§Œ")
            return "\n".join(items)
        examples = (
            '{"1000":"í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°","1130":"ë§¤ì¶œì±„ê¶Œ","5110":"ê´‘ê³ ì„ ì „ë¹„","2210":"ë‹¨ê¸°ì°¨ì…ê¸ˆ","8100":"ë²•ì¸ì„¸ë¹„ìš©"}'
        )
        expected = (
            '{"1000":"TREASURY_INVEST","1130":"REV","5110":"PUR","2210":"TREASURY_FINANCE","8100":"TAX"}'
        )
        prompt = (
            "ì•„ë˜ 'í—ˆìš© ë¼ë²¨' ì¤‘ í•˜ë‚˜ë¡œ ê° ê³„ì •ì½”ë“œë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. í•œ í•­ëª©ì€ ì˜¤ì§ í•˜ë‚˜ì˜ ë¼ë²¨.\n"
            "'OTHER'ëŠ” ìµœí›„ìˆ˜ë‹¨ì…ë‹ˆë‹¤. ì• ë§¤í•˜ë©´ ê°€ì¥ ê°€ê¹Œìš´ ë¼ë²¨ì„ ê³ ë¥´ì„¸ìš”.\n"
            "ë°˜ë“œì‹œ JSON(object)ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì¶”ê°€ í…ìŠ¤íŠ¸/ì„¤ëª… ê¸ˆì§€.\n\n"
            f"í—ˆìš© ë¼ë²¨(ì •ì˜):\n{short_defs()}\n\n"
            f"ì˜ˆì‹œ ì…ë ¥:\n{examples}\n\nì˜ˆì‹œ ì¶œë ¥(JSONë§Œ):\n{expected}\n\n"
            f"ì…ë ¥(JSON):\n{json.dumps(account_names, ensure_ascii=False)}"
        )
        raw = client.generate("You are an expert accountant.", prompt, force_json=True, max_tokens=2000)
        data = json.loads(raw)
        out: Dict[str, str] = {}
        if isinstance(data, dict):
            for code, lab in data.items():
                out[str(code)] = _normalize_cycle_label(lab)
        return out
    except Exception:
        return {}




==============================
ğŸ“„ FILE: services/__init__.py
==============================






==============================
ğŸ“„ FILE: tests/conftest.py
==============================

# tests/conftest.py
import os, sys
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)



==============================
ğŸ“„ FILE: tests/test_anomaly.py
==============================

# Z-Score ë° ìœ„í—˜ ì ìˆ˜ ë‹¨ì¡°ì„± í…ŒìŠ¤íŠ¸
import pandas as pd
from analysis.anomaly import calculate_grouped_stats_and_zscore, _risk_from

def test_zscore_and_risk_monotonic():
    df = pd.DataFrame({
        "ê³„ì •ì½”ë“œ": ["100"]*5 + ["200"]*5,
        "ì°¨ë³€":     [0,0,0,0,0] + [0,0,0,0,0],
        "ëŒ€ë³€":     [10,20,30,40,50] + [5,5,5,5,5],
    })
    out = calculate_grouped_stats_and_zscore(df, target_accounts=["100","200"])
    assert "Z-Score" in out.columns
    a1 = _risk_from(1.0, amount=1_000, pm=100_000)[-1]
    a2 = _risk_from(3.0, amount=1_000, pm=100_000)[-1]
    b1 = _risk_from(1.0, amount= 50_000, pm=100_000)[-1]
    b2 = _risk_from(1.0, amount=150_000, pm=100_000)[-1]
    assert a2 > a1
    assert b2 > b1





==============================
ğŸ“„ FILE: tests/test_architecture_imports.py
==============================

from pathlib import Path

FORBIDDEN = ("from services", "import services")


def test_analysis_not_import_services():
    bad = []
    for p in Path("analysis").rglob("*.py"):
        try:
            t = p.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        if any(x in t for x in FORBIDDEN):
            bad.append(str(p))
    assert not bad, f"analysis must not import services: {bad}"





==============================
ğŸ“„ FILE: tests/test_evidence_schema.py
==============================

from dataclasses import asdict
from analysis.anomaly import run_anomaly_module
from analysis.contracts import LedgerFrame
import pandas as pd


def test_anomaly_emits_evidence_minimal():
    # ê°„ë‹¨ ê°€ì§œ DF
    df = pd.DataFrame({
        "row_id":["a","b","c"],
        "íšŒê³„ì¼ì": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03"]),
        "ê³„ì •ì½”ë“œ": ["400","400","400"],
        "ê³„ì •ëª…":   ["ë§¤ì¶œ","ë§¤ì¶œ","ë§¤ì¶œ"],
        "ì°¨ë³€": [0, 0, 0],
        "ëŒ€ë³€": [10_000_000, 100, 50],
    })
    lf = LedgerFrame(df=df, meta={})
    mod = run_anomaly_module(lf, target_accounts=["400"], topn=2, pm_value=500_000_000)
    assert isinstance(mod.evidences, list)
    assert len(mod.evidences) >= 1
    d = asdict(mod.evidences[0])
    for key in ["row_id","reason","anomaly_score","financial_impact","risk_score","is_key_item","impacted_assertions","links"]:
        assert key in d




==============================
ğŸ“„ FILE: tests/test_kdmeans_basic.py
==============================

# ê°„ë‹¨ ë‹¨ìœ„í…ŒìŠ¤íŠ¸: KDMeansê°€ ì˜ í•™ìŠµë˜ê³  ì†ì„±ë“¤ì´ ì±„ì›Œì§€ëŠ”ì§€ í™•ì¸
import numpy as np
from analysis.kdmeans_shim import HDBSCAN

def test_kdmeans_fixed_k():
    rng = np.random.default_rng(0)
    X = np.vstack([
        rng.normal(loc=[0,0], scale=0.1, size=(25,2)),
        rng.normal(loc=[3,3], scale=0.1, size=(25,2)),
    ])
    model = HDBSCAN(n_clusters=2, random_state=0).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ == 2
    assert set(model.labels_) == {0,1}
    assert np.allclose(model.probabilities_, 1.0)

def test_kdmeans_auto_k_runs():
    rng = np.random.default_rng(1)
    X = rng.normal(size=(200, 4))
    model = HDBSCAN(n_clusters=None, random_state=1).fit(X)
    assert model.labels_.shape[0] == X.shape[0]
    assert model.chosen_k_ is not None




==============================
ğŸ“„ FILE: tests/test_report_units.py
==============================

# ë¦¬í¬íŠ¸ ë‚´ ì›(â‚©) ë‹¨ìœ„ ê°•ì œ ë³€í™˜ í…ŒìŠ¤íŠ¸
from analysis.report import _enforce_won_units

def test_unit_enforcement():
    s = "ì´ì•¡ì€ 3ì–µ 5,072ë§Œ ì›ì´ë©° ì´ì „ì—ëŠ” 2ì–µ ì›ì´ì—ˆë‹¤."
    out = _enforce_won_units(s)
    assert "350,720,000ì›" in out
    assert "200,000,000ì›" in out





==============================
ğŸ“„ FILE: tests/test_risk_boundaries.py
==============================

import math
import pytest
from analysis.anomaly import _risk_from


@pytest.mark.parametrize("z_abs", [0.0, 1.0, 3.0, 10.0])
@pytest.mark.parametrize("pm_value", [0.0, 1.0, 1e6, 1e12])
def test_risk_from_boundary_is_finite(z_abs, pm_value):
    a, f, k, score = _risk_from(z_abs=z_abs, amount=1_000_000, pm=pm_value)
    assert 0.0 <= score <= 1.0
    assert math.isfinite(score)


def test_risk_from_monotonic_in_z_when_pm_fixed():
    pm = 1e6
    s1 = _risk_from(0.5, amount=1_000_000, pm=pm)[-1]
    s2 = _risk_from(3.0, amount=1_000_000, pm=pm)[-1]
    s3 = _risk_from(10.0, amount=1_000_000, pm=pm)[-1]
    assert s1 <= s2 <= s3


@pytest.mark.parametrize("pm_lo,pm_hi", [(0.0, 1e9)])
def test_risk_from_non_decreasing_in_pm(pm_lo, pm_hi):
    z = 3.0
    slo = _risk_from(z, amount=1_000_000, pm=pm_lo)[-1]
    shi = _risk_from(z, amount=1_000_000, pm=pm_hi)[-1]
    assert slo <= shi




==============================
ğŸ“„ FILE: tests/test_risk_score.py
==============================

import math
from analysis.anomaly import _risk_from
from config import Z_SIGMOID_SCALE


def test_risk_from_basic():
    a, f, k, score = _risk_from(z_abs=3.0, amount=1_000_000_000, pm=500_000_000)
    exp_a = 1.0 / (1.0 + math.exp(-(3.0/float(Z_SIGMOID_SCALE or 1.0))))
    assert abs(a - exp_a) < 1e-9
    assert f == 1.0                 # PM ëŒ€ë¹„ ìº¡ 1
    assert k == 1.0                 # KIT
    # ê°€ì¤‘í•©: 0.5*a + 0.4*1 + 0.1*1
    expected = 0.5*a + 0.4 + 0.1
    assert abs(score - expected) < 1e-9


def test_risk_from_zero_pm_guard():
    a, f, k, score = _risk_from(z_abs=0.0, amount=0, pm=0)
    assert f == 0.0 and k == 0.0    # ë¶„ëª¨ ê°€ë“œ ë™ì‘
    assert 0.0 <= a <= 0.5          # z=0ì´ë©´ aëŠ” 0.5 ê·¼ì²˜




==============================
ğŸ“„ FILE: tests/test_risk_score_more.py
==============================

from analysis.anomaly import _risk_from
from analysis.anomaly import _assertions_for_row


def test_risk_from_none_and_negative_pm():
    for pm in (None, -1, -1000):
        a, f, k, score = _risk_from(z_abs=2.0, amount=1_000_000, pm=pm)
        assert f == 0.0 and k == 0.0
        assert 0.0 <= a <= 1.0
        assert 0.0 <= score <= 1.0

def test_assertions_mapping_rules():
    # í•­ìƒ A í¬í•¨
    assert "A" in _assertions_for_row(0.0)
    # í° ì–‘ì˜ ì´íƒˆ â†’ E í¬í•¨
    assert set(_assertions_for_row(+2.5)) >= {"A","E"}
    # í° ìŒì˜ ì´íƒˆ â†’ C í¬í•¨
    assert set(_assertions_for_row(-2.5)) >= {"A","C"}




==============================
ğŸ“„ FILE: tests/test_snapshot_core.py
==============================

import pandas as pd
from dataclasses import asdict
from analysis.contracts import LedgerFrame
from analysis.anomaly import run_anomaly_module


def _mini_df():
    df = pd.DataFrame({
        "row_id": ["file|L:2","file|L:3","file|L:4","file|L:5"],
        "íšŒê³„ì¼ì": pd.to_datetime(["2024-01-01","2024-01-02","2024-01-03","2024-01-04"]),
        "ê³„ì •ì½”ë“œ": ["101","101","201","201"],
        "ê³„ì •ëª…":   ["í˜„ê¸ˆ","í˜„ê¸ˆ","ë§¤ì¶œ","ë§¤ì¶œ"],
        "ì°¨ë³€": [0, 0, 0, 0],
        "ëŒ€ë³€": [5_000_000, 100, 50_000_000, 200],
    })
    return df


def test_snapshot_evidence_and_matrix_stable():
    lf = LedgerFrame(df=_mini_df(), meta={})
    mod = run_anomaly_module(lf, target_accounts=["101","201"], topn=10, pm_value=500_000_000)
    # Evidence ìŠ¤ëƒ…ìƒ·(í•µì‹¬ í•„ë“œë§Œ ë¹„êµ)
    snap = [{
        "row_id": e.row_id,
        "risk_score": round(e.risk_score, 6),
        "is_key_item": e.is_key_item,
        "assertions": tuple(e.impacted_assertions),
        "acct": e.links.get("account_name") or e.links.get("account_code")
    } for e in mod.evidences]
    # ê³ ì • ê¸°ëŒ€ê°’(ë¦¬ìŠ¤í¬ ê°€ì¤‘ì¹˜/PMì´ ë°”ë€Œë©´ ì‹¤íŒ¨í•˜ë„ë¡)
    assert any(s["acct"] == "ë§¤ì¶œ" for s in snap)
    # (ìœ„í—˜í‰ê°€/ë§¤íŠ¸ë¦­ìŠ¤ ì„ì‹œ ì œê±°: ê´€ë ¨ ë‹¨ì–¸ ì‚­ì œ)




==============================
ğŸ“„ FILE: tests/test_timeseries_naive.py
==============================

# ì‹œê³„ì—´ ëª¨ë“ˆì˜ ê°„ë‹¨ ë°±ì—”ë“œ(EMA ë“±) ë™ì‘ì„± í…ŒìŠ¤íŠ¸
import pandas as pd
import numpy as np
from analysis.timeseries import run_timeseries_module, model_registry, z_and_risk, run_timeseries_for_account


def test_timeseries_naive_backend():
    df = pd.DataFrame({
        "account": ["A"]*7 + ["B"]*7,
        "date":    list(range(1,8)) + list(range(1,8)),
        "amount":  [10,11,10,12,11,10, 20] + [8,8,8,8,8,8, 5],
    })
    res = run_timeseries_module(df, account_col="account", date_col="date",
                                amount_col="amount", backend="ema", window=3)
    assert set(res["account"]) == {"A","B"}
    assert set(res["assertion"]).issubset({"E","C"})


def test_z_and_risk_basic():
    # í•œê¸€: 0 ì¤‘ì‹¬ ëŒ€ì¹­ ì”ì°¨ì— ëŒ€í•´ |z|ê°€ í¬ë©´ riskê°€ ì»¤ì§„ë‹¤
    resid = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])
    z, r = z_and_risk(resid)
    assert len(z) == len(resid)
    assert len(r) == len(resid)
    assert float(r[0]) > float(r[1])
    assert float(r[-1]) == float(r[0])


def test_model_registry_keys_present():
    reg = model_registry()
    for k in ["ma","ema","arima","prophet"]:
        assert k in reg
    assert reg["ma"] is True and reg["ema"] is True


def test_run_timeseries_for_account_dual():
    # í•œê¸€: 12ê°œì›” ìƒ˜í”Œë¡œ flow ëˆ„ì  balance ìƒì„±í•˜ì—¬ ë‘ íŠ¸ë™ ëª¨ë‘ ë°˜í™˜
    dates = pd.period_range("2024-01", periods=12, freq="M").to_timestamp()
    flow = np.linspace(100, 210, num=12)
    df = pd.DataFrame({"date": dates, "flow": flow})
    df["balance"] = df["flow"].cumsum()
    out = run_timeseries_for_account(df, account="ë§¤ì¶œì±„ê¶Œ", is_bs=True, flow_col="flow", balance_col="balance")
    assert set(out["measure"]).issuperset({"flow","balance"})
    assert set(out.columns) == {"date","account","measure","actual","predicted","error","z","risk","model"}





==============================
ğŸ“„ FILE: tests/test_zbins.py
==============================

import numpy as np, pandas as pd
from analysis.anomaly import _z_bins_025_sigma


def test_zbins_label_and_count():
    s = pd.Series(np.linspace(-4, 4, 101))
    df, order = _z_bins_025_sigma(s)
    assert len(df) == len(order) == 26      # í…Œì¼ í¬í•¨ 26ê°œ
    assert int(df["ê±´ìˆ˜"].sum()) == 101     # ì´í•© ë³´ì¡´




==============================
ğŸ“„ FILE: ui/inputs.py
==============================

import re
import streamlit as st

# KRW ì…ë ¥ ìœ„ì ¯(ì²œë‹¨ìœ„ ì‰¼í‘œ) - ì•ˆì •í˜•
# - ì‚¬ìš©ìëŠ” ììœ ë¡­ê²Œ íƒ€ì´í•‘(ì‰¼í‘œ/ê³µë°±/ë¬¸ì ì„ì—¬ë„ ë¬´ì‹œ)í•˜ê³ ,
#   í¬ì»¤ìŠ¤ ì•„ì›ƒ/ì—”í„° ì‹œì—ë§Œ ì •ê·œí™”(ìˆ«ìë§Œ ìœ ì§€ â†’ ì‰¼í‘œ í¬ë§·)í•©ë‹ˆë‹¤.
# - ì‹¤ì œ ìˆ«ìê°’ì€ session_state[key] (int)ë¡œ ë³´ê´€í•©ë‹ˆë‹¤.
# - í‘œì‹œìš© ë¬¸ìì—´ì€ session_state[f"{key}__txt"] ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

def _parse_krw_text(s: str) -> int:
    """ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ ì•ˆì „í•˜ê²Œ intë¡œ ë³€í™˜(ìŒìˆ˜ ë°©ì–´, ê³µë€=0)."""
    if s is None:
        return 0
    s = str(s).replace(",", "").strip()
    s = re.sub(r"[^\d]", "", s)  # ìˆ«ì ì´ì™¸ ì œê±°
    if s == "":
        return 0
    try:
        return max(0, int(s))
    except Exception:
        return 0

def _fmt_krw(n: int) -> str:
    """ì •ìˆ˜ë¥¼ ì²œë‹¨ìœ„ ì‰¼í‘œ ë¬¸ìì—´ë¡œ í¬ë§·."""
    try:
        return f"{int(n):,}"
    except Exception:
        return "0"

def krw_input(label: str, key: str, default_value: int = 0, help_text: str = "") -> int:
    """
    KRW ì…ë ¥(ì²œë‹¨ìœ„ ì‰¼í‘œ) í†µí•© ìœ„ì ¯.
    - ìˆ«ì ìƒíƒœ: st.session_state[key] (int)
    - í‘œì‹œ ìƒíƒœ: st.session_state[f"{key}__txt"] (str, '1,234,567')
    - on_change ì‹œì—ë§Œ ì •ê·œí™”í•˜ì—¬ ì”ê³ ì¥(500,00 ë“±) ë°©ì§€
    """
    # ì´ˆê¸° ìƒíƒœ ë³´ì •
    if key not in st.session_state:
        st.session_state[key] = int(default_value)
    if f"{key}__txt" not in st.session_state:
        st.session_state[f"{key}__txt"] = _fmt_krw(st.session_state[key])

    def _commit():
        """ì‚¬ìš©ì ì…ë ¥ ì™„ë£Œ(í¬ì»¤ìŠ¤ ì•„ì›ƒ/ì—”í„°) ì‹œ ìˆ«ì/ë¬¸ì ìƒíƒœ ë™ê¸°í™”."""
        raw = st.session_state.get(f"{key}__txt", "")
        val = _parse_krw_text(raw)
        st.session_state[key] = val
        st.session_state[f"{key}__txt"] = _fmt_krw(val)
        # Streamlitì€ on_change í›„ ìë™ rerun â†’ ê·¸ë˜í”„/í‘œ ê°±ì‹ ì— ì¶©ë¶„

    # í‘œì‹œ ì…ë ¥ì°½(íƒ€ì´í•‘ ì¤‘ì—ëŠ” í¬ë§· ê°•ì œí•˜ì§€ ì•ŠìŒ)
    st.text_input(
        label,
        key=f"{key}__txt",
        help=help_text,
        placeholder="ì˜ˆ: 500,000,000",
        on_change=_commit,
    )
    return int(st.session_state.get(key, int(default_value)))





==============================
ğŸ“„ FILE: ui/validation_helpers.py
==============================

from __future__ import annotations

import streamlit as st
import pandas as pd
import plotly.express as px

from typing import Optional
from analysis.timeseries import build_trend_validation_data
from utils.viz import add_period_guides, add_materiality_threshold


def render_trend_validation_section(
    monthly: pd.DataFrame,
    *,
    section_id: str,
    is_bs: bool,
    pm_value: Optional[float] = None,
    title: str = "ë°ì´í„° ê²€ì¦: ì›”ë³„ ì¶”ì„¸ ë¶„ì„(ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì§ì ‘ ëŒ€ì¡°)",
):
    """
    - monthly: ['date','flow'] (+ 'balance' ì„ íƒ)
    - section_id: í† ê¸€ ê³ ìœ í‚¤ì— ì„ì„ ì‹ë³„ì(ê³„ì •ì½”ë“œ ë“±)
    - is_bs: BS ì—¬ë¶€
    - pm_value: PM í‘œì‹œ(ì ì„  ë³´ì¡°ì„ )
    """
    df_val = build_trend_validation_data(monthly, is_bs=is_bs)
    if df_val.empty:
        st.info("ê²€ì¦ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.subheader(title)

    # ê³ ìœ  keyë¡œ í† ê¸€ ì¶©ëŒ ë°©ì§€
    show_cfg = st.toggle("ì´ ì°¨íŠ¸ì˜ í†µê³„ ì„¤ì • ë³´ê¸°", key=f"trend_cfg_{section_id}")

    # ë§‰ëŒ€: ì‹¤ì¸¡, ì„ : ì˜ˆì¸¡
    fig = px.bar(
        df_val,
        x="date",
        y="actual",
        color="measure",
        barmode="group",
        title="actual vs predicted (ê²€ì¦)",
    )
    # ë™ì¼ xì¶• ìœ„ì— ì˜ˆì¸¡ì„  ì˜¤ë²„ë ˆì´
    for m in df_val["measure"].unique():
        sub = df_val[df_val["measure"] == m]
        fig.add_scatter(
            x=sub["date"],
            y=sub["predicted"],
            mode="lines+markers",
            name=f"predicted ({m})",
        )

    add_period_guides(fig, df_val["date"])
    if pm_value:
        add_materiality_threshold(fig, pm_value)

    # ìˆ«ì í¬ë§·
    fig.update_yaxes(separatethousands=True, tickformat=",.0f", showexponent="none", exponentformat="none")
    fig.update_traces(hovertemplate="month=%{x}<br>value=%{y:,.0f}<extra></extra>")

    st.plotly_chart(fig, use_container_width=True)

    if show_cfg:
        by_m = df_val.groupby("measure").agg(
            n_points=("date", "count"),
            last_actual=("actual", "last"),
            last_pred=("predicted", "last"),
        )
        st.dataframe(by_m, use_container_width=True)





==============================
ğŸ“„ FILE: ui/__init__.py
==============================

# íŒ¨í‚¤ì§€ ì´ˆê¸°í™” (ë¹„ì–´ìˆì–´ë„ ë¬´ë°©)




==============================
ğŸ“„ FILE: utils/drilldown.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional


def ensure_rowid(df: pd.DataFrame, id_col: str = "row_id") -> pd.DataFrame:
    """row_id ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜(ê³„ì•½ ì¤€ìˆ˜ëŠ” ìƒìœ„ ë‹¨ê³„ì—ì„œ)."""
    return df if id_col in df.columns else df


def attach_customdata(df: pd.DataFrame, cols: Iterable[str], id_col: str = "row_id"):
    """
    Plotlyì— ì˜¬ë¦´ customdata ë°°ì—´ ìƒì„±.
    ë°˜í™˜: (df, customdata(ndarray), header_labels(list))
    """
    import numpy as np
    use_cols = [c for c in cols if c in df.columns]
    if id_col not in use_cols and id_col in df.columns:
        use_cols = [id_col] + use_cols
    arr = df[use_cols].to_numpy()
    return df, np.asarray(arr), use_cols


def fmt_money(x) -> str:
    try:
        return f"{float(x):,.0f}"
    except Exception:
        return str(x)





==============================
ğŸ“„ FILE: utils/helpers.py
==============================

from __future__ import annotations
import pandas as pd
from typing import Iterable, Optional

def find_column_by_keyword(columns: Iterable[str], keyword: str) -> Optional[str]:
    """ì—´ ì´ë¦„ì—ì„œ keyword(ë¶€ë¶„ì¼ì¹˜, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)ë¥¼ ìš°ì„  íƒìƒ‰."""
    keyword = str(keyword or "").lower()
    cols = [str(c) for c in columns]
    # 1) ì™„ì „ ì¼ì¹˜ ìš°ì„ 
    for c in cols:
        if c.lower() == keyword:
            return c
    # 2) ë¶€ë¶„ ì¼ì¹˜
    for c in cols:
        if keyword in c.lower():
            return c
    return None

def add_provenance_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ì—…ë¡œë“œ ì¶œì²˜ ì •ë³´ê°€ ì—†ì–´ë„ row_idë¥¼ ê°•ì œë¡œ ë¶€ì—¬."""
    out = df.copy()
    if "row_id" not in out.columns:
        out["row_id"] = out.reset_index().index.astype(str)
    return out

def add_period_tag(df: pd.DataFrame) -> pd.DataFrame:
    """ì—°ë„ ìµœëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ CY/PY/Other íƒœê·¸ë¥¼ ë¶€ì—¬."""
    out = df.copy()
    if "ì—°ë„" not in out.columns:
        out["period_tag"] = "Other"
        return out
    y_max = out["ì—°ë„"].max()
    out["period_tag"] = out["ì—°ë„"].apply(lambda y: "CY" if y == y_max else ("PY" if y == y_max - 1 else "Other"))
    return out


# --- NEW: ì°¨íŠ¸ ì „ìš© ëŒ€ë³€ê³„ì • íŒì •(ê·¸ë˜í”„ì—ì„œë§Œ ë¶€í˜¸ ë°˜ì „) ---
def is_credit_account(account_type: str | None, dc: str | None = None) -> bool:
    """
    ê³„ì • ì„±ê²©ì´ ëŒ€ë³€(Credit)ì¸ì§€ íŒì •í•©ë‹ˆë‹¤.
    - dcê°€ ì£¼ì–´ì§€ë©´ ìš°ì„  ì‚¬ìš©(ì˜ˆ: 'ì°¨ë³€'/'ëŒ€ë³€' ë˜ëŠ” 'D'/'C')
    - ì•„ë‹ˆë©´ account_typeìœ¼ë¡œ ê°„ì ‘ íŒì •: ë¶€ì±„/ìë³¸/ìˆ˜ìµ â†’ Credit
    """
    try:
        if dc is not None:
            s = str(dc).strip().upper()
            return s.startswith("C") or ("ëŒ€ë³€" in s)
    except Exception:
        pass
    try:
        t = str(account_type or "").strip()
        return t in {"ë¶€ì±„", "ìë³¸", "ìˆ˜ìµ"}
    except Exception:
        return False


# --- NEW: ëª¨ë¸ ì„ íƒ ì´ìœ  ì„¤ëª… í…ìŠ¤íŠ¸ ---
def model_reason_text(name: str, d: dict) -> str:
    """
    ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ MoR ì„ íƒ ì‚¬ìœ ë¥¼ ìì—°ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    ê¸°ëŒ€ í‚¤: cv_mape_rank, seasonality_strength(0~1), stationary(bool), recent_trend(bool), n_points(int)
    """
    try:
        why = []
        nm = str(name or "").lower()
        if d.get("cv_mape_rank") == 1:
            why.append("êµì°¨ê²€ì¦ì—ì„œ ê°€ì¥ ë‚®ì€ MAPEë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.")
        if float(d.get("seasonality_strength", 0.0)) > 0.4 and nm.startswith("prophet"):
            why.append("ì—°/ë¶„ê¸° ìˆ˜ì¤€ì˜ ê³„ì ˆì„±ì´ ê°•í•˜ê²Œ ê´€ì¸¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if bool(d.get("stationary")) and nm.startswith("arima"):
            why.append("ì°¨ë¶„ í›„ ì •ìƒì„±ì´ í™•ë³´ë˜ì–´ ARIMA ì í•©ì´ ìœ ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        if bool(d.get("recent_trend")) and (nm.startswith("ema") or nm.startswith("holt") or nm.startswith("exp")):
            why.append("ìµœê·¼ ì¶”ì„¸ ë³€í™”ê°€ ì»¤ì„œ ìµœê·¼ê°’ ê°€ì¤‘ ëª¨ë¸ì´ ë” ì˜ ë§ì•˜ìŠµë‹ˆë‹¤.")
        if int(d.get("n_points", 0)) < 18 and (nm.startswith("ma") or nm.startswith("ema")):
            why.append("ê´€ì¸¡ì¹˜ê°€ ì§§ì•„ ë‹¨ìˆœ ì´ë™í‰ê·  ê³„ì—´ì´ ê³¼ì í•© ìœ„í—˜ì´ ë‚®ì•˜ìŠµë‹ˆë‹¤.")
        if not why:
            why.append("ì˜¤ì°¨ì§€í‘œ(MAE/MAPE)ì™€ ì •ë³´ëŸ‰(AIC/BIC)ì„ ì¢…í•©í•´ ìµœì  ëª¨ë¸ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        return " / ".join(why)
    except Exception:
        return "ì˜¤ì°¨ì§€í‘œ(MAE/MAPE)ì™€ ì •ë³´ëŸ‰(AIC/BIC)ì„ ì¢…í•©í•´ ìµœì  ëª¨ë¸ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤."


# --- NEW: ì‹œê³„ì—´ìš© ë‚ ì§œ/ê¸ˆì•¡ ì»¬ëŸ¼ ìë™ íƒìƒ‰ ---
def guess_time_and_amount_cols(df: pd.DataFrame):
    """ì‹œê³„ì—´ìš© ë‚ ì§œ/ê¸ˆì•¡ ì»¬ëŸ¼ì„ ìœ ì—°í•˜ê²Œ íƒìƒ‰í•œë‹¤."""
    date_candidates = ["íšŒê³„ì¼ì", "ì „í‘œì¼ì", "ê±°ë˜ì¼ì", "ì¼ì", "date", "Date"]
    amt_candidates  = [
        "ê±°ë˜ê¸ˆì•¡", "ë°œìƒì•¡", "ê¸ˆì•¡", "ê¸ˆì•¡(ì›)", "ê±°ë˜ê¸ˆì•¡_ì ˆëŒ€ê°’",
        "ë°œìƒì•¡_ì ˆëŒ€ê°’", "ìˆœì•¡", "ìˆœì•¡(ì›)"
    ]
    cols = list(df.columns) if df is not None else []
    date_col = next((c for c in date_candidates if c in cols), None)
    amt_col  = next((c for c in amt_candidates  if c in cols), None)
    return date_col, amt_col


# --- NEW: ê³µìš© add_or_replace (df.insert ëŒ€ì²´) ---
def add_or_replace(df: pd.DataFrame, loc: int, col: str, values):
    """df.insert ëŒ€ì²´: ì´ë¯¸ ìˆìœ¼ë©´ êµì²´, ì—†ìœ¼ë©´ ì§€ì • ìœ„ì¹˜ì— ì¶”ê°€."""
    import pandas as pd
    if col in df.columns:
        df[col] = values
        return df
    df.insert(loc, col, values)
    return df


==============================
ğŸ“„ FILE: utils/monthly.py
==============================

import pandas as pd


def monthly_flow(df: pd.DataFrame, date_col: str, amt_col: str) -> pd.DataFrame:
    d = df[[date_col, amt_col]].copy()
    p = pd.to_datetime(d[date_col]).dt.to_period("M")
    g = d.assign(_p=p).groupby("_p", as_index=False)[amt_col].sum()
    g = g.rename(columns={"_p": "month"}).rename(columns={amt_col: "flow"})
    g["label"] = g["month"].astype(str)
    try:
        g["order"] = g["month"].astype(int)
    except Exception:
        # Fallback: use numeric YYYYMM from label
        g["order"] = g["label"].str.replace("-", "", regex=False).astype(int)
    return g[["month", "label", "order", "flow"]]


def monthly_balance_from_col(df: pd.DataFrame, date_col: str, bal_col: str) -> pd.DataFrame:
    d = df[[date_col, bal_col]].copy()
    p = pd.to_datetime(d[date_col]).dt.to_period("M")
    g = d.assign(_p=p).sort_values(date_col).groupby("_p", as_index=False).last()
    g = g.rename(columns={"_p": "month", bal_col: "balance"})
    g["label"] = g["month"].astype(str)
    try:
        g["order"] = g["month"].astype(int)
    except Exception:
        g["order"] = g["label"].str.replace("-", "", regex=False).astype(int)
    return g[["month", "label", "order", "balance"]]


def monthly_balance_from_flow(flow_df: pd.DataFrame, opening: float = 0.0) -> pd.DataFrame:
    g = flow_df[["month", "label", "order", "flow"]].copy()
    g["balance"] = float(opening) + g["flow"].astype(float).cumsum()
    return g[["month", "label", "order", "balance"]]





==============================
ğŸ“„ FILE: utils/viz.py
==============================

# utils/viz.py
# ëª©ì : Materiality(Performance Materiality, PM) ë³´ì¡°ì„ /ë°°ì§€ ì¶”ê°€ ìœ í‹¸
# - Plotly Figureì— ë¹¨ê°„ ì ì„ (ê°€ë¡œì„ ) + "PM=xxxì›" ë¼ë²¨ì„ ì•ˆì „í•˜ê²Œ ì¶”ê°€
# - í˜„ì¬ yì¶• ë²”ìœ„ì— PMì´ ì—†ìœ¼ë©´ ì¶•ì„ ìë™ í™•ì¥í•´ì„œ ì„ ì´ ë³´ì´ê²Œ í•¨
# - Pareto(ë³´ì¡°ì¶• ìˆìŒ)ì—ì„œë„ 1ì°¨ yì¶•ì— ì •í™•íˆ ê·¸ë ¤ì¤Œ

from __future__ import annotations
from typing import Optional
import pandas as pd
import math


def _is_plotly_fig(fig) -> bool:
    try:
        # ì§€ì—° ì„í¬íŠ¸ (í™˜ê²½ì— plotly ë¯¸ì„¤ì¹˜ì¼ ë•Œë„ í•¨ìˆ˜ ìì²´ëŠ” import ê°€ëŠ¥í•˜ë„ë¡)
        import plotly.graph_objects as go  # noqa: F401
        from plotly.graph_objs import Figure
        return isinstance(fig, Figure)
    except Exception:
        return False


def _get_primary_y_data_bounds(fig):
    """
    1ì°¨ yì¶• ë°ì´í„°ì˜ (min, max) ì¶”ì •.
    - secondary_y=Trueë¡œ ì˜¬ë¼ê°„ traceëŠ” ì œì™¸
    - trace.yê°€ ìˆ˜ì¹˜ ë°°ì—´ì¼ ë•Œë§Œ ì§‘ê³„
    """
    ymin, ymax = math.inf, -math.inf
    for tr in getattr(fig, "data", []):
        # ë³´ì¡°ì¶• ì—¬ë¶€: trace.yaxis ê°€ 'y2'/'y3'... ì´ë©´ ë³´ì¡°ì¶•
        yaxis = getattr(tr, "yaxis", "y")
        if yaxis and str(yaxis).lower() != "y":  # 'y2' ë“±ì€ ì œì™¸
            continue
        y = getattr(tr, "y", None)
        if y is None:
            continue
        try:
            for v in y:
                if v is None:
                    continue
                fv = float(v)
                if math.isfinite(fv):
                    ymin = min(ymin, fv)
                    ymax = max(ymax, fv)
        except Exception:
            # ìˆ«ì ë°°ì—´ì´ ì•„ë‹ˆë©´ ìŠ¤í‚µ
            continue
    if ymin is math.inf:  # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
        return (0.0, 0.0)
    return (ymin, ymax)


def _ensure_y_contains(fig, y_value: float, pad_ratio: float = 0.05):
    """
    y_valueê°€ yì¶• ë²”ìœ„ì— í¬í•¨ë˜ë„ë¡ ë ˆì´ì•„ì›ƒì„ ì¡°ì •.
    - ê¸°ì¡´ auto-rangeë¼ë„ PMì´ ì¶• ë°–ì´ë©´ ê°•ì œë¡œ range ë¶€ì—¬
    - pad_ratioë§Œí¼ ì—¬ìœ ë¥¼ ë‘¬ì„œ ë¼ë²¨ì´ ì˜ë¦¬ì§€ ì•Šê²Œ í•¨
    """
    if not math.isfinite(y_value):
        return
    # í˜„ì¬ 1ì°¨ yì¶• ë°ì´í„° ë²”ìœ„ ì¶”ì •
    ymin_data, ymax_data = _get_primary_y_data_bounds(fig)
    # ë°ì´í„°ê°€ ì „ë¶€ ìŒìˆ˜ì´ê±°ë‚˜ ì „ë¶€ ì–‘ìˆ˜ì¼ ìˆ˜ ìˆìŒ â†’ PMì´ ë” í° ìª½ì— ìˆìœ¼ë©´ í™•ì¥
    base_min = min(0.0, ymin_data) if math.isfinite(ymin_data) else 0.0
    base_max = max(0.0, ymax_data) if math.isfinite(ymax_data) else 0.0
    tgt_min = min(base_min, y_value)
    tgt_max = max(base_max, y_value)
    if tgt_min == tgt_max:
        # ì™„ì „ í‰í‰í•˜ë©´ ì‚´ì§ í­ ì¶”ê°€
        span = abs(y_value) if y_value != 0 else 1.0
        tgt_min -= span * 0.5
        tgt_max += span * 0.5
    # ì—¬ìœ  íŒ¨ë”©
    span = (tgt_max - tgt_min) or 1.0
    pad = span * float(pad_ratio)
    final_min = tgt_min - pad
    final_max = tgt_max + pad
    # yaxisëŠ” ë ˆì´ì•„ì›ƒ í‚¤ 'yaxis' (ì„œë¸Œí”Œë¡¯ ì•„ë‹Œ ê¸°ë³¸ ë„ë©´ ê¸°ì¤€)
    if "yaxis" not in fig.layout:
        fig.update_layout(yaxis=dict(range=[final_min, final_max]))
    else:
        fig.layout.yaxis.update(range=[final_min, final_max])


def add_materiality_threshold(fig, pm_value: Optional[float], *, label: bool = True):
    """
    Plotly Figureì— PM ê°€ë¡œ ì ì„  + ë¼ë²¨ ì¶”ê°€.
    - pm_valueê°€ None/0/ìŒìˆ˜ë©´ ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    - Pareto(ë³´ì¡°ì¶•)ë„ 1ì°¨ yì¶•ì— ë¼ì¸ì„ ê·¸ë¦¼ (yref='y')
    - ì¶• ë²”ìœ„ë¥¼ ìë™ í™•ì¥í•´ì„œ í•­ìƒ ë³´ì´ê²Œ í•¨
    ë°˜í™˜: ë™ì¼ Figure (in-place ìˆ˜ì • í›„)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    # yì¶• ë²”ìœ„ì— PMì´ í¬í•¨ë˜ë„ë¡ ë¨¼ì € ë³´ì¥
    _ensure_y_contains(fig, pm, pad_ratio=0.08)

    # ì ì„  ë¼ì¸ ì¶”ê°€
    # xref='paper'ë¡œ 0~1 ì „í­ì— ê±¸ì³ ìˆ˜í‰ì„ , yref='y'ë¡œ 1ì°¨ yì¶• ê¸°ì¤€ ê³ ì •
    line_shape = dict(
        type="line",
        xref="paper", x0=0, x1=1,
        yref="y",     y0=pm, y1=pm,
        line=dict(color="red", width=2, dash="dot"),
        layer="above"
    )
    shapes = list(fig.layout.shapes) if getattr(fig.layout, "shapes", None) else []
    shapes.append(line_shape)
    fig.update_layout(shapes=shapes)

    # ë¼ë²¨(ì˜¤ë¥¸ìª½ ë)
    if label:
        annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
        annotations.append(dict(
            x=1.0, xref="paper",
            y=pm, yref="y",
            xanchor="left", yanchor="bottom",
            text=f"PM {pm:,.0f}ì›",
            showarrow=False,
            font=dict(color="red", size=11),
            bgcolor="rgba(255,255,255,0.6)",
            bordercolor="red",
            borderwidth=0.5,
            align="left"
        ))
        fig.update_layout(annotations=annotations)

    return fig


def add_pm_badge(fig, pm_value: Optional[float], *, text: str | None = None):
    """
    Heatmapì²˜ëŸ¼ ì„ ì„ ê¸‹ê¸° ì• ë§¤í•œ ê·¸ë˜í”„ì— ìš°ì¸¡ ìƒë‹¨ ë°°ì§€ ì¶”ê°€.
    ë°˜í™˜: ë™ì¼ Figure (in-place)
    """
    if not _is_plotly_fig(fig):
        return fig
    try:
        pm = float(pm_value) if pm_value is not None else 0.0
    except Exception:
        pm = 0.0
    if pm <= 0:
        return fig

    label = text or f"PM {pm:,.0f}ì›"
    annotations = list(fig.layout.annotations) if getattr(fig.layout, "annotations", None) else []
    annotations.append(dict(
        x=0.995, xref="paper",
        y=0.995, yref="paper",
        xanchor="right", yanchor="top",
        text=label,
        showarrow=False,
        font=dict(color="red", size=11),
        bgcolor="rgba(255,255,255,0.6)",
        bordercolor="red",
        borderwidth=0.5,
        align="right"
    ))
    fig.update_layout(annotations=annotations)
    return fig



def add_time_dividers(fig, xdates, show_quarter: bool = True, show_year_label: bool = True):
    """
    xdates: datetime Series/List (ì°¨íŠ¸ì˜ x ê°’)
    - ì—°ë„ ê²½ê³„(1/1): êµµì€ ì‹¤ì„ 
    - ë¶„ê¸° ê²½ê³„(4/1, 7/1, 10/1): ì–‡ì€ ì ì„ 
    """
    if xdates is None:
        return fig
    try:
        ts = pd.to_datetime(pd.Series(xdates)).dropna().sort_values()
    except Exception:
        return fig
    if ts.empty:
        return fig

    # ì—°ë„ ê²½ê³„ (ì²« í•´ëŠ” ì œì™¸, ë‹¤ìŒ í•´ 1/1 ì§€ì )
    years = ts.dt.year.unique()
    for y in years[1:]:
        x = pd.Timestamp(year=y, month=1, day=1)
        if x < ts.iloc[0] or x > ts.iloc[-1]:
            continue
        try:
            fig.add_vline(
                x=x, line_width=2, line_dash="solid",
                line_color="rgba(0,0,0,0.35)",
                annotation_text=(str(y) if show_year_label else None),
                annotation_position="top", annotation_font_color="rgba(0,0,0,0.55)"
            )
        except Exception:
            continue

    # ë¶„ê¸° ê²½ê³„: 4/1, 7/1, 10/1
    if show_quarter:
        start = pd.Timestamp(ts.iloc[0].year, ts.iloc[0].month, 1)
        end   = pd.Timestamp(ts.iloc[-1].year, ts.iloc[-1].month, 1)
        for x in pd.date_range(start, end, freq="MS"):
            if x.month in (4, 7, 10):
                try:
                    fig.add_vline(
                        x=x, line_width=1, line_dash="dot",
                        line_color="rgba(0,0,0,0.18)"
                    )
                except Exception:
                    continue
    return fig


def add_period_guides(fig, x_series):
    """
    ì›” ë‹¨ìœ„ ì‹œê³„ì—´ì— ì—°/ë¶„ê¸° ê²½ê³„ì„  ì¶”ê°€.
    - ì—°ë§(12ì›”): êµµì€ ì ì„ (ê²€ì •)
    - ë¶„ê¸°ë§(3/6/9/12ì›”): ì–‡ì€ ì ì„ (íšŒìƒ‰)
    """
    import pandas as _pd
    try:
        xs = _pd.to_datetime(x_series)
    except Exception:
        try:
            xs = _pd.to_datetime(_pd.Index(x_series))
        except Exception:
            return fig
    if xs is None or len(xs) == 0:
        return fig
    x_min, x_max = xs.min(), xs.max()
    if _pd.isna(x_min) or _pd.isna(x_max):
        return fig
    months = _pd.date_range(x_min, x_max, freq="M")

    # ì—°ë§: 12ì›”(êµµì€ ì„ )
    year_ends = [m for m in months if m.month == 12]
    for x in year_ends:
        try:
            fig.add_vline(x=x, line=dict(width=2, dash="dash"), line_color="black")
        except Exception:
            continue

    # ë¶„ê¸°ë§: 3/6/9/12ì›”(ì–‡ì€ ì ì„ )
    quarter_ends = [m for m in months if m.month in (3, 6, 9, 12)]
    for x in quarter_ends:
        try:
            fig.add_vline(x=x, line=dict(width=1, dash="dot"), line_color="gray")
        except Exception:
            continue
    return fig




==============================
ğŸ“„ FILE: utils/__init__.py
==============================




==============================
ğŸ“„ FILE: viz/guides.py
==============================

# Materiality ê°€ì´ë“œë¼ì¸(ë¶‰ì€ ì ì„ ) ìœ í‹¸
# - matplotlib/plotly ëª¨ë‘ ì§€ì›. ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ì¶° í˜¸ì¶œë§Œ ë¶™ì´ë©´ ë¨.

def add_materiality_lines_matplotlib(ax, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # yì¶• ê¸°ì¤€ ìˆ˜í‰ì„ 
    if y_threshold is not None and ax is not None:
        ax.axhline(y_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)  # ë¶‰ì€ ì ì„ 
        ax.text(ax.get_xlim()[0], y_threshold, f"{label_prefix}: {y_threshold:,.0f}",
                va="bottom", ha="left", fontsize=9, color="red", alpha=0.9)
    # xì¶• ê¸°ì¤€ ìˆ˜ì§ì„ 
    if x_threshold is not None and ax is not None:
        ax.axvline(x_threshold, linestyle="--", color="red", linewidth=1.25, alpha=0.9)
        ax.text(x_threshold, ax.get_ylim()[1], f"{label_prefix}: {x_threshold:,.0f}",
                va="top", ha="right", fontsize=9, color="red", alpha=0.9)


def add_materiality_lines_plotly(fig, *, y_threshold=None, x_threshold=None, label_prefix="Materiality"):
    # Plotly Figureì— ê°€ì´ë“œë¼ì¸ ì¶”ê°€
    if fig is None:
        return fig
    if y_threshold is not None:
        try:
            fig.add_hline(y=y_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(xref="paper", x=0.0, y=y_threshold, yref="y",
                               text=f"{label_prefix}: {y_threshold:,.0f}",
                               showarrow=False, align="left", yanchor="bottom", font=dict(color="red", size=10))
        except Exception:
            pass
    if x_threshold is not None:
        try:
            fig.add_vline(x=x_threshold, line_dash="dash", line_color="red", opacity=0.9)
            fig.add_annotation(yref="paper", y=1.0, x=x_threshold, xref="x",
                               text=f"{label_prefix}: {x_threshold:,.0f}",
                               showarrow=False, align="right", xanchor="right", font=dict(color="red", size=10))
        except Exception:
            pass
    return fig




